id,title,subtitles
0,NVIDIA's Image Restoration AI: Almost Perfect!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Image denoising is an area where we have a noisy image as an input, and we wish to get a clear, noise-free image. Neural network-based solutions are amazing at this, because we can feed them a large amount of training data with noisy inputs and clear outputs. And if we do that, during the training process, the neural network will be able to learn the concept of noise, and when presented with a new, previously unseen noisy image, it will be able to clear it up. However, with light transport simulations, creating a noisy image means following the path of millions and millions of light rays, which can take up to hours per training sample. And we need thousands, or potentially hundreds of thousands of these! There are also other cases where creating the clean images for the training set is not just expensive, but flat out impossible. Low-light photography, astronomical imaging, or magnetic resonance imaging, MRI in short are great examples of this. In these cases, we cannot use our neural networks simply because we cannot build such a training set as we don't have access to the clear images. In this collaboration between NVIDIA, Aalto University and MIT, scientists came up with an insane idea: let's try to train a neural network without clear images and use only noisy data. Normally, we would say that this is clearly impossible and end this research project. However, they show that under a suitable set of constraints, for instance, one reasonable assumption about the distribution of the noise opens up the possibility of restoring noisy signals without seeing clean ones. This is an insane idea that actually works, and can help us restore images with significant outlier content. Not only that, but it is also shown that this technique can do close to or just as well as other previously known techniques that have access to clean images. You can look at these images, many of which have many different kinds of noise, like camera noise, noise from light transport simulations, MRI imaging, and images severely corrupted with a ton of random text. The usual limitations apply, in short, it of course cannot possibly recover content if we cut out a bigger region from our images. This severely hamstrung training process can be compared to a regular neural denoiser that has access to the clean images, and the differences are negligible most of the time. So how about that, we can teach a neural network to denoise without ever showing it the concept of denoising. Just the thought of this boggles my mind so much it keeps me up at night. This is such a remarkable concept I hope there will soon be followup papers that extend this idea to other problems as well. If you enjoyed this episode and you feel, that about 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. We use these funds to make better videos for you, and a small portion is also used to fund research conferences. You can find us at patreon.com/TwoMinutePapers and there is also a link to it in the video description. You know the drill, one dollar is almost nothing, but it keeps the papers coming. Thanks for watching and for your generous support, and I'll see you next time!"
1,NVIDIA's AI Makes Amazing Slow-Mo Videos!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. How about some slow-motion videos! If we would like to create a slow-motion video and we don't own an expensive slow-mo camera, we can try to shoot a normal video and simply slow it down. This sounds good on paper, however, the more we slow it down, the more space we have between our individual frames, and at some point, our video will feel more like a slideshow. To get around this problem, in a previous video, we discussed two basic techniques to fill in these missing frames: one was a naive technique called frame blending that basically computes the average of two images. In most cases, this doesn't help all that much because it doesn't have an understanding of the motion that takes place in the video. The other one was optical flow. Now this one is much smarter as it tries to estimate the kind of translation and rotational motions that take place in the video and they typically do much better. However, the disadvantage of this is that it usually takes forever to compute, and, it often introduces visual artifacts. So now, we are going to have a look at NVIDIA's results, and the main points of interest are always around the silhouettes of moving objects, especially around regions where the foreground and the background meet. Keep an eye out for these regions throughout this video. For instance, here is one example I've found. Let me know in the comments section if you have found more. This technique builds on U-Net, a superfast convolutional neural network architecture that was originally used to segment biomedical images from limited training data. This neural network was trained on a bit over a 1.000 videos and computes multiple approximate optical flows and combines them in a way that tries to minimize artifacts. As you see in these side by side comparisons, it works amazingly well. Some artifacts still remain, but are often hard to catch. And, this architecture blazing fast. Not real-time yet, but creating a few tens of these additional frames takes only a few seconds. The quality of the results is also evaluated and compared to other works in the paper, so make sure to have a look! As the current, commercially available tools are super slow and take forever, I cannot wait to be able to use this technique to make some more amazing slow motion footage for you Fellow Scholars! Thanks for watching and for your generous support, and I'll see you next time!"
2,DeepMind's AI Takes An IQ Test!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Throughout this series, we have seen many impressive applications of artificial intelligence. These techniques are capable of learning the piano from the masters of the past, beat formidable teams in complex games like DOTA 2, perform well in the game Sonic, The Hedgehog, or help us revive and impersonate famous actors who are not with us anymore. However, what is often not spoken about is how narrow, or how general these AI programs are. A narrow AI means an agent that can perform one task really well, but cannot perform other, potentially easier tasks. The holy grail of machine learning research is a general AI that is capable of obtaining new knowledge by itself through abstract reasoning. This is similar to how humans learn and is a critical step in obtaining a general AI. And to tackle this problem, scientists at DeepMind created a program that is able to generate a large amount of problems that test abstract reasoning capabilities. They are inspired by human IQ-tests with all the these questions about sizes, colors and progressions. They designed the training process in a way that the algorithm is given training data on the progression of colors, but it is never shown similar progression examples that involve object sizes. The concept is the same, but the visual expression of the progression is different. A human easily understands the difference, but teaching abstract reasoning like this to a computer sounds almost impossible. However, now we have a tool that can create many of these questions and the correct answers to them. And I will note that some of these are not as easy as many people would expect. For instance, a vertical number progression is relatively easy to spot, but have a good look at these ones...not so immediately apparent, right? Going back to being able to generate lots and lots of data...the black-belt Fellow Scholars know exactly what this means. This means that we can train a neural network to perform this task! Unfortunately, existing techniques and architectures perform quite poorly. Despite the fact that we have a ton of training data, they could only get 22-42% of the answers right. However, these networks are amazing at doing other things, like writing novels or image classification, therefore this means that their generalization capabilities are not too great when we go outside their core domain. This new technique goes by the name ""Wild Relation Network"" and is trained in a way that encourages reasoning. It is also designed in a way that it not only outputs a guess for the results, but also tries to provide a reason for it, which interestingly, further improved the accuracy of the network. And what is this accuracy we are talking about? It finds the correct solution 62.6% of the time. But it gets better, because this result was measured in the presence of distractor objects like these annoying lines and circles. This is quite confusing, even for humans, so a result above 60% is quite remarkable. And it gets even better, because if we don't use these distractions, it is correct 78% of the time. Wow! This is indeed a step towards teaching an AI how to reason, and as the authors made this dataset publicly available for everyone, I expect a reasonable amount of research works appearing in this area in the near future who knows, perhaps even in the next few months. Thanks for watching and for your generous support, and I'll see you next time!"
3,DeepMind Has A Superhuman Level Quake 3 AI Team!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. After having a look at OpenAI's effort to master the DOTA 2 game, of course, we all know that scientists at DeepMind are also hard at work on an AI that beats the Capture The Flag game mode in Quake 3. Quake III Arena is an iconic first person shooter game and Capture the Flag is a fun game mode where each team tries to take the other team's flag and carry it to their own base while protecting their own. This game mode requires good aiming skills, map presence, reading the opponents well, and tons of strategy. A nightmare situation for any kind of AI. Not only that, but in this version, the map changes from game to game, therefore the AI has to learn general concepts and be able to pull them off in a variety of different previously unseen conditions. This doesn't seem to be within the realm of possibilities to pull off. The minimaps here always show the location of the players, each are color coded to blue or red to indicate their teams. Much like humans, these AI agents learned by looking at the video output of the game and have never been told anything about the game or what the rules are. These scientists at DeepMind ran a tournament with 40 human players who were matched up against these agents randomly, both as opponents and teammates. In this tournament, a team of average human players had a win probability of 43%, where a team of strong players won slightly more than half, 52% of their games. And now hold on to your papers, because the agents were able to win 74% of their games. So the difference between the average and strong human player's winrate is 9%, and the difference between the strongest humans and the AI is more than twice that margin, 22%. This is insanity. And as you see, it barely matters what the size or the layout of the map is or how many teammates there are, the AI's winrate is always remarkably high. These agents showcase many humanlike behaviors such as staying at their own base to defend it, camping within the opponent's base, or following teammates. This builds on a new architecture by the name For The Win, FTW in short, good work folks. Instead of training one agent, it uses a population of agents that train and evolve from each other to make sure that a diverse set of playstyles are discovered. This uses recurrent neural networks, these are neural network variants that are able to learn and produce sequences of data. Here, two of these are used, a fast and a slow one that operate on different timescales but share a memory module. This means that one of them has a very accurate look at the near past, and the other one has a more coarse look, but can look back more into the past in return. If these two work together correctly, decisions can be made that are both good locally, at this point in time, and globally, to maximize the probability of winning the whole game. This is really huge because this algorithm can perform long-term planning, which is one of the key reasons why many difficult games and tasks still remain unsolved. Well, as it seems now, not for long. An additional challenge is that the game score is not necessarily subject to maximization like in most games, but there is a mapping from the scores into an internal reward, which means that the algorithm has to be able to predict its own progress towards winning. And note that even though Quake 3 and Capture The Flag is an excellent way to demonstrate the capabilities of this algorithm, this architecture can be generalized to other problems. I am going to give you a few more tidbits that I have found super interesting, but before, if you are enjoying this episode and would like to pick up some cool perks like early access, deciding the topic of future episodes or getting your name listed in the video description as a key supporter, why not support the show on Patreon? With this, you also help us make better videos in the future. You can find us at Patreon.com/TwoMinutePapers and we also support Bitcoin and other cryptocurrencies, the addresses are available in the video description. And now, onwards to the cool tidbits: a human+agent team has been able to defeat an agent+agent team 5% of the time, indicating that these AIs are able to coordinate and play together with anyone they are given. I get goosebumps from this. Love it. The reaction time and accuracy of the agents is better than that of humans, but not nearly perfect as many people would think. However, they outclass humans even if we artificially reduce their accuracy and reaction times. In another experiment, two agents were paired up against two professional game tester humans who could freely communicate and train against the same agents for 12 hours to see if they can learn their patterns and force them to make mistakes. Even with this, humans had only won 25% of these games. Given the other numbers we have, it is very likely that this unfair advantage made no difference whatsoever. How about that. If there are any more questions, make sure to have a look at the paper that describes every possible tidbit you can possibly imagine. Thanks for watching and for your generous support, and I'll see you next time!"
4,This is How You Hack A Neural Network,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a mind-boggling new piece of work from scientists at Google Brain on how to hack and reprogram neural networks to make them perform any task we want. A neural network is given by a prescribed number of layers, neurons within these layers and weights, or in other words, the list of conditions under which these neurons will fire. By choosing the weights appropriately, we can make the neural network perform a large variety of tasks, for instance, to tell us what an input image depicts, or predict new camera viewpoints when looking at a virtual scene. So this means that by changing the weights of a neural network, we can reprogram it to perform something completely different, for instance, solve a captcha for us. That is a really cool feature. This work reveals a new kind of vulnerability by performing this kind of reprogramming of neural networks in an adversarial manner, forcing them to perform tasks that they were originally not intended to do. It can perform new tasks that it has never done before, and these tasks are chosen by the adversary. So how do adversarial attacks work in general? What does this mean? Let's have a look at a classifier. These neural networks are trained on a given, already existing dataset. This means that they look at a lot of images of buses, and from these, they learn the most important features that are common across buses. Then, when we give them a new, previously unseen image of a bus, they will now be able to identify whether we are seeing a bus or an ostrich. A good example of an adversarial attack is when we present such a classifier with not an image of a bus, but a bus plus some carefully crafted noise that is barely perceptible, that forces the neural network to misclassify it as an ostrich. And in this new work, we are not only interested in forcing the neural network to make a mistake, but we want it to make exactly the kind of mistake we want! That sounds awesome, but also, quite nebulous, so let's have a look at an example. Here, we are trying to reprogram an image classifier to count the number of squares in our images. Step number one, we create a mapping between the classifier's original labels to our desired labels. Initially, this network was made to identify animals like sharks, hens, and ostriches. Now, we seek to get this network to count the number of squares in our images, so we make an appropriate mapping between their domain and our domain. And then, we present the neural network with our images, these images are basically noise and blocks, where the goal is to create these in a way that they coerce the neurons within the neural network to perform our desired task. The neural network then says tiger shark and ostrich, which when mapped to our domain, means 4 and 10 squares respectively, which is exactly the answer we were looking for. Now as you see, the attack is not subtle at all, but it doesn't need to be. Quoting the paper: ""The attack does not need to be imperceptible to humans, or even subtle, in order to be considered a success. Potential consequences of adversarial reprogramming include theft of computational resources from public facing services, and repurposing of AI-driven assistants into spies or spam bots."". As you see, it is of paramount importance that we talk about AI safety within this series and my quest is to make sure that everyone is vigilant that now, tools like this exist. Thank you so much for coming along on this journey, and if you're enjoying it, make sure to subscribe and hit the bell icon to never miss a future episode, some of which will be on followup papers on this super interesting topic. Thanks for watching and for your generous support, and I'll see you next time!"
5,DeepMind's AI Learns The Piano From The Masters of The Past,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, we will listen to a new AI from DeepMind that is capable of creating beautiful piano music. Because there are many other algorithms that do that, to put things into perspective, let's talk about the two key differentiating factors that set this method apart from previously existing techniques. One, music is typically learned from high-level representations, such as the score or MIDI data. This is a precise representation of what needs to be played, but they don't tell us how to play them. These small nuances are what makes the music come alive, and this is exactly what is missing from most of the synthesis techniques. This new method is able to learn these structures and generates not midi signals but raw audio waveforms. And two, it is better at retaining stylistic consistency. Most previous techniques create music that is consistent on a shorter time-scale, but do not take into consideration what was played 30 seconds ago, and therefore they lack the high-level structure that is the hallmark of quality songwriting. However, this new method shows stylistic consistency over longer time periods. Let's give it a quick listen and talk about the architecture of this learning algorithm after that. While we listen, I'll show you the composers it has learned from to produce this. I have  never heard any AI-generated music before with such articulation and the harmonies are also absolutely amazing. Truly stunning results. It uses an architecture that goes by the name autoregressive discrete autoencoder. This contains an encoder module that takes a raw audio waveform and compresses it down into an internal representation, where the decoder part is responsible for reconstructing the raw audio from this internal representation. Both of them are neural networks. The autoregressive part means that the algorithm looks at previous time steps in the learned audio signals when producing new notes, and is implemented in the encoder module. Essentially, this is what gives the algorithm longer-term memory to remember what it played earlier. As you have seen the dataset the algorithm learned from as the music was playing, I am also really curious how we can exert artistic control over the output by changing the dataset. Essentially, you can likely change what the student learns by changing the textbooks used to teach them. For now, let's marvel at one more sound sample. This is already incredible, and I can only imagine what we will be able to do not ten years from now, just a year from now. Thanks for watching and for your generous support, and I'll see you next time!"
6,OpenAI + DOTA2: 180 Years of Learning Per Day,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. You know that I am always excited to tell you about news where AI players manage to beat humans at more and more complex games. Today we are going to talk about DOTA 2 which is a multiplayer online battle arena game with a huge cult following and world championship events with a prize pool of over 40 million dollars. This is not just some game, and just to demonstrate how competitive it is and how quickly it is growing, last time we talked about this in Two Minute Papers episode 180 where an AI beat some of the best players of the game in a limited 1 versus 1 setting, and the prize pool was 20 million dollars back then. This was a huge milestone as this game requires long-term strategic planning, has incomplete information and a high-dimensional continuous action space, which is a classical nightmare situation for any AI. Then, the next milestone was set to defeat a human team in the full 5 versus 5 game, and I promised to report back when there is something new on this project. So here we go. If you look through the forums and our YouTube comments, it is generally believed that this is so complex that it would never ever happen. I would agree that the search space is indeed stupendously large and the problem is notoriously difficult, but whoever thinks this will never be solved has clearly not been watching enough Two Minute Papers. Now, you better hold on to your papers right away, because this video dropped 10 months ago, in August 2017, and since then, the AI has played 180 years worth of gameplay against itself every single day.  80% of these games it played against itself, and 20% against its past self, and even though five of these bots are supposed to work together as a team, there is no explicit communication channel between them. And now, it is ready to play 5 versus 5 matches. Some limitations still apply, but since then, the AI was able get a firm understanding of the importance of teamfighting, predicting the outcome of future actions and encounters, ganking, or in other words, ambushing unsuspecting opponents and many other important pieces of the game. The May 15th version of the AI was evenly matched against OpenAI's in-house team, which is a formidable result, and I find it really amusing that these scientists were beaten by their own algorithm. This is, however, not a world class DOTA 2 team. And the crazy part is that the next version of the AI was tested three weeks later, and it not only beat the in-house team easily, but also defeated several other teams and a semi-professional team as well. As it is often incorrectly said on several forums that these algorithms defeat humans because they can click faster, so I will note that that these bots perform about 150-170 actions per minute, which is approximately in line with an intermediate human player, and it is also to be noted that DOTA2 is not that sensitive to this metric. More clicking does really not mean more winning here at all. The human players were also able to train with an earlier version of this AI. There will be an upcoming event on July 28th where these bots will challenge a team of top players, so stay tuned for some more updates on this! There is no paper yet, but I've put a link to a blog post and the full video in the description, and it is a gold mine of information and was such a joy to read through. So, what do you think? Who will win? And is a 5 versus 5 game in DOTA 2 more complex than playing Starcraft 2? If you wish to hear more about this, please consider helping us tell this story to more people and convert them into Fellow Scholars by supporting the series through Patreon, and as always, we also accept Bitcoin, Ethereum, and Litecoin, the addresses are in the video description. And if you are now in the mood to learn some more about DOTA 2, I recommend taking a look at Day9's channel, I've put a link to a relevant series in the video description. Highly recommended. So there you go, a fresh Two Minute Papers episode, that's not two minutes and it's not about a paper. Yet. Love it. Thanks for watching and for your generous support, and I'll see you next time!"
7,OpenAI's Gaming AI Contest: Results,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a contest by OpenAI where a bunch of AIs compete to decide who has the best transfer learning capabilities. Transfer learning means that the training and the testing environments differ significantly, therefore only the AIs that learn general concepts prevail, and the ones that try to get by with memorizing things will quickly fall. In this experiment, these programs start playing Sonic the Hedgehog, and are given a bunch of levels to train on. However, like in a good test at school, the levels for the final evaluation are kept secret! So, the goal is that only high-quality general algorithms prevail and we can't cheat through the program as we don't know what the final exam will entail. We only know that we have to make the most of the training materials to pass. Sonic is a legendary platform game where we have to blaze through levels by avoiding obstacles and traps often while traveling with the speed of sound. Here you can see the winning submission taking the exam on a previously unseen level. After one minute of training, as expected, the AI started to explore the controls, but is still quite inept and does not make any meaningful progress on the level. After 30 minutes, things look significantly better as the AI now understands the basics of the game. And look here, almost got up there, and, got it. It is clearly making progress as it collects some coins, defeats enemies, goes through the loop, and gets stuck seemingly because it doesn't yet know how being under water changes how high it can jump. This is quite a bit of a special case, so, we are getting there. After only 60-120 minutes, it became a competent player and was able to finish this challenging map with only a few mistakes. Really impressive transfer learning in just about an hour. And note that the algorithm has never seen this level before. Here you see a really cool visualization of three different AI's progress on the map, where the red dots indicate the movement of the character for earlier episodes, and the bluer colors show the progress at later stages of the training. I could spend all day staring at these. Videos are available for many many submissions, some of which even opened up their source code and there are a few high-quality write-ups as well, so make sure to have a look! There's gonna be lots of fun to be had there. This competition gives us something that is scientifically interesting, practical, and super fun at the same time. What more could you possibly want? Huge thumbs up for the OpenAI team for organizing this, and of course, congratulations to the participants. And now you see that we have a job where we train computers to play video games, and we are even paid for it. What a time to be alive. By the way, if you wish to unleash the inner scholar in you, Two Minute Papers shirts are available in many sizes and colors. We have mugs too! The links are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
8,Style Transfer...For Smoke and Fluids!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Fluid and smoke simulations are widely used in computer games and in the movie industry, and are capable of creating absolutely stunning video footage. We can very quickly put together a coarse simulation and run it cheaply, however, the more turbulent motion we're trying to simulate, the more resources and time it will take. If we wish to create some footage with the amount of visual quality that you see here, well, if you think the several hour computation time for light transport algorithms was too much, better hold on to your papers because it will take not hours, but often from days to weeks to compute. And to ease the computation time of such simulations, this is a technique that performs style transfer, but this time, not for paintings, but for fluid and smoke simulations! How cool is that! It takes the low-resolution source and detailed target footage, dices them up into small patches and borrows from image and texture synthesis techniques to create a higher resolution version of our input simulation. The challenge of this technique is that we cannot just put more swirly motion on top of our velocity fields because this piece of fluid has to obey to the laws of physics to look natural. Also, we have to make sure that there is not too much variation from patch to patch, so we have to perform some sort of smoothing on the boundaries of these patches. Also, our smoke plumes also have to interact with obstacles, which is anything but trivial to do well. Have a look at the ground truth results from the high resolution simulation this is the one that would take a long time to compute. There are clearly deviations, but given how coarse the input footage was, I'll take this any day of the week. We can now look forward to seeing even higher quality smoke and fluids in the animation movies of the near future. There was a similar technique by the name Wavelet Turbulence, which is one of my all-time favorite papers that has been showcased in the very first Two Minute Papers episode. This is what it looked like and we are now celebrating its tenth anniversary. Imagine what a bomb this was ten years ago, and you know what, it is still going strong. Thanks for watching and for your generous support, and I'll see you next time!"
9,DeepMind's AI Learns To See,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a recent DeepMind paper on neural rendering where they taught a learning-based technique to see things the way humans do. What's more, it has an understanding of geometry, viewpoints, shadows, occlusion, even self-shadowing and self-occlusion, and many other difficult concepts. So what does this do and how does it work exactly? It contains a representation and a generation network. The representation network takes a bunch of observations, a few screenshots if you will, and encodes this visual sensory data into a concise description that contains the underlying information in the scene. These observations are made from only a handful of camera positions and viewpoints. The neural rendering or seeing part means that we choose a position and viewpoint that the algorithm hasn't seen yet, and ask the generation network to create an appropriate image that matches reality. Now, we have to hold on to our papers for a moment and understand why this is such a crazy idea. Computer graphics researchers work so hard on creating similar rendering and light simulation programs that take tons of computational power to compute all aspects of light transport and in return, give us a beautiful image. If we slightly change the camera angles, we have to redo most the same computations, whereas a learning-based algorithm may just say ""don't worry, I got this"", and from previous experience, guesses the remainder of the information perfectly. I love it. And what's more, by leaning on what these two networks learned, it generalizes so well that it can even deal with previously unobserved scenes. If you remember, I have also worked on a neural renderer for about 3000 hours and created an AI that predicts photorealistic images perfectly. The difference was that this one took a fixed camera viewpoint, and predicted what the object would look like if we started changing its material properties. I'd love to see a possible combination of these two works, oh my! Super excited for this. There is a link in the video description to both of these works. Can you think of other possible uses for these techniques? Let me know in the comments section! And, if you wish to decide the order of future episodes or get your name listed as a key supporter for the series, hop over to our Patreon page and pick up some cool perks. We use these funds to improve the series and empower other research projects and conferences. As this video series is on the cutting edge of technology, of course, we also support cryptocurrencies like Bitcoin, Ethereum, and Litecoin. The addresses are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
10,Infinite Walking in Virtual Reality,"We are over 260 episodes into the series, but believe it or not, we haven't had a single episode on virtual reality. So at this point, you probably know that this paper has to be really good. The promise of virtual reality is indeed truly incredible. Doctors could be trained to perform surgery in a virtual environment, or even perform surgery from afar, we could enhance military training by putting soldiers into better flight simulators, expose astronauts to virtual zero-gravity simulations, you name it. And of course, games. As you see, virtual reality or VR in short, is on the rise these days and there is a lot of research going on on how to make more killer applications for it. The basics are simple we put on a VR headset and walk around in our room and perform gestures, and these will be performed in a virtual would by our avatar. Sounds super fun, right? Well, yes, however, we have this headset on, and we don't really see our surroundings within the room, which makes it easy to bump into objects, or smash the controller into the wall, which is exactly what I did in the NVIDIA lab in Switzerland not so long ago. My greetings to all the kind people there, and sorry folks! So, what could be a possible solution? Creating virtual worlds with smaller scales? That kind of defeats the purpose, doesn't it? There has to be a better solution. So, how about redirection? Redirection is a simple concept that changes our movement in the virtual world so it deviates from our real path in the room in a way that both lets us explore the virtual world well, and not bump into walls and objects in the meantime. Most existing techniques out there either don't do redirection and make us bump into objects and walls within our room, or they do redirection, at the cost of introducing distortions and other disturbing changes into the virtual environment. This is not easy to perform well because it has to feel natural, but the changes we apply to the path deviates from what is natural. Here you can see how the blue and orange lines deviate, which means that the algorithm is at work. With this, we can wander about in a huge and majestic virtual landscape or a cramped bar, even when being confined to a small physical room. Loving the idea. This technique takes into consideration even other moving players in the room and dynamically remap our virtual paths to make sure we don't bump into them. There is a lot more in the paper that describes how the whole method adapts to human perception. Papers like this make me really happy because there are thousands of papers in the domain of human perception within computer graphics, many of which will now see quite a bit of practical use. VR is going to be a huge enabler for this area. Thanks for watching and for your generous support, and I'll see you next time!"
11,An AI For Image Manipulation Detection,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As facial reenactment videos are improving at a rapid pace, it is getting easier and easier to create video impersonations of other people by transferring our gestures onto their faces. We have recently discussed a technique that is able to localize the modified regions within these videos, however, this technique was limited to human facial reenactment. That is great, but what about the more general case with manipulated photos? Well, do not worry for second, because this new learning-based algorithm can look at any image and highlight the regions that were tampered with. It can detect image splicing, which means that we take part of a different image and add it to this one. Or, copying an object and pasting it into the image elsewhere. Or, removing an object from a photo and filling in the hole with meaningful information harvested from the image. This we also refer to as image inpainting, and this is something that we also use often to edit our thumbnail images that you see here on YouTube. Believe it or not, it can detect all of these cases. And it uses a two-stream convolutional neural network to accomplish this. So what does this mean exactly? This means a learning algorithm that looks at one, the color data of the image to try to find unnatural contrast changes along edges and silhouettes, and two, the noise information within the image as well and see how they relate to each other. Typically, if the image has been tampered with, either the noise, or the color data is disturbed, or, it may also be that they look good one by one, but the relation of the two has changed. The algorithm is able to detect these anomalies too. As many of the images we see on the internet are either resized or compressed, or both, it is of utmost importance that the algorithm does not look at compression artifacts and thinks that the image has been tampered with. This is something that even humans struggle with on a regular basis, and this is luckily not the case with this algorithm. This is great because a smart attacker may try to conceal their mistakes by recompressing an image and thereby adding more artifacts to it. It's not going to fool this algorithm. However, as you Fellow Scholars pointed out in the comments of a previous episode, if we have a neural network that is able to distinguish forged images, with a little modification, we can perhaps turn it around and use it as a discriminator to help training a neural network that produces better forgeries. What do you think about that? It is of utmost importance that we inform the public that these tools exist. If you wish to hear more about this topic, and if you think that a bunch of videos like this a month is worth a dollar, please consider supporting us on Patreon. You know the drill, a dollar a month is almost nothing, but it keeps the papers coming. Also, for the price of a coffee, you get exclusive early access to every new episode we release, and there are even more perks on our Patreon page, patreon.com/TwoMinutePapers. We also support cryptocurrencies like Bitcoin, Ethereum and Litecoin, the addresses are available in the video description. With your help, we can make better videos in the future. Thanks for watching and for your generous support, and I'll see you next time!"
12,"Beautiful Layered Materials, Instantly","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As animation movies and computer game graphics become more and more realistic, they draw us more and more into their own worlds. However, when we see cars, mugs, minerals and paintings, and similar materials, we often feel that something is not right there, and the illusion quickly crumbles. This is such a peculiar collection of materials...so what is the common denominator between them? Normally, to create these beautiful images, we use programs that create millions and millions of rays of light and simulate how they bounce off of the objects within the scene. However, most of these programs bounce these rays off of the surface of these objects, where in reality, there are many sophisticated multi-layered materials with all kinds of coatings and varnishes. Such a simple surface model is not adequate to model these multiple layers. This new technique is able to simulate not only these surface interactions, but how light is scattered, transferred and absorbed within these layers, enabling us to create even more beautiful images with more sophisticated materials. We can envision new material models with any number of layers and it will be able to handle it. However, I left the best part for last. What is even cooler is that it takes advantage of the regularity of the data and builds a statistical model that approximates what typically happens with our light rays within these layers. What this results in is a real-time technique that still remains accurate. This is not normal. This used to take hours! This is insanity! And the whole paper was written by only one author, Laurent Belcour and was accepted to the most prestigious research venue in computer graphics. So huge congrats to Laurent for accomplishing this. If you would like to learn more about light transport, I am holding an Master-level course on it at the Technical University of Vienna. This course used to take place behind closed doors, but I feel that the teachings shouldn't only be available for the 20-30 people who can afford a University education, but they should be available for everyone. So, we recorded the entirety of the course and it is now available for everyone, free of charge. If you are interested, have a look at the video description to watch them. Thanks for watching and for your generous support, and I'll see you next time!"
13,Faceforensics: This AI Detects DeepFakes!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. With the recent ascendancy of several new AI-based techniques for human facial reenactment, we are now able to create videos where we transfer our gestures onto famous actors or politicians and impersonate them. Clearly, as this only needs a few minutes of video as training data from the target, this could be super useful for animating photorealistic characters for video games and movies, reviving legendary actors who are not with us anymore, and much more. And understandably, some are worried about the social implications of such a powerful tool. In other words, if there are tools to create forgery, there should be tools to detect forgery, right? If we can train an AI to impersonate, why not train an other AI to detect impersonation? This has to be an arms race. However, this is no easy task to say the least. As an example, look here. Some of these faces are real, some are fake. What do you think, which is which? I will have to admit, my guesses weren't all that great. But what about you? Let me know in the comments section. Compression is also an issue. Since all videos you see here on YouTube are compressed in some way to reduce filesize, some of the artifacts that appear may easily throw off not only an AI, but a human as well. I bet there will be many completely authentic videos that will be thought of as fakes by humans in the near future. So how do we solve these problems? First, to obtain a neural network-based solution, we need a large dataset to train it on. This paper contains a useful dataset with over a 1000 videos that we can use to train such a neural network. These records contain pairs of original and manipulated videos, along with the input footage of the gestures that were transferred. After the training step, the algorithm will be able to pick up on the smallest changes around the face and tell a forged footage from a real one, even in cases where we humans are unable to do that. This is really amazing. These green to red colors showcase regions that the AI thinks were tampered with. And it is correct. Interestingly, this can not only identify regions that are forgeries, but it can also improve these forgeries too. I wonder if it can detect footage that is has improved itself? What do you think? Thanks for watching and for your generous support, and I'll see you next time!"
14,Better Video Impersonations with AI,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Earlier, we talked about an amazing technique where the inputs were a source video of ourselves, and a target actor. And the output was a reenactment, in other words, a video of this target actor with our facial gestures. This requires only a few minutes of video from the target which is usually already available on the internet. Essentially, we can impersonate other people, at least for one video. A key part of this new technique is that it extracts additional data such as pose and eye positions both from the source and target videos, and uses this data for the reconstruction. As opposed to this original Face2face technique from 2 years ago, which was already mind-blowing, you see here that this results in a new learning-based method that supports the reenactment of eyebrows and blinking, changing the background, plus head and gaze positioning as well. So far, this would still be similar to a non learning-based technique we've seen a few episodes ago. And now, hold on to your papers, because this algorithm enables us to not only impersonate, but also control the characters in the output video. The results are truly mesmerizing, I almost fell out of the chair when I've first seen them. And what's more, we can create really rich reenactments by editing the expressions, pose, and blinking separately by hand. What also needs to be emphasized here is that we see and talk to other human beings all the time, so we have a remarkably keen eye for these kinds of gestures. If something is off by just a few millimeters or is not animated in a way that is close to perfect, the illusion immediately falls apart. And the magical thing about these techniques is that every single iteration we get something that is way beyond the capabilities of the previous methods. And they come in quick succession. There are plenty of more comparisons in the paper as well, so make sure to have a look. It also contains a great idea that opens up the possibility of creating quantitative evaluations against ground truth footage. Turns out that we can have such a thing as ground truth footage. I wonder when we will see the first movie with this kind of reenactment of an actor who passed away. Do you have some other cool applications in mind? Let me know in the comments section. And, if you enjoyed this episode, make sure to pick up some cool perks on our Patreon page where you can manage your paper addiction by getting early access to these episodes and more. We also support cryptocurrencies, the addresses are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
15,Curiosity-Driven AI: How Effective Is It?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There are many research projects about teaching an AI to play video games well. We have seen some amazing results from DeepMind's Deep Q Learning algorithm that performed on a superhuman level on many games, but faltered on others. What really made the difference is the sparsity of rewards and the lack of longer-term planning. What this means is that the more often we see the score change on our screen, the faster we know how well we are doing and change our strategy if needed. For instance, if we make a mistake in Atari Breakout, we lose a life almost immediately, but in a strategy game, a bad decision may come back to haunt us up to an hour after committing it. So, what can we do to build an AI that can deal with these cases? So far when we have talked about extrinsic rewards that come from the environment, for instance, our score in a video game, and most existing AIs are for all intents and purposes, extrinsic score-maximizing machines. And this work is about introducing an intrinsic reward by endowing an AI with one of the most humanlike attributes: curiosity. But hold on right there, how can a machine possibly become curious? Well, curiosity is defined by whatever mathematical definition we attach to it. In this work, curiosity is defined as the AI's ability to predict the results of its own actions. This is big, because it gives the AI tools to pre-emptively start learning skills that don't seem useful now, but might be useful in the future. In short, this AI is driven to explore, even if it hasn't been told how well it is doing. It will naturally start exploring levels in Super Mario, even without seeing the score. And now comes the great part: this curiosity really teaches the AI to learn new skills, and when we drop it into a new, previously unseen level, it will perform much better than a non-curious one. When playing Doom, the legendary first-person shooter game, it will also start exploring the level and is able to rapidly solve hard exploration tasks. The comparisons reveal that an AI infused with curiosity performs significantly better on easier tasks, but the even cooler part is that with curiosity, we can further increase the difficulty of the games and the sparsity of external rewards and can expect the agent to do well, even when previous algorithms failed. This will be able to play much harder games than previous works. And remember, games are only used to demonstrate the concept here, this will be able to do so much more. Love it. Thanks for watching and for your generous support, and I'll see you next time!"
16,Neural Image Stitching And Morphing,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Consider this problem: we have a pair of images that are visually quite different, but have similar semantic meanings, and we wish to map points between them. Now this might sound a bit weird, so bear with me for a moment. For instance, geese and airplanes look quite different, but both have wings, and front, and back regions. The paw of a lion looks quite different from a cat's foot, but they share the same function and are semantically similar. This is an AI-based technique that is able to find these corresponding points between our pair of images, in fact, the point pairs you've seen so far have been found by this AI. The main difference between this and previous non-learning-based techniques is that instead of pairing up regions based on pixel color similarities, it measures how similar they are in terms of the neural network's internal representation. This makes all the difference. So far this is pretty cool, but is that it? Mapping points? Well if we can map points effectively, we can map regions as a collection of points. This enables two killer applications. One, this can augment already existing artistic tools so that we can create a hybrid between two images. And the cool thing is that we don't even need to have any drawing skills because we only have to add these colored masks and the algorithm finds and stitches together the corresponding images. And two, it can also perform cross-domain image morphing. That's an amazing term, but what does this mean? This means that we have our pair of images from earlier, and we are not interested in stitching together a new image from their parts, but we want an animation where the starting point is one image, the ending point is the other, and we get a smooth and meaningful transition between the two. There are some really cool use-cases for this. For example, we can start out from a cartoon drawing, set our photo as an endpoint, and witness this beautiful morphing between the two. Kind of like in style transfer, but we have more fine-grained control over the output. Really cool. And note that many images inbetween are usable as-is. No artistic skills needed. And of course, there is a mandatory animation that makes a cat from a dog. As usual, there are lots of comparisons to other similar techniques in the paper. This tool is going to be invaluable for, I was about to say artists, but this doesn't require any technical expertise, just good taste and a little bit of imagination. What an incredible time to be alive. Thanks for watching and for your generous support, and I'll see you next time!"
17,NVIDIA's AI Removes Objects From Your Photos!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Ever had an experience when you shot an almost perfect photograph of for instance, an amazing landscape but unfortunately it was littered with unwanted objects. If only we had an algorithm that could perform image inpainting, in other words, delete a small part of an image and have it automatically filled in! So let's have a look at NVIDIA's AI-based solution. On the left, you see the white regions that are given to the algorithm to correct, and on the right, you see the corrected images. So it works amazingly well, but the question is why? This is an established research field, so what new can an AI-based approach bring to the table? Well, traditional non-learning approaches either try to fill these holes in with other pixels from the same image that have similar neighborhoods, copy-paste something similar if you will, or they try to record the distribution of pixel colors and try to fill in something using that knowledge. And here comes the important part none of these traditional approaches have an intuitive understanding of the contents of the image. And that is the main value proposition of the neural network-based learning techniques! This work also borrows from earlier artistic style transfer methods to make sure that not only the content, but the style of the inpainted regions also match the original image. It is also remarkable that this new method works with images that are devoid of symmetries and can also deal with cases where we cut out really crazy, irregularly shaped holes. Of course, like every good piece of research work, it has to be compared to previous algorithms. As you can see here, the quality of different techniques is measured against a reference output, and it is quite clear that this new method produces more convincing results than its competitors. For reference, PatchMatch is a landmark paper from almost 10 years ago that still represents the state of the art for non learning-based techniques. The paper contains a ton more of these comparisons, so make sure to have a look. Without doubt, this is going to be an invaluable tool for artists in the future. In fact, in this very series, we use Photoshop's built-in image inpainting tool on a daily basis, so this will make our lives much easier. Loving it. Also, did you know that you can get early access to each of these videos? If you are addicted to the series, have a look at our Patreon page, patreon.com/TwoMinutePapers, or just click the link in the video description. There are also other really cool perks like getting your name as a key supporter in the video description, or deciding the order of the next few episodes. We also support cryptocurrencies, the addresses are in the video description. And with this, you also help us make better videos in the future. Thanks for watching and for your generous support, and I'll see you next time!"
18,This Technique Impersonates People,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Two years ago, in 2016, we talked about a paper that enabled us to sit in front of a camera, and transfer our gestures onto a virtual actor. This work went by the name Face2face and showcased a bunch of mesmerizing results containing reenactments of famous political figureheads. It was quite amazing. But it is nothing compared to this one. And the reason for this is that the original Face2face paper only transferred expressions, but this new work is capable of transferring head and torso movements as well. Not only that, but mouth interiors also appear more realistic and more gaze directions are also supported. You see in the comparisons here that the original method disregarded many of these features and how much more convincing this new one is. This extended technique opens up the door to several, really cool new applications. For instance, consider this self-reenactment application. This means that you can re-enact yourself. Now, what would that be useful for, you may ask? Well, of course, you can appear to be the most professional person during a virtual meeting, even when sitting at home in your undergarment. Or, you can quickly switch teams based on who is winning the game. Avatar digitization is also possible, this basically means that we can create a stylized version of our likeness to be used in a video game. Somewhat similar to the Memoji presented in Apple's latest keynote with the iPhone X. And the entire process takes place in real time without using neural networks. This is as good as it gets. What a time to be alive! Of course, like every other technique, this one also has its own set of limitations. For instance, illumination changes in the environment are not always taken into account, and long-haired subjects with extreme motion may cause artifacts to appear. In short, don't use this for rock concerts. And with this, we are also one step closer to full character re-enactment for movies, video games and telepresence applications. This is still a new piece of technology, and may offer many more applications that we haven't thought of yet. After all, when the internet was invented, who thought that it could be used to order pizza or transfer Bitcoin? Or order pizza and pay with bitcoin. Anyway, if you have some more applications in mind, let me know in the comments section. Thanks for watching and for your generous support, and I'll see you next time!"
19,This AI Learned To See In The Dark!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If you start watching reviews of some of the more recent smartphones, you will almost always see a dedicated section to low-light photography. The result is almost always that cameras that work remarkably well in well-lit scenes produce almost unusable results in dim environments. So unless we have access to a super expensive camera, what can we really do to obtain more usable low-light images? Well, of course, we could try brightening the image up by increasing the exposure. This would help maybe a tiny bit, but would also mess up our white balance and also amplify the noise within the image. I hope that by now you are getting the feeling that there must be a better AI-based solution. Let's have a look! This is an image of a dark indoor environment, I am sure you have noticed. This was taken with a relatively high light sensitivity that can be achieved with a consumer camera. This footage is unusable. And this image was taken by an expensive camera with extremely high light sensitivity settings. This footage is kinda usable, but is quite dim and is highly contaminated by noise. And now, hold on to your papers, because this AI-based technique takes sensor data from the first, unusable image, and produces this. Holy smokes! And you know what the best part is? It produced this output image in less than a second. Let's have a look at some more results. These look almost too good to be true, but luckily, we have a paper at our disposal so we can have a look at some of the details of the technique! It reveals that we have to use a Convolutional Neural Network to learn the concept of this kind of image translation, but that also means that we require some training data. The input should contain a bunch of dark images, these are the before images, this can hardly be a problem, but the output should always be the corresponding image with better visibility. These are the after images. So how do we obtain them? The key idea is to use different exposure times for the input and output images. A short exposure time means that when taking a photograph, the camera aperture is only open for a short amount of time. This means that less light is let in, therefore the photo will be darker. This is perfect for the input images as these will be the ones to be improved. And the improved versions are going to be the images with a much longer exposure time. This is because more light is let in, and we'll get brighter and clearer images. This is exactly what we're looking for! So now that we have the before and after images that we referred to as input and output, we can start training the network to learn how to perform low-light photography well. And as you see here, the results are remarkable. Machine learning research at its finest. I really hope we get a software implementation of something like this in the smartphones of the near future, that would be quite amazing. And as we have only scratched the surface, please make sure to look at the paper as it contains a lot more details. Thanks for watching and for your generous support, and I'll see you next time!"
20,AI-Based Large-Scale Texture Synthesis,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When an artist is in the process of creating digital media, such as populating a virtual world for an animation movie or a video game, or even in graphic design, the artist often requires a large number of textures for these kinds of works. Concrete walls, leaves, fabrics are materials that we know well from the real world and sometimes the process of obtaining textures is as simple as paying for a texture package and using it. But the problem quite often occurs that we wish to fill an entire road with a concrete texture, but we only have a small patch at our disposal. In this case, the easiest and worst solution is to copy-paste this texture over and over, creating really unpleasant results that are quite repetitive and suffer from seams. So what about an AI-based technique that looks at a small patch, and automatically continues it in a way that looks natural and seamless. This is an area within computer graphics and AI that we call texture synthesis. Periodic texture synthesis is simple, but textures with structure are super difficult. The selling point of this particular work is that it is highly efficient at taking into consideration the content and symmetries of the image. For instance, it knows that it has to take into consideration the concentric nature of the wood rings when synthesizing this texture, and it can also adapt to the regularities of this water texture and create a beautiful, high-resolution result. This is a neural-network based technique, so first, the question is, what should the training data be? Let's take a database of high-resolution images. Let's cut out a small part and pretend that we don't have access to the bigger image and ask a neural network to try to expand this small cutout. This sounds a little silly, so what is this trickery good for? Well, this is super useful because after the neural network has expanded the results, we now have a reference result in our hands that we can compare to and this way, teach the network to do better. Note that this architecture is a generative adversarial network, where two neural networks battle each other. The generator network is the creator that expands the small texture snippets, and the discriminator network takes a look and tries to tell it from the real deal. Over time, the generator network learns to be better at texture synthesis, and the discriminator network becomes better at telling synthesized results from real ones. Over time, this rivalry leads to results that are of extremely high quality. And as you can see in this comparison, this new technique smokes the competition. The paper contains a ton of more results and comparisons, and one of the most exhaustive evaluation sections I've seen in texture synthesis so far. I highly recommend reading it. If you would like to see more episodes like this, make sure to pick up one of the cool perks we offer through Patreon, such as deciding the order of future episodes, or getting your name in the video description of every episode as a key supporter. We also support cryptocurrencies like Bitcoin, Ethereum and Litecoin. We had a few really generous pledges in the last few weeks. I am quite stunned to be honest, and I regret that I cannot come in contact with these Fellow Scholars. If you can contact me, that would be great, if not, thank you so much everyone for your unwavering support. This is just incredible. Thanks for watching and for your generous support, and I'll see you next time!"
21,We Taught an AI To Synthesize Materials,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Due to popular request, here's a more intuitive explanation of our latest work. Believe it or not, when I have started working on this, Two Minute Papers didn't even exist. In several research areas, there are cases where we can't talk about our work until it is published. I knew that the paper would not see the light of the day for quite a while, if ever, so I started Two Minute Papers to be able to keep my sanity and deliver a hopefully nice piece of work on a regular basis. In the end, this took more than 3000 work hours to complete, but it is finally here, and I am so happy to finally be able to present it to you. This work is in the intersection of computer graphics and AI, which you know is among my favorites. So what do we see here? This beautiful scene contains more than a 100 different materials, each of which has been learned and synthesized by an AI. None of these daisies and dandelions are alike, each of them have a different material model. The goal is to teach an AI the concept of material models such as metals, minerals and translucent materials. Traditionally, when we are looking to create a new material model with a light simulation program, we have to fiddle with quite a few parameters, and whenever we change something, we have to wait from 40 to 60 seconds until a noise-free result appears. In our solution, we don't need to play with these parameters. Instead, our goal is to grab a gallery of random materials, assign a score to each of them saying that I liked this one, I didn't like that one, and get an AI to learn our preferences and recommend new materials for us. This is quite useful when we’re looking to synthesize not only one, but many materials. So this is learning algorithm number one, and it works really well for a variety of materials. However, these recommendations still have to be rendered with a light simulation program, which takes several hours for a gallery like the one you see here. Here comes learning algorithm number two to the rescue, a neural network that replaces this light simulation program and creates photorealistic visualizations. It is so fast, it not only does this in real time, but it is more than 10 times faster than real time. We call this a neural renderer. So we have a lot of material recommendations, and they are all photorealistic that we can visualize in real time. However, it is always a possibility that we have a recommendation that is almost exactly what we had in mind, but need a few adjustments. That’s an issue, because to do that, we would have to go back to the parameter fiddling, which we really wanted to avoid in the first place. No worries, because the third learning algorithm is coming to the rescue. What this can do is take our favorite material models from the gallery, and map them onto a nice 2D plane where we can explore similar materials. If we combine this with the neural renderer, we can explore these photorealistic visualizations and everything is appears not in a few hours, but in real time. However, without a little further guidance, we get a bit lost because we still don’t know which regions in this 2D space are going to give us materials that are similar to the one we wish to fine-tune. We can further improve this by exploring different combinations of the three learning algorithms. In the end, we can assign these colors to the background that describe either whether the AI expects us to like the output, or how similar the output will be. A nice use-case of this is where we have this glassy still life scene, but the color of the grapes is a bit too vivid for us. Now, we can go to this 2D latent space, and adjust it to our liking in real time. Much better. No material modeling expertise is required. So I hope you have found this explanation intuitive. We tried really hard to create something that is both scientifically novel and also useful for the computer game and motion picture industry. We had to throw away hundreds of other ideas until this final system materialized. Make sure to have a look at the paper in the description where every single element and learning algorithm is tested and evaluated one by one. If you are a journalist and you would like to write about this work, I would be most grateful, and I am also more than happy to answer questions in an interview format as well. Please reach out if you’re interested. We also try to give back to the community, so for the fellow tinkerers out there, the entirety of the paper is under the permissive Creative Commons license, and the full source code and pre-trained neural networks are also available under the even more permissive MIT license. Everyone is welcome to reuse it or build something cool on top of it. Thanks for watching and for your generous support, and I'll see you next time!"
22,This Evolving AI Finds Bugs in Games,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is a learning algorithm that chooses a set of actions in an environment to maximize a score. This class of techniques enables us to train an AI to master a large variety of video games and has many more cool applications. For instance, in the game of Q*bert, at every time step, the AI has to choose the appropriate actions to control this orange character and light up all the cubes without hitting the purple enemy. This work proposes an interesting alternative to reinforcement learning and is named evolution strategies and it aims to train not one agent, but an entire population of agents in parallel. The efficiency of this population is assessed, much like how evolution works in nature, and new offsprings are created from the best performing candidates. Note that this is not the first paper using evolution strategies this is a family of techniques that dates back to the 70s. However, an advantage of this variant is that it doesn't require long trial and error sessions to find an appropriate discount factor. But wait, what does this discount factor mean exactly? This is a number that describes whether the AI should focus only on immediate rewards at all costs, or whether it should be willing to temporarily make worse decisions for a better payoff in the future. This optimal number is different for every game, and depends on how much long-term planning it requires. With this evolutionary algorithm, we can skip this step entirely. And the really cool thing about this is that it is not only able to master many games, but after only 5 hours of training, it was able to find a way to abuse game mechanics in Q*bert in the most creative ways. It has found a glitch where it sacrifices itself to lure the purple blob into dropping down after it. And much to our surprise, it found that there is a bug if it drops down from this position, it should lose a life for doing it, but due to a bug, it doesn't. It also learned another cool technique where it waits for the adversary to make a move and immediately goes the other way. Here's the same scene slowed down. It had also found and exploited another serious bug which was to the best of my knowledge, previously unknown after completing the first level, it starts jumping around in a seemingly random manner. A moment later, we see that the game does not advance to the next level, but cubes start blinking and the AI is free to score as many points as it wishes. After this video, a human player was able to reproduce this, I've put a link to it in the video description. It also found out the age-old trick in breakout, where we dig a tunnel through the bricks, lean back, start reading a paper, and let physics solve the rest of the level. One of the greatest advantages of this technique is that instead of training only one agent, it works on an entire population. These agents can be trained independently, making the algorithm more parallelizable, which means that it is fast and maps really well to modern processors and graphics cards with many cores. And these algorithms are not only winning the game, they are breaking the game. Loving it. What a time to be alive! I think this is an incredible story that everyone needs to hear about. If you wish help us with our quest and get exclusive perks for this series, please consider supporting us on Patreon. We are available through patreon.com/TwoMinutePapers, and a link with the details is available in the video description. We also use part of these funds to give back to the community and empower research projects and conferences. For instance, we recently sponsored a conference aimed to teach young scientists to write and present their papers at international venues. We are hoping to invest some more into upgrading our video editing rig in the near future. We also support cryptocurrencies, such as Bitcoin, Ethereum and Litecoin. I am really grateful for your support. And this is why every video ends with... Thanks for watching and for your generous support, and I'll see you next time!"
23,AI Learns Painterly Harmonization,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When we show a photograph to someone, most of the time we are interested in sharing our memories. Graduation, family festivities beautiful landscapes are common examples of this. With the recent ascendancy of these amazing neural style transfer techniques, we can take a painting, or any other source image, and transfer the style of this image to our contents. The style is transfered, but the contents remains unchanged. This takes place by running the images through a deep neural network, which, in its deeper layers, learns about high level concepts such as artistic style. This work has sparked a large body of followup research works. Feedforward real-time style transfer, temporally coherent style transfer for videos, you name it. However, these techniques are always about taking one image for content, and one for style. How about a new problem formulation where we paste in a part of a foreign image with a completely different style? For instance, if you feel that this ancient artwork is sorely missing a Captain America shield, or if Picasso's self-portrait is just not cool enough without shades, then this algorithm is for you. However, if we just drop in this part of a foreign image, anyone can immediately tell because of the differences in color and style. A previous, non-AI-based technique does way better, but it is still apparent that the image has been tampered with. But as you can see here, this new technique is able to do it seamlessly. It works by first performing style transfer from the painting to the new region, and then, in the second step, additional refinements are made to it to make sure that the response of our neural network is similar across the entirety of the painting. It is conjectured that if the neural network is stimulated the same way by every part of the image, then there shouldn't be outlier regions that look vastly different. And as you can see here, it works remarkably well on a range of inputs. I hope these scroll animations come out really smooth and creamy. This video took a long time to render in 4K resolution with 60 frames per second, and was only possible because of your support on Patreon. If you wish to help us create better videos in the future, please click the Patreon link in the video description and support the series. To validate this work, a user study was done that revealed that the users preferred the new technique over the older ones in 15 out 16 images. I think it is fair to say that this work smokes the competition. But what about comparisons to real paintings? A different user study was also created to answer this question. And the answer is that users were mostly unable to identify whether the painting was tampered with. Excellent work. The source code is also available, so let the experiments begin! Thanks for watching and for your generous support, and I'll see you next time!"
24,This AI Reproduces Human Perception,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Assessing how similar two images are has been a long standing problem in computer graphics. For instance, if we write a new light simulation program, we have to compare our results against the output of other algorithms and a noise-free reference image. However, this often means that we have many noisy images where the structure of the noise is different. This leads to endless arguments on which algorithm is favorable to the others, since who really gets to decide what kind of noise is favorable and what is not? These are important and long-standing questions that we need to find answers to. In an other application, we took a photorealistic material model and wanted to visualize other materials that look similar to it. However, in order to do this, we need to explain to the computer what it means that two images are similar. This is what we call a similarity metric. Have a look at this reference image, and these two variants of it. Which one is more similar to it, the blurred or the warped version? Well, according to most humans, warping is considered a less intrusive operation. However, some of the most ubiquitous similarity metrics, like computing a simple per-pixel difference thinks otherwise. Not good. What about this comparison? Which image is closer to the reference? The noisy or the blurry one? Most humans say that the noisy image is more similar, perhaps because with enough patience, one could remove all the noise pixel by pixel and get back the reference image, but in the blurry image, lots of features are permanently lost. Again, the classical error metrics think otherwise. Not good. And now comes the twist, if we build a database from many of these human decisions, feed it into a deep neural network, we'll find that this network will be able to learn and predict how humans see differences in images. This is exactly what we are looking for! You can see the agreement between this new similarity metric and these example differences. However, this shows the agreement on only three images. That could easily happen by chance. So, this chart shows how different techniques correlate with how humans see differences in images. The higher the number, the higher the chance that it thinks similarly to humans. The ones labeled with LPIPS denote the new proposed technique used on several different classical neural network architectures. This is really great news for all kinds of research works that include working with images. I can't wait to start experimenting with it! The paper also contains a more elaborate discussion on failure cases as well, so make sure to have a look. Also, if you would like to help us do more to spread the word about these incredible works and pick up cool perks, please consider supporting us on Patreon. Each dollar you contribute is worth more than a thousand views, which is a ton of help for the channel. We also accept cryptocurrencies, such as Bitcoin, Ethereum and Litecoin. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
25,This AI Learns From Its Dreams ,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we are going to talk about an AI that not only plays video games really well, but can also dream up new, unseen scenarios, and more. This is an interesting new framework that contains a vision model that compresses what it has seen in the game into an internal code. As you see here, these latent variables are responsible to capture different level designs, and this variable simulates time and shows how the fireballs move towards us over time. This is a highly compressed internal representation that captures the most important aspects of the game. We also have a memory unit that not only stores previous experiences, but similarly to how an earlier work predicted the next pen strokes of a drawing, this can also dream up new gameplay. Finally, it is also endowed with a controller unit that is responsible for making decisions as to how to play the game. Here, you see the algorithm in action: on the left, there is the actual gameplay, and on the right you can see its compressed internal representation. This is how the AI thinks about the game. The point is that it is lossy, therefore some information is lost, but the essence of the game is retained. So, this sounds great, the novelty is clear, but how well does it play the game? Well, in this racing game, on a selection of a 100 random tracks, its average score is almost three times that of DeepMind's groundbreaking Deep Q-Learning algorithm. This was the AI that took the world by storm when DeepMind demonstrated how it learned to play Atari Breakout and many other games on a superhuman level. This is almost three times better than that on the racetrack game, though it is to be noted that DeepMind has also made great strides since their original DQN work. And now comes the even more exciting part! Because it can create an internal dream representation of the game, and this representation really captures the essence of the game, then it means that it is also be able to play and train within these dreams. Essentially, it makes up dream scenarios and learns how to deal with them without playing the actual game. It is a bit like how a we prepare for a first date, imagining what to say, and how to say it, or, imagining how we would incapacitate an attacker with our karate chops if someone were to attack us. And the cool thing is that with this AI, this dream training actually works, which means that the newly learned dream strategies translate really well to the real game. We really have only scratched the surface, so make sure to read the paper in the description. This is a really new and fresh idea, and I think it will give birth to a number of followup papers. Cannot wait to report on these back to you, so stay tuned and make sure to subscribe and hit the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!"
26,This Robot Adapts Like Animals,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about building a robot that works even when being damaged, and you will see that the results are just unreal. There are many important applications for such a robot where sending out humans may be too risky, such as putting out forest fires, finding earthquake survivors under rubble, or shutting down a malfunctioning nuclear plant. Since these are all dangerous use cases, it is a requirement that such a robot works even when damaged. The key idea to accomplish this is that we allow the robot to perform tasks such as walking not only in one, optimal way, but to explore and build a map of many alternative motions relying on different body parts. Some of these limping motions are clearly not optimal, but, whenever damage happens to the robot, it will immediately be able to choose at least one alternative way to move around, even with broken or missing legs. After building the map, it can be used as additional knowledge to lean on when the damage occurs, and the robot doesn't have to re-learn everything from scratch. This is great, especially given that damage usually happens in the presence of danger, and in these cases, reacting quickly can be a matter of life and death. However, creating such a map takes a ton of trial and error, potentially more than what we can realistically get the robot to perform. And now comes my favorite part, which is, starting a project in a computer simulation, and then, in the next step, deploying the trained AI to a real robot. This previously mentioned map of movements contains over 13,000 different kinds of gaits, and since we are in a simulation, it can be computed efficiently and conveniently. In software, we can also simulate all kinds of damage for free without dismembering our real robot. And since no simulation is perfect, after this step, the AI is deployed to the real robot that evaluates and adjusts to the differences. By the way, this is the same robot that surprised us in the previous episode when it showed that it can walk around just fine without any foot contact with the ground by jumping on its back and using its elbows. I can only imagine how much work this project took, and the results speak for themselves. It is also very easy to see the immediate utility of such a project. Bravo. I also recommend looking at the press materials. For instance, in the frequently asked questions, many common misunderstandings are addressed. For instance, it is noted that the robot doesn't understand the kind of damage that occurred, and doesn't repair itself in the strict sense but it tries to find alternative ways to function. Thanks for watching and for your generous support, and I'll see you next time!"
27,AI Learns Real-Time 3D Face Reconstruction,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we have two extremely hard problems on the menu. One is facial alignment and the other is 3D facial reconstruction. For both problems, we have an image as an input, and the output should be either a few lines that mark the orientation of the jawline, mouth and eyes, and in the other case, we are looking for a full 3D computer model of the face. And all this should happen automatically, without any user intervention. This is extremely difficult, because this means that we need an algorithm that takes a 2D image, and somehow captures 3D information from this 2D projection, much like a human would. This all sounds great and would be super useful in creating 3D avatars for Skype calls, or scanning real humans to place them in digital media such as feature movies and games. That would be amazing, but, is this really possible? This work uses a convolutional neural network to accomplish this, and it not only provides high-quality outputs, but it creates them in less than 10 milliseconds per image, which means that it can process a hundred of them every second. That is great news indeed, because it also means that doing this for video in real time is also a possibility! But not so fast, because if we are talking about video, new requirements arise. For instance, it is important that such a technique is resilient against changes in lighting. This means that if we have different lighting conditions, the output geometry the algorithm gives us shouldn't be wildly different. The same applies to camera and pose as well. This algorithm is resilient against all three, and it has some additional goodies. For instance, it finds the eyes properly through glasses, and can deal with cases where the jawline is occluded by the hair, or infer its shape when one side is not visible at all. One of the key ideas is to give additional instruction to the convolutional neural network to focus more of its efforts to reconstruct the central region of the face because that region contains more discriminative features. The paper also contains a study that details the performance of this algorithm. It reveals that it is not only five to eight times faster than the competition, but also provides higher quality solutions. Since these are likely to be deployed in real-world applications very soon, it is a good time to start brainstorming about possible applications for this. If you have ideas beyond the animation movies and games line, let me know in the comments section. I will put the best ones in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
28,AI Photo Translation,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Recently, a new breed of AI techniques surfaced that were capable of this new thing called image translation. And by image translation I mean that they can translate a drawn map to a satellite image, take a set of color labels and make a photorealistic facade, or take a sketch and create a photo out of it. This is done through a Generative Adversarial Network. This is an architecture where we have a pair of neural networks, one that learns to generate new images, and the other learns to tell a fake image from a real one. As they compete against each other, they get better and better without any human interaction. In these earlier applications, unfortunately, the output is typically one image, and since there are many possible shoes that could satisfy our initial sketch, it is highly unlikely that the one we are offered is exactly what we envisioned. This improved version enhanced this algorithm to be able to produce not one, but an entire set of outputs. And as you can see here, we have a night image, and a set of potential daytime translations on the right that are quite diverse. I really like how it has an intuitive understanding of the illumination differences of the building during night and daytime. It really seems to know how to add lighting to the building. It also models the atmospheric scattering during daytime, creates multiple kinds of pretty convincing clouds, or puts hills in the background. The results are both realistic, and the additional selling point is that this technique offers an entire selection of outputs. What I found to be really cool about the next comparisons is that ground truth images are also attached for reference. If we can take a photograph of a city at nighttime, we have access to the same view during the daytime too, or we can take a photograph of a shoe and draw the outline of it by hand. And as you can see here, there are not only lots of high-quality outputs, but in some cases, the ground truth image is really well approximated by the algorithm. This means that we give it a crude drawing, and it could translate this drawing into a photorealistic image that is very close to reality. I think that is mind blowing! The validation section of the paper reveals that this technique provides a great tradeoff between diversity and quality. There are previous methods that perform well if we need one high-quality solution, or many not so great ones, but overall this one provides a great package for artists working in the industry, and this will be a godsend for any kind of content generation scenario. The source code of this project is also available, and make sure to read the license before starting your experiments. Thanks for watching and for your generous support, and I'll see you next time!"
29,4 Experiments Where the AI Outsmarted Its Creators!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, I am really excited to show you four experiments where AI researchers were baffled by the creativity and unexpected actions of their own creations. You better hold on to your papers. In the first experiment, robots were asked to walk around while minimizing the amount of foot contact with the ground. Much to the scientists' surprise, the robots answered that this can be done with 0% contact, meaning that they never ever touch the ground with the feet. The scientists wondered how that is even possible, and pulled up a video of the proof. This proof showed a robot flipping over and walking using its elbows. Talk about thinking outside the box! Wow. A different robot arm experiment also came to a surprising conclusion. At first, the robot arm had to use its grippers to grab a cube, which it successfully learned to perform. However, in a later experiment, the gripper was crippled, making the robot unable to open its fingers. Scientists expected a pathetic video with the robot to push the box around and always failing to pick up the cube. Instead, they have found this. You see it right, instead of using the fingers, the robot finds the perfect angle to smash the hand against the box to force the gripper to open and pick up the box. That is some serious dedication to solving the task at hand. Bravo! In the next experiment, a group of robots were tasked to find food and avoid poisonous objects in an environment, and were equipped with a light and no further instructions. First, they learned to use the lights to communicate the presence of food and poison to each other and cooperate. This demonstrates that when trying to maximize the probability of the survival of an entire colony, the concept of communication and cooperation can emerge even from simple neural networks. Absolutely beautiful. And what is even more incredible, is that later, when a new reward system was created that fosters self-preservation, the robots learned to deceive each other by lighting up the the food signal near the poison to take out their competitors and increase their chances. And these behaviors emerge from a reward system and a few simple neural networks. Mind blowing. A different AI was asked to fix a faulty sorting computer program. Soon, it achieved a perfect score without changing anything because it noticed that by short circuiting the program itself, it always provides an empty output. And of course, you know, if there are no numbers, there is nothing to sort. Problem solved. Make sure to have a look at the paper, there are many more experiments that went similarly, including a case where the AI found a bug in a physics simulation program to get an edge. With AI research improving at such a rapid pace, it is clearly capable of things that surpasses our wildest imagination, but we have to make sure to formulate our problems with proper caution, because the AI will try to use loopholes instead of common sense to solve them. When in a car chase, don't ask the car AI to unload all unnecessary weights to go faster, or if you do, prepare to be promptly ejected from the car. If you have enjoyed this episode, please make sure to have a look at our Patreon page in the video description where you can pick up really cool perks, like early access to these videos or getting your name shown in the video description, and more. Thanks for watching and for your generous support, and I'll see you next time!"
30,Gaussian Material Synthesis (SIGGRAPH 2018),"Creating high-quality photorealistic materials for light transport simulations typically includes direct hands-on interaction with a principled shader. This means that the user has to tweak a large number of material properties by hand, and has to wait for a new image of it to be rendered after each interaction. This requires a fair bit expertise and the best setups are often obtained through a lengthy trial and error process. To enhance this workflow, we present a learning-based system for rapid mass-scale material synthesis. First, the user is presented with a gallery of materials and the assigned scores are shown in the upper left. Here, we learn the concept of glassy and transparent materials. By leaning on only a few tens of high-scoring samples, our system is able to recommend many new materials from the learned distributions. The learning step typically takes a few seconds, where the recommendations take negligible time and can be done on a mass scale. Then, these recommendations can be used to populate a scene with materials. Typically, each recommendation takes 40-60 seconds to render with global illumination, which is clearly unacceptable for real-world workflows for even mid-sized galleries. In the next step, we propose a convolutional neural network that is able to predict images of these materials that are close to the ones generated via global illumination, and takes less than 3 milliseconds per image. Sometimes a recommended material is close the one envisioned by the user, but requires a bit of fine-tuning. To this end, we embed our high-dimensional shader descriptors into an intuitive 2D latent space where exploration and adjustments can take place without any domain expertise. However, this isn't very useful without additional information because the user does not know which regions offer useful material models that are in line with their scores. One of our key observations is that this latent space technique can be combined with Gaussian Process Regression to provide an intuitive color coding of the expected preferences to help highlighting the regions that may be of interest. Furthermore, our convolutional neural network can also provide real-time predictions of these images. These predictions are close to indistinguishable from the real rendered images and are generated in real time. Beyond the preference map, this neural network also opens up the possibility of visualizing the expected similarity of these new materials to the one we seek to fine-tune. By combining the preference and similarity maps, we obtain a color coding that guides the user in this latent space towards materials that are both similar and have a high expected score. To accentuate the utility of our real-time variant generation technique, we show a practical case where one of the grape materials is almost done, but requires a slight reduction in vividity. This adjustment doesn't require any domain expertise or direct interaction with a material modeling system and can be done in real time. In this example, we learn the concept of translucent materials from only a handful of high-scoring samples and generate a large amount of recommendations from the learned distribution. These recommendations can then be used to populate a scene with relevant materials. Here, we show the preference and similarity maps of the learned translucent material space and explore possible variants of an input material. These recommendations can be used for mass-scale material synthesis, and the amount of variation can be tweaked to suit the user's artistic vision. After assigning the appropriate materials, displacements and other advanced effects can be easily added to these materials. We have also experimented with an extended, more expressive version of our shader that also includes procedural textured albedos and displacements. The following scenes were populated using the material learning and recommendation and latent space embedding steps. We have proposed a system for mass-scale material synthesis that is able to rapidly recommend a broad range of new material models after learning the user preferences from a modest number of samples. Beyond this pipeline, we also explored powerful combinations of the three used learning algorithms, thereby opening up the possibility of real-time photorealistic material visualization, exploration and fine-tuning in a 2D latent space. We believe this feature set offers a useful solution for rapid mass-scale material synthesis for novice and expert users alike and hope to see more exploratory works combining the advantages of multiple state of the art learning algorithms in the future."
31,Evolving Generative Adversarial Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. With the recent ascendancy of neural network-based techniques, we have witnessed amazing algorithms that are able to take an image from a video game and translate it into reality and the other way around. Or, they can also translate daytime images to their nighttime versions, or change summer to winter and back. Some AI-based algorithms can also create near photorealistic images from our sketches. So the first question is, how is this wizardry even possible? These techniques are implemented by using Generative Adversarial Networks, GANs in short. This is an architecture where two neural networks battle each other: the generator network is the artist who tries to create convincing, real-looking images. The discriminator network is the critic that tries to tell a fake image from a real one. The artist learns from the feedback of the critic and will improve itself to come up with better quality images, and in the meantime, the critic also develops a sharper eye for fake images. These two adversaries push each other until they both become adept at their tasks. However, the training of these GANs is fraught with difficulties. For instance, it is not guaranteed that this process converges to a point, and therefore it matters a great deal when we stop training the networks. This makes reproducing some works very challenging and is generally not a desirable property of GANs. It is also possible that the generator starts focusing on a select set of inputs and refuses to generate anything else, a phenomenon we refer to as mode collapse. So how could we possibly defeat these issues? This work presents a technique that mimics the steps of evolution in nature: evaluation and selection and variation. First, this means that not one, but many generator networks are trained, and only the ones that provide sufficient quality and diversity in their images will be preserved. We start with an initial population of generator networks, and evaluate the fitness of each of them. The better and more diverse images they produce more fit they are, the more fit they are, the more likely they are to survive the selection step where we eliminate the most unfit candidates. Okay, so we now see how a subset these networks become the victim of evolution. This is how networks get eaten, if you will. But, how do we produce new ones? And this is how we arrive to the variation step, where new generator networks are created by introducing variations to the networks that are still alive in this environment. This simulates the creation of an offspring, and will provide the next set of candidates for the next selection step, and we hope that if we play this game over a long time, we get more and more resilient offsprings. The resulting algorithm can be trained in a more stable way, and it can create new bedroom images when being shown a database of bedrooms. When compared to the state of the art, we see that this evolutionary approach offers higher quality images and more diversity in the outputs. It can also generate new human faces that are quite decent. They are clearly not perfect, but a technique that can pull this off consistently will be an excellent baseline for newer and better research works in the near future. We are also getting very close to an era where we can generate thousands of convincing digital characters from scratch to name just one application. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
32,Das täuscht deine Sicht | Two Minute Papers # 241,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Neural networks are amazing at recognizing objects when being shown an image, and in some cases, like traffic sign recognition, their performance can reach superhuman levels. But as we discussed in the previous episode, most of these networks have an interesting property where we can add small changes to an input photo and have the network misclassify it to something completely different. This is called an adversarial attack. A super effective neural network can be reduced to something that is less accurate than a coinflip with a properly crafted adversarial attack. So, of course, we may think that neural networks are much smaller and simpler than the human brain, and because of that, of course, we cannot perform such an adversarial attack on the human vision system...right? Or is it possible that some of the properties of machine vision systems can be altered to fool the human vision? And now, hold on to your papers, I think you know what's coming. This algorithm performs an adversarial attack on you. This image depicts a cat. And this image depicts, uh... a dog? Surely it's a dog, right? Well, no. This is an image of a previous cat plus some carefully crafted noise that makes it look like a dog. This is such a peculiar effect I am staring at it, and I know for a fact that this is not a dog, this is cat plus noise, but I cannot not see it as a dog. Wow, this is certainly something that you don't see every day. So let's look at what changes were made to the image. Clearly, the nose appears to be longer and thicker, so that's a dog-like feature. But, it is of utmost importance that we don't overlook the fact that several cat-specific features still remain in the image, for instance, the whiskers are very cat-like. And despite that, we still see it as a dog. This is insanity. The technique works by performing an adversarial attack against an AI model, and modifying the noise generator model to better match the human visual system. Of course, the noise we have to add depends on the architecture of the neural network, and by this I mean the number of layers and the number of neurons within these layers, and many other parameters. However, a key insight of the paper is that there are still features that are shared between most architectures. This means that if we create an attack that works against 5 different neural network architectures, it is highly likely that it will also work on an arbitrary sixth network that we haven't seen yet. And it turns out that some of these noise distributions are also useful against the human visual system. Make sure to have a look at the paper, I have found it to be an easy read, and quite frankly I am stunned by the result. It is clear that machine learning research is progressing at a staggering pace, but I haven't expected this. I haven't expected this at all. If you are enjoying the series, please make sure to have a look at our Patreon page to pick up cool perks, like watching these episodes in early access, or getting your name displayed in the video description as a key supporter. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
33,One Pixel Attack Defeats Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We had many episodes about new wondrous AI-related algorithms, but today, we are going to talk about an AI safety which is an increasingly important field of AI research. Deep neural networks are excellent classifiers, which means that after we train them on a large amount of data, they will be remarkably accurate at image recognition. So generally, accuracy is subject to maximization. But, no one has said a word about robustness. And here is where these new neural-network defeating techniques come into play. Earlier we have shown that we can fool neural networks by adding carefully crafted noise to an image. If done well, this noise is barely perceptible and can fool the classifier into looking at a bus and thinking that it is an ostrich. We often refer to this as an adversarial attack on a neural network. This is one way of doing it, but note that we have to change many-many pixels of the image to perform such an attack. So the next question is clear. What is the lowest number of pixel changes that we have to perform to fool a neural network? What is the magic number? One would think that a reasonable number would at least be a hundred. Hold on to your papers because this paper shows that many neural networks can be defeated by only changing one pixel. By changing only one pixel in an image that depicts a horse, the AI will be 99.9% sure that we are seeing a frog. A ship can also be disguised as a car, or, amusingly, almost anything can be seen as an airplane. So how can we perform such an attack? As you can see here, these neural networks typically don't provide a class directly, but a bunch of confidence values. What does this mean exactly? The confidence values denote how sure the network is that we see a labrador or a tiger cat. To come to a decision, we usually look at all of these confidence values and choose the object type that has the highest confidence. Now clearly, we have to know which pixel position to choose and what color it should be to perform a successful attack. We can do this by performing a bunch of random changes to the image and checking how each of these changes performed in decreasing the confidence of the network in the appropriate class. After this, we filter out the bad ones and continue our search around the most promising candidates. This process we refer to as differential evolution, and if we perform it properly, in the end, the confidence value for the correct class will be so low that a different class will take over. If this happens, the network has been defeated. Now, note that this also means that we have to be able to look into the neural network and have access to the confidence values. There is also plenty of research works on training more robust neural networks that can withstand as many adversarial changes to the inputs as possible. I cannot wait to report on these works as well in the future! Also, our next episode is going to be on adversarial attacks on the human vision system. Can you believe that? That paper is absolutely insane, so make sure to subscribe and hit the bell icon to get notified. You don't want to miss that one! Thanks for watching and for your generous support, and I'll see you next time!"
34,DeepMind's AI Learns Complex Behaviors From Scratch,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is a learning algorithm that chooses a set of actions in an environment to maximize a score. This class of techniques enables us to train an AI to master a large variety of video games and has many more cool applications. Reinforcement learning typically works well when the rewards are dense. What does this mean exactly? This means that if we play a game, and after making a mistake, we immediately die, it is easy to identify which action of ours was the mistake. However, if the rewards are sparse, we are likely playing something that is akin to a long-term strategy planner game. If we lost, it is possible that we were outmaneuvered in the final battle, but it is also possible that we lost the game way earlier due to building the wrong kind of economy. There are a million other possible reasons, because we get feedback on how well we have done only once, and much much after we have chosen our actions. Learning from sparse rewards is very challenging, even for humans. And it gets even worse. In this problem formulation, we don't have any teachers that guide the learning of the algorithm, and no prior knowledge of the environment. So this problem sounds almost impossible to solve. So what did DeepMind's scientists come up with to at least have a chance at approaching it? And now, hold on to your papers, because this algorithm learns like a baby learns about its environment. This means that before we start solving problems, the algorithm would be unleashed into the environment to experiment and master basic tasks. In this case, our final goal would be to tidy up the table. First, the algorithm learns to activate its haptic sensors, control the joints and fingers, then, it learns to grab an object, and then to stack objects on top of each other. And in the end, the robot will learn that tidying up is nothing else but a sequence of these elementary actions that it had already mastered. The algorithm also has an internal scheduler that decides which should be the next action to master, while keeping in mind that the goal is to maximize progress on the main task, which is, tidying up the table in this case. And now, onto validation. When we are talking about software projects, the question of real-life viability often emerges. So, the question is, how would this technique work in reality, and what else would be the ultimate test than running it on a real robot arm! Let's look here and marvel at the fact that it easily finds and moves the green block to the appropriate spot. And note that it had learned how to do it from scratch, much like a baby would learn to perform such tasks. And also note that this was a software project that was deployed on this robot arm, which means that the algorithm generalizes well for different control mechanisms. A property that is highly sought after when talking about intelligence. And if earlier progress in machine learning research is indicative of the future, this may learn how to perform backflips and play video games on a superhuman level within two followup papers. I cannot wait to see that, and I'll be here to report on that for you. Thanks for watching and for your generous support, and I'll see you next time!"
35,DeepMind's AI Masters Even More Atari Games,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is a learning algorithm that we can use to choose a set of actions in an environment to maximize a score. There are many applications of such learners, but we typically cite video games because of the diverse set of challenges they can present the player with. And in reinforcement learning, we typically have one task, like learning backflips, and one agent that we wish to train to perform it well. This work is DeepMind's attempt to supercharge reinforcement learning by training one agent that can do a much wider variety of tasks. Now, this clearly means that we have to acquire more training data and also be prepared to process all this data as effectively as possible. By the way, the test suite you see here is also new where typical tasks in this environment involve pathfinding through mazes, collecting objects, finding keys to open their matching doors, and more. And every Fellow Scholar knows that the paper describing its details is of course, available in the description. This new technique builds upon an earlier architecture that was also published by DeepMind. This earlier architecture, A3C unleashes a bunch of actors into the wilderness, each of which gets a copy of the playbook that contains the current strategy. These actors then play the game independently, and periodically stop and share what worked and what didn't to this playbook. With this new IMPALA architecture, there are two key changes to this. One, in the middle, we have a learner, and the actors don't share what worked and what didn't to this learner, but they share their experiences instead. And later, the centralized learner will come up with the proper conclusions with all this data. Imagine if each football player in a team tries to tell the coach the things they tried on the field and what worked. That is surely going to work at least okay, but instead of these conclusions, we could aggregate all the experience of the players into some sort of centralized hive mind, and get access to a lot more, and higher quality information. Maybe we will see that a strategy only works well if executed by the players who are known to be faster than their opponents on the field. The other key difference is that with traditional reinforcement learning, we play for a given number of steps, then stop and perform learning. With this technique, we have decoupled the playing and learning, therefore it is possible to create an algorithm that performs both of them continuously. This also raises new questions, make sure to have a look at the paper, specifically the part with the new off-policy correction method by the name V-Trace. When tested on 30 of these levels and a bunch of Atari games, the new technique was typically able to double the score of the previous A3C architecture, which was also really good. And at the same time, this is at least 10 times more data-efficient, and its knowledge generalizes better to other tasks. We have had many episodes on neural network-based techniques, but as you can see research on the reinforcement learning side is also progressing at a remarkable pace. If you have enjoyed this episode, and you feel that 8 science videos a month is worth a dollar, please consider supporting us on Patreon. You can also pick up cool perks like early access to these episodes. The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
36,AI Learns Human Pose Estimation From Videos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This project is a collaboration between INRIA and Facebook AI research and is about pose estimation. Pose estimation means that we take an input photo or in the cooler case, video of people, and the output should be a description of their postures. This is kind of like motion capture for those amazing movie and computer game animations, but without the studio and the markers. This work goes even further and tries to offer a full 3D reconstruction of the geometry of the bodies, and it is in fact doing way more than that as you will see in a minute. Neural networks are usually great at these tasks, provided that we have a large number of training samples to train them. So, the first step is gathering a large amount of annotated data. This means an input photograph of someone, which is paired up with the correct description of their posture. This is what we call one training sample. This new proposed dataset contains 50 thousand of these training samples, and using that, we can proceed to step number two, training the neural network to perform pose estimation. But, there is more to this particular work. Normally, this pose estimation takes place with a 2D skeleton, which means that most techniques output a stick figure. But not in this case, because the dataset contains segmentations and dense correspondances between 2D images and 3D models, therefore, the network is also able to output fully 3D models. There are plenty of interesting details shown in the paper, for instance, since the annotated ground truth footage in the training set is created by humans, there is plenty of missing data that is filled in by using a separate neural network that is specialized for this task. Make sure to have a look at the paper for more cool details like this. This all sounds good in theory, but a practical application has to be robust against occlusions and rapid changes in posture, and the good thing is that the authors published plenty of examples with these that you can see here. Also, it has to be able to deal with smaller and bigger scales when people are closer or further away from the camera. This is also a challenge. The algorithm does a really good job at this, and remember, no markers and studio setup is required, and everything that you see here is performed interactively. The dataset will appear soon and it will be possible to reuse it for future research works, so I expect plenty of more collaboration and followup works for this problem. We are living amazing times indeed. Thanks for watching and for your generous support, and I'll see you next time!"
37,AI-Based Animoji Without The iPhone X,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Many of you have surely heard the word Animoji, which refers to these emoji figures that are animated in real time and react to our facial gestures. This is implemented in the new iPhone X phones, however, to accomplish this, it uses a dot projector get a good enough understanding of the geometry of the human face. So, how about a technique that doesn't need any specialized gear, takes not even a video of you, but one photograph as an input, and creates a digital avatar of us that can be animated in real time. Well, sign me up! Have a look at these incredible results. As you can see, the final result also includes secondary components like eyes, teeth, tongue, and gum. Now, these avatars don't have to be fully photorealistic, but have to capture the appearance and gestures of the user well enough so they can be used in video games or any telepresence application where a set of users interact in a virtual world. As opposed to many prior works, the hair is not reconstructed strand by strand, because doing this in real time is not feasible. Also, note that the information we are given is highly incomplete, because the backside of the head is not captured, but these characters also have a quite appropriate looking hairstyle there. So, how is this even possible? Well, first, the input image is segmented into the face part and the hair part. Then, the hair part is run through a neural network that tries to extract attributes like length, spikiness, are there hair buns, is there a ponytail, where the hairline is, and more. This is an extremely deep neural network with over 50 layers and it took 40 thousand images of different hairstyles to train. Now, since it is highly unlikely that the input photo shows someone with a hairstyle that was never ever worn by anyone else, we can look into a big dataset of already existing hairstyles and choose the closest one that fits the attributes extracted by the neural network. Such a smart idea, loving it. You can see how well this works in practice, and in the next step, the movement and appearance of the final hair geometry can be computed in real time through a novel polygonal strip representation. The technique also supports retargeting, which means that our gestures can be transferred to different characters. The framework is also very robust to different lighting conditions, which means that a differently lit photograph will lead to very similar outputs. The same applies for expressions. This is one of those highly desirable details that makes or breaks the usability of a new technique in production environments, and this one passed with flying colors. In these comparisons, you can also see that the quality of the results also smokes the competition. A variant of the technology can be downloaded through the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
38,Eine Foto verbessernde KI,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Some time ago, smartphone cameras were trying to outpace each other by adding more and more megapixels to their specification sheet. The difference between a half megapixel image and a 4 megapixel image was night and day. However, nowadays, we have entered into diminishing returns as most newer mobile cameras support 8 or more megapixels. At this point, a further resolution increase doesn't lead to significantly more convincing photos and here is where the processing software takes the spotlight. This paper is about an AI-based technique that takes a poor quality photo and automatically enhances it. Here, you can already see what a difference software can make to these photos. Many of these photos were taken with an 8-year-old mobile camera and were enhanced by the AI. This is insanity. Now, before anyone thinks that by enhancement, I am referring to the classic workflow of adjusting white balance, color levels and hues. No, no, no. By enhancement, I mean the big, heavy hitters like recreating lost details via super resolution and image inpainting, image deblurring, denoising, and recovering colors that were not even recorded by the camera. The idea is the following: first, we shoot a lot of photos from the same viewpoint with a bunch of cameras, ranging from a relatively dated iPhone 3GS, other mid-tier mobile cameras, and a state of the art DSLR camera. Then, we hand over this huge bunch of data to a neural network that learns the typical features that are preserved by the better cameras and lost by the worse ones. The network does the same with relating the noise patterns and color profiles to each other. And then, we use this network to recover these lost features and pump up the quality of our lower tier camera to be as close as possible to a much more expensive model. Super smart idea, loving it. And you know what is even more brilliant? The validation of this work can take place in a scientific manner, because we don't need to take a group of photographers who will twirl their moustaches and judge these photos. Though, I'll note that this was also done for good measure. But, since we have the photos from the high-quality DSLR camera, we can take the bad photos, enhance them with the AI, and compare this output to the real DSLR's output. Absolutely brilliant. The source code and pre-trained networks and an online demo is also available. So, let the experiments begin, and make sure to leave a comment with your findings! What do you think about the outputs shown in the website? Did you try your own photo? Let me know in the comments section. A high-quality validation section, lots of results, candid discussion of the limitations in the paper, published source code, pretrained networks and online demos that everyone can try free of charge. Scientists at ETH Zürich maxed this paper out. This is as good as it gets. If you have enjoyed this episode and would like to help us make better videos in the future, please consider supporting us on Patreon by clicking the letter P at the end screen of this video in a moment, or just have a look at the video description. Thanks for watching and for your generous support, and I'll see you next time!"
39,Building Blocks of AI Interpretability,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Hold on to your papers because this is an exclusive look at a new neural network visualization paper that came from a collaboration between Google and the Carnegie Mellon University. The paper is as fresh as it gets because this is the first time I have been given an exclusive look before the paper came out, and this means that this video and the paper itself will be published at the same time. This is really cool and it's quite an honor, thank you very much! Neural networks are powerful learning-based tools that are super useful for tasks that are difficult to explain but easy to demonstrate. For instance, it is hard to mathematically define what a traffic sign is, but we have plenty of photographs of them. So the idea is simple, we label a bunch of photographs with additional data that says ""this is a traffic sign"", and ""this one isn't"", and feed this to the learning algorithm. As a result, neural networks have been able to perform traffic sign detection at a superhuman level for many years now. Scientists at Google DeepMind have also shown us that if we combine a neural network with reinforcement learning, we can get it to look at a screen and play computer games on a very high level. It is incredible to see problems that seemed impossible for many decades crumble one by one in quick succession over the last few years. However, we have a problem, and that problem is interpretability. There is no doubt that these neural networks are efficient, however, they cannot explain their decisions to us, at least not in a way that we can interpret. To alleviate this, earlier works tried to visualize these networks on the level of neurons, particularly, what kinds of inputs make these individual neurons extremely excited. This paper is about combining previously known techniques to unlock more powerful ways to visualize these networks. For instance, we can combine the individual neuron visualization with class attributions. This offers a better way of understanding how a neural network decides whether a photo depicts a labrador or a tiger cat. Here, we can see which part of the image activates a given neuron and what the neuron is looking for. Below, we see the final decision as to which class this image should belong to. This next visualization technique shows us which set of detectors contributed to a final decision, and how much they contributed exactly. Another way towards better interpretability is to decrease the overwhelming number of neurons into smaller groups with more semantic meaning. This process is referred to as factorization or neuron grouping in the paper. If we do this, we can obtain highly descriptive labels that we can endow with intuitive meanings. For instance, here, we see that in order for the network to classify the image as a labrador, it needs to see a combination of floppy ears, doggy forehead, doggy mouth, and a bunch of fur. And we can also construct a nice activation map to show which part of the image makes our groups excited. Please note that we have only scratched the surface. This is a beautiful paper, and it has tons of more results available exactly from this moment, with plenty of interactive examples you can play with. Not only that, but the code is open sourced, so you are also able to reproduce these visualizations with little to no setup. Make sure to have a look at it in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
40,Why Should We Trust An AI?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Through over 200 episodes of this series, we talked about many learning-based algorithms that are able to solve problems that previously seemed completely impossible. They can look at an image and describe what they depict in a sentence, or even turn video game graphics into reality and back. Amazing new results keep appearing every single week. However, an important thing that we need to solve is that if we deploy these neural networks in a production environment, we would want to know if we're relying on a good or bad AI's decision. The narrative is very simple: if we don't trust a classifier, we won't use it. And perhaps the best way of earning the trust of a human would be if the AI could explain how it came to a given decision. Strictly speaking, a neural network can explain it to us, but it would show us hundreds of thousands of neuron activations that are completely unusable for any sort of intuitive reasoning. So, what is even more difficult to solve is that this explanation happens in a way that we can interpret. An earlier approach used decision trees that described what the learner looks at and how it uses this information to arrive to a conclusion. This new work is quite different. For instance, imagine that a neural network would look at all the information we know about a patient and tell us that this patient likely has the flu. And, in the meantime, it could tell us that the fact that the patient has a headache and sneezes a lot contributed to the conclusion that he has the flu, but, the lack of fatigue is notable evidence against it. Our doctor could take this information and instead of blindly relying on the output, could make a more informed decision. A fine example of a case where AI does not replace, but augment human labor. An elegant tool for a more civilized age. Here, we see an example image where the classifier explains which region contributes to the decision that this image depicts a cat, and which region seems to be counterevidence. We can use this not only for tabulated patient data and images, but text as well. In this other example, we try to find out whether a piece of written text is about Christinanity or Atheism. Note that the decision itself is not as simple as looking for a few keywords, even a mid-tier classifier is much more sophisticated than that. But, it can tell us about the main contributing factors. A big additional selling point is that this technique is model agnostic, which means that it can be applied to other learning algorithms that are able to perform classification. It is also a possibility that an AI is only right by chance, and if this is the case, we should definitely know about that. And here, in this example, with the additional explanation, it is rather easy to find out that we have a bad model that looks at the background of the image and thinks it is the fur of a wolf. The tests indicate that humans make significantly better decisions when they lean on explanations that are extracted by this technique. The source code of this project is also available. Thanks for watching and for your generous support, and I'll see you next time!"
41,"DeepMind's WaveNet, 1000 Times Faster","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Due to popular demand, here is the new DeepMind paper on WaveNet. WaveNet is a text to speech algorithm that takes a sentence as an input, and gives us audio footage of these words being uttered by a person of our choice. Let's listen to some results from the original algorithm, note that these are all synthesized by the AI. All this requires is some training data from this person's voice, typically 10-30 hours, and, a ton of computational power. The computational power part is especially of interest, because we have to produce over 16 to 24 thousand samples for each second of continuous audio footage. And unfortunately, as you can see here, these new samples are generated one by one. And since today's graphics cards are highly parallel, this means that it is a waste to get them to have one compute unit that does all the work while the others are sitting there twiddling their thumbs. We need to make this more parallel somehow. So, the solution is simple, instead of one, we can just simply make more samples in parallel! No, no, no, it doesn't work like that, and the reason for this is that speech is not like random noise it is highly coherent where the new samples are highly dependent on the previous ones. We can only create one new sample at a time. So how can we create the new waveform in one go using these many compute units in parallel? This new WaveNet variant starts out from white noise and applies changes to it over time to morph it into the output speech waveform. The changes take place in parallel over the entirety of the signal, so that's a good sign. It works by creating a reference network that is slow, but correct. Let's call this the teacher network. And the new algorithm arises as a student network, which tries to mimic what the teacher does, but the student tries to be more efficient at that. This has a similar vibe to Generative Adversarial Networks where we have two networks: one is actively trying to fool the other one, while this other one tries to better distinguish fake inputs from real ones. However, it is fundamentally different because of the fact that the student does not try to fool the teacher, but mimic it while being more efficient. And, this yields a blistering fast version of WaveNet that is over a 1000 times faster than its predecessor. It is not real time, it is 20 times faster than real time. And you know what the best part is? Usually, there are heavy tradeoffs for this, but this time, the validation section of the paper reveals that there is no perceived difference in the outputs from the original algorithm. Hell yeah! So, where can we try it? Well, it is already deployed online in Google Assistant in multiple English and Japanese voices. So, as you see, I was wrong. I said that a few papers down the line, it will definitely be done in real time. Apparently, with this new work, it is not a few more papers down the line, it is one, and it is not a bit faster but a thousand times faster. Things are getting out of hand real quick, and I mean this in the best possible way. What a time to be alive! This is one incredible, and highly inspiring work. Make sure to have a look at the paper, perfect training for the mind. As always, it is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
42,Bubble Collision Simulations in Milliseconds,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is about simulating on a computer what happens when bubbles collide. Prepare for lots of beautiful footage. This is typically done by simulating the Navier-Stokes equations that describe the evolution of the velocity and pressure within a piece of fluid over time. However, because the world around us is a continuum, we cannot compute these quantities in an infinite number of points. So, we have to subdivide the 3D space into a grid, and compute them only in these gridpoints. The finer the grid, the more details appear in our simulations. If we try to simulate what happens when these bubbles collide, we would need to create a grid that can capture these details. This is an issue, because the thickness of a bubble film is in the order of 10-800 nanometers, and this would require a hopelessly fine high-resolution grid. By the way, measuring the thickness of bubbles is a science of its own, there is a fantastic reddit discussion on it, I put a link to it in the video description, make sure to check it out. So, these overly fine grids take too long to compute, so what do we do? Well, first, we need to focus on how to directly compute how the shape of soap bubbles evolves over time. Fortunately, from Belgian physicist Joseph Plateau we know that they seek to reduce their surface area, but, retain their volume over time. One of the many beautiful phenomena in nature. So, this shall be the first step we simulate forces that create the appropriate shape changes and proceed into an intermediate state. However, by pushing the film inwards, its volume has decreased, therefore this intermediate state is not how it should look in nature. This is to be remedied now where we apply a volume correction step. In the validation section, it is shown that the results follow to Plateau's laws quite closely. Also, you know well that my favorite kind of validation is when we let reality be our judge, and in this work, the results have been compared to a real-life experimental setup and proved to be very close to it. Take a little time to absorb this. We can write a computer program that reproduces what would happen in reality and result in lots of beautiful video footage. Loving it! And, the best part is that the first surface evolution step is done through an effective implementation of the hyperbolic mean curvature flow, which means that the entirety of the process is typically 3 to 20 times faster than the state of the art while being more robust in handling splitting and merging scenarios. The computation times are now in the order of milliseconds instead of seconds. The earlier work in this comparison was also showcased in Two Minute Papers, if I see it correctly, it was in episode number 18. Holy mother of papers, how far we have come since. I've put a link to it in the video description. The paper is beautifully written, and there are plenty of goodies therein, for instance, an issue with non-manifold junctions is addressed, so make sure to have a look. The source code of this project is also available. Thanks for watching and for your generous support, and I'll see you next time!"
43,This AI Sings,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about building an AI vocoder that is able to synthesize believable singing from MIDI and lyrics as inputs. But first, what is a vocoder? It works kinda like this. Fellow Scholars who are fans of Jean-Michel Jarre's music are likely very familiar with this effect, I've put a link to an example song in the video description. Make sure to leave a comment with your favorite songs with vocoders so I and other Fellow Scholars can also nerd out on them. And now about the MIDI and lyrics terms. The lyrics part is a simple text file containing the words that this synthesized voice should sing, and the MIDI is data that describes the pitch, length and the velocity of each sound. With a little simplification, we could say that the score is given as an input, and the algorithm has to output the singing footage. We will talk about the algorithm in a moment, but for now, let's listen to it. Wow. So this is a vocoder. This means it separates the pitch and timbre components of the voice, therefore the waveforms are not generated directly, which is a key difference from Google DeepMind's WaveNet. This leads to two big advantages: One, the generation times are quite favorable. And by favorable, I guess you're hoping for real time. Well, hold on to your papers, because it is not real time, it is 10-15 times real-time! And two, this way, the algorithm will only need a modest amount of training data to function well. Here, you can see the input phonemes that make up the syllables of the lyrics, each typically corresponding to one note. This is then connected to a modified WaveNet architecture that uses 2-by-1 dilated convolutions. This means that the dilation factor is doubled in each layer, thereby introducing an exponential growth in the receptive field of the model. This helps us keep the parameter count down, which enables training on small datasets. As validation, the mean opinion scores have been recorded, in a previous episode, we discussed that this is a number that describes how a sound sample would pass as genuine human speech or singing. The test showed that this new method is well ahead of the competition, approximately midway between the previous works and the reference singing footage. There are plenty of other tests in the paper, this is just one of many, so make sure to have a look. This is one important stepping stone towards synthesizing singing that is highly usable in digital media and where generation is faster than real time. Creating a MIDI input is a piece of cake with a midi master keyboard, or we can even draw the notes by hand in many digital audio workstation programs. After that, writing the lyrics is as simple as it gets and doesn't need any additional software. Tools like this are going to make this process accessible to everyone. Loving it. If you would like to help us create more elaborate videos, please consider supporting us on Patreon. We also support one-time payments through cryptos like Bitcoin, Ethereum and Litecoin. Everything is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
44,Pruning Makes Faster and Smaller Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When we are talking about deep learning, we are talking about neural networks that have tens, sometimes hundreds of layers, and hundreds of neurons within these layers. This is an enormous number of parameters to train, and clearly, there should be some redundancy, some duplication in the information within. This paper is trying to throw out many of these neurons of the network without affecting its accuracy too much. This process we shall call pruning and it helps creating neural networks that are faster and smaller. The accuracy term I used typically means a score on a classification task, in other words, how good this learning algorithm is in telling what an image or video depicts. This particular technique is specialized for pruning Convolutional Neural Networks, where the neurons are endowed with a small receptive field and are better suited for images. These neurons are also commonly referred to as filters. So here, we have to provide a good mathematical definition of a proper pruning. The authors proposed a definition where we can specify a maximum accuracy drop that we deem to be acceptable, which will be denoted with the letter ""b"" in a moment. And the goal is to prune as many filters as we can, without going over the specified accuracy loss budget. The pruning process is controlled by an accuracy and efficiency term, and the goal is to have some sort of balance between the two. To get a more visual understanding of what is happening, here, the filters you see outlined with the red border are kept by the algorithm, and the rest are discarded. As you can see, the algorithm is not as trivial as many previous approaches that just prune away filters with weaker responses. Here you see the table with the b numbers. Initial tests reveal that around a quarter of the filters can be pruned with an accuracy loss of 0.3%, and with a higher b, we can prune more than 75% of the filters with a loss of around 3%. This is incredible. Image segmentation tasks are about finding the regions that different objects inhabit. Interestingly, when trying the pruning for this task, it not only introduces a minimal loss of accuracy, in some cases, the pruned version of the neural network performs even better. How cool is that! And of course, the best part is that we can choose a tradeoff that is appropriate for our application. For instance, if we are we looking for a light cleanup, we can use the first option at a minimal penalty, or, if we wish to have a tiny tiny neural network that can run on a mobile device, we can look for the more heavy-handed approach by sacrificing just a tiny bit more accuracy. And, we have everything in between. There is plenty more validation for the method in the paper, make sure to have a look! It is really great to see that new research works make neural networks not only more powerful over time, but there are efforts in making them smaller and more efficient at the same time. Great news indeed. Thanks for watching and for your generous support, and I'll see you next time!"
45,Google's Text Reader AI: Almost Perfect,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Earlier, we talked about Google's WaveNet, a learning-based text-to-speech engine. This means that we give it a piece of written text and after a training step using someone's voice, it has to read it aloud using this person's voice as convincingly as possible. And this followup work is about making it even more convincing. Before we go into it, let's marvel at these new results together. Hm-hm! As you can hear, it is great at prosody, stress and intonation, which leads to really believable human speech. The magic component in the original WaveNet paper was introducing dilated convolutions for this problem. This makes large skips in the input data so we have a better global view of it. It is a bit like increasing the receptive field of the eye so we can see the entire landscape, and not only a tree on a photograph. The magic component in this new work is using Mel spectrograms as an input to WaveNet. This is an intermediate representation that is based on the human perception that records not only how different words should be pronounced, but the expected volumes and intonations as well. The new model was trained on about 24 hours of speech data. And of course, no research work should come without some sort of validation. The first is recording the mean opinion scores for previous algorithms, this one and real, professional voice recordings. The mean opinion score is a number that describes how a sound sample would pass as genuine human speech. The new algorithm passed with flying colors. An even more practical evaluation was also done in the form of a user study where people were listening to the synthesized samples and professional voice narrators, and had to guess which one is which. And this is truly incredible, because most of the time, people had no idea which was which if you don't believe it, we'll try this ourselves in a moment. A very small, but statistically significant tendency towards favoring the real footage was recorded, likely because some words, like ""merlot"" are mispronounced. Automatically voiced audiobooks, automatic voice narration for video games. Bring it on. What a time to be alive! Note that producing these waveforms is not real time and still takes quite a while. To progress along that direction, scientists as DeepMind wrote a heck of a paper where they sped WaveNet up a thousand times. Leave a comment if you would like to hear more about it in a future episode. And of course, new inventions like this will also raise new challenges down the line. It may be that voice recordings will become much easier to forge and be less useful as evidence unless we find new measures to verify their authenticity, for instance, to sign them like we do with software. In closing, a few audio sample pairs, one of them is real, one of them is synthesized. What do you think, which is which? Leave a comment below. I'll just leave a quick hint here that I found on the webpage. Hopp! There you go. If you have enjoyed this episode, please make sure to support us on Patreon. This is how we can keep the show running, and you know the drill, one dollar is almost nothing, but it keeps the papers coming. Thanks for watching and for your generous support, and I'll see you next time!"
46,SLAC Dataset From MIT and Facebook ,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This project is about a dataset created through a joint effort between MIT and Facebook. As it turns out, this dataset is way more useful than I initially thought, I'll tell you in a moment why. Datasets are used to train and test the quality of learning algorithms. This particular dataset contains short video clips. These clips are passed to a neural network which is asked to classify the kind of activity that is taking place in the video. In this dataset, there are many cases where everything is given to come to a logical answer that is wrong. We may be in a room with the climbing wall, but exercising is not necessarily happening. We could be around the swimming pool, but swimming is not necessarily happening. I am pretty sure this has happened to you too. This is a brilliant idea, because it is super easy for a neural network to assume that if there is a swimming pool, swimming is probably happening, but it takes a great deal of understanding to actually know what constitutes the swimming part. A few episodes ago, we discussed that this could potentially be a stepping stone towards creating machines that think like humans. Without looking into it, it would be easy to think that creating a dataset is basically throwing a bunch of training samples together and calling it a day. I can assure you that this is not the case and that creating a dataset like this was a herculean effort as it contains more than half a million videos, and almost two million annotations for 200 different activities. And, there are plenty of pre-processing steps that one has to perform to make it usable. The collection procedure contains a video crawling step where a large number of videos are obtained from YouTube, which are to be deduplicated, which means removing videos that are too similar to one already contained in the database. A classical case is many different kinds of commentary on the same footage. This amounted to the removal of more than 150 thousand videos. Then, all of these videos undergo a shot and person detection step where relevant subclips are extracted that contain some kind of human activity. These are then looked at by two different classifiers, and depending on whether there was a consensus between the two, a decision is made whether the clip is to be discarded or not. This step helps balancing the ratio of videos where there is some sort of relevant action compared to clips where there is no relevant action happening. This also makes the negative samples much harder because the context may be correct, but the expected activity may not be there. This is the classical hard case with the swimming pool and people in swimming suits twiddling their thumbs instead of swimming. And here comes the more interesting part when trying to train a neural network for other, loosely related tasks, using this dataset for pre-training improves the scores significantly. I'll try to give a little context for the numbers, because these numbers are absolutely incredible. There are cases where the success rate is improved by over 30%, which speaks for itself. However, there are other cases where the difference is about 10-15% percent, that is also remarkable when we are talking about high numbers because the closer the classifier gets to a 100%, the more difficult the remaining corner cases are that improve the accuracy. In these cases, even a 3% improvement is remarkable. And before we go, greetings and best regards to Lucas, the little scholar who seems to be absorbing the papers along with the mother's milk. Excellent. You can't start early enough in the pursuit of knowledge. Thanks for watching and for your generous support, and I'll see you next time!"
47,DeepMind Control Suite,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This footage that you see here came freshly from Google DeepMind's lab, and is about benchmarking reinforcement learning algorithms. Here, you see the classical cartpole swing-up task from this package. As the algorithm starts to play, a score is recorded that indicates how well it is doing, and the learner has to choose the appropriate actions depending on the state of the environment to maximize this score. Reinforcement learning is an established research subfield within machine learning with hundreds of papers appearing every year. However, we see that most of them cherry-pick a few problems and test against previous works on this very particular selection of tasks. This paper describes a package that is not about the algorithm itself, but about helping future research projects to be able to test their results against previous works on an equal footing. This is a great idea, which has been addressed earlier by OpenAI with their learning environment by the name Gym. So the first question is, why do we need a new one? The DeepMind Control Suite provides a few differentiating features. One, Gym contains both discrete and continuous tasks, where this one is concentrated on continuous problems only. This means that state, time and action are all continuous which is usually the hallmark of more challenging and life-like problems. For an algorithm to do well, it has to be able to learn the concept of velocity, acceleration and other meaningful physical concepts and understand their evolution over time. Two, there are domains where the new control suite is a superset of Gym, meaning that it offers equivalent tasks, and then some more. And three, the action and reward structures are standardized. This means that the results and learning curves are much more informative and easier to read. This is crucial because research scientists read hundreds of papers every year, and this means that they don't necessarily have to look at videos, they immediately have an intuition of how an algorithm works and how it relates to previous techniques just by looking at the learning curve plots. Many tasks also include a much more challenging variant with more sparse rewards. We discussed these sparse rewards in a bit more in detail in the previous episode, if you are interested, make sure to click the card on the lower right at the end of this video. The paper also contains an exciting roadmap for future development, including quadruped locomotion, multithreaded dynamics and more. Of course, the whole suite is available, free of charge for everyone. The link is available in the description. Super excited to see a deluge of upcoming AI papers and see how they beat the living hell out of each other in 2018. Thanks for watching and for your generous support, and I'll see you next time!"
48,Reinforcement Learning With Noise (OpenAI),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about improving reinforcement learning. Reinforcement learning is a learning algorithm that we can use to choose a set of actions in an environment to maximize a score. Our classical example applications are helicopter control, where the score to be maximized would be proportional to the distance that we traveled safely, or any computer game of your choice where a score can describe how well we are doing. For instance, in Frostbite, our score describes how many jumps we have survived without dying and this score is subject to maximization. Earlier, scientists at DeepMind combined a reinforcement learner with a deep neural network so the algorithm could look at the screen and play the game much like a human player would. This problem is especially difficult when the rewards are sparse. This is similar to what a confused student would experience after a written exam when only one grade is given, but the results for the individual problems are not shown. It is quite hard to know where we did well and where we missed the mark, and it is much more challenging to choose the appropriate topics to study to do better next time. When starting out, the learner starts exploring the parameter space and performs crazy, seemingly nonsensical actions until it finds a few scenarios where it is able to do well. This can be thought of as adding noise to the actions of the agent. Scientists at OpenAI proposed an approach where they add noise not directly to the actions, but the parameters of the agent, which results in perturbations that depend on the information that the agent senses. This leads to less flailing and a more systematic exploration that substantially decreases the time taken to learn tasks with sparse rewards. For instance, it makes a profound difference if we use it in the walker game. As you can see here, the algorithm with the parameter space noise is able to learn the concept of galloping, while the traditional method does, well, I am not sure what it is doing to be honest, but it is significantly less efficient. The solution does not come without challenges. For instance, different layers respond differently to this added noise, and, the effect of the noise on the outputs grows over time, which requires changing the amount of noise to be added depending on its expected effect on the output. This technique is called adaptive noise scaling. There are plenty of comparisons and other cool details in the paper, make sure to have a look, it is available in the video description. DeepMind's deep reinforcement learning was published in 2015 with some breathtaking results and superhuman plays on a number of different games, and it has already been improved leaps and bounds beyond its initial version. And we are talking about OpenAI, so of course, the source code of this project is available under the permissive MIT license. In the meantime, we have recently been able to upgrade the entirety of our sound recording pipeline through your support on Patreon. I have been yearning for this for a long-long time now and not only that, but we could also extend our software pipeline with sound processing units that use AI and work like magic. Quite fitting for the series, right? Next up is a recording room or recording corner with acoustic treatment, depending on our budget. Again, thank you for your support, it makes a huge difference. A more detailed write-up on this is available in the video description, have a look. Thanks for watching and for your generous support, and I'll see you next time!"
49,DeepMind's AI Learns Object Sounds,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about creating an AI that can perform audio-visual correspondence. This means two really cool tasks: One, when given a piece of video and audio, it can guess whether they match each other. And two, it can localize the source of the sounds heard in the video. Hm-hmm! And wait, because this gets even better! As opposed to previous works, here, the entire network is trained from scratch and is able to perform cross-modal retrieval. Cross-modal retrieval means that we are able to give it an input sound and it will be able to find pictures that would produce similar sounds. Or vice versa. For instance, here, the input is the sound of a guitar, note the loudspeaker icon in the corner, and it shows us a bunch of either images or sounds that are similar. Marvelous. The training is unsupervised, which means that the algorithm is given a bunch of data and learns without additional labels or instructions. The architecture and results are compared to a previous work by the name Look, Listen & Learn that we covered earlier in the series, the link is available in the video description. As you can see, both of them run a convolutional neural network. This is one of my favorite parts about deep learning the very same algorithm is able to process and understand signals of very different kinds: video and audio. The old work concatenates this information and produces a binary yes/no decision whether it thinks the two streams match. This new work tries to produce number that encodes the distance between the video and the audio. Kind of like the distance between two countries on a map, but both video and audio signals are embedded in the same map. And the output decision always depends on how small or big this distance is. This distance metric is quite useful: if we have an input video or audio signal, choosing other video and audio snippets that have a low distance is one of the important steps that opens up the door to this magical cross-modal retrieval. What a time to be alive! Some results are very easy to verify, others may spark some more debate, for instance, it is quite interesting to see that the algorithm highlights the entirety of the guitar string as a sound source. If you are curious about this mysterious blue image here, make sure to have a look at the paper for an explanation. Now this is a story that we would like to tell to as many people as possible. Everyone needs to hear about this. If you would like to help us with our quest, please consider supporting us on Patreon. You can also pick up some cool perks, like getting early access to these videos or deciding the order of upcoming episodes. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
50,Building Machines That Learn and Think Like People,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper discusses possible roadmaps towards building machines that are endowed with humanlike thinking. And before we go into that, the first question would be, is there value in building machines that think like people? Do they really need to think like people? Isn't it a bit egotistical to say ""if they are to become any good at this and this task, they have to think like us""? And the answer is, well, in some cases, yes. If you remember DeepMind's Deep Q-Learning algorithm, it was able to play on a superhuman level on 29 out of 49 different Atari games. For instance, it did quite well in Breakout, but less so in Frostbite. And by Frostbite, I mean not the game engine, but the Atari game from 1983 where we need to hop from ice floe to ice floe and construct an igloo. However, we are not meant to jump around arbitrarily we can gather these pieces by jumping on the active ice floes only, and these are shown with white color. Have a look at this plot. It shows the score it was able to produce as a function of game experience in hours. As you can see, the original DQN is doing quite poorly, while the extended versions of the technique can reach a relatively high score over time. This looks really good...until we look at the x axis, because then we see that this takes around 462 hours and the scores plateau afterwards. Well, compare that to humans can do at least as well, or a bit better after a mere 2 hours of training. So clearly, there are cases where there is an argument to be made for the usefulness of humanlike AI. The paper describes several possible directions that may help us achieve this. Two of them is understanding intuitive physics and intuitive psychology. Even young infants understand that objects follow smooth paths and expect liquids to go around barriers. We can try to endow an AI with similar knowledge by feeding it with physics simulations and their evolution over time to get an understanding of similar phenomena. This could be used to augment already existing neural networks and give them a better understanding of the world around us. Intuitive psychology is also present in young infants. They can tell people from objects, or distinguish other social and anti-social agents. They also learn goal-based reasoning quite early. This means that a human who looks at an experienced player play Frostbite can easily derive the rules of the game in a matter of minutes. Kind of like what we are doing now. Neural networks also have a limited understanding of compositionality and causality, and often perform poorly when describing the content of images that contain previously known objects interacting in novel, unseen ways. There are several ways of achieving each of these elements described in the paper. If we manage to build an AI that is endowed with these properties, it may be able to think like humans, and through self-improvement, may achieve the kind of intelligence that we see in all these science fiction movies. There is lots more in the paper learning to learn, approximate models for thinking faster, model-free reinforcement learning, and a nice Q&A section with responses to common questions and criticisms. It is a great read and is easy to understand for everyone, I encourage you to have a look at the video description for the link to it. Scientists at Google DeepMind have also written a commentary article where they largely agree with the premises described in this paper, and add some thoughts about the importance of autonomy in building humanlike intelligence. Both papers are available in the video description and both are great reads, so make sure to have a look at them! It is really cool that we have plenty of discussions on potential ways to create a more general intelligence that is at least as potent as humans in a variety of different tasks. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
51,This Autonomous Robot Models Your House Interior,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The goal of this work is to have a robot that automatically creates a 3D model of an indoor space, including path planning and controlling attention. Now this immediately sounds like quite a challenging task. This robot uses an RGBD camera, so beyond the colors, it also gets some depth information that describes how far the viewed objects are from the observer. From this information, it tries to create a 3D digital model of these interiors. It not only does that, but it also constantly replans its trajectory based on newly found areas. These paths have to be smooth and walkable without bumping into objects, and of course, adapt to the topology of the building. You'll see in a moment why even this mundane sounding smooth path part is really challenging to accomplish. Spoiler alert: it's about singularities. With this proposed technique, the robot takes a lap in the building and builds a rough representation of it, kind of like a minimap in your favorite computer games. Previous techniques worked with potential and gradient fields to guide the navigation. The issue with these is that the green dots that you see here represent singularities. These are degenerate points that introduce ambiguity to the path planning process and reduce the efficiency of the navigation. The red dots are sinks, which are even worse, because these can trap the robot. This new proposed tensor field representation contains a lot fewer singularities, and its favorable mathematical properties make it sink-free. This leads to much better path planning which is crucial for maintaining high reconstruction quality. If you have a look at the paper, you'll see several occurrences of the word ""advection"". This is particularly cool because these robot paths are planned in these gradient or tensor fields represent the vortices and flow directions similarly to how fluid flows are computed in simulations. Many of which you have seen in this series. Beautiful, love it. However, as advection can't guarantee that we'll have a full coverage of the building, this proposed technique borrows classic structures from graph theory. Graph theory is used to model the connections between railway stations or to represent people and their relationships in a social network. And here, a method to construct a minimum spanning tree was borrowed to help deciding which direction to take at intersections for optimal coverage with miminal effort. Robots, fluids, graph theory, some of my favorite topics of all time, so you probably know how happy I was to see all these theories come together to create something really practical. This is an amazing paper, make sure to have a look at it, it is available in the video description. The source code of this project is also available. If you have enjoyed this episode, make sure to subscribe to the series and click the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!"
52,High-Resolution Neural Texture Synthesis,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Deep Learning means that we are working with neural networks that contain many inner layers. As neurons in each layer combine information from the layer before, the deeper we go in these networks, the more elaborate details we're going to see. Let's have a look at an example. For instance, if we train a neural network to recognize images of human faces, first we'll see an edge detector, and as a combination of edges, object parts will emerge in the next layer. And in the later layers, a combination of object parts create object models. Neural texture synthesis is about creating lots of new images based on an input texture, and these new images have to resemble, but not copy the input. Previous works on neural texture synthesis focused on how different features in a given layer relate to the ones before and after it. The issue is that because neurons in convolutional neural networks are endowed with a small receptive field, they can only look at an input texture at one scale. So for instance, if you look here, you see that with previous techniques, trying to create small-scale details in a synthesized texture is going to lead to rather poor results. This new method is about changing the inputs and the outputs of the network to be able to process these images at different scales. These scales range from coarser to finer versions of the same images. Sound simple enough, right? This simple idea makes all the difference here, you can see the input texture, and here is the output. As you can see, it has different patterns but has very similar properties to the input, and if we zoom into both of these images, we see that this one is able to create beautiful, high-frequency details as well. Wow, this is some really, really crisp output. Now, it has to be emphasized that this means that the statistical properties of the original image are being mimiced really well. What it doesn't mean is that it takes into consideration the meaning of these images. Just have a look at the synthesized bubbles or the flowers here. The statistical properties of the synthesized textures may be correct, but the semantic meaning of the input is not captured well. In a future work, it would be super useful to extend this algorithm to have a greater understanding of the structure and the symmetries of the input images into consideration. The source code is available under the permissive MIT license, so don't hold back those crazy experiments. If you have enjoyed this episode and you think the series provides you value or entertainment, please consider supporting us on Patreon. One-time payments and cryptocurrencies like Bitcoin or Ethereum are also supported, and have been massively successful, I am really out of words, thank you so much. The details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
53,Efficient Viscoelastic Fluid Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. It has been a while now since we've talked about fluid simulations, and now, it is time for us to have a look at an amazing technique that creates simulations with viscoelastic fluids, plus rigid and deformable bodies. This is a possibility with previous techniques but takes forever to compute and typically involves computational errors that add up over time and lead to a perceptible loss of viscoelasticity. I'll try to explain these terms and what's going on in a moment. There will be lots of eye candy to feast your eyes on throughout the video, but I can assure you that this scene takes the cake. The simulation phase here took place at one frame per second, which is still not yet fast enough for the much coveted real-time applications, however, is super competitive compared previous off-line techniques that could do this. So what does viscosity mean? Viscosity is the resistance of a fluid against deformation. Water has a low viscosity and rapidly takes the form of the cup we pour it into, where honey, ketchup and peanut butter have a higher viscosity and are much more resistant to these external forces. Elasticity is a little less elusive concept that describes to what degree a piece of fluid behaves like elastic solids. However, viscous and elastic are not binary yes or no concepts, there is a continuum between the two, and if we have the proper machinery, we can create viscoelastic fluid simulations. The tau parameter that you see here controls how viscous or elastic the fluid should be, and is referred to in the paper as relaxation time. As tau is increased towards infinity, the friction dominates the internal elastic forces and the polymer won't be able to recover its structure well. The opposite case is where we reduce the tau parameter to zero, then, the internal elastic forces will dominate and our polymer will be more rigid. The alpha parameter stands for compliance, which describes the fluidity of the model. Using a lower alpha leads to more solid behavior, and higher alphas lead to more fluid behavior. The cool thing is that as a combination of these two parameters, we can produce a lot of really cool materials ranging from viscous to elastoplastic to inviscid fluid simulations. Have a look at this honey pouring scene. Mmmm! This simulation uses more than a 100 thousand particles and 6 of these frames can be simulated in just 1 second. Wow. If we reduce the number of particles to a few tens of thousands, real-time human interaction with these simulations also becomes a possibility. A limitation of this technique is that most of our decisions involving the physical properties of the fluid are collapsed into the tau and alpha parameters, if we are looking for more esoteric fluid models, we should look elsewhere. I am hoping that since part of the algorithm runs on the graphics card, the speed of this technique can be further improved in the near future. That would be awesome. Admittedly, we've only been scratching the surface, so make sure to have a look at the paper for more details. Thanks for watching and for your generous support, and I'll see you next time!"
54,Deep Image Prior,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about performing useful image restoration tasks with a convolutional neural network with an additional twist. Its main use cases are as follows: One, in the case of JPEG artifact removal, the input is this image with many blocky artifacts that materialized during compression, and the output is a restored version of this image. Two, image inpainting where some regions of the input image are missing and are to be filled with useful and hopefully plausible information. Three, super resolution where the input image is intact, but is very coarse and has low resolution, and the output should be a more detailed, higher resolution version of the same image. This is the classic enhance scenario from these CSI TV series. It is typically hard to do because there is a stupendously large number of possible high resolution image solutions that we could come up with as an output. Four, image denoising is also a possibility. The standard way of doing these is that we train such a network on a large database of images so that they can learn the concept of many object classes, such as humans, animals and more, and also the typical features and motifs that are used to construct such images. These networks have some sort of understanding of these images and hence, can perform these operations better than most handcrafted algorithms. So let's have a look at some comparisons. Do you see these bold lettered labels that classify these algorithms as trained or untrained? The bicubic interpolation is a classical untrained algorithm that almost naively tries to guess the pixel colors by averaging its neighbors. This is clearly untrained because it does not take a database of images to learn on. Understandably, the fact that the results are lackluster is to show that non-learning-based algorithms are not great at this. The SRResNet is a state of the art learning-based technique for super resolution that was trained on a large database of input images. It is clearly doing way better than bicubic interpolation. And look, here we have this Deep Prior algorithm that performs comparably well, but is labeled to be untrained. So what is going on here? And here comes the twist this Convolutional Neural Network is actually untrained. This means that the neuron weights are randomly initialized, which generally leads to completely useless results on most problems. So no aspect of this network works through the data it has learned on, all the required information is contained within the structure of the network itself. We all know that the structure of these neural networks matter a great deal, but in this case, it is shown that it is at least as important as the training data itself. A very interesting and esoteric idea indeed, please make sure to have a look at the paper for details as there are many details to be understood to get a more complete view of this conclusion. In the comparisons, beyond the images, researchers often publish this PSNR number you see for each image. This is the Peak Signal To Noise ratio, which means how close the output image is to the ground truth, and this number is, of course, always subject to maximization. Remarkably, this untrained network performs well both on images with natural patterns and man-made objects. Reconstruction from a pair of flash and no-flash photography images is also a possibility and the algorithm does not contain the light leaks produced by a highly competitive handcrafted algorithm, a joint bilateral filter. Quite remarkable indeed. The supplementary materials and the project website contain a ton of comparisons against competing techniques, so make sure to have a look at that if you would like to know more. The source code of this project is available under the permissive Apache 2.0 license. If you have enjoyed this episode and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. One dollar is almost nothing, but it keeps the papers coming. Recently, we have also added the possibility of one-time payments through Paypal and cryptocurrencies. I was stunned to see how generous our crypto-loving Fellow Scholars are. Since most of these crypto donations are anonymous and it is not possible to say thank you to everyone individually, I would like to say a huge thanks to everyone who supports the series. And this applies to everyone regardless of contribution just watching the series and spreading the word is already a great deal of help for us. Thanks for watching and for your generous support, and I'll see you next time!"
55,Distilling Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Since the latest explosion in AI research, virtually no field of science remains untouched by neural networks. These are amazing tools that help us solve problems where the solutions are easy to identify, but difficult to explain. For instance, we all know a backflip when we see one, but mathematically defining all the required forces, rotations and torque is much more challenging. Neural networks excel at these kinds of tasks, provided that we can supply them a large number of training samples. If we peek inside these neural networks, we'll see more and more layers and more and more neurons within these layers as years go by. The final decision depends on what neurons are being activated by our inputs. They are highly efficient, however, trying to understand how a decision is being made by these networks is going to be a fruitless endeavor. This is especially troublesome when the network gives us a wrong answer that we, without having access to any sort of explanation, may erroneously accept without proper consideration. This piece of work is about distillation, which means that we take a neural network and try to express its inner workings in the form of a decision tree. Decision trees take into consideration a series of variables and provide a clear roadmap towards a decision based on them. For instance, they are useful in using the age and amount of free time of people to try to guess whether they are likely to play video games, or deciding who should get a loan from the bank based on their age, occupation and income. Yeah! This sounds great! However, the main issue is that decision trees are not good substitutes for neural networks. The theory says that we have a generalization versus interpretability tradeoff situation, which means that trees that provide us good decisions overfit the training data and generalize poorly, and the ones that are easy to interpret are inaccurate. So in order to break out of this tradeoff situation, a key idea of this piece of work is to ask the neural network to build a decision tree by taking an input dataset for training, trying to generate more training data that follows the same properties, and feed all this to the decision tree. Here are some results for the classical problem of identifying digits in the MNIST dataset. As each decision is meant to cut the number of output options in half, it shows really well that we can very effectively perform the classification in only 4 decisions from a given input. And not only that, but it also shows what it is looking for. For instance, here, we can see that the final decision before we conclude whether the input number is a 3 or 8, it looks for the presence of a tiny area that joins the ends of the 3 to make an 8. A different visualization of the Connect4 game dataset reveals that the neural network quickly tries to distinguish two types of strategies: one where the players start playing on the inner, and one with the outer region of the board. It is shown that these trees perform better than traditional decision trees. What's more, they are only slightly worse than the corresponding neural networks, but can explain their decisions much more clearly and are also faster. In summary, the rate of progress in machine learning research is truly insane these days, and I and am all for papers that try to provide us a greater understanding of what is happening under the hood. I am loving this idea in particular. We had an earlier episode on how to supercharge these decision trees via tree boosting. If you are interested in learning more about it, the link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
56,AI Learns Semantic Image Manipulation,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This technique is about creating high resolution images from semantic maps. A semantic map is a colorful image where each of the colors denote an object class, such as pedestrians, cars, traffic signs and lights, buildings, and so on. Normally, we use light simulation programs or rasterization to render such an image, but AI researchers asked the question: why do we even need a renderer if we can code up a learning algorithm that synthesizes the images by itself? Whoa. This generative adversarial network takes this input semantic map, and synthesizes a high-resolution photorealistic image from it. Previous techniques were mostly capable of creating coarser, lower resolution images, and also they were rarely photorealistic. And get this, this one produces 2k by 1k pixel outputs, which is close to full HD in terms of pixel count. If we wish to change something in a photorealistic image, we'll likely need a graphic designer and lots of expertise in photoshop and similar tools. In the end, even simpler edits are very laborious to make because the human eye is very difficult to fool. An advantage of working with these semantic maps is that they are super easy to edit without any expertise. For instance, we can exert control on the outputs by choosing from a number of different possible options to fill the labels. These are often not just reskinned versions of the same car or road but can represent a vastly different solution, like changing the material of the road from concrete to dirt. Or, it is super easy to replace trees with buildings, all we have to do is rename the labels in the input image. These results are not restricted to outdoors traffic images. Individual parts of human faces are also editable. For instance, adding a moustache has never been easier. The results are compared to a previous technique by the name pix2pix and against cascaded refinement networks. You can see that the quality of the outputs vastly outperforms both of them, and the images are also of visibly higher resolution. It is quite interesting to say that these are ""previous work"", because both of these papers came out this year, for instance, our episode on pix2pix came 9 months ago and it has already been improved by a significant margin. The joys of machine learning research. Part of the trick is that the semantic map is not only used by itself, but a boundary map is also created to encourage the algorithm to create outputs with better segmentation. This boundary information turned out to be just as useful as the labels themselves. Another trick is to create multiple discriminator networks and run them on a variety of coarse to fine scale images. There is much, much more in the paper, make sure to have a look for more details. Since it is difficult to mathematically evaluate the quality of these images, a user study was carried out in the paper. In the end, if we take a practical mindset, these tools are to be used by artists and it is reasonable to say that whichever one is favored by humans should be accepted as a superior method for now. This tool is going to be a complete powerhouse for artists in the industry. And by this, I mean right now because the source code of this project is available to everyone, free of charge. Yipee! In the meantime, we have an opening at our Institute at the Vienna University of Technology for one PhD student and one PostDoc. The link is available in the video description, read it carefully to make sure you qualify, and if you apply through the e-mail address of Professor Michael Wimmer, make sure to mention Two Minute Papers in your message. This is an excellent opportunity to turn your life around, live in an amazing city, learn a lot and write amazing papers. It doesn't get any better than that. Deadline is end of January. Thanks for watching and for your generous support, and I'll see you next time!"
57,AlphaZero: DeepMind's New Chess AI,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. After defeating pretty much every highly ranked professional player in the game of Go, Google DeepMind now ventured into the realm of Chess. They recently challenged not the best humans, no-no-no, that was long ago. They challenged Stockfish, the best computer chess engine in existence in quite possibly the most exciting chess-related event since Kasparov's matches against Deep Blue. I will note that I was told by DeepMind that this is the preliminary version of the paper, so now we shall have an initial look, and perhaps make a part 2 video with the newer results when the final paper drops. AlphaZero is based on a neural network and reinforcement learning and is trained entirely through self-play after being given the rules of the game. It is not to be confused with AlphaGo Zero that played Go. It is also noted that this is not simply AlphaGo Zero applied to chess. This is a new variant of the algorithm. The differences include: one, the rules of chess are asymmetric, for instance pawns only move forward, castling is different on kingside and queenside, and this means that neural network-based techniques are less effective at it. two, the algorithm not only has to predict a binary win or loss probability when given a move, but draws are also a possibility and that is to be taken into consideration. Sometimes a draw is the best we can do, actually. There are many more changes to the previous incarnation of the algorithm, please make sure to have a look at the paper for details. Before we start with the results and more details, a word on Elo ratings for perspective. The Elo rating is a number that measures the relative skill level of a player. Currently, the human player with the highest Elo rating, Magnus Carlssen is hovering around 2800. This man played chess blindfolded against 10 opponents simultaneously in Vienna a couple years ago and won most of these games. That's how good he is. And Stockfish is one of the best current chess engines, with Elo rating over 3300. A difference of 500 Elo points means that if it were to play against Magnus Carlssen, it would be expected to win at least 95 games out of a 100. Though it is noted that there is a rule suggesting a hard cutoff at around a 400 point difference. The two algorithms then played each other. AlphaZero versus Stockfish. They were both given 60 seconds of thinking time per move, which is considered to be plenty given that both of the algorithms take around 10 seconds at most per move. And here are the results. AlphaZero was able to outperform Stockfish in about 4 hours of learning from scratch. They played a 100 games AlphaZero won 28 times, drew 72 times and never lost to Stockfish. Holy mother of papers, do you hear that? Stockfish is already unfathomably powerful compared to even the best human prodigies, and AlphaZero basically crushed it after four hours of self-play. And, it was run with a similar hardware as AlphaGo Zero, one machine with 4 Tensor Processing Units. This is hardly commodity hardware, but given the trajectory of the improvements we've seen lately, it might very well be in a couple of years. Note that Stockfish does not use machine learning and is a handcrafted algorithm. People like to refer to computer opponents in computer games as AI, but it is not doing any sort of learning. So, you know what the best part is? AlphaZero is a much more general algorithm that can also play Shogi on an extremely high level, which is also referred to as Japanese chess. And this is one of the most interesting points AlphaZero would be highly useful even it if were slightly weaker than Stockfish, because it is built on more general learning algorithms that can be reused for other tasks without investing significant human effort. But in fact, it is more general, and it also crushes Stockfish. With every paper from DeepMind, the algorithm becomes better AND more and more general. I can tell you, this is very, very rarely the case. Total insanity. Two more interesting tidbits about the paper: one, all the domain knowledge the algorithm is given is stated precisely for clarity. two, one might think that as computers and processing power increases over time, all we have to do is add more brute force to the algorithm and just evaluate more positions. If you think this is the case, have a look at this it is noted that AlphaZero was able to reliably defeat Stockfish WHILE evaluating ten times fewer positions per second. Maybe we could call this the AI equivalent of intuition, in other words, being able to identify a small number of promising moves and focusing on them. Chills run down my spine as I read this paper. Being a researcher is the best job in the world. And we are even being paid for this. Unreal. This is a hot paper, there is lot of discussions out there on this, lots of chess experts analyze and try to make sense of the games. I had a ton of fun reading and watching through some of these, as always, Two Minute Papers encourages you to explore and read more, and the video description is ample in useful materials. You will find videos with some really cool analysis from Grandmaster Daniel King, International Chess Master Daniel Rensch, and the YouTube channel ChessNetwork. All quality materials. And, if you have enjoyed this episode and you think that 8 of these videos a month is worth a few dollars, please throw a coin our way on Patreon, or, if you favor cryptocurrencies instead, you can throw Bitcoin or Ethereum our way. You support has been amazing as always and thanks so much for keeping with us through thick and thin, even in times when weird Patreon decisions happen. Luckily, this last one has been reverted. I am honored to have supporters like you Fellow Scholars. Thanks for watching and for your generous support, and I'll see you next time!"
58,AI Learns Noise Filtering For Photorealistic Videos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is another one of those amazing papers that I am really excited about. And the reason for that is that this is in the intersection of computer graphics and machine learning, which, as you know, is already enough to make me happy, but when I've first seen the quality of the results, I was delighted to see that it delivered exactly what I was hoping for. Light simulation programs are an important subfield of computer graphics where we try to create a photorealistic image of a 3D digital scene by simulating the path of millions and millions of light rays. First, we start out with a noisy image, and as we compute more paths, it slowly clears up. However, it takes a very long time to get a perfectly clear image, and depending on the scene and the algorithm, it can take from minutes to hours. In an earlier work, we had a beautiful but pathological scene that took weeks to render on several machines, if you would like to hear more about that, the link is available in the video description. So in order to alleviate this problem, many noise filtering algorithms surfaced over the years. The goal of these algorithms is that instead of computing more and more paths until the image clears up, we stop at a noisy image and try to guess what the final image would look like. This often happens in the presence of some additional depth and geometry information, additional images that that are often referred to as feature buffers or auxiliary buffers. This information helps the noise filter to get a better understanding of the scene and produce higher quality outputs. Recently, a few learning-based algorithms emerged with excellent results. Well, excellent would be an understatement since these can take an extremely noisy image that we rendered with one ray per pixel. This is as noisy as it gets I'm afraid, and it is absolutely stunning that we can still get usable images out of this. However, these algorithms are not capable of dealing with sequences of data and are condemned to deal with each of these images in isolation. They have no understanding of the fact that we are dealing with an animation. What does this mean exactly? What this means is that the network has no memory of how it dealt with the previous image, and if we combine it with the fact that a trace amount of noise still remains in the images, we get a disturbing flickering effect. This is because the remainder of the noise is different from image to image. This technique uses a Recurrent Neural Network, which is able to deal with sequences of data, for instance, in our case, video. It remembers how it dealt with the previous images a few moments ago, and, as a result, it can adjust and produce outputs that are temporally stable. Computer graphics researchers like to call this spatiotemporal filtering. You can see in this camera panning experiment how much more smoother this new technique is. Let's try the same footage slowed down and see if we get a better view of the flickering. Yup, all good! Recurrent Neural Networks are by no means easy to train and need quite a few implementation details to get it right, so make sure to have a look at the paper for details. Temporally coherent light simulation reconstruction of noisy images from one sample per pixel. And for video. This is insanity. I would go out on a limb and say that in the very near future, we'll run learning-based noise filters that take images that are so noisy, they don't even have one ray sample per pixel. Maybe one every other pixel or so. This is going to be the new milestone. If someone told me that this would be possible when I started doing light transport as an undergrad student, I wouldn't have believed a word of it. Computer games, VR and all kinds of real-time applications will be able to get photorealistic light simulation graphics in real time. And, temporally stable. I need to take some time to digest this. Thanks for watching and for your generous support, and I'll see you next time!"
59,AI Beats Radiologists at Pneumonia Detection,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this work, a 121-layer convolutional neural network is trained to recognize pneumonia and 13 different diseases. Pneumonia is an inflammatory lung condition that is responsible for a million hospitalizations and 50,000 deaths per year in the US alone. Such an algorithm requires a training set of formidable size to work properly. This means a bunch of input-output pairs. In this case, one training sample is an input frontal X-ray image of the chest, and the outputs are annotations by experts who mark which of the 14 different sought diseases are present in this sample. So they say like this image contains pneumonia here, and this doesn't. This is not just a binary yes or no answer, but a more detailed heatmap of possible regions that fit the diagnosis. The training set used for this algorithm contained over a 100.000 images of over 30.000 patients. This is then given to the neural network, and its task is to learn the properties of these diseases by itself. Then, after the learning process took place, previously unseen images are given to the algorithm and a set of radiologists. This is called a test set, and of course, it is crucial that both the training and the test sets are reliable. If the training and test set is created by one expert radiologist, and then we again benchmark a neural network against a different, randomly picked radiologist, that's not a very reliable process because each of the humans may be wrong in more than a few cases. Instead, the training and test annotation data is created by asking multiple radiologists and taking a majority vote on their decisions. So now that the training and test data is reliable, we can properly benchmark a human versus a neural network. And here's the result: this learning algorithm outperforms the average human radiologist. The performance was measured in a 2D space, where sensitivity and specificity were the two interesting metrics. Sensitivity means the proportion of positive samples that were classified as positive, and specificity means the portion of negative samples that were classified as negative. The crosses mean the human doctors, and as you can see, whichever radiologist we look at, even though they have different false positive and negative ratios, they are all located below the blue curve which denotes the results of the learning algorithm. This is a simple diagram, but if you think about what it actually means, this is an incredible application of machine intelligence. And now, a word on limitations. It is noted that this was an isolated test, for instance, the radiologists were only given one image, and usually, when diagnosing someone, they know more about the history of the patient that may further help their decisions. For instance, a history of a strong cough and high fever is highly useful supplementary information for humans when diagnosing someone who may have pneumonia. Beyond only the frontal view of the chest, it is also standard practice to use the lateral views as well if the results are inconclusive. These views are not available in this dataset and it is conjectured that it may sway the comparison towards humans. However, I'll note that this information may also benefit the AI just as much as the radiologists, and this seems like a suitable direction for future work. Finally, this is not the only algorithm for pneumonia detection, and it has been compared to the state of the art for all 14 diseases, and this new technique came out on top on all of them. Also, have a look at the paper for details because training a 121-layer neural network requires some clever shenanigans as this was the case here too. It is really delightful to see that these learning algorithms can help diagnosing serious illnesses, and provide higher quality healthcare to more and more people around the world, especially in places where access to expert radiologists is limited. Everyone needs to hear about this. If you wish to help us spreading the word and telling these incredible stories to even more people, please consider supporting us on Patreon. We also know that many of you are crazy for Bitcoin, so we also set up a Bitcoin address as well. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
60,Universal Neural Style Transfer,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Let's have a look at some recent results on neural style transfer. You know the drill, we take a photo with some content, and for example, a painting with the desired style, and the output is an image where this style is applied to our content. If this is done well and with good taste, it really looks like magic. However, for pretty much all of the previous techniques, there are always some mysterious styles that result in failure cases. And the reason for this is the fact that these techniques are trained on a set of style images, and if they face a style that is wildly different from these training images, the results won't be very usable. This new algorithm is also based on neural networks, it doesn't need to be trained on these style images, but it can perform high-quality style transfer, and it works on arbitrary styles. This sounds a bit like black magic. So how does this happen exactly? First, an autoencoder is trained for image reconstruction. An autoencoder is a neural network where the input and output image is supposed to be the same thing. So far, this doesn't make any sense, because all the neural network does is copy and pasting the inputs to the outputs. Not very useful. However, if we reduce the number of neurons in one of the middle layers to very very few neurons compared to the others, we get a bottleneck. This bottleneck essentially hamstrings the neural network, and forces it to first come up with a highly compressed representation of an image, this is the encoder network, and then reconstruct the full image from this compressed representation. This is called the decoder network. So encoding is compression, decoding is decompression or more intuitively, reconstruction. This compressed representation can be thought of as the essence of the image which is a very concise representation, but carefully crafted such that a full reconstruction of the image can take place based on it. Autoencoders are previous work, and if you would like to hear more about them, check the video description as we have dedicated an earlier episode to it. And now, the value proposition of this work comes from the fact that, we don't just use the autoencoder as-is, but rip this network in half, and use the encoder part on both the input style and content images. This way, the concept of style transfer is much much simpler in this compressed representation. In the end, we're not stuck with this compressed result, because if you remember, we also have a decoder, which is the second part of the neural network that performs a reconstruction of an image from this compressed essence. As a result, we don't have to train this neural network on the style images, and it will work with any chosen style. Hell yeah! With most style transfer techniques, we are given an output image and we either take it or leave it because and we can't apply any meaningful edits to it. A cool corollary of this design decision is that we can also get closer to our artistic vision by fiddling with parameters. For instance, the scale and weight of the style transfer can be changed on the fly to our liking. As always, the new technique is compared to a bunch of other competing algorithms. Due to the general and lightweight nature of this method, it seems to perform more consistently across a set of widely varying input styles. We can also create some mattes for our target image and apply different artistic styles to different parts of it. Local parts of a style can also be transferred. Remember, the first style transfer technique was amazing, but very limited and took an hour on a state of the art graphics card in a desktop computer. This one takes less than a second and works for any style. Now, as and more new phones contain chips for performing deep learning, we can likely look forward to a totally amazing future where style transfer can be done in our pockets and in real time. What a time it is to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
61,This Neural Network Optimizes Itself,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As we know from the series, neural network-based techniques are extraordinarily successful in defeating problems that were considered to be absolutely impossible as little as ten years ago. When we'd like to use them for something, choosing the right kind of neural network is one part of the task, but usually the even bigger problem is choosing the right architecture. Architecture typically, at a bare minimum, means the type and number of layers in the network, and the number of neurons to be used in each layer. Bigger networks can learn solutions for more complex problems. So it seems that the answer is quite easy: just throw the biggest possible neural network we can at the problem and hope for the best. But if you think that it is that easy or trivial, you need to think again. Here's why. Bigger networks come at a cost: they take longer to train, and even worse, if we have a networks that are too big, we bump into the problem of overfitting. Overfitting is the phenomenon when a learning algorithm starts essentially memorizing the training data without actually doing the learning. As a result, its knowledge is not going to generalize for unseen data at all. Imagine a student in a school who has a tremendous aptitude in memorizing everything from the textbook. If the exam questions happen to be the same, this student will do extremely well, but in the case of even the slightest deviations, well, too bad. Even though people like to call this rote learning, there is nothing about the whole process that resembles any kind of learning at all. A smaller neural network, a less knowledgeable student, who has done their homework properly would do way, way better. So this is overfitting, the bane of so many modern learning algorithms. It can be kind of defeated by using techniques like L1 and L2 regularization or dropout, these often help, but none of them are silver bullets. If you would like to hear more about these, we've covered them in an earlier episode, actually, two episodes, as always, the links are in the video description for the more curious Fellow Scholars out there. So, the algorithm itself is learning, but for some reason, we have to design their architecture by hand. As we discussed, some architectures, like some students, of course, significantly outperform other ones and we are left to perform a lengthy trial end error to find the best ones by hand. So, speaking about learning algorithms, why don't we make them learn their own architectures? And, this new algorithm is about architecture search that does exactly that. I'll note that this is by far not the first crack at this problem, but it definitely is a remarkable improvement over the state of the art. It represents the neural network architecture as an organism and makes it evolve via genetic programming. This is just as cool as you would think it is and not half as complex as you may imagine at first we have an earlier episode on genetic algorithms, I wrote some source code as well which is available free of charge, for everyone, make sure to have a look at the video description for more on that, you'll love it! In this chart, you can see the number of evolution steps on the horizontal x axis, and the performance of these evolved architectures over time on the vertical y axis. Finally, after taking about 1.5 days to perform these few thousand evolutionary steps, the best architectures found by this algorithm are only slightly inferior to the best existing neural networks for many classical datasets, which is bloody amazing. Please refer to the paper for details and comparisons against state of the art neural networks and other architecture search approaches, there are lots of very easily readable results reported there. Note that this is still a preliminary work and uses hundreds of graphics cards in the process. However, if you remember how it went with AlphaGo, the computational costs were cut down by a factor of ten within a little more than a year. And until that happens, we have learning algorithms that learn to optimize themselves. This sounds like science fiction. How cool is that? Thanks for watching and for your generous support, and I'll see you next time!"
62,How Do Neural Networks See The World? Pt 2.,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This one is going to be a treat. As you know all too well after watching at least a few episodes of this series, neural networks offer us amazingly powerful tools to defeat problems that we didn't stand a chance against for a long long time. We are now in the golden age of AI and no business or field of science is going to remain unaffected by this revolution. However, this approach comes with its own disadvantage compared to previous handcrafted algorithms, it is harder to know what is really happening under the hood. That's also kind of the advantage of neural networks, because they can deal with complexities that we, humans are not built to comprehend. But still, it is always nice to peek within a neural network and see if it is trying to learn the correct concepts that are relevant to our application. Maybe later we'll be able to take a look into a neural network, learn what it is trying to do, simplify it and create a more reliable, handcrafted algorithm that mimics it. What's even more, maybe they will be able to write this piece of code by themselves. So clearly, there is lots of value to be had from the visualizations, however, this topic is way more complex than one would think at first. Earlier, we talked about a technique that we called activation maximization, which was about trying to find an input that makes a given neuron as excited as possible. Here you can see what several individual neurons have learned, when I trained them to recognize wooden patterns. In this first layer, it is looking for colors, then in the second layer, some basic patterns emerge. As we look into the third layer, we see that it starts to recognize horizontal, vertical and diagonal patterns, and in the fourth and fifth layers, it uses combinations of the previously seen features, and as you can see, beautiful, somewhat symmetric figures emerge. If you would like to see more on this, I put a link to a previous episode in the video description. Then, a followup work came for multifaceted neuron visualizations, that unveiled even more beautiful and relevant visualizations, a good example was showing which neuron is responsible for recognizing groceries. A new Distill article on this topic has recently appeared by Christopher Olah and his colleagues at Google. Distill is a journal that is about publishing clear explanations to common interesting phenomena in machine learning research. All their articles so far are beyond amazing, so make sure to have a look at this new journal as a whole, as always, the link is available in the video description. They usually include some web demos that you can also play with, I'll show you one in a moment. This article gives a nice rundown of recent works in optimization-based feature visualization. The optimization part can take place in a number different ways, but it generally means that we start out with a noisy image, and look to change this image to maximize the activation of a particular neuron. This means that we slowly morph this piece of noise into an image that provides us information on what the network has learned. It is indeed a powerful way to perform visualization, often more informative than just choosing the most exciting images for a neuron from the training database. It unveils exactly the information the neuron is looking for, not something that only correlates with that information. There is more about not only visualizing the neurons in isolation, but getting a more detailed understanding of the interactions between these neurons. After all, a neural network produces an output as a combination of these neuron activations, so we might as well try to get a detailed look at how they interact. Different regularization techniques to guide the visualization process towards more informative results are also discussed. You can also play with some of these web demos, for instance, this one shows the neuron activations with respect to the learning rates. There is so much more in the article, I urge you to read the whole thing, it doesn't take that long and it is a wondrous adventure into the imagination of neural networks. How cool is that? If you have enjoyed this episode, you can pick up some really cool perks on Patreon, like early access, voting on the order of the next few episodes, or getting your name in the video description as a key contributor. This also helps us make better videos in the future and we also use part of these funds to empower research projects and conferences. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
63,Meta Learning Shared Hierarchies,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is a technique where we have a virtual creature that tries to learn an optimal set of actions to maximize a reward in a changing environment. Playing video games, helicopter control, and even optimizing light transport simulations are among the more awesome example use cases for it. But if we train a reinforcement learner from scratch, we'll see that it typically starts out with a brute force search in the space of the simplest, lowest level actions. This not only leads to crazy behavior early on, but is also highly ineffective, requires way more experience than humans do, and, the obtained knowledge cannot be reused for similar tasks. It can learn the game it was trained on, often even on a superhuman level, but if we need it to function in a new environment, all this previous knowledge has to be thrown away. And this algorithm is very much like how humans learn it breaks down a big and complex task into sequences of smaller actions. These are called sub-policies and can be shared between tasks. Learning to walk and crawl are excellent examples of that and will likely be reused for a variety of different problems and will lead to rapid learning on new, unseen tasks, even if they differ significantly from the previously seen problems. Not only that, but the search space over sub-policies can easily be a hundred or more times smaller than the original search space of all possible actions, therefore this kind of search is way more efficient than previous techniques. Of course, creating a good selection of sub-policies is challenging, because they have to be robust enough to be helpful on many possible tasks, but not too specific to one problem, otherwise they lose their utility. A few episodes ago, we mentioned a related technique by the name Neural Task Programming, and it seems that this one is capable of generalization not only over different variations of the same task, but across different tasks as well. These ants were trained to traverse several different mazes one after another and quickly realized that the basic movement directions should be retained. Creating more general learning algorithms is one of the holy grail problems of AI research, and this one seems to be a proper, proper step towards defeating it. We're not there yet, but it's hard not to be optimistic with this incredible rate of progress each year. Really excited to see how this area improves over the next few months. The source code of this project is also available. Oh, and before we go, make sure to check out the channel of Robert Miles who makes excellent videos about AI, and I'd recommend starting with one of his videos that you are objectively guaranteed to enjoy. If you wish to find out why, you'll see the link in the video description or just click the cat picture appearing here on the screen in a moment. If you indeed enjoyed it, make sure to subscribe to his channel. Thanks for watching and for your generous support, and I'll see you next time!"
64,Image Matting With Deep Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Image matting is the process of taking an input image, and separating its foreground from the background. It is an important preliminary step for creating visual effects where we cut an actor out from green-screen footage and change the background to something else, and image matting is also an important part of these new awesome portrait mode selfies where the background looks blurry and out of focus for a neat artistic effect. To perform this properly, we need to know how to separate the foreground from the background. Matting human hair and telling accurately which hair strand is the foreground and which is the background is one of the more difficult parts of this problem. This is also the reason for many of the failure cases of the portrait mode photos made with new iPhone and Pixel cameras. The input of this problem formulation is a colored image or video, and the output is an alpha matte where white and lighter colors encode the foreground, and darker colors are assigned to the background. After this step, it is easy to separate and cut out the different layers and selectively replace some of them. Traditional techniques rely on useful heuristics, like assuming that the foreground and the background are dominated by different colors. This is useful, but of course, it's not always true, and clearly, we would get the best results if we had a human artist creating these alpha mattes. Of course, this is usually prohibitively expensive for real-world use and costs a ton of time and money. The main reason why humans are successful at this is that they have an understanding of the objects in the scene. So perhaps, we could come up with a neural network-based learning solution that could replicate this ideal case. The first part of this algorithm is a deep neural network that takes images as an input and outputs an alpha matte, which was trained on close to 50 thousand input-output pairs. So here comes the second, refinement stage where we take the output matte from the first step and use a more shallow neural network to further refine the edges and sharper details. There are a ton of comparisons in the paper, and we are going to have a look at some of them, and as you can see, it works remarkably well for difficult situations where many tiny hair strands are to be matted properly. If you look closely here, you can also see the minute differences between the results of the raw and refined steps, and it is shown that the refined version is more similar to the ground truth solution and is abbreviated with GT here. By the way, creating a dataset with tons of ground truth data is also a huge endeavor in and of itself, so thank you very much for the folks at alphamatting.com for creating this dataset, and you can see how important this kind of work is to make it easier to compare state of the art research works more easily. Adobe was a part of this research project so if everything goes well, we can soon expect such a feature to appear in their products. Also, if you're interested, we also have some nice Two Minute Papers shirts for your enjoyment. If you are located in the US, check twominutepapers.com, and for worldwide shipping, check the video description for the links. All photos of you wearing them are appreciated. Plus scholarly points if it depicts you reading a paper! Thanks for watching and for your generous support, and I'll see you next time!"
65,Terrain Generation With Deep Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have recently witnessed the emergence of neural network-based techniques that are able to synthesize all sorts of images. Our previous episode was about NVIDIA's algorithm that created high resolution images of imaginary celebrities that was a really cool application of Generative Adversarial Networks. This architecture means that we have a pair of neural networks, one that learns to generate new images, and the other learns to tell a fake image from a real one. As they compete against each other, they get better and better without any human interaction. So we can clearly use them to create 2D images, but why stop there? Why not use this technique, for instance, to create assets for digital media? So instead of 2D images, let's try to adapt these networks to generate high-resolution 3D models of terrains that we can use to populate a virtual world. Both computer games and the motion picture industry could benefit greatly from such a tool. This process is typically done via procedural generation, which is basically a sort of guided random terrain generation. Here, we can have a more direct effect on the output without putting in tens of hours of work to get the job done. In the first training step, this technique learns how an image of a terrain corresponds to input drawings. Then, we will be able to sketch a draft of a landscape with rivers, ridges, valleys and the algorithm will output a high quality model of the terrain itself. During this process, we can have a look at the current output and refine our drawings in the meantime, leading to a super efficient process where we can go from a thought to a high-quality final result within a few seconds without being bogged down with the technical details. What's more, it can also not only deal with erased subregions, but it can also automatically fill them with sensible information to save time for us. What an outstanding convenience feature! And, the algorithm can also perform physical manipulations like erosion to the final results. After the training for the erosion step is done, the computational cost is practically zero, for instance, running an erosion simulator on this piece of data would take around 40 seconds, where the neural network can do it in 25 milliseconds. The full simulation would almost be a minute, where the network can mimic its results practically instantaneously. A limitation of this technique is that if the input is too sparse, unpleasant grid artifacts may appear. There are tons of more cool features in the paper, make sure to have a look, as always, it is available in the video description. This is a really well thought out and well-presented work that I expect to be a true powerhouse for terrain authoring in the future. And, in the meantime, we have reached a hundred thousand subscribers. A hundred thousand Fellow Scholars. Wow. This is absolutely amazing and honestly I never thought that this would ever happen. So, happy paperversary, thank you very much for coming along on this journey of science and I am very happy to see that the series brings joy and learning to more people than ever. Thanks for watching and for your generous support, and I'll see you next time!"
66,NVIDIA's AI Dreams Up Imaginary Celebrities!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Hold on to your papers because these results are completely out of this world, you'll soon see why. In this work, high-resolution images of imaginary celebrities are generated via a generative adversarial network. This is an architecture where two neural networks battle each other: the generator network is the artist who tries to create convincing, real-looking images and the discriminator network, the critic tries to tell a fake image from a real one. The artist learns from the feedback of the critic and will improve itself to come up with better quality images, and in the meantime, the critic also develops a sharp eye for fake images. These two adversaries push each other until they are both adept at their tasks. A classical drawback of this architecture is that it is typically extremely slow to train and these networks are often quite shallow, which means that we get low-resolution images that are devoid of sharp details. However, as you can see here, these are high resolution images with tons of details. So, how is that possible? So here comes the solution from scientists at NVIDIA. Initially, they start out with tiny, shallow neural networks for both the artist and the critic, and as time goes by, both of these neural networks are progressively grown. They get deeper and deeper over time. This way, the training process is more stable than using deeper neural networks from scratch. It not only generates pictures, but it can also compute high resolution intermediate images via latent space interpolation. It can also learn object categories from a bunch of training data and generate new samples. And, if you take a look at the roster of scientists on this project, you will see that they are computer graphics researchers who recently set foot in the world of machine learning. And man, do they know their stuff and how to present a piece of work! And now comes something, that is the absolute most important part of the evaluation that should be a must for every single paper in this area. These neural networks were trained on a bunch of images of celebrities, and are now generating new ones. However, if all we are shown is a new image, we don't know how close it is to the closest image in the training set. If the network is severely overfitting, it would essentially copy/paste samples from there. Like a student in class who hasn't learned a single thing, just memorized the textbook. Actually, what is even worse is that this would mean that the worst learning algorithm that hasn't learned anything but memorized the whole database would look the best! That's not useful knowledge. And here you see the nearest neighbors, the images that are the closest in this database to the newly synthesized images. It shows really well that the AI has learned the concept of a human face extremely well and can synthesize convincing looking new images that are not just copy-pasted from the training set. The source code, pre-trained network and one hour of imaginary celebrities are also available in the description, check them out! Premium quality service right there. And, if you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. You can also get really cool additional perks like early access, and it helps us to make better videos, grow, and tell these incredible stories to a larger audience. Details are available in the description. Thanks for watching and for your generous support, and I'll see you next time!"
67,Generalizing AI With Neural Task Programming,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. One of the holy grail problems of machine learning research is to achieve artificial general intelligence, AGI in short. Deep Blue was able to defeat the genius Kasparov in Chess, but it was unable to tell us what the time was. Algorithms of this type we often refer to as a weak AI, or narrow AI, a technique that excels, or is maybe even on a superhuman level at a task, but has zero or no knowledge about anything else. A key to extend these algorithms would be to design them in a way that their knowledge generalizes well to other problems. This is what we call transfer learning, and this collaboration between the Stanford AI lab and Caltech goes by the name Neural Task Programming, and tries to tackle this problem. A solution to practically any problem we're trying to solve can be written as series of tasks. These are typically complex actions, like cleaning a table, or performing a backflip that are difficult to transfer to a different problem. This technique is a bit like divide and conquer type algorithms that aggressively try to decompose big, difficult tasks into smaller, more manageable pieces. The smaller and easier to understand the pieces are, the more reusable they are and the better they generalize. Let's have a look at an example. For instance, in a problem where we need to pick and place objects, this series of tasks can be decomposed into picking and placing. These can be further diced into a series of even smaller tasks, such as gripping, moving, and releasing actions. However, if the learning takes place like this, we can now specify different variations of these tasks, and the algorithm will quickly understand how to adapt the structure of these small tasks to efficiently to solve new problems. The new algorithm generalizes really well for tasks with different lengths, topologies, and changing objectives. If you take a look at the paper, you'll also find some more information on adversarial dynamics, which lists some problem variants where a really unpleasant adversary pushes things around on the table from time to time to mess with the program, and there are some results that show that the algorithm is able to recover from these failure states quite well. Really cool. Now, please don't take this as a complete solution for AGI, because it is a fantastic piece of work, but it's definitely not that. However, it may be a valuable puzzle piece to build towards the final solution. This is research. We advance one step at a time. Man, what an amazing time to be alive. Thanks for watching and for your generous support, and I'll see you next time!"
68,AI Competitive Self-Play,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Earlier we had an episode about OpenAI's absolutely amazing algorithm that mastered DOTA 2, a competitive online battle arena game, and managed to beat some of the best players in the world in a relatively limited 1 versus 1 game mode. While the full 5 versus 5 version of this learning algorithm is still in the works, scientists at OpenAI experimented with some self-play in other games and have found some remarkable results. You can see here that most of these amusing experiments take place in a made up 3D game with simulated physics. For instance, performing well with these humanoid creatures means controlling 17 actuated joints properly. These agents use a reinforcement learning algorithm to maximize a reward, for instance, a sumo warrior gets a thousand points for pushing their opponent out of the ring. The first interesting thing is that a learning curriculum was used, which means that the algorithm was allowed to explore on their own by relaxing the strict scores that are given only when winning. This is combined with the fact that these agents play against themselves led to some remarkable emergent behaviors. Here you can see with the score how much of a difference this curriculum makes, and you also see that whenever a plot is symmetric that means that they are zero-sum games, so if one agent wins a given number of points, the other loses the same amount. The self-play part is also particularly interesting as many agents are being trained in parallel at the same time, and if we're talking about one versus one games, we have to create some useful logic to decide who to pair with whom. It seems that training against an older version of a previously challenged opponent was the best strategy. This makes sense because they are running a similar algorithm, and for self-play, this means that the algorithm is asked to defeat an older version of itself. If it can reliably do that, it will lead to a smooth and predictable learning process. It is kinda incredible to think about the fact that we have a virtual world with a bunch of simulated learning creatures and we are omnipotent beings trying to craft the optimal learning experience for them. The perks of being a researcher in machine learning. And we are even being paid for this! Isn't this incredible? Psst, don't tell anyone about this. There are so many interesting results here and so much to talk about. For instance, we haven't even talked about transfer learning, where these creatures learn to generalize their knowledge learned from previous tasks to tackle new challenges more efficiently. Make sure to have a look at the paper, and the source code is available for everyone free of charge. If you are one of those fellow tinkerers, you'll be more than happy to look into the video description. If you wish to hear more about transfer learning, subscribe and turn on notifications because the next episode is going to about some really cool results in this area. Thanks for watching and for your generous support, and I'll see you next time!"
69,Disney's AI Learns To Render Clouds,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a fully in-house Disney paper on how to teach a neural network to capture the appearance of clouds. This topic is one of my absolute favorites because it is in the intersection of the two topics I love most computer graphics and machine learning. Hell yeah! Generally, we use light simulation programs to render these clouds, and the difficult part of this is that we have to perform something that is called volumetric path tracing. This is a technique where we have to simulate rays of light that do not necessarily bounce off of the surface of objects, but may penetrate their surfaces and undergo many scattering events. Understandably, in the case of clouds, capturing volumetric scattering properly is a key element in modeling their physical appearance. However, we have to simulate millions and millions of light paths with potentially hundreds of scattering events, which is a computationally demanding task even in the age of rapidly improving hardware. As you can see here, the more we bump up the number of possible simulated scattering events, the closer we get to reality, but the longer it takes to render an image. In the case of bright clouds here, rendering an image like this can take up to 30 hours. In this work, a nice hybrid approach is proposed where a neural network learns the concept of in-scattered radiance and predicts it rapidly so this part we don't have to compute ourselves. It is a hybrid because some parts of the renderer are still using the traditional algorithms. The dataset used for training the neural network contains 75 different clouds, some of which are procedurally generated by a computer, and some are drawn by artists to expose the learning algorithm to a large variety of cases. As a result, these images can be rendered in a matter of seconds to minutes. Normally, this would take many-many hours on a powerful computer. Here's another result with traditional path tracing. And now the same with deep scattering. Yep, that's how long it takes. The scattering parameters can also be interactively edited without us having to wait for hours to see if the new settings are better than the previous ones. Dialing in the perfect results typically takes an extremely lengthy trial and error phase which now can be done almost instantaneously. The technique also supports a variety of different scattering models. As with all results, they have to be compared to the ground truth renderings, and as you can see here, they seem mostly indistinguishable from reality. It is also temporally stable, so animation rendering can take place flicker-free as is demonstrated here in the video. I think this work is also a great testament to show how these incredible learning algorithms can accelerate progress in practically all fields of science. And given that this work was done by Disney, I am pretty sure we can expect tons of photorealistic clouds in their upcoming movies in the near future. There are tons of more details discussed in the paper, which is remarkably well produced, make sure to have a look, the link is in the video description. This is a proper, proper paper, you don't want to miss out on this one. And, if you enjoyed this episode and you feel that the series provides you value in the form of enjoyment or learning, please consider supporting us on Patreon. You can pick up cool perks there like deciding the order of the next few episodes, and you also help us make better videos in the future. Details are available in the description. Thanks for watching and for your generous support, and I'll see you next time!"
70,Video Game Graphics To Reality And Back,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Researchers at NVIDIA have been hitting it out of the park lately, and this work is no different. This technique performs image translation, which means that the input is an image of our choice, and the output is the same image with somewhat different semantics. For instance, an image of a city can be translated to a map of this city, or a daytime photo or video can be converted to appear as if it were shot during the night. And throughout the video, you'll see so much more of these exciting applications. A typical way of accomplishing this is done by using Generative Adversarial Networks. This is a pair of neural networks where the generator network creates new synthetic images trying to fool a discriminator network which learns to tell a fake, synthesized image from a real one. These two neural networks learn together, where one tries to come up with better solutions to fool the discriminator, where the discriminator seeks to get better at telling forgeries from the real photographs. In the end, this rivalry makes both of them get better and better and the final result is an excellent technique to create convincing image translations. In this work, not two, but 6 of these networks are being used, so make sure to have a look at the paper for details. There was an earlier work that was able to perform image translation by leaning on a novel cycle consistency constraint. This means that we assume that the source image can be translated to the target image and then this target image can be translated back to look exactly like the source. This kinda means that these translations are not arbitrary and are mathematically meaningful operations. Here, the new technique builds on a novel assumption that there exists a latent space in which the input and output images can both coexist. This latent space is basically an intuitive and concise representation of some more complicated data. For instance, earlier, we experimented a bit with fonts and had seen that even though the theory of font design is not easy, we can create a 2 dimensional latent space that encodes simple properties like curvature that can describe many many fonts in an intuitive manner. Remarkably, with this new work, converting dogs and cats into different breeds is also a possibility. Interestingly, it can also perform real to synthetic image translation and vice versa. So that means that it can create video game footage from our real world videos, and even more remarkably, convert video game footage to real world video. This is insanity, one of the craziest ideas I've seen in a while. Bravo. And now, hold on to your papers because it can also perform attribute-based image translation. This means that for instance, we can grab an image of a human face and transform the model's hair to blonde, add sunglasses or smiles to it at will. A limitation of this technique is that training is still non-trivial as it still relies on generative adversarial networks, and it is not yet clear whether there is a point to which the training converges or not. The source code of this project is also available. Make sure to take a good look at the license before doing anything because it is under the creative commons non-commercial and no derivatives license. Thanks for watching and for your generous support, and I'll see you next time!"
71,Transferring AI To The Real World (OpenAI),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we talk a lot about different AI algorithms that solve a variety of super difficult tasks. These are typically tested within a software environment in the form of a simulation program. However, this often leaves the question open whether these algorithms would really work in real world environments. So what about that? This work from OpenAI goes by the name domain randomization, and is about training an AI on relatively crude computer simulations in a way that can be transferred to the real world. The problem used to demonstrate this was localizing and grasping objects. Note that this algorithm has never seen any real images and was trained using simulated data. It only played a computer game, if you will. Now, the question we immediately think about is what the term domain randomization has to do with transferring simulation knowledge into reality? The key observation is that using simulated training data is okay, but we have to make sure that the AI is exposed to a diverse enough set of circumstances to obtain knowledge that generalizes properly, hence the term domain randomization. In these experiments, the following parameters were heavily randomized: number of shapes and distractor objects on the table, positions and textures on the objects, table and the environment, number of lights, material properties, and the algorithm was even exposed to some random noise as well in the images. And it turns out that if we do this properly, leaning on the knowledge of only a few thousand images, when the algorithm is uploaded to a real robot arm, it becomes capable of grasping the correct prescribed objects. In this case, the objective was, spam detection. Very amusing. I think the very interesting part is that it is not even using photorealistic rendering and light simulations these programs are able to create high quality images that resemble the real world around us, and it is mostly clear that those would be useful to train such an algorithm. However, this only uses extremely crude data and the knowledge of the AI still generalizes to the real world. How about that! Thanks for watching and for your generous support, and I'll see you next time!"
72,New DeepMind AI Beats AlphaGo 100-0,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Hold on to your papers, because this work on AlphaGo is absolute insanity. In the game of Go, the players put stones on a table where the objective is to surround more territory than the opponent. This is a beautiful game that is particularly interesting for AI research, because the space of possible moves is vastly larger than in chess, which means that using any sort of exhaustive search is out of question and we have to resort to smart algorithms that are able to identify a small number of strong moves within this stupendously large search space. The first incarnation of DeepMind's Go AI, AlphaGo uses a combination of a policy network that is responsible for predicting the moves, and a value network that predicts the winner of the game after it plays it to the end against itself. These are both deep neural networks and they are then combined with a technique called Monte Carlo Tree Search to be able to narrow down the search in this this large search space. This algorithm started out with a bootstrapping process where it was shown thousands of games that were used to learn the basics of Go. Based on this, it is clear that such an algorithm can learn to be as good as formidable human players. But the big question was, how could it possibly become even better than the professionals that it has observed? How could the disciple become better than its master? The solution is that after it has learned what it can from these games, it plays against itself many-many times to improve its skills. This second phase is the main part of the training that takes the most time. Let's call this base algorithm AlphaGo Fan, which was used to play against Fan Hui a 2-dan European Go champion, who was defeated 5 to 0. This was a historic moment and the first time an AI beat a professional Go player without a handicap. Fan Hui described his experience as playing against a very strong and stable player and he also mentioned that the algorithm felt very human-like. Some voiced their doubts within the Go community and noted that the algorithm would never be able to beat Lee Sedol, a 9-dan world champion, and winner of 18 international titles. Just to give you an intuition of the difference, based on their Elo points, Lee Sedol is expected to beat Fan Hui 97 times out of 100 games. So a few months later, DeepMind organized a huge media event where they would challenge him to play against AlphaGo. This was a slightly modified version of the base algorithm that used a deeper neural network with more layers and was trained using more resources than the previous version. There was also an algorithmic change to the policy networks, the details on this are available in the paper in the description, it is a great read, make sure to have a look. Let's call this algorithm AlphaGo Lee. This event was watched all around the world and can perhaps be compared to Kasparov's public chess games against Deep Blue. I have the fondest memories of waking up super early in the morning, jumping out of the bed in excitement to watch all these Go matches. And in a long and nailbiting series, Lee Sedol was defeated 4 to 1 by the AI. With significantly less media attention, the next phase came bearing the name AlphaGo Master, which used around ten times less tensor processing units than the AlphaGo Lee and became an even stronger player. This algorithm played against human professionals online in January 2017 and won all 60 matches it had played. This is insanity, but if you think that's it, well, hold on to your papers now. In this newest work, AlphaGo has reached its next form, AlphaGo Zero. This variant does not have access to any human played games in the first phase and learns completely through self-play. It starts out from absolutely nothing, with just the knowledge of the rules of the game. It was trained for 40 days, and by day 3, it reached the level of AlphaGo Lee, this is above World champion level. Around day 21, it hits the level of AlphaGo Master, which is practically unbeatable to all human beings. And get this, at 40 days, this version surpasses all previous AlphaGo versions and defeats the previously published worldbeater version 100-0. This has kept me up for several nights now and I am completely out of words. In this version, the two neural networks are fused into one, which can be trained more efficiently. It is beautiful to see these curves as they show this neural network starting from a random initialization. It knows the rules, but beyond that, it is completely clueless about the game itself, and it rapidly becomes practically unbeatable. And I left the best part for last it uses only one single machine. I think it is fair to say that is history unfolding before our eyes. What a time to be alive! Congratulations to the DeepMind team for this remarkable achievement. And, for me, I love talking about research to a wider audience and it is a true privilege to be able to tell these stories to you. Thank you very much for your generous support on Patreon and making me able to spend more and more time with what I love most. Absolutely amazing. And now, I know it's a bit redundant, but from muscle memory, I'll sign out the usual way. Thanks for watching and for your generous support, and I'll see you next time!"
73,Real-Time Global Illumination With Radiance Probes,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is our 200th episode, so I know you're expecting something great. See how you like this one! One of the most sought after effect in light transport simulations is capturing indirect illumination. This is a beautiful effect where the color of multiple diffuse, matte surfaces bleed onto each other. And of course, computing such an effect is as costly as it is beautiful because it requires following the path of millions and millions of light rays. This usually means several hours of waiting time. There have been countless research papers written on how to do this in real time, but the limitations were often much too crippling for practical use. But this time around, you will see soon that these results are just outstanding, and we will have a word on limitations at the end of this video. The key contribution of this work is that instead of computing the light transport between all possible point pairs in the scene, it uses radiance probes that measure the nearby illumination, and tries to reconstruct the missing information from this sparse set of radiance probes. After that, we place a bunch of receiver points around the scene to places where we would like to know how the indirect illumination looks. There are several things to be taken care of in the implementation of this idea. For instance, in previous works, the hierarchy of these sender and receiver points was typically fixed. In this new work, it is shown that a much sparser set of carefully placed radiance probes is sufficient to create high-quality reconstructions. This seemingly small difference also gives rise to a lot of ambiguous cases that the researchers needed to work out how to deal with. For instance, possible occlusions between the probes and receiver points need special care. The entire algorithm is explained in a remarkably intuitive way in the paper, make sure to have a look at that. And, given that we can create images by performing much less computation, with this technique, we can perform real-time light simulations. As you can see, 3.9 milliseconds is a typical value for computing an entire image, which means that this can be done with over 250 frames per second. That's not only real time, that's several times real time, if you will. Outstanding! And of course, now that we know that this technique is fast, the next question is: how accurate is it? As expected, the outputs are always compared to the reference footage so we can see how accurate the proposed technique is. Clearly, there are differences, however, probably many of us would fail to notice that we're not looking at the reference footage, especially if we don't have access to it, which is the case in most applications. And note that normally, we would have to wait for hours for results like this. Isn't this incredible? There are also tons of more comparisons in the paper, for instance, it is also shown how the density of radiance probes relates to the output quality and where the possible sweet spots are for industry practitioners. It is also tested against many competing solutions. Not only the results, but the number and quality of comparisons is also top tier in this paper. However, like with all research works, no new idea comes without limitations. This method works extremely well for static scenes where not a lot of objects move around. Some movement is still fine as it is shown in the video here, but drastic changes to the structure of the scene, like a large opening door that remains unaccounted for by the probes will lead to dips in the quality of the reconstruction. I think this is an excellent direction for future research works. If you enjoyed this episode, make sure to subscribe and click the bell icon, we have some more amazing papers coming up, you don't want to miss that. Thanks for watching and for your generous support, and I'll see you next time!"
74,Learning to Model Other Minds (OpenAI),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work doesn't have a ton of viewable footage, but I think it is an absolutely amazing piece of craftsmanship, so in the first half of this video, we'll roll some footage from earlier episodes, and in the second half, you'll see the new stuff. In this series, we often talk about reinforcement learning, which is a learning technique where an agent chooses an optimal series of actions in an environment to maximize a score. Playing computer games is a good example of a clearly defined score that is to be maximized. As long as we can say that the higher the score, the better the learning, the concept will work for helicopter control, choosing the best spot for wifi connectivity or a large variety of different tasks. However, what about environments where multiple agents or players are present? Not all games are single player focused, and not all helicopters have to fly alone. So what about that? To deal with cases like this, scientists at OpenAI and the University of Oxford came up with a work by the name ""Learning with Opponent-Learning Awareness"", LOLA in short, or lola. I have to say that the naming game at OpenAI has been quite strong lately. This is about multiplayer reinforcement learning, if you will. This new agent does not only care about maximizing its own score but also inserts a new term into the equation which is about anticipating the actions of other players in the environment. It is not only possible to do this, but they also show that it can be done in an effective way, and, the best part is that it also gives rise to classical strategies that game theory practitioners will immediately recognize. For instance, it can learn tit for tat, which is a strategy that mirrors the other player's actions. This means that if the other player is cooperative, it will remain cooperative, but if it gets screwed over, it will also try to screw others over. You'll see in a moment why this is a big deal. The prisoner's dilemma is a game where two criminals are caught and are independently interrogated, and have to choose whether they snitch on the other one or not. If any one snitches out, there will be hell to pay for the other one. If they both defect, they both serve a fair amount time in prison. The score to be minimized is therefore this time spent in prison. The optimal solution of this game is when both criminals remain silent, and this strategy is something that we call the Nash equilibrium. In other words, this is the best set of actions if we consider the options of the other actor as well and expect that they do the same for us. And now, the first cool result is that if we run the prisoner's dilemma with two of these new LOLA agents, they quickly find the Nash equilibrium. This is great. But wait, we have talked about this tit for tat thing, so what's the big deal with that? There is an iterated version of the prisoner's dilemma game, where this snitching or cooperating game is replayed many many times. It is an ideal benchmark because an advanced agent would know that we cooperated the last time, so it is likely that we can partner up this time around too! And now comes the even cooler thing! This is where the tit for tat strategy emerges these LOLA agents know that if the previous time, they cooperated, they will immediately give each other another chance, and again, get away with the least amount of prison time. As you can see here, the results vastly outperform other naive agents, and from the scores it seems that previous techniques enter a snitching revenge war against each other and both will serve plenty of time in prison. Other games are also benchmarked against naive, uncooperative agents, vastly outperforming them. This is a fantastic paper, make sure to check it out in the video description for more details. I found it to be very readable, so do not despair if your math kung fu is not that strong. Just dive into it! Videos like this tend to get less views because they have less visual fireworks than most other works we're discussing in the series. Fortunately, we are super lucky because we have your support on Patreon and can tell these important stories without worrying about going viral. And, if you have enjoyed this episode and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. One buck is almost nothing, but it keeps the papers coming. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
75,AI Learns 3D Face Reconstruction,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Now that facial recognition is becoming more and more of a hot topic, let's talk a bit about 3D face reconstruction! This is a problem where we have a 2D input photograph, or a video of a person, and the goal is to create a piece of 3D geometry from it. To accomplish this, previous works often required a combination of proper alignment of the face, multiple photographs and dense correspondences, which is a fancy name for additional data that identifies the same regions across these photographs. But this new formulation is the holy grail of all possible versions of this problem, because it requires nothing else but one 2D photograph. The weapon of choice for this work was a Convolutional Neural Network, and the dataset the algorithm was trained on couldn't be simpler: it was given a large database of 2D input image and 3D output geometry pairs. This means that the neural network can look at a lot of these pairs and learn how these input photographs are mapped to 3D geometry. And as you can see, the results are absolutely insane, especially given the fact that it works for arbitrary face positions and many different expressions, and even with occlusions. However, this is not your classical Convolutional Neural Network, because as we mentioned, the input is 2D and the output is 3D. So the question immediately arises: what kind of data structure should be used for the output? The authors went for a 3D voxel array, which is essentially a cube in which we build up the face from small, identical Lego pieces. This representation is similar to the terrain in the game Minecraft, only the resolution of these blocks is finer. The process of guessing how these voxel arrays should look based the input photograph is referred to in the research community as volumetric regression. This is what this work is about. And now comes the best part! An online demo is also available where we can either try some prepared images, or, we can also upload our own. So while I run my own experiments, don't leave me out of the good stuff and make sure you post your results in the comments section! The source code is also available for you fellow tinkerers out there. The limitations of this technique includes the inability of detecting expressions that are very far away from the ones seen in the training set, and as you can see in the videos, temporal coherence could also use some help. This means that if we have video input, the reconstruction has some tiny differences in each frame. Maybe a Recurrent Neural Network, like some variant of Long Short Term Memory could address this in the near future. However, those are trickier and more resource-intensive to train properly. Very excited to see how these solutions evolve, and of course, Two Minute Papers is going to be here for you to talk about some amazing upcoming works. Thanks for watching and for your generous support, and I'll see you next time!"
76,AI Learns Video Frame Interpolation,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. With today's graphics technology, we can enjoy many really smooth videos that were created using 60 frames per second. We love it too, and we hope that you noticed that our last hundred or maybe even more episodes have been available in 60hz. However, it oftentimes happens that we're given videos that have anything from 20 to 30 frames per second. This means that if we play them on a 60 fps timeline, half or even more of these frames will not provide any new information. As we try to slow down the videos for some nice slow-motion action, this ratio is even worse, creating an extremely choppy output video. Fortunately, there are techniques that are able to guess what happens in these intermediate frames and give them to us. This is what we call frame interpolation. We have had some previous experiments in this area where we tried to create an amazing slow motion version of a video with some bubbles merging. A simple and standard way of doing frame interpolation is called frame blending, which is a simple averaging of the closest two known frames. The more advanced techniques are optical flow-based, which is a method to determine what motions happened between the two frames, and create new images based on that knowledge, leading to higher quality results in most cases. This technique uses a convolutional neural network to accomplish something similar, but in the end, it doesn't give us an image, but a set of convolution kernels. This is a transformation that is applied to the previous and the next frame to produce the intermediate image. It is not the image itself, but a recipe of how to produce it, if you will. We've had a ton of fun with convolutions earlier, where we used them to create beautiful subsurface scattering effects for translucent materials in real time, and our more loyal Fellow Scholars remember that at some point, I also pulled out my guitar and showed what it would sound like inside a church using a convolution-based reverberation technique. The links are available in the video description, make sure to check them out! Since we have a neural network over here, it goes without saying that the training takes place on a large number of before-after image pairs, so that the network is able to produce these convolution kernels. Of course, to validate this algorithm, we also need to have access to a ground truth reference to compare against we can accomplish this by withholding some information about a few intermediate frames so we have the true images which the algorithm would have to reproduce without seeing it. Kinda like giving a test to a student when we already know the answers. You can see such a comparison here. And now, let's have a look at these results! As you can see, they are extremely smooth, and the technique retains a lot of high-frequency details in these images. The videos also seem temporally coherent, which means that it's devoid of the annoying flickering effect where the reconstruction takes place in a way that's a bit different in each subsequent frame. None of that happens here, which is an excellent property of this technique. The python source code for this technique is available and is free for non-commercial uses. I've put a link in the description, if you have given it a try and have some results of your own, make sure to post them in the comments section or our subreddit discussion. The link is available in the description. Thanks for watching and for your generous support, and I'll see you next time!"
77,Deep Learning From Human Preferences,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this new age of AI, there is no shortage of articles and discussion about AI safety, and of course, rightfully so: these new learning algorithms started solving problems that were previously thought to be impossible in quick succession. Only ten years ago, if we told someone about half of the things that have been covered in the last few Two Minute Papers episodes, we'd have been declared insane. And of course, having such powerful algorithms, we have to make sure that they are used for good. This work is a collaboration between OpenAI and DeepMind's security team and is about introducing more human control in reinforcement learning problems. The goal was to learn to perform a backflip through reinforcement learning. This is an algorithm that tries to perform a series of actions to maximize a score. Kind of like playing computer games. For instance, in Atari Breakout if we break a lot of bricks, we get a high score so we know we did something well. If we see that happening, we keep doing what led to this result, if not, we go back to the drawing board and try something new. But this work is about no ordinary reinforcement learning algorithm, because the score to be maximized comes from a human supervisor and we're trying to teach a digital creature to perform a backflip. I particularly like the choice of the backflip here because we can tell when we see one, but a mathematical specification of this in terms of movement actions is rather challenging. This is a problem formulation in which humans can overlook and control the learning process, which is going to be an increasingly important aspect of learning algorithms in the future. The feedback option is very simple: we just specify whether this sequence of motions achieved our prescribed goal or not. Did it fall or did it perform the backflip successfully. After around 700 human feedbacks, the algorithm was able to learn the concept of a backflip, which is quite remarkable given that these binary yes/no scores are extremely difficult to use for any sort of learning. In an earlier episode, we illustrated a similar case with a careless teacher who refuses to give out points for each problem on a written exam and only announces whether we have failed or passed. This clearly makes a dreadful learning experience, and it is incredible that the algorithm is still able to learn using these. We provide feedback on less than 1% of the actions the algorithm makes, and it can still learn difficult concepts off of these extremely sparse and vague rewards. Low-quality teaching leads to high-quality learing. How about that!? This is significantly more complex than what other techniques were able to learn with human feedback. And, it works with other games too! A word about the collaboration itself. When a company hires a bunch of super smart scientists and a spends a ton of money on research, it is understandable that they want to get an edge through these projects, which often means keeping the results for themselves. This leads to excessive secrecy and a lack of collaboration with other groups as everyone wants to keep their cards close to their chest. The fact that such collaborations can happen between these two AI research giants is a testament to how devoted they are to working together and sharing their findings with everyone, free of charge for the greater good. Awesome. As the media is all up in arms about the demise of the human race I feel that it is important to show the other side of the coin as well. We have top people working on AI safety right now. If you wish to help us tell these stories to more people, please consider supporting us on Patreon. Details are available in the video description, or just click the letter p that appears on the screen in a moment. Thanks for watching and for your generous support, and I'll see you next time!"
78,AI Learns To Recreate Computer Games,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In most video games that we've seen running for at least a few moments, we learn to anticipate what is going to happen in the next second, and even more, if given the patience and skills, we could attempt to recreate parts of the game itself. And what you see here in this work is actually even better, because it requires neither the patience nor the skills to do that. So here is the million dollar idea: let's have a learning algorithm look at some video game footage, and then, ask it to recreate that so we can indulge in playing it. The concept is demonstrated on the Super Mario game and later, you will also see some results with the millenial childhood favorite, Mega Man. There are many previous works that hook into the source code of these games and try to read and predict what happens next by reading the code-level instructions. But not in this case, because this technique looks at the video output and the learning takes place on the level of pixels, therefore, no access to the inner workings of the game is necessary. The algorithm is given two things for the learning process: one, a sprite pallette that contains all the possible elements that can appear in the game, including landscape tiles, enemies, coins and so on. And two, we also provide an input video sequence with one playthrough of the game to demonstrate the mechanics and possible interactions between these game elements. A video is a series of frames, from which the technique learns how a frame can be advanced to the next one. After it has been exposed to enough training samples, it will be able to do this prediction by itself on unknown frames that it hasn't seen before. This pretty much means that we can start playing the game that it tries to mimic. And there are similarities across many games that could be exploited, endowing the learning algorithms with knowledge reused from other games, making them able to recreate even higher quality computer games, even in cases where a given scenario hasn't played out in the training footage. It used to be the privilege of computer graphics researchers to play video games during work hours, but apparently, scientists in machine learning also caught up in this regard. Way to go! A word about limitations. As the predictions are not very speedy and are based off of a set of facts learned from the video sequences, it is a question as to how well this technique would generalize to more complex 3d video games. As almost all research works, this is a stepping stone, but a very important one at that as this is a proof of concept for a really cool idea. You know the drill, a couple papers down the line and we'll see the idea significantly improved. The results are clearly not perfect, but it is a nice demonstration of a new concept, and knowing the rate of progress in machine learning research, you will very soon see some absolutely unreal results. What's even more, I expect that new levels, enemy types and mechanics will soon be synthesized to already existing games via generative adversarial networks, or generative latent optimization. If you would like to hear more about these, as always, the links are available in the video description. Also, if you enjoyed this episode, please make sure to help us tell these incredible stories to more and more people by supporting us on Patreon. Your support has always been absolutely amazing. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
79,Audio To Obama: AI Learns Lip Sync from Audio,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is doing something truly remarkable: if we have a piece of audio of a real person speaking, and a target video footage, it will retime and change the video so that the target person appears to be uttering these words. Whoa! This is different from what we've seen a few episodes ago, where scientists at NVIDIA worked on synthesizing lip sync geometry for digital characters solely relying on audio footage. The results were quite amazing, have  a look. This was great for animating digital characters when all we have is sound. But this time around, we're interested in reanimating the footage or real, existing people. A prerequisite to do this with a learning algorithm is to have a ton of data to train on which we have in our possession as there are many hours of footage of the former president speaking during his weekly address. This is done using a recurrent neural network. Recurrent neural networks are learning algorithms where the inputs and outputs can be sequences of data. So here, in the first part, the input can be a piece of audio with the person saying something, and it is able to synthesize the appropriate mouth shapes and their evolution over time to match the audio. The next step is creating an actual mouth texture from this rough shape that comes from the learning algorithm, which is then used as an input to the synthesizer. Furthermore, the algorithm is also endowed with an additional pose matching module to make sure that the synthesized mouth texture aligns with the posture of the head properly. The final retiming step makes sure that the head motions follow the speech correctly. If you have any doubts whether this is required, here are some results with and without the retiming step. You can see that this indeed substantially enhances the realism of the final footage. Even better, when combined with Google DeepMind's WaveNet, given enough training data, we could skip the audio footage altogether and just write a piece of text, making Obama, or someone else say what we've written. There are also a ton of other details to be worked out, for instance, there are cases where the mouth moves before the person starts to speak, which is to be taken into consideration. The dreaded ""umm""-s and ""ahh""-s are classical examples of that There is also an important jaw correction step and more. This is a brilliant piece of work with many non-trivial decisions that are described in the paper make sure to have a look at it for details, as always, there is a link to it is available the video description. The results are also compared to the Face2face paper from last year that we also covered in the series. It is absolutely insane to see this rate of progress over the lapse of only one year. If you have enjoyed this episode and you feel that eight of these videos a month is worth a dollar, please consider supporting us on Patreon. You can pick up some really cool perks there and it is also a great deal of help for us to make better videos for you in the future. Earlier I also wrote a few words about the changes we were able to make because of your amazing support. Details are available in the description. Thanks for watching and for your generous support, and I'll see you next time!"
80,Light Transport on Specular Microstructure,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Using light simulation programs, we are able to populate a virtual scene with objects, assign material models to them, and create a beautiful, photorealistic image of this digital scene. The theory of light transport follows the physics of light, therefore these images should be indistinguishable from reality, and fortunately, they often are. However, there are some cases when we can tell them apart from real images. And the reason for this is not the inaccuracies of the light transport algorithm, but the oversimplifications used in the geometry and material models. The main issue is that in our mathematical models, these materials are often defined to be too perfect. But in reality, metals are rarely perfectly polished, and the classical material models in light transport can rarely capture these microstructures that make surfaces imperfect. This algorithm is about rendering new material models that can represent the imperfect materials like scratched coating and metallic flakes on carpaint, a leather sofa, wooden floor, or a teapot. Just look at these phenomenal images. Previous techniques exist to solve this problem, but they take extremely long and are typically limited to flat surfaces. One of the main difficulties of the problem is that these tiny flakes and scratches are typically orders of magnitude smaller than a pixel, and therefore they require a lot of care and additional computations to render. This work provides an exact, closed-form solution to this that is highly efficient to render. It is over a hundred times faster than previous techniques, has less limitations as it works on curved surfaces, and, it only takes 40% longer to render it compared to the standard perfect material models. Only 40% more time for this? Sign me up! It is truly incredible that we can create images of this sophistication using science. It is also highly practical as it can be plugged in as a standard material model without any crazy modifications to the simulation program. Looking forward to seeing some amazing animations using more and more realistic material models in the near future. If you would like to learn more about light simulations, I have been holding a full master-level course on it at the Technical University of Vienna for a few years now. After a while, I got a strong feeling that the teachings shouldn't only be available for the lucky 30 people in the classroom who can afford a college education. The teachings should be available for everyone. And now, the entirety of this course is available free of charge for everyone where we learn the theory of light from scratch and implement a really cool light simulation program together. If you want to solve a few infinite dimensional integrals with me, give it a go! As always, details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
81,Hindsight Experience Replay,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is an awesome algorithm that is able to play computer games, navigate helicopters, hit a baseball, or even defeat Go champions when combined together with a neural network and Monte Carlo tree search. It is a quite general algorithm that is able to take on a variety of difficult problems that involve observing an environment and coming up with a series of actions to maximize a score. In a previous episode, we had a look at DeepMind's algorithm where a set of movement actions had to be chosen to navigate in a difficult 3D environment efficiently. The score to be maximized was the distance measured from the starting point, the further our character went, the higher score it was given, and it has successfully learned the concept of locomotion. Really cool! A prerequisite for a reinforcement learner to work properly is that it has to be given informative reward signals. For instance, if we go to a written exam, as an output, we would like to get a detailed breakdown of the number of points we got for each problem. This way, we know where we did well and which kinds of problems need some more work. However, imagine having a really careless teacher who never tells us the points, but would only tell us whether we have failed of passed. No explanation, no points for individual tasks, no telling whether we failed by a lot or just by a tiny bit. Nothing. First attempt, we failed. Next time, we failed again. And again and again and again. Now this would be a dreadful learning experience because we would have absolutely no idea what to improve. Clearly, this teacher would have to be fired. However, when formulating a reinforcement learning problem, instead of using more informative scores, it is much easier to just tell whether the algorithm was successful or not. It is very convenient for us to be this careless teacher. Otherwise, what score would make sense for a helicopter control problem when we almost crash into a tree? This part is called reward engineering and the main issue is that we have to adapt the problem to the algorithm, where the best would be if the algorithm would adapt to the problem. This has been a long-standing problem in reinforcement learning research, and a potential solution would open up the possibility of solving even harder and more interesting problems with learning algorithms. And this is exactly the what researchers at OpenAI try to solve by introducing Hindsight Experience Replay, HER, or her in short. Very apt. This algorithm takes on problems where the scores are binary, which means that it either passed or failed the prescribed task. A classic careless teacher scenario. And these rewards are not only binary, but very sparse as well, which further exacerbates the difficulty of the problem. In the video, you can see a comparison with a previous algorithm with and without the HER extension. The higher the number of epochs you see above, the longer the algorithm was able to train. The incredible thing here is that it is able to achieve a goal even if it had never been able to reach it during training. The key idea is that we can learn just as much from undesirable outcomes as from desirable ones. Let me quote the authors. Imagine that you are learning how to play hockey and are trying to shoot a puck into a net. You hit the puck but it misses the net on the right side. The conclusion drawn by a standard reinforcement learning algorithm in such a situation would be that the performed sequence of actions does not lead to a successful shot, and little (if anything) would be learned. It is however possible to draw another conclusion, namely that this sequence of actions would be successful if the net had been placed further to the right. They have achieved this by storing and replaying previous experiences with different potential goals. As always, the details are available in the paper, make sure to have a look. Now, it is always good to test things out whether the whole system works well in software, however, its usefulness has been demonstrated by deploying it on a real robot arm. You can see the goal written on the screen alongside with the results. A really cool piece of work that can potentially open up new ways of thinking about reinforcement learning. After all, it's great to have learning algorithms that are so good, they can solve problems that we formulate in such a lazy way that we'd have to be fired. And here's a quick question: do you think 8 of these videos a month is worth a dollar? If you have enjoyed this episode and your answer is yes, please consider supporting us on Patreon. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
82,Latent Space Human Face Synthesis,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In many previous episodes, we talked about generative adversarial networks, a recent new line in machine learning research with some absolutely fantastic results in a variety of areas. They can synthesize new images of animals, create 3d models from photos, or dream up new products based on our edits of an image. A generative adversarial network means that we have two neural networks battling each other in an arms race. The generator network tries to create more and more realistic images, and these are passed to the discriminator network which tries to learn the difference between real photographs and fake, forged images. During this process, the two neural networks learn and improve together until they become experts at their own craft. And as you can see, the results are fantastic. However, training these networks against each other is anything but roses and sunshine. We don't know if the process converges or if we reach Nash equilibrium. Nash equilibrium is a state where both actors believe they have found an optimal strategy while taking into account the other actor's possible decisions, and neither of them have interest in changing their strategy. This is a classical scenario in game theory where two convicted criminals are pondering whether they should snitch on each other without knowing how the other decided to act. If you wish to hear more about the Nash-equilibrium, I've a put a link to Khan Academy's video in the description, make sure to check it out, you'll love it! I find it highly exciting that there are parallels in AI and game theory, however, the even cooler thing is that here, we try to build a system where we don't have to deal with such a situation. This is called Generative Latent Optimization, GLO in short and it is about introducing tricks to do this by only using a generator network. If you have ever read up on font design, you know that it is a highly complex field. However, if we'd like to create a new font type, what we're typically interested in is only a few features, like how curvy they are, or whether we're dealing with a serif kind of font, and simple descriptions like that. The same principle can be applied to human faces, animals, and most topics you can imagine. This means that there are many complex concepts that contain a ton of information, most of which can be captured by a simple description with only a few features. This is done by projecting this high-dimensional data onto a low-dimensional latent space. This latent space helps eliminating adversarial optimization, which makes this system much easier to train, and the main selling point is that it still retains the attractive properties of generative adversarial networks. This means that it can synthesize new samples from the learned dataset. If it had learned the concept of birds, it will be able to synthesize new bird species. It can perform continuous interpolation between data points. This means that for instance, we can produce intermediate states between two chosen furniture types or light fixtures. It is also able to perform simple arithmetic operations between any number of data points. For instance, if A is males with sunglasses, B are males without sunglasses, and C are females, then A-B+C is going to generate females in sunglasses. It can also do super resolution and much, much more, make sure to have a look at the paper in the video description. Now, before we go, we shall address the elephant in the room: these images are tiny. Our seasoned Fellow Scholars know that for generative adversarial networks, there are plenty of works on how to synthesize high resolution images with more details. This means that this is a piece of work that opens up exciting new horizons, but it is not to be measured against the tenth followup work on top of a more established line of research. Two Minute Papers will be here for you to keep you updated on the progress, which is, as we know, staggeringly quick in machine learning research. Don't forget to subscribe and click the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!"
83,DeepMind's AI Learns Locomotion From Scratch,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have talked about some awesome previous works where we used learning algorithms to teach digital creatures to navigate in complex environments. The input is a terrain and a set of joints, feet, and movement types, and the output has to be a series of motions that maximizes some kind of reward. This previous technique borrowed smaller snippets of movements from a previously existing database of motions and learned to stitch them together in a way that looks natural. And as you can see, these results are phenomenal. And the selling point of this new one, which you might say, looks less elaborate, however, it synthesizes them from scratch. This problem is typically solved via reinforcement learning, which is a technique that comes up with a series of decisions to maximize a prescribed score. This score typically needs to be something reasonably complex, otherwise the algorithm is given too much freedom to maximize it. For instance, we may want to teach a digital character to run or jump hurdles, but it may start crawling instead, which is still completely fine if our objective is too simple, for instance, just maximizing the distance from the starting point. To alleviate this, we typically resort to reward engineering, which means that we add additional terms to this reward function to regularize the behavior of these creatures. For instance, we can specify that throughout these motions, the body has to remain upright which likely favors locomotion-type solutions. However, one of the main advantages of machine learning is that we can reuse our solutions for a large set of problems. If we have to specialize our algorithm for all terrain and motion types, and different kinds of games, we lose out on one of the biggest advantage of learning techniques. So researchers at DeepMind decided that they are going to solve this problem with a reward function which is nothing else but forward progress. That's it. The further we get, the higher score we obtain. This is amazing because it doesn't require any specialized reward function but at the same time, there are a ton of different solutions that get us far in these terrains. And as you see here, beyond bipeds, a bunch of different agent types are supported. The key factors to make this happen is to apply two modifications to the original reinforcement learning algorithm. One makes the learning process more robust and less dependent on what parameters we choose, and the other one makes it more scalable, which means that it is able to efficiently deal with larger problems. Furthermore, the training process itself happens on a rich, carefully selected set of challenging levels. Make sure to have a look at the paper for details. A byproduct of this kind of problem formulation, is, as you can see, that even though this humanoid does its job with its lower body well, but in the meantime, it is flailing its arms like a madman. The reason is likely because there is not much of a difference in the reward between different arm motions. This means that we most likely get through a maze or a heightfield even when flailing, therefore the algorithm doesn't have any reason to favor more natural looking movements for the upper body. It will probably choose a random one, which is highly unlikely to be a natural motion. This creates high quality, albeit amusing results that I am sure some residents of the internet will honor with a sped-up remix video with some Benny Hill music. In summary, no precomputed motion database, no handcrafting of rewards, and no additional wizardry needed. Everything is learned from scratch with a few small modifications to the reinforcement learning algorithm. Highly remarkable work. If you've enjoyed this episode and would like to help us and support the series, have a look at our Patreon page. Details and cool perks are available in the video description, or just click the letter P at the end of this video. Thanks for watching and for your generous support, and I'll see you next time!"
84,What is The Best Way To Simulate Liquids?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As you know from this series, fluid simulation techniques that are able to create high fidelity video footage are in abundance in computer graphics research. These techniques all have their own tradeoffs, and when we evaluate them, we often use terms like first or second-order accuracy, which are mathematical terms. We often have to evaluate these techniques against each other by means of mathematics, because this way, we can set up consistent and unbiased comparisons that everyone understands and agrees upon. However, ultimately, in the show business, what matters is how the viewers perceive the end result, whether they think it looks fake, or if it keeps their suspension of disbelief. We have the choice of not only simulation techniques, but all of them also have their own set of parameters. For instance, the higher the resolution of our simulations, the more high-frequency details appear in the footage. However, after a point, increasing the resolution further is extremely costly, and while we know what is to be gained in terms of mathematics, it is still unknown how well it would do with the users. So the ultimate question is this: what do I get for my money and time? This paper provides an exhaustive user study to answer this question, where the users are asked to look at two different simulations and as a binary choice, tell us which is the one they perceived to be closer to the reference. The reference footage is a real-world video of a water sloshing in a tank, and the other footage that is to be judged is created via a fluid simulation algorithm. Turns out that the reference footage can be almost anything as long as there are some splashing and sloshing going on in it. It also turns out that after a relatively favorable breaking point which is denoted by 2x, further increasing the resolution does not make a significant difference in the user scores. But boy, does it change the computation times! So this is why such studies are super useful, and it's great to see that the accuracy of these techniques are measured both mathematically, and also how convincing they actually look for users. Another curious finding is that if we deny access to the reference footage, we see a large change in different responses and a similar jump in ambiguity. This means that we are reasonably bad at predicting the fine details, therefore, if the simulation pushes the right buttons, the users will easily believe it to be correct even if it is far away from the ground truth solution. Here is a matrix with a ton of rendered footage. Horizontally, you see the same thing with different simulation techniques, and vertically, we slowly go from transparent above to opaque below. To keep things fair and really reveal which choices are the best bang for the buck, there are also comparisons between techniques that have a similar computation time. In these cases, the Fluid Implicit Particle. FLIP in short and the Affine Particle in Cell, are almost unanimously favorable. These are advanced techniques that combine particle and grid-based simulations. I think this is highly useful information for more time critical applications, so make sure to have a look at the paper for details. There are similar user studies with glossy and translucent material models and much more in the paper. The source code of this project is also available. Thanks for watching and for your generous support, and I'll see you next time!"
85,AI Learns To Improve Smoke Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about using AI to create super detailed smoke simulations. Typically, creating a crude simulation doesn't take very long, but as we increase the resolution, the execution time and memory consumption skyrockets. In the age of AI, it only sounds logical to try to include some learning algorithms in this process. So what if we had an AI-based technique that would have some sort of understanding of smoke simulations, take our crude data and add the fine details to it? This way, we could obtain a high resolution smoke simulation without waiting several days or weeks for the computation. Now if you are a truly seasoned Fellow Scholar, you may remember an earlier work by the name Wavelet Turbulence, which is one of my favorite papers of all time. So much so that it got the distinction of being showcased in the very first Two Minute Papers episode. I was a sophomore college student back then when I've first seen it and was absolutely shocked by the quality of the results. That was an experience I'll never forget. It also won a technical Oscar award and it is not an overstatement to say that this was one of the most influential works that made me realize that research is my true calling. The link to the first episode is available in the video description and if you want to see how embarrassing it is, make sure to check it out. It did something similar, but instead of using AI, it used some heuristics that describe what is the ratio and distribution of smaller and bigger vortices in a piece of fluid or smoke. Using this information, it could create a somewhat similar effect, but ultimately, that technique had an understanding of smoke simulations in general, but it didn't know anything about the scene that we have at hand right now. Another work that is related to this is showing a bunch of smoke simulation videos to an AI and teach it to continue these simulations by itself. I would place this work as a middle ground solution, because this work says that we should take a step back and not try to synthesize everything from scratch. Let's create a database of simulations, dice them up into tiny tiny patches, look at the same footage in low and high resolutions, and learn how they relate to each other. This way, we can hand the neural network some low resolution footage and it will be able to make an educated guess as to which high resolution patch should be the best match for it. When we found the right patch, we just switch the coarse simulation to the most fitting high-resolution patch in the database. You might say that in theory, creating such a Frankenstein smoke simulation sounds like a dreadful idea. But have a look at the results, as they are absolutely brilliant! And as you can see, it takes a really crude base simulation and adds so many details to it, it's truly an incredible achievement. One neural network is trained to capture similarities in densities, and one for vorticity. Using the two neural networks in tandem, we can take a low resolution fluid flow and synthesize the fine details on top of it in a way that is hardly believable. It also handles boundary conditions, which means that these details are correctly added even if our smoke puff hits an object. This was an issue with Wavelet Turbulence which had to be addressed with several followup works. There are also comparisons against this legendary algorithm, and as you can see, the new technique smokes it. However, it took 9 years to do this. This is exactly 9 eternities in the world of research, which is a huge testament to how powerful the original algorithm was. It is also really cool to get more and more messages where I get to know more about you Fellow Scholars. I was informed that the series is used in school classes in Brazil, it is also used to augment college education, and it is a great topic for fun family conversations over dinner. That's just absolutely fantastic. Loving the fact that the series is an inspiration for many of you. Thanks for watching and for your generous support, and I'll see you next time!"
86,Physics-based Image and Video Editing,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about changing already existing images and videos by adding new objects to them, or editing their physical attributes. These editable attributes include gravity, mass, stiffness, or we can even add new physical forces to the scene. For instance, we can change the stiffness of the watches in this Dalí painting and create an animation from it. Physically accurate animations from paintings. How cool is that? This is approaching science fiction levels of craziness. Animating a stationary clothesline by adding a virtual wind effect to the scene, or bending a bridge by changing its mass is also a possibility. The first reaction I had when I've looked at this work was ""are you kidding me? you can't edit a photograph!"". Especially that I've seen plenty of earlier works that tried to do something similar but each time the limitations were just too crippling for real-world usage. And the other ultimate question is always how much user interaction does this need? Is this trivial to use, or is it a laborious process? What we need to do is roughly highlight the outline of the object that we'd like to manipulate. The algorithm uses a previously published technique to make sure that the outlines are accurately captured, and then tries to create a 3D digital model from the selected area. We need one more step where we align the 3D model to the image or video input. Finally, the attribute changes and edits take place not on the video footage, but on this 3D model through a physics simulation technique. A truly refreshing combination of old and new techniques with some killer applications. Loving it! The biggest challenge is to make sure that the geometry and the visual consistency of the scene is preserved through these changes. There are plenty of details discussed in the paper, make sure to have a look at that, a link to it is available in the video description. As these 2D photo to 3D model generator algorithms improve, so will the quality of these editing techniques in the near future. Our previous episode was on this topic, make sure to have a look at that. Also, if you would like to get more updates on the newest and coolest works in this rapidly improving field, make sure to subscribe and click the bell icon to be notified when new Two Minute Papers videos come up. Thanks for watching and for your generous support, and I'll see you next time!"
87,AI Creates 3D Models From Images,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we're going to talk about a task that humans are remarkably good at, but learning algorithms mostly flounder. And that is creating 3D geometry by looking at a 2D color image. In video games and animation films, this is a scenario that comes up very often if we need a new weapon model in the game, we typically give the artist a photo, who will sit down with a 3D modeler program, and spends a few hours sculpting a similar 3D geometry. And I will quickly note that our binocular vision is not entirely necessary to make this happen. We can look at 2D images all day long and still have a good idea about the shape of an airplane, even with one eye closed. We had previous episodes on this problem, and the verdict was that that the results with previous techniques are great, but not very detailed. Mathematicians like to say that this algorithm has a cubic complexity or cubic scaling, which means that if we wish to increase the resolution of the 3D model just a tiny bit, we have wait not a tiny bit longer, but significantly longer. And the cubic part means that this tradeoff becomes unbearable even for moderately high resolutions. This paper offers a technique to break through this limitation. This new technique still uses a learning algorithm to predict the geometry, but it creates these 3D models hierarchically. This means that it starts out approximating the coarse geometry of the output, and restarts the process by adding more and more fine details to it. The geometry becomes more and more refined over several steps. Now, this refinement doesn't just work unless we have a carefully designed algorithm around it. The refinement happens by using additional information in each step from the created model. Namely, we imagine our predicted 3D geometry as a collection of small blocks, and each block is classified as either free space, occupied space, or as a surface. After this classification happened, we have the possibility to focus our efforts on refining the surface of the model, leading to a significant improvement in the execution time of the algorithm. As a result, we get 3D models that are of higher quality than the ones offered by previous techniques. The outputs are still not super high resolution, but they capture a fair number of surface detail. And you know the drill, research is a process, and every paper is a stepping stone. And this is one of those stepping stones that can potentially save many hours of work for 3D artists in the industry. Thanks for watching and for your generous support, and I'll see you next time!"
88,AI Creates Facial Animation From Audio,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about creating facial animation from speech in real time. This means that after recording the audio footage of us speaking, we give it to a learning algorithm, which creates a high-quality animation depicting our digital characters uttering these words. This learning algorithm is a Convolutional Neural Network, which was trained on as little as 3 to 5 minutes of footage per actor, and was able to generalize its knowledge from this training data to a variety of real-world expressions and words. And if you think you've seen everything, you should watch until the end of the video as it gets better than that because of two reasons. Reason number one, it not only takes audio input, but we can also specify an emotional state that the character should express when uttering these words. Number two, and this is the best part, we can also combine this together with DeepMind's WaveNet, which synthesizes audio from our text input. It basically synthesizes a believable human voice and says whatever text we write down. And then that sound clip can be used with this technique to make a digital character say what we've written. So we can go from text to speech with WaveNet, and put the speech onto a virtual actor with this work. This way, we get a whole pipeline that works by learning and does everything for us in the most convenient way. No actors needed for voiceovers. No motion capture for animations. This is truly incredible. And if you look at the left, side, you can see that in their video, there is some Two Minute Papers action going on. How cool is that? Make sure to have a look at the paper to see the three-way loss function the authors came up with to make sure that the results work correctly for longer animations. And of course, in research, we have to prove that our results are better than previous techniques. To accomplish this, there are plenty of comparisons in the supplementary video. But we need more than that. Since these results cannot be boiled down to a mathematical theorem that we need to prove, we have to do it some other way. And the ultimate goal is that a human being would judge these videos as being real with a higher chance than one made with a previous technique. This is the core idea behind the user study carried out in the paper. We bring in a bunch of people, present them with a video of the old and new technique without knowing which is which, and ask them which one they feel to be more natural. And the result was not even close the new method is not only better overall, but I haven't found a single case, scenario or language where it didn't come out ahead. And that's extremely rare in research. Typically, in a maturing field, new techniques introduce a different kind of tradeoff, for instance, less execution time but at the cost of higher memory consumption is a classical case. But here, it's just simply better in every regard. Excellent. If you enjoyed this episode, and would like to help us make better videos in the future, please consider supporting us on Patreon. You can pick up cool perks like watching these episodes in early access. Details are available in the video description. Beyond telling these important research stories, we're also using part of these funds to empower other research projects. I just made a small write-up about this which is available on our Patreon page. That link is in the video description, make sure to have a look. Thanks for watching and for your generous support, and I'll see you next time!"
89,DeepMind's AI Learns Audio And Video Concepts By Itself,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In our earlier episodes, when it came to learning techniques, we almost always talked about supervised learning. This means that we give the algorithm a bunch of images, and some additional information, for instance, that these images depict dogs or cats. Then, the learning algorithm is exposed to new images that it had never seen before and has to be able to classify them correctly. It is kind of like a teacher sitting next to a student, providing supervision. Then, the exam comes with new questions. This is supervised learning, and as you have seen from more than 180 episodes of Two Minute Papers, there is no doubt that it is an enormously successful field of research. However, this means that we have to label our datasets, so we have to add some additional information to every image we have. This is a very laborious task, which is typically performed by researchers or through crowdsourcing, both of which takes a lot of funding and hundreds of work hours. But if we think about it, we have a ton of videos on the internet, you always hear these mind melting new statistics on how many hours of video footage is uploaded to YouTube every day. Of course, we could hire all the employees in the world to annotate these videos frame by frame to tell the algorithm that this is a guitar, this is an accordion, or a keyboard, and we would still not be able to learn on most of what's uploaded. But it would be so great to have an algorithm that can learn on unlabeled data. However, there are learning techniques in the field of unsupervised learning, which means that the algorithm is given a bunch of images, or any media, and is instructed to learn on it without any additional information. There is no teacher to supervise the learning. The algorithm learns by itself. And in this work, the objective is to learn both visual and audio-related tasks in an unsupervised manner. So for instance, if we look at the this layer of the visual subnetwork, we'll find neurons that get very excited when they see, for instance, someone playing an accordion. And each of the neurons in this layer belong to different object classes. I surely have something like this for papers. And here comes the Károly goes crazy part one: this technique not only classifies the frames of the videos, but it also creates semantic heatmaps, which show us which part of the image is responsible for the sounds that we hear. This is insanity! To accomplish this, they ran a vision subnetwork on the video part, and a separate audio subnetwork to learn about the sounds, and at the last step, all this information is fused together to obtain Károly goes crazy part two: this makes the network able to guess whether the audio and the video stream correspond to each other. It looks at a man with a fiddle, listens to a sound clip and will say whether the two correspond to each other. Wow! The audio subnetwork also learned the concept of human voices, the sound of water, wind, music, live concerts and much, much more. And the answer is yes, it is remarkably close to human-level performance on sound classification. And all this is provided by the two networks that were trained from scratch, and, no supervision is required. We don't need to annotate these videos. Nailed it. And please don't get this wrong, it's not like DeepMind has suddenly invented unsupervised learning, not at all. This is a field that has been actively researched for decades, it's just that we rarely see really punchy results like these ones here. Truly incredible work. If you enjoyed this episode, and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
90,Photorealistic Fur With Multi-Scale Rendering,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Creating a photorealistic image with fur and hair is hard. It is typically done by using light simulation programs where we use the laws of physics to simulate the path of millions and millions of light rays as they bounce off of different objects in a scene. This typically takes from minutes to hours if we're lucky. However, in the presence of materials like hair and fur, this problem becomes even more difficult, because fur fibers have inner scattering media. This means that we not only have to bounce these rays off of the surface of objects, but also have to simulate how light is transmitted between these inner layers. And initially, we start out with a noisy image, and this noise gets slowly eliminated as we compute more and more rays for the simulation. Spp means samples per pixel, which is the number of rays we compute for each pixel in our image. And you can see that with previous techniques, using 256 samples per pixel leads to a very noisy image and we need to spend significantly more time to obtain a clear, converged image. And this new technique enables us to get the most out of our samples, and if we render an image with 256 spp, we get a roughly equivalent quality to a previous technique using around six times as many samples. If we had a film studio and someone walked up on us and said that we can render the next Guardians of The Galaxy film six times cheaper, we'd surely be all over it. This would save us millions of dollars. The main selling point is that this work introduces a multi-scale model for rendering hair and fur. This means that it computes near and far-field scattering separately. The far-field scattering model contains simplifications, which means that it's way faster to compute. This simplification is sufficient if we look at a model from afar or we look closely at a hair model that is way thinner than human hair strands. The near-field model is more faithful to reality, but also more expensive to compute. And the final, most important puzzle piece is stitching together the two: whenever we can get away with it, we should use the far-field model, and compute the expensive near-field model only when it makes a difference visually. And one more thing: as these hamsters get closer or further away from the camera, we need to make sure that there is no annoying jump when we're switching models. And as you can see, the animations are buttery smooth, and when we look at it, we see beautiful rendered images, and if we didn't know a bit about the theory, we would have no idea about the multi-scale wizardry under the hood. Excellent work. The paper also contains a set of decompositions for different light paths, for instance, here, you can see a fully rendered image on the left, and different combinations of light reflection and transmission events. For instance, R stands for one light reflection, TT for two transmission events, and so on. The S in the superscript denotes light scattering events. Adding up all the possible combinations of these Ts and Rs, we get the photorealistic image on the left. That's really cool, loving it! If you would like to learn more about light simulations, I am holding a full master-level course on it at the Technical University of Vienna. And the entirety of this course is available free of charge for everyone. I got some feedback from you Fellow Scholars that you watched it and enjoyed it quite a bit. Give it a go! As always, details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
91,DeepMind Publishes StarCraft II Learning Environment,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This topic has been perhaps the most highly anticipated by you Fellow Scholars and I am extremely excited to show you the first joint paper between DeepMind and Blizzard on creating an AI program to play Stacraft II. Hell yeah! And fortunately, we have a paper where every detail is meticulously described, so there's much less room for misunderstandings. And before we start, note that this is a preliminary work, so please don't expect superhuman performance. However difficult you thought this problem was, you'll see in a minute that it's way more complex than most people would think. But before we start what is Starcraft 2? It is a highly technical strategy game which will be a huge challenge to write a formidable AI for because of the three reasons. One, we have imperfect information with a partially observed map. If we wish to see what the other opponent is up to, we have to devote resources to scouting, which may or may not be successful depending on the vigilance of the other player. Two, we need to select and control hundreds of units under heavy time pressure. One wrong decision, and we can quickly lose most of our units and become unable to recover from it. And three, perhaps the most important part long-term strategies need to be developed, where a poor decision in the early game can lead to a crushing defeat several thousands of actions later. These cases are especially difficult to identify and learn. However, we ran a bit too far ahead to the gameplay part. What needs to be emphasized is that there is a step number one before that. And that step number one is making sure that the AI is able communicate and interact with the game, which requires a significant engineering effort. In this paper, a Python-based interface is described to make this happen. It is great to have companies like DeepMind and OpenAI who are devoted to lay down the foundations for such an interface, which is a herculean task. This work would likely had never seen the light of day if AI research would only take place in academia. Huge respect and much thanks for the DeepMind guys for making this happen. To play the game, Deep Reinforcement Learning is used, which you heard about earlier in the series. This is a powerful learning algorithm where a neural network is used to process the video input and is combined with a reinforcement learner. With reinforcement learning, we're observing the environment around us and choose the next action to maximize a score or reward. However, defining score was very easy in Atari Breakout, because we knew that if the number of our lives drops to zero, we lost, and if we break a lot of bricks, our score improves. Simple. Not so much in Starcraft 2, because how do we know exactly if we're winning? What is the score we're trying to maximize? In this work, there are two definitions for score: one that we get to know at the very end that describes whether we won, had a tie, or lost. This is the score that ultimately matters. However, this information is not available throughout the game to drive the reinforcement learner, so there is an intermediate score that is referred to as Blizzard score in the paper, which involves a weighted sum of current resources and upgrades, as well as our units and buildings. This we have access to throughout the game. This sounds good for a first approximation, since it is monotonically increasing if we win battles and manage our resources well, and decreases when we're losing. However, there are many matches where the player with the more resources does not have enough time to spend it and ultimately loses a deciding encounter. It remains to be seen whether this is exactly what we need to maximize to beat a formidable human player. There are also non-trivial engineering decisions on how to process the video stream. The current system uses a set of feature layers, which encode relevant information for the AI, such as terrain height, the camera location, hit points for the units on the screen and much, much more. There is a huge amount of information that the convolutional neural network has to make sense of. And I think it is now easy to see that starting out with throwing the AI in the deep water and expecting it to perform well on a full one versus one match, at this point, is a forlorn effort. The paper describes a set of minigames, where the algorithm can learn different aspects of the game in isolation, such as picking up mineral shards scattered around the map, defeating enemy units in small skirmishes, building units or harvesting resources. In these minigames, the AI has reached the level of a novice human player, which is quite amazing given the magnitude and the complexity of the problem. The authors also encourage the community to create more minigames for the AI to train on. I really love the openness and the community effort aspects of this work! And we've only just scratched the surface, there is so much more in the paper, with a lot more non-trivial design decisions and a database with tens of thousands of recorded games. And, the best part is that the source code for this environment is available right now for the fellow tinkerers out there. I've put a link to this in the video description. This is going to be one heck of a challenge for even the brightest AI researchers of our time. I can't wait to get my hands on the code and also, I am very excited to read some followup papers on this. I expect there will be many of those in the following months. In the meantime, as we know, OpenAI is also working on DOTA with remarkable results, and there's lots of discussion whether a DOTA 5 versus 5 or a Starcraft 2 1 versus 1 game is more complex for the AI to learn. If you have an opinion on this, make sure to leave a comment below this video. Which is more complex? Why? This also signals that there's going to be tons of fun to be had with AI and video games this year. Stay tuned! Thanks for watching and for your generous support, and I'll see you next time!"
92,Real-Time Noise Filtering For Light Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we talk a lot about photorealistic rendering, which is one of the most exciting areas in computer graphics research. Photorealistic rendering means that we put virtual objects in a scene, assign material models to them, and then run a light simulation program to create a beautiful image. This image depicts how these objects would look like in reality. This is particularly useful for the film industry because we can create highly realistic scenes and set them up in a way that we couldn't do in real life. We can have any possible object we can imagine, light sources that we wouldn't ever be able to buy, change the time of the day, or even the planet we're on. Practically, we have an infinite budget. That's amazing! However, creating such an image takes a long time, often in the order of hours to days. Here you can see me render an image and even if the footage is sped up significantly, you can see that this is going to take a long, long time. Spp means samples per pixel, so this the number of light rays we compute for every pixel. The more spp, the cleaner, more converged image we get. This technique performs spatiotemporal filtering. This means that we take a noisy input video and try to eliminate the noise and try to guess how the final image would look like. And it can create almost fully converged images from extremely noisy inputs. Well, as you can see, these videos are created with one sample per pixel, which is as noisy as it gets. These images with the one sample per pixel can be created extremely quickly, in less than ten milliseconds per image, and this new denoiser also takes around 10 milliseconds to reconstruct the final image from the noisy input. And yes, you heard it right, this is finally a real-time result. This all happens through decoupling the direct and indirect effect of light sources and denoising them separately. In the meantime, the algorithm also tries to estimate the amount of noise in different parts of the image to provide more useful information for the denoising routines. The fact that the entire pipeline runs on the graphics card is a great testament to the simplicity of this algorithm. Whenever you see the term SVGF, you'll see the results of the new technique. So we have these noisy input videos with 1 spp. And...look at that! Wow! This is one of those papers that looks like magic. And, no neural networks or learning algorithms have been used in this work. Not so long ago, I speculated, or more accurately, hoped that real-time photorealistic rendering would be a possibility during my lifetime. And just a few years later, this paper appears. We know that the rate of progress in computer graphics research is just staggering, but this is, this is too much to handle. Super excited to see where the artists will take this, and of course, I'll be here to show you the coolest followup works. Thanks for watching and for your generous support, and I'll see you next time!"
93,OpenAI's Bot Beats DOTA World Champion Dendi,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. It is time for some minds to be blown. DOTA 2 is a multiplayer online battle arena game with a huge cult following and world championship events with a prize pool of over 20 million dollars. In this game, players form two teams and control a hero each and use their strategy and special abilities to defeat the other team. OpenAI recently created an AI for this game that is so good that they challenged the best players in the world. Now note that this program is not playing the full feature set of the game, but a version that is limited to one versus one encounters with several other elements of the game disabled. Since lots of strategy is involved, we always discuss in these episodes that long-term planning is the Achilles-heel of these learning algorithms. A small blunder in the early game can often snowball out of control by the end of the match, and it is hard for the AI, and sometimes, to even humans to identify these cases. And this game is a huge challenge because unlike chess and go, it has lots of incomplete information, and even the simplified one versus one mode involves a reasonable amount of long-term planning. It also involves attacks, trickery and deceiving an opponent and can be imagined as a strategy game that also requires significant technical prowess to pull off the most spectacular moves. This game is also designed in a way that new and unfamiliar situations come up all the time which require lots of experience and split-second decisionmaking to master. This is a true test for any kind of AI. And note that this AI wasn't told anything about the game, not even the rules, and was just instructed to try to find a way to win. The algorithm was trained in 24 hours, and during this time, it not only learned the rules and objectives of the game, but it also pulls off remarkable tactics. For instance, other players were very surprised that the bot didn't take the bait, which typically means a smart tactic involving giving up a smaller battle in favor of winning a bigger objective. The AI has a ton of experience playing the game and typically sees through these shenanigans. In this game, there are also neutral units that we call creep. When killed, they grant precious gold and experience to our opponent, so we typically try to deny that. If these units encounter an obstacle, they go around it, so players developed a technique by the name creep blocking, which is the art of holding them up by the hero character to minimize the distance traveled by them in a unit of time. And the AI has not only learned about the existence of this technique by itself, but it also executes it with stunning precision, which is quite remarkable. And again, during the training phase, it had never seen any human play the game and do something like this. The other remarkable thing is that when a player disappears in the darkness, the AI predicts what he could be doing, plans around it, and strikes where the player is expected to show up. If you remember, DeepMind's initial Go algorithm contained a bootstrapping step where it was fed a large amount of games by players to grasp the basics. The truly remarkable thing is that none of that happened here. This algorithm was trained for only 24 hours and it only played against itself. When it finally played against Dendi, the reigning world champion, the first match was such a treat, and I was shocked to see that the AI has outplayed him. In the second game, the player tried to create a situation that he thought the AI hasn't encountered before by giving up some creep to it. The program ruthlessly took advantage of this mistake and defeated him almost immediately. OpenAI's bot not only won, but apparently also broke the will of Dendi, who tapped out after two matches. I feel like someone being hit by a sledgehammer. I didn't even know this was being worked on! This is such a remarkable achievement. Usually, the first argument I hear is that of course, the AI can play non-stop without bathroom breaks or sleep. While, admittedly, this is also true for some players, the algorithm was only trained for 24 hours. Note that this still means a stupendous amount of games played, but in terms of training time, given that these algorithms typically take from weeks to months to train properly, 24 hours is nothing. The second argument that I often hear is that the AI should of course win every time, because it has close to 0 reaction time and can perform thousands of actions every second. For instance, if we would play a game where the goal is to perform the most amount of actions per minute, clearly, humans with biological limitations would stand no chance against a computer program. However, in this case, the number of actions that this algorithm performs in a minute is comparable to that of a human player. This means that these results stem from superior technical abilities and planning, and not from the fact that we're talking about a computer. We can look at this result from two different directions. One could be saying, well, no big deal, because this is only a highly limited and hamstrung version of the game, which is way less complex than a fully-fledged 5 versus 5 team match. Or, two, we could say that the algorithm had shown a remarkable aptitude for learning highly sophisticated technical maneuvers and longer-term strategy in a difficult game. And the rest is only a matter of time. In fact, in 5 versus 5, there is even more room for a highly intelligent program to shine and create new tactics that we've never thought of. I would bet that if anything, we're going to be even more surprised by the 5 versus 5 results later. We are still lacking in details a bit, but I have contacted the OpenAI guys who noted that there will be more information available in the next few days. Whenever something new appears, I'll be here to cover it for you Fellow Scholars. If you are new to the series and enjoyed this episode, make sure to subscribe and click the bell icon for two super fun science videos a week. And if you find yourself interested in DOTA 2, and admittedly, it's hard not to, and would like to catch up a bit on the basics, make sure to visit Day9's channel who has a really nice playlist about the fundamentals of the game. There is a link in the description for it, check it out. If you go to his channel, make sure to leave him a kind scholarly comment. Let the world see how courteous the Two Minute Papers listeners are. Thanks for watching and for your generous support, and I'll see you next time!"
94,Verifying Mission-Critical AI Programs,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper does not contain the usual fireworks that you're used to in Two Minute Papers, but I feel that this is a very important story that needs to be told to everyone. In computer science, we encounter many interesting problems, like finding the shortest path between two given streets in a city, or measuring the stability of a bridge. Up until a few years ago, these were almost exclusively solved by traditional, handcrafted techniques. This means a class of techniques that were designed by hand by scientists and are often specific to the problem we have at hand. Different problem, different algorithm. And, fast forward to a few years ago, we witnessed an amazing resurgence of neural networks and learning algorithms. Many problems that were previously thought to be unsolvable, crumbled quickly one after another. Now it is clear that the age of AI is coming, and clearly, there are possible applications of it that we need to be very cautious with. Since we design these traditional techniques by hand, the failure cases are often known because these algorithms are simple enough that we can look under the hood and make reasonable assumptions. This is not the case with deep neural networks. We know that in some cases, neural networks are unreliable. But it is remarkably hard to identify these failure cases. For instance, earlier, we talked about this technique by the name pix2pix where we could make a crude drawing of a cat and it would translate it to a real image. It worked spectacularly in many cases, but twitter was also full of examples with really amusing failure cases. Beyond the unreliability, we have a much bigger problem. And that problem is adversarial examples. In an earlier episode, we discussed an adversarial algorithm, where in an amusing example, they added a tiny bit of barely perceptible noise to this image, to make the deep neural network misidentify a bus for an ostrich. We can even train a new neural network that is specifically tailored to break the one we have, opening up the possibility of targeted attacks against it. To alleviate this problem, it is always a good idea to make sure that these neural networks are also trained on adversarial inputs as well. But how do we know how many possible other adversarial examples exist that we haven't found yet? The paper discusses a way of verifying important properties of neural networks. For instance, it can measure the adversarial robustness of such a network, and this is super useful, because it gives us information whether there are possible forged inputs that could break our learning systems. The paper also contains a nice little experiment with airborne collision avoidance systems. The goal here is avoiding midair collisions between commercial aircrafts while minimizing the number of alerts. As a small-scale thought experiment, we can train a neural network to replace an existing system, but in this case, such a neural network would have to be verified. And it is now finally a possibility. Now, make no mistake, this does not mean that there are any sort of aircraft safety systems deployed in the industry that are relying on neural networks. No no no, absolutely not. This is a small-scale ""what if"" kind of experiment that may prove to be a first step towards something really exciting. This is one of those incredible papers that, even without the usual visual fireworks, makes me feel that I am a part of the future. This is a step towards a future where we can prove that a learning algorithm is guaranteed to work in mission critical systems. I would also like to note that even if this episode is not meant to go viral on the internet, it is still an important story to be told. Normally, creating videos like this would be a financial suicide, but we're not hurt by this at all because we get stable support from you on Patreon. And that's what it is all about worrying less about views and spending more time talking about what's really important. Absolutely amazing. Thanks for watching and for your generous support, and I'll see you next time!"
95,DeepMind's AI Learns Imagination-Based Planning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A bit more than two years ago, the DeepMind guys implemented an algorithm that could play Atari Breakout on a superhuman level by looking at the video feed that you see here. And the news immediately took the world by storm. This original paper is a bit more than 2 years old and has already been referenced in well over a thousand other research papers. That is one powerful paper! This algorithm was based on a combination of a neural network and reinforcement learning. The neural network was used to understand the video feed, and reinforcement learning is there to come up with the appropriate actions. This is the part that plays the game. Reinforcement learning is very suitable for tasks where we are in a changing environment and we need to choose an appropriate action based on our surroundings to maximize some sort of score. This score can be for instance, how far we've gotten in a labyrinth, or how many collisions we have avoided with a helicopter, or any sort of score that reflects how well we're currently doing. And this algorithm works similarly to how an animal learns new things. It observes the environment, tries different actions and sees if they worked well. If yes, it will keep doing that, if not, well, let's try something else. Pavlov's dog with the bell is an excellent example of that. There are many existing works in this area and it performs remarkably well for a number of problems and computer games, but only if the reward comes relatively quickly after the action. For instance, in Breakout, if we miss the ball, we lose a life immediately, but if we hit it, we'll almost immediately break some bricks and increase our score. This is more than suitable for a well-built reinforcement learner algorithm. However, this earlier work didn't perform well on any other games that required long-term planning. If Pavlov gave his dog a treat for something that it did two days ago, the animal would have no clue as to which action led to this tasty reward. And this work's subject is a game where we control this green character and our goal is to push the boxes onto the red dots. This game is particularly difficult, not only for algorithms, but even humans, because of two important reasons: one, it requires long-term planning, which, as we know, is a huge issue for reinforcement learning algorithms. Just because a box is next to a dot doesn't mean that it is the one that belongs there. This is a particularly nasty property of the game. And two, some mistakes we make are irreversible, for instance, pushing a box in a corner can make it impossible to complete the level. If we have an algorithm that tries a bunch of actions and sees if they stick, well, that's not going to work here! It is now hopefully easy to see that this is an obscenely difficult problem, and the DeepMind guys just came up with Imagination-Augmented Agents as a solution for it. So what is behind this really cool name? The interesting part about this novel architecture is that it uses imagination, which is a routine to cook up not only one action, re plans consisting of several steps, and finally, choose one that has the greatest expected reward over the long term. It takes information about the present and imagines possible futures, and chooses the one with the most handsome reward. And as you can see, this is only the first paper on this new architecture and it can already solve a problem with seven boxes. This is just unreal. Absolutely amazing work. And please note that this is a fairly general algorithm that can be used for a number of different problems. This particular game was just one way of demonstrating the attractive properties of this new technique. The paper contains more results and is a great read, make sure to have a look. Also, if you've enjoyed this episode, please consider supporting Two Minute Papers on Patreon. Details are available in the video description, have a look! Thanks for watching and for your generous support, and I'll see you next time!"
96,AI Learns Semantic Style Transfer,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Style transfer is an amazing area in machine learning and AI research, where we take two images. Image number one is an input photograph, and image number two, is the desired style. And the output of this process is the content of image number one with the style of image number two. This first paper opened up an incredible new area of research. As a result, a ton of different variants have emerged in the last two years. Feedforward style transfer for close to real-time results, temporally coherent style transfer for videos, and much, much more. And this one not only outperforms previously existing techniques, but also broadens the horizon of possible style transfer applications. And obviously, a human would be best at doing this because a human has an understanding of the objects seen in these images. And now, hold on to your papers, because the main objective of this is method to create semantically meaningful results for style transfer. It is meant to do well with input image pairs that may look completely different visually, but have some semantic components that are similar. For instance, a photograph of a human face and a drawing of a virtual character is excellent example of that. In this case, this learning algorithm recognizes that they both have noses and uses this valuable information in the style transfer process. As a result, it has three super cool applications. First, the regular photo to style transfer that we all know and love. Second, it is also capable of swapping the style of two input images. Third, and hold on to your papers because this is going to be even more insane. Style or sketch to photo. And we have a plus one here as well, so, fourth, it also supports color transfer between photographs, which will allow creating amazing time-lapse videos. I always try to lure you Fellow Scholars into looking at these papers, so make sure to have a look at the paper for some more results on this. And you can see here that this method was compared to several other techniques, for instance, you can see the cycle consistency paper and PatchMatch. And this is one of those moments when I get super happy, because more than a 170 episodes into the series, we can not only appreciate the quality of these these new results, but we also had previous episodes about both of these algorithms. As always, the links are available in the video description, make sure to have a look, it's going to be a lot of fun. The source code of this project is also available. We also have a ton of episodes on computer graphics in the series, make sure to have a look at those as well. Every now and then I get e-mails from viewers who say that they came for the AI videos, and just in case, watched a recent episode on computer graphics and were completely hooked. Thanks for watching and for your generous support, and I'll see you next time!"
97,Elastoplastic Hair and Cloth Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a piece of elastic cloth modeled from more than a million tiny triangles and its interaction with 7 million colored grains of sand. This is super challenging because of two things: one, we have to compute the elastic deformations when these millions of tiny elements collide, and two, all this, while maintaining two-way coupling. This means that the cloth has an effect on the sand, but the effect of the sand is also simulated on the cloth. In elastic deformations, there are potential interactions between distant parts of the same material and self-collisions may also occur. Previous state of the art techniques were either lacking in these self-collision effects, or the ones that were able to process that also included the fracturing of the material. With this novel work, it is possible to simulate both elastic deformations as you can see here, but, it also supports simulating plasticity as you can see here with the cloth pieces sliding off of each other. Beautiful! This new technique also supports simulating a variety of different types of materials, knitted cloth ponchos, shag carpets, twisting cloth, hair, tearing fiber and more. And it does all this with a typical execution time between 10 to 90 seconds per frame. In these black screens, you see the timing information and the number of particles and triangles used in these simulations. And you will see that there are many scenes where millions of triangles and particles are processed in very little time. It is very rare that we can implement only one technique that takes care of so many kinds of interactions while still obtaining results very quickly. This is insanity. This paper is absolutely top tier bang for the buck and I am really excited to see some more elastoplastic simulations in all kinds of digital media in the future. You know our motto, a couple more papers down the line and having something like this in real-time applications may become a reality. Really cool! If you enjoyed this episode and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. One dollar per month really doesn't break the bank but it is a great deal of help for us in keeping the series going. And your support has always been absolutely amazing and I am so grateful to have so many devoted Fellow Scholars like you in our ranks. Details are in the video description, have a look. Thanks for watching and for your generous support, and I'll see you next time!"
98,Animating Elastic Rods With Sound,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In the series, we talk a lot about photorealistic rendering and making sure that the appearance of our virtual objects is simulated properly. A lot of works on how things look. However, in order to create a more complete sensorial experience, we also have to simulate how these things sound. And today, we're going to have a look at a really cool piece of work that simulates the sound of virtual elastic rods made of aluminum, steel, oak tree, and rubber. And of course, before you ask, this also means that there will be sound simulations of everyone's favorite toy the walking slinky. As for all papers that have anything to do with sound synthesis, I recommend using a pair of headphones for this episode. The sound emerging from these elastic rods is particularly difficult to simulate because of the fact that sound frequencies vary quite a bit over time, and the objects themselves are also in motion and subject to deformations during the simulation. And as you will see with the slinky, we potentially have tens of thousands of contact events in the meantime. Let's have a look at some results! For the Fellow Scholars who are worried about the validity of these Star Wars sounds, I know you're out there, make sure to watch the video until the end. The authors of the paper proposed a dipole model to create these simulations. Dipoles are typically used to approximate electric and magnetic fields in physics, and in this case, it is really amazing to see an application of it for sound synthesis. For instance, in most cases, these sound waves are typically symmetric around 2D cross sections of these objects, which can be described by a dipole model quite well. Also, it is computationally quite effective and can eliminate these lengthy pre-computation steps that are typically present in previous techniques. There are also comparisons against the state of the art, and we can hear how much richer the sound of this new technique is. And as you know all too well, I love all papers that have something to do with the real world around us. And the reason for this is that we can try the very best kind of validation for these algorithms and this is, when we let reality be our judge. Some frequency plots are also available to validate the output of the algorithm against the real-world sound samples from the lab. It is really amazing to see that we can use science to breathe more life in our virtual worlds. Thanks for watching and for your generous support, and I'll see you next time!"
99,Interactive Green-Screen Keying,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In the film industry, we can often see footage of a human walking on the Moon, fighting underwater, or appearing in any environment without actually going there. To do this, a piece of footage of the actor is recorded in front of a green screen, and then, the background of the scene is changed to something else. This process is called green-screen keying, and in theory, this sounds simple enough, but make no mistake this is a challenging problem. Here's why. Issue number one is that separating the foreground from the background is non-trivial and is not a fully automatic process. Let's call this semi-automatic because the compositing artist starts drawing these separation masks, and even though there is some help from pre-existing software, it still takes quite a bit of manual labor. For instance, in this example, it is extremely difficult to create a perfect separation between the background and the hair of the actor. Our eyes are extremely keen on catching such details, so even the slightest inaccuracies are going to appear as glaring mistakes. This takes a ton of time and effort from the side of the artist, and we haven't even talked about tracking the changes between frames as we're talking about video animations. I think it is now easy to see that this is a hugely relevant problem in the post production of feature films. And now, on to issue number two, which is subtracting indirect illumination from this footage. This is a beautiful light transport effect where the color of different diffuse objects bleed onto each other. In this case, the green color of the background bleeds onto the karate uniform. That is normally a beautiful effect, but here, it is highly undesirable because if we put this character in a different environment, it won't look like it belongs there. It will look more like one of those super fake Photoshop disasters that we see everywhere on the internet. And this technique offers a novel solution to this keying problem. First, we are asked to scribble on the screen and mark the most dominant colors of the scene. This we only have to do once even though we're processing an entire video. As a result, we get an initial map where we can easily fix some of the issues. This is very easy and intuitive, not like those long sessions spent with pixel-by-pixel editing. These colors are then propagated to the entirety of the animation. The final results are compared to a ton of already existing methods on the market and this one smokes them all. However, what is even more surprising is that it is also way better than what an independent artist produced which took ten times that long. Similar comparisons are also made for removing indirect illumination, which is also referred to as color unmixing in the paper. It is also shown that the algorithm is not too sensitive to this choice of dominant colors, so there is room for amazing followup papers to make the process a bit more automatic. Thanks for watching and for your generous support, and I'll see you next time!"
100,Refocusing Videos With Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Whenever we take an image with our camera, and look at it after an event, we often feel that many of them are close to perfect, if only it was less blurry, or the focus distance was a bit further away. But the magic moment is now gone, and there is nothing to do other than cursing at the blurry footage that we're left with when showing it to our friends. However, if we have access to light fields, we can change some camera parameters after the photo was taken. This includes changing the focal distance, or even slightly adjusting the viewpoint of the camera. How cool is that! This can be accomplished by a light field camera which is also referred to as a plenoptic camera. This tries to record not only light intensities, but the direction of incoming light as well. Earlier, this was typically achieved by using an array of cameras. That's both expensive and cumbersome. And here comes the problem with using only one light field camera: because of the increased amount of data that they have to record, current light field cameras are only able to take 3 frames per second. That's hardly satisfying if we wish to do this sort of post-editing for videos. This work offers a novel technique to remedy this situation by attaching a standard camera to this light field camera. The goal is that the standard camera has 30, so tons of frames per second, but with little additional information, and the light field camera, which has only a few frames per second, but, with a ton of additional information. If we stitch all this information together in a smart way, maybe it is a possibility to get full light field editing for videos. Earlier, we have talked about interpolation techniques that can fill some of the missing frames in videos. This way, we can fill in maybe every other frame in a footage, or we can be a bit more generous than that. However, if we're shown 3 frames a second, and we have to create a smooth video by filling the blanks would almost be like asking an algorithm to create a movie from a comic book. This would be awesome, but we're not there yet. Too much information is missing. This stitching process works with a bit more information than this, and the key idea is to use two convolutional neural networks to fill in the blanks: one is used to predict flows, which describe the movements and rotations of the objects in the scene, and one to predict the final appearance of the objects. Basically, one for how they move, and one for how they look. And the results are just absolutely incredible. It is also blazing fast and takes less than a tenth of a second to create one of these new views. Here, you can see how the final program is able to change the focal distance of any of the frames in our video, or we can even click on something in the image to get it in focus. And all this is done after the video has been taken. The source code of this project is also available. With some more improvements, this could be tremendously useful in the film industry, because the directors could adjust their scenes after the shooting, and not just sigh over the inaccuracies and missed opportunities. And this is just one of many possible other applications. Absolutely amazing. If you enjoyed this episode, don't forget to subscribe to Two Minute Papers and also make sure to click the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!"
101,Phace: Physics-based Face Modeling and Animation,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about transferring our gestures onto a virtual human's face in a way that is physically correct. This means that not only the changes in the facial geometry are transferred to a digital character, no, no, no. Here's how it works. This piece of work uses a really cool digital representation of our face that contains not only geometry, but there is also information about the bone and flesh and muscle structures as well. This means that it builds on a physically accurate model, which synthesizes animations where this human face is actuated by the appropriate muscles. We start out with a surface scan of the user, which through a registration step, is then converted to a set of expressions that we wish to achieve. The inverse physics module tries to guess exactly which muscles are used and how they are used to achieve these target expressions. The animation step takes the information of how the desired target expressions evolve in time, and some physics information, such as gravity or wind, and the forward physics unit computes the final simulation of the digital character. So while we're talking about the effects of gravity and wind, here you can see how this can create more convincing outputs because these characters really become a part of their digital environment. As a result, the body mass index of a character can also be changed in both directions, slimming or fattening the face. Lip enhancement is also a possibility. If we had super high resolution facial scans, maybe a followup work could simulate the effects of botox injections. How cool would that be? Also, one of my favorite features of this technique is that it also enables artistic editing. By means of drawing, we can also specify a map of stiffness and mass distributions, and if we feel cruel enough, we can create a barely functioning human face to model and animate virtual zombies. Imagine what artists could do with this, especially in the presence of super high resolution textures and photorealistic rendering. Oh, my! Another glimpse of the future of computer graphics and animation. Make sure to have a look at the paper for more applications, for instance, they also demonstrate the possibility of modifying the chin and the jawbone. They even have some result in simulating the effect of Bell's palsy, which is the paralysis of facial muscles on one side. While we're at this high note of illnesses, if you enjoyed this episode and would like to support us, you can pick up really cool perks like early access for all of these episodes on Patreon. The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time."
102,Real-Time Hair Rendering With Deep Opacity Maps,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In earlier episodes, we've seen plenty of video footage about hair simulations and rendering. And today we're going to look at a cool new technique that produces self-shadowing effects for hair and fur. In this image pair, you can see this drastic difference that shows how prominent this effect is in the visual appearance of hair. Just look at that. Beautiful. But computing such a thing is extremely costly. Since we have a dense piece of geometry, for instance, hundreds of thousands of hair strands, we have to know how each one occludes the other ones. This would take hopelessly long to compute. To even get a program that executes in a reasonable amount of time, we clearly need to simplify the problem further. An earlier technique takes a few planes that cut the hair volume into layers. These planes are typically regularly spaced outward from the light sources and it is much easier to work with a handful of these volume segments than with the full geometry. The more planes we use, the more layers we obtain, and the higher quality results we can expect. However, even if we can do this in real time, we will produce unrealistic images when using around 16 layers. Well of course, we should then crank up the number of layers some more! If we do that, for instance by now using 128 layers, we can expect better quality results, but we'll be able to process an image only twice a second, which is far from competitive. And even then, the final results still contain layering artifacts and are not very close to the ground truth. There has to be a better way to do this. And with this new technique called Deep Opacity Maps, these layers are chosen more wisely, and this way, we can achieve higher quality results with only using 3 layers, and it runs easily in real time. It is also more memory efficient than previous techniques. The key idea is that if we look at the hair from the light source's point of view, we can record how far away different parts of the geometry are from the light source. Then, we can create the new layers further and further away according to this shape. This way, the layers are not planar anymore, they adapt to the scene that we have at hand and contain significantly more useful occlusion information. As you can see, this new technique blows all previous methods away and is incredibly simple. I have found an implementation from Philip Rideout, the link to this is available in the video description. If you have found more, let me know and I'll include your findings in the video description for the fellow tinkerers out there. The paper is ample in comparisons, make sure to have a look at that too. And sometimes I get some messages saying ""Károly, why do you bother covering papers from so many years ago, it doesn't make any sense!"". And here you can see that part of the excitement of Two Minute Papers is that the next episode can be about absolutely anything. The series has been mostly focusing on computer graphics and machine learning papers, but don't forget, that we also have an episode on whether we're living in a simulation, or the Dunning-Kruger effect and so much more. I've put a link to both of them in the video description for your enjoyment. The other reason for covering older papers is that a lot of people don't know about them and if we can help just a tiny bit to make sure these incredible works see more widespread adoption, we've done our job well. Thanks for watching and for your generous support, and I'll see you next time!"
103,Visualizing Fluid Flow With Clebsch Maps,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Everyone who watches this series knows that among many other scientific topics, I am severely addicted to fluid simulations, and today, it's time to relapse! And this time, we're going to run wind tunnel tests on hummingbirds. Typically, when a new engine, airplane, or even a new phone is being designed, we're interested in knowing how the heat flow and dissipation will look like, preferably before we're designing an object. To do so, we often run some virtual wind tunnel tests and optimize our design until we're happy with the results. Then, we can proceed to build these new contraptions. Simulating the pressure distribution and aerodynamic forces is a large topic, however, visualizing these results is at least as well-studied and difficult as writing a simulator. What is it exactly that we're interested in? Even if we have an intuitive particle-based simulation, millions and millions of particles, it is clearly impossible to show the path for every one of them. Grid-based simulations are often even more challenging to visualize well. So how do we choose what to visualize and what not to show on the screen? And in this paper, we can witness a new way of visualizing velocity and vorticity fields. And this visualization happens through Clebsch-maps. This is a mathematical transformation where we create a sphere, and a set of points on this sphere correspond to vortex lines and their evolution over time. However, if instead of only points, we pick an entire region on this sphere, as you can see the north and south pole regions here, we obtain vortex tubes. These vortex tubes provide an accurate representation of the vorticity information within the simulation, and this is one of the rare cases where the validity of such a solution can also be shown. Such a crazy idea, loving it! And with this, we can get a better understanding of the air flow around the wings of a hummingbird, but we can also learn more from pre-existing NASA aircraft datasets. Have a look at these incredible results. Publishing a paper at the SIGGRAPH conference is an incredible feat that typically takes a few brilliant guys and several years of unbelievably hard work. Well, apparently this is not such a challenge for Albert Chern, who was also the first author of this and the Schrödinger's Smoke paper just a year ago that we reported on. He is doing incredible work at taking a piece of mathematical theory and showing remarkable applications of it in new areas where we would think it doesn't belong at all. The link is available in the video description, for both this and the previous works, make sure to have a look. There is lots of beautifully written mathematics to be read there that seems to be from another world. It's a truly unique experience. The paper reports that the source code is also available, but I was unable to find it yet. If you have found a public implementation, please let me know and I'll update the video description with your link. Thanks for watching and for your generous support, and I'll see you next time."
104,AI Learns Visual Common Sense With New Dataset,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, we are going to talk about a new endeavor to teach some more common sense to learning algorithms. If you remember, in an earlier episode, we talked about an excellent work by Andrej Karpathy, who built an algorithm that looked at an input image, and described, in a full, well-formed sentence what is depicted there. By the way, he recently became director of AI at Tesla. Before that, he worked at OpenAI, freshly after graduating with a PhD. Now that is a scholarly career if I've ever seen one! Reading about this earlier work was one of those moments when I really had to hold on to my papers not to fall out of the chair, but of course, as it should be with every new breakthrough, the failure cases were thoroughly discussed. One of the the motivations for this new work is that we could improve the results by creating a video database that contains a ton of commonly occurring events that would be useful to learn. These events include, moving and picking up, or holding, poking, throwing, pouring, or plugging in different things, and much more. The goal is that these neural algorithms would get tons of training data for these, and would be able to distinguish whether a human is showing them something, or just moving things about. The already existing video databases are surprisingly sparse in this sort of information, and in this new, freshly published dataset, we can learn on a 100.000 labeled videos to accelerate research in this direction. I love how many these works are intertwined and how followup research works try to address the weaknesses of previous techniques. Some initial results with learning on this dataset are also reported to kick things off, and they seem quite good if you look at the results here, but since this was not the focus of the paper, we shouldn't expect superhuman performance. However, as almost all papers in research are stepping stones, two more followup papers down the line, this will be an entirely different discussion. I'd love to report back to you on the progress later. Super excited for that. Thanks for watching and for your generous support, and I'll see you next time."
105,DeepMind's AI Learns Superhuman Relational Reasoning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is from the Google DeepMind guys, and is about teaching neural networks to be capable of relational reasoning. This means that we can present the algorithm with an image and ask it relatively complex relational questions. For instance, if we show it this image and ask ""What is the color of the object that is closest to the blue object?"", it would answer ""red"". This is a particularly difficult problem because all the algorithm has access to is a bunch of pixels. In computer code, it is near impossible to mathematically express that in an image, something is below or next to something else, especially in 3 dimensional scenes. Beyond a list of colors, this requires a cognitive understanding of the entirety of the image. This is something that we humans are amazingly good at, but computer algorithms are dreadful for this type of work. And this work almost feels like teaching common sense to a learning algorithm. This is accomplished by augmenting and already existing neural network with a relational network module. This is implemented on top of a recurrent neural network that we call long short-term memory or LSTM, that is able to process sequences of information, for instance, an input sentence. The more seasoned Fellow Scholars know that we've talked about LSTMs in earlier episodes, and of course, as always, the video description contains these episodes for your enjoyment. Make sure to have a look, you'll love it. As you can see in this result, this relational reasoning also works for three dimensional scenes as well. The aggregated results in the paper show that this method is not only leaps and bounds beyond the capabilities of already existing algorithms, but, and now, hold on to your papers in many cases, it also shows superhuman performance. I love seeing these charts in machine learning papers where several learning algorithms and humans are benchmarked on the same tasks. This paper was barely published and there is already a first, unofficial public implementation and two research papers have already referenced it. This is such a great testament to the incredible pace of machine learning research these days. To say that it is competitive would be a huge understatement. Achieving high quality results in relational reasoning is an important cornerstone for achieving general intelligence, and even though there are still much, much more to do, today is one of those days when we can feel that we're a part of the future. The failure cases are also reported in the paper and are definitely worthy of your time and attention. When I asked for permissions to cover this paper in the series, all three scientists from DeepMind happily answered yes within 30 minutes. That's unbelievable. Thanks guys! Also, some of these questions sound like ones that we would get in the easier part of an IQ test. I wouldn't be very surprised to see a learning algorithm complete a full IQ test with flying colors in the near future. If you enjoyed this episode, and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. This way, we can make better videos for your enjoyment. We've recently reached a new milestone, which means that part of these funds will be used to empower research projects. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time."
106,Text-based Editing of Audio Narration,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Close enough! As you have probably noticed, today we are going to talk about text to speech, or TTS in short. TTS means that we write a piece of text, and a computer synthesized voice will read it aloud for us. This is really useful for reading the news, or creating audiobooks that don't have any official voiceovers. This work was done by researchers at Princeton University and Adobe and is about text-based audio narration editing. This one is going to be crazy good. The Adobe guys like to call this the Photoshop of voiceovers. In a normal situation, we have access to a waveform and if we wish to change anything in a voiceover, we need to edit it. Editing waveforms by hand is extremely difficult traditional techniques often can't even reliably find the boundaries between words or letters, let alone edit them. And with this technique, we can cut, copy, and even edit this text and the waveforms will automatically be transformed appropriately using the same voice. We can even use new words that have never been uttered in the original narration. It solves an optimization problem where the similarity, smoothness and the pace of the original footage is to be matched as closely as possible. One of the excellent new features is that we can even choose from several different voicings for the new word and insert the one that we deem the most appropriate. For expert users, the pitch and duration is also editable. It is always important to have a look at a new technique and make sure that it works well in practice, but in science, this is only the first step. There has to be more proof that a new proposed method works well in a variety of cases. In this case, a theoretical proof by means of mathematics is not feasible, therefore a user study was carried out where listeners were shown synthesized and real audio samples and had to blindly decide which was which. The algorithm was remarkably successful at deceiving the test subjects. Make sure to have a look at the paper in the description for more details. This technique is traditional in a sense that it doesn't use any sort of neural networks, however, there are great strides being made in that area as well, which I am quite excited to show you in future episodes. And due to some of these newer video and audio editing techniques, I expect that within the internet forums, fake news is going to be an enduring topic. I hope that in parallel with better and better text and video synthesis, there will be an arms race with other methods that are designed to identify these cases. A neural detective, if you will. And now, if you excuse me, I'll give this publicly available TTS one more try and see if I can retire from narrating videos. Thanks for watching and for your generous support, and I'll see you next time. Yep. Exact same thing. Bet you didn't even notice it."
107,Efficient Yarn-based Cloth Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is about creating stunning cloth simulations that are rich in yarn-to-yarn contact. Normally, this is a challenging problem because finding and simulating all the possible contacts between tens of thousands of interlinked pieces of geometry is a prohibitively long process. Also, due to the many different kinds of possible loop configurations, these contacts can take an awful lot of different shapes, which all need to be taken into consideration. Since we are so used to see these garments moving about in real life, if someone writes a simulator that is off just by a tiny bit, we'll immediately spot the difference. I think it is now easy to see why this is a highly challenging problem. This technique optimizes this process by only computing some of the forces that emerge from these yarns pulling each other, and only trying to approximate the rest. The good news is that this approximation is carried out with temporal coherence. This means that these contact models are retained through time and are only rebuilt when it is absolutely necessary. The regions marked with red in these simulations show the domains that are found to be undergoing significant deformation, therefore we need to focus most of our efforts in rebuilding the simulation model for these regions. Look at these results, this is unbelievable. There is so much detail in these simulations. And all this was done seven years ago. In research and technology, this is an eternity. This just blows my mind. The results are also compared against the expensive reference technique as well. And you can see that the differences are minuscule, but the new, improved technique offers a 4 to 5-time speedup over that. For my research project, I also run many of these simulations myself, and many of these tasks take several all nighters to compute. If someone would say that each of my all-nighters would now count as 5, I'd be absolutely delighted. If you haven't subscribed to the series, please make sure to do so and please also click the bell icon to never miss an episode. We have tons of awesome papers to come in the next few episodes. Looking forward to seeing you there! Thanks for watching and for your generous support, and I'll see you next time!"
108,Iridescent Light Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. From many fond childhood memories, most of us are quite fond of the colorful physical appearance of bubbles and fuel-water mixtures. We also know surprisingly little about this peculiar phenomenon where the color of an object changes when we either turn our head or change the lighting. This happens in a very colorful manner, and physicists like to call this iridescence or goniochromism. What is even less known, is that if we try to use a light simulation program to make an image with leather, we'll be surprised to see that it also shows a pronounced goniochromatic effect. An even more less known fact is that quite a few birds, insects, minerals, sea shells, and even some fruits are iridescent as well. I've added links to some really cool additional readings to the video description for your enjoyment. This effect is caused by materials that scatter different colors of light in different directions. A white incoming light is therefore scattered not in one direction, but in a number of different directions, sorted by their colors. This is why we get these beautiful, rainbow-colored patterns that we all love so much. Now that we know what iridescence is, the next step is obviously to infuse our light simulation programs to have this awesome feature. This paper is about simulating this effect with microfacets, which are tiny microstructures on the surface of rough objects. And with this, it is now suddenly possible to put a thin iridescent film onto a virtual object and create a photorealistic image out of it. If you are into math and would like to read about some tasty spectral integration in the frequency space with Fourier transforms, this paper is for you. If you are not a mathematician, also make sure to have a look because the production quality of this paper is through the roof. The methodology, derivations, comparisons are all really crisp. Loving it. If you have a look, you will get a glimpse of what it takes to create a work of this quality. This is one of the best papers in photorealistic rendering I've seen in a while. In the meantime, I am getting more and more messages from you Fellow Scholars who tell their stories on how they chose to turn their lives around and started studying science because of this series. Wow. That's incredibly humbling, and I really don't know how to express my joy for this. I always say that it's so great to be a part of the future, and I am delighted to see that some of you want to be a part of the future and not only as an observer, but as a research scientist. This sort of impact is stronger than the absolute best case scenario I have ever dreamed of for the series. Thanks for watching and for your generous support, and I'll see you next time!"
109,Simulating Cuts On Virtual Bodies,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is about the absolute favorite thing of computer graphics researchers: destroying virtual objects in the most creative ways. This is the only place on Earth where words like deformable bodies and cutting can be used in the same sentence and be delighted about it. This time around, we are going to cut and dismember every virtual object that stands in our way. And then some more. In these animations, we have complex 3D geometry, and the objective is to change these geometries in a way that remains physically correct even in the presence of complex cut surfaces. When such a cut happens, traditional techniques typically delete and duplicate parts of the geometry close to the cut. This is a heavily simplified solution that leads to inaccurate results. Other techniques try to rebuild parts of the geometry that are affected by the cut. This is what computer graphics researchers like to call remeshing, and it works quite well, but it takes ages to perform. Also, is still has drawbacks, for instance, quantities like temperature and deformations also have to be transferred to the new geometry, which is non-trivial to execute properly. In this work, a new technique is proposed that is able to process really complex cuts without creating new geometry. No remeshing takes place, but the mass and stiffness properties of the materials are retained correctly. Also, the fact that it minimizes the geometric processing overhead leads to a not only simpler, but a more efficient solution. There is so much visual detail in the results that I could watch this video ten times and still find something new in there. There are also some horrifying, Game of Thrones kinda experiments in this footage. Watch out! Ouch! The presentation of the results and the part of the video that compares against a previous technique is absolutely brilliant. You have to see it. The paper is also remarkably well written, make sure to have a look at that too. The link is available in the video description. I am really itching to make some longer videos where we can go into some of these derivations and build a strong intuitive understanding of them. That sounds like a ton of fun, and if this could ever become a full time endeavor, I am more than enthused to start doing more and work on bonus videos like that. If you enjoyed this episode, don't forget to hit the like button and subscribe to the series. Normally, it's up to YouTube to decide whether you get a notification or not, so make sure to click the bell icon as well to never miss a Two Minute Papers episode. Thanks for watching and for your generous support, and I'll see you next time!"
110,DeepMind's AI Creates Images From Your Sentences,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is one of those new, absolutely insane papers from the Google DeepMind guys. You are going to see a followup work to an algorithm that looks at a bunch of images and from that, it automatically learns the concept of birds, human faces or coral reefs, so much so that we're able to write a new sentence, and it will generate a new, close to photorealistic image from this written description. This network is capable of creating images that are significantly different than the ones it has been trained on. This already sounds like science fiction. Completely unreal. This work goes by the name PixelCNN. We'll discuss a followup work to that in a moment. The downside of this method is that these images are generated pixel by pixel, and many of these pixels depend on their neighborhoods. For instance, if I start to draw one pixel of the beak of a bird, the neighboring pixels have to adhere to this constraint and have to be the continuation of the beak. Clearly, these images have a lot of structure. This means that we cannot do this process in parallel, but create these new images one pixel at a time. This is an extremely slow and computationally expensive process, and hence, the original paper showed results with 32x32 and 64x64 images at most. As we process everything sequentially, the execution time of the algorithm scales linearly with the number of pixels we can generate. It is like a factory where there are a ton of assembly lines, but only one person to run around and operate all of them. Here, the goal was to start generating different regions of these images independently, but only in cases when these pixels are not strongly correlated. For instance, doing this with neighbors is a no-go. This is possible, but extremely challenging, and the paper contains details on how to select these pixels and when we can pretend them to be independent. And now, feast your eyes upon these spectacular results. If we're looking for ""A yellow bird with a black head, orange eyes and an orange bill"", we're going to see much more detailed images. The complexity of the new algorithm scales with the number of pixels not linearly, but in a logarithmic manner, which is basically the equivalent of winning the jackpot in terms of parallelization, and it often results in a more than 100 times speedup. This is a factory that's not run by one guy, but one that works properly. The lead author, Scott Reed has also published some more amazing results on twitter as well. In these examples, we can see the evolution of the final image that is generated by the network. It is an amazing feeling to be a part of the future. And note that there is a ton of challenges with the idea, this is one of those typical cases when the idea is only the first step, and execution is king. Make sure to have a look at the paper for more details. According to our regular schedule, we try our best to put out two videos every week. That's eight episodes a month. If you feel that eight of these episodes is worth a dollar for you, please consider supporting us on Patreon. This way, we can create more elaborate episodes for you. The channel is growing at a remarkable rate, and your support has been absolutely amazing. I am honored to have an audience like you Fellow Scholars. We are quite close to hitting our next milestone. And this milestone will be about giving back more to the scientific community and empowering other research projects. I've put a link to our Patreon page with the details in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
111,Style Transfer For Fluid Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We've seen a lot of fluid and smoke simulations throughout the series. In each of these cases, the objective was to maximize the realism of these animations, often to the point where they are indistinguishable from reality. However, there are cases where creating photorealistic footage is not the main objective. Artists often seek to imbue these fluid and smoke simulations with their own distinctive style, and this style needs not to be photorealistic. It can be cartoonish, black and white, or take a variety of different color schemes. But unfortunately, to obtain such an effect, we have to sit down, get a bunch of papers, and draw the entirety of the animation frame by frame. And of course, to accomplish this, we also need to be physicists and know the underlying laws of fluid dynamics. That's not only borderline impossible, but extremely laborious as well. It would be really cool to have an algorithm that is somehow able to learn our art style and apply it to a fluid or smoke simulation sequence. But the question is, how do we exactly specify this style? Have a look at this really cool technique, I love the idea behind it: first we compute a classical smoke simulation, then, we freeze a few frames and get the artist to colorize them. After that, the algorithm tries to propagate this artistic style to the entirety of the sequence. Intuitively, this is artistic style transfer for fluid animations, but, without using any machine learning techniques. Here, we are doing patch-based regenerative morphing. This awesome term refers to a technique that is trying to understand the direction of flows and advect the colored regions according to it in a way that is both visually and temporally coherent. Visually coherent means that it looks as close to plausible as we can make it, and temporally coherent means that we're not looking only at one frame, but a sequence of frames, and the movement through these neighboring frames has to be smooth and consistent. These animation sequences were created from 8 to 9 colorized frames, and whatever you see happening in between was filled in by the algorithm. And again, we're talking about the artistic style here, not the simulation itself. A fine, handcrafted work in the world dominated by advanced learning algorithms. This paper is a bit like a beautiful handmade automatic timepiece in the era of quartz watches. If you enjoyed this episode, please make sure to leave a like on the video, and don't forget to subscribe to get a glimpse of the future on the channel, twice a week. Thanks for watching and for your generous support, and I'll see you next time!"
112,AI Learns To Create User Interfaces (pix2code),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Creating applications for mobile Android and iOS devices is a laborious endeavor which most of the time, includes creating a graphical user interface. These are the shiny front-end interfaces that enable the user to interact with the back-end of our applications. So what about an algorithm that learns how to create these graphical user interfaces and automates part of this process? This piece of work takes one single input image that we can trivially obtain by making a screenshot of the user interface, and it almost immediately provides us with the code that is required to recreate it. What an amazing idea! The algorithm supports several different target platforms. For instance, it can give us code for iOS and Android devices. This code we can hand over to a compiler which will create an executable application. This technique also supports html as well for creating websites with the desired user interface. Under the hood, a domain specific language is being learned, and using this, it is possible to have a concise text representation of a user interface. Note that's by no means the only use of domain specific languages. The image of the graphical user interface is learned by a classical convolutional neural network, and this text representation is learned by a technique machine learning researchers like to call Long Short Term Memory. LSTM in short. This is a neural network variant that is able to learn sequences of data and is typically used for language translation, music composition, or learning all the novels of Shakespeare and writing new ones in his style. If you were wondering why these examples are suspiciously specific, we've had an earlier episode about this, I've put a link to it in the video description. Make sure to have a look, you are going to love it. Also, this year it will have its twentieth year anniversary. Live long and prosper, little LSTM! Now, I already see the forums go up in flames. Sweeping generalizations, far-reaching statements on front end developers around the world getting fired and all that. I'll start out by saying that I highly doubt that this work would mean the end of front end development jobs in the industry. However, what I do think is that with a few improvements, it can quickly prove its worth by augmenting human labor and cutting down the costs of implementing graphical user interfaces in the future. This is another testament to the variety of tasks modern learning algorithms can take care of. The author also has a GitHub repository with a few more clarifications, stating that the source code of the project and the dataset will be available soon. Tinkerers rejoice! Thanks for watching and for your generous support, and I'll see you next time!"
113,Simulating Wet Sand,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. After around 160 episodes into Two Minute Papers, I think it is no secret to anyone that I am helplessly addicted to fluid simulations, so you can already guess what this episode will be about. I bet you will be as spellbound by this beautiful footage of wet sand simulations as I was when I've first seen it. Before you ask, yes, I have attempted to prepare some slow-motion action too! As you remember, simulating the motion of fluids involves solving equations that tell us how the velocity and the pressure evolves in time. Now, the 3D world we live in is a continuum, and we cannot solve these quantities everywhere because that would take an infinite amount of time. To alleviate this, we can put a grid in our virtual world and obtain these quantities only in these gridpoints. The higher the resolution the grid is, the more realistic the animations are, but the computation time also scales quite poorly. It is really not a surprise that we have barely seen any wet sand simulations in the visual effects industry so far. Here, we have an efficient algorithm to handle these cases, and as you will see, this is not only extremely expensive to compute, but nasty stability issues also arise. Have a look at this example here. These are sand simulations with different cohesion values. Cohesion means the strength of intermolecular forces that hold the material together. The higher cohesion is, the harder it is to break the sand up, the bigger the clumps are. This is an important quantity for our simulation because the higher the water saturation of this block of sand, the more cohesive it is. Now, if we try to simulate this effect with traditional techniques on a coarse grid, we'll encounter a weird phenomenon: namely, the longer our simulation runs, the larger the volume of the sand becomes. An excellent way to demonstrate this phenomenon is using these hourglasses, where you can clearly see that after only a good couple turns, the amount of sand within is significantly increased. This is particularly interesting, because normally, in classical fluid simulations, if our grid resolution is insufficient, we typically encounter water volume dissipation, which means that the total amount of mass in the simulation decreases over time. Here, we have the exact opposite, like in a magic trick, after every turn, the volume gets inflated. That's a really peculiar and no less challenging problem. This issue can be alleviated by using a finer grid, which is, as we know, extremely costly to compute, or, the authors proposed a volume fixing method to take care of this without significantly increasing the execution time of the algorithm. Make sure to have a look at the paper, which is certainly my kind of paper: lots of beautiful physics and a study on how to solve these equations so that we can obtain an efficient wet sand simulator. And also, don't forget, a fluid paper a day keeps the obsessions away. In the meantime, a word about the Two Minute Papers shirts. I am always delighted to see you Fellow Scholars sending over photos of yourselves proudly posing with your newly obtained shirts for the series. Thanks so much and please, keep them coming! They are available through twominutepapers.com for the US, and the EU and worldwide link is also available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
114,Algorithmic Beautification of Selfies,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we're going to talk about a rigorous scientific topic, none other than the creation of the perfect selfie photo. By definition selfies are made by us, which means that these are typically short-range photos, and due to the perspective distortion of the camera lens, we often experience unpleasant effects like the heavy magnification the nose and the forehead. And get this, this technique enables us to take a photo and after that, edit the perceived camera distance for it without changing anything else. Basically, algorithmic beautification! This technique works the following way: we analyze the photo and try to figure out how distant the camera was when the photo was taken. Then, we create a digital model of the perspective camera and create a 3D model of the face. This is a process that mathematicians like to call fitting. It means that if we know the optics of perspective cameras, we can work backwards from the input photo that we have, and find an appropriate setup that would result in this photo. Then, we will be able to adjust this distance to even out the camera lens distortions. But that's not all, because as we have a digital 3D model of the face, we can do even more. For instance, we can also rotate it around in multiple directions. To build such a 3D model, we typically try to locate several well-recognizable hotspots on the face, such as the chin, eyebrows, nose stem, the region under the nose, eyes and lips. However, as these hotspots lead to a poor 3D representation of the human face, the authors added a few more of these hotspots to the detection process. This still takes less than 5 seconds. Earlier, we also talked about a neural network-based technique that judged our selfie photos by assigning a score to them. I would absolutely love to see how that work would react to a before and after photo that comes from this technique. This way, we could formulate this score as a maximization problem, and as a result, we could have an automated technique that truly creates the perfect selfie photo through these warping operations. The best kind of evaluation is when we let reality be our judge, and use images that were taken closer or farther away and compare the output of this technique against them. These true images bear the ground truth label throughout this video. The differences are often barely perceptible, and to provide a better localization of the error, some difference images are shown in the paper. If you are into stereoscopy, there is also an entire section about that as well. The authors also uploaded an interactive version of their work online that anyone can try, free of charge. So as always, your scholarly before and after selfie experiments are more than welcome in the comments section. Whether you are already subscribed to the series or just subscribing now, which you should absolutely do, make sure to click the bell icon to never miss an episode. We have lots of amazing works coming up in the next few videos. Hope to see you there again. Thanks for watching and for your generous support, and I'll see you next time!"
115,Simulating Honey Coiling,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This episode is about simulating a beautiful phenomenon in nature, the buckling and coiling effect of honey. Mmm! This effect is due to the high viscosity of materials like honey, which means that they are highly resistant against deformation. Water, however, is much less viscous as it is held together by weaker intermolecular forces, therefore it is easier to deform, making it so easy to pour it into a glass. We had an earlier episode on honey buckling, and as every seasoned Fellow Scholar already knows, the link is available in the video description. One key difference of this work is that the older solution was built upon a Lagrangian approach, which means that the simulation consists of computing the velocities and the pressure that acts on these particles. It is a particle-based simulation. Here, a solution is proposed for the Eulerian approach, which means that we do not compute these quantities everywhere in the continuum of space, but we use a fine 3D grid, and we compute these quantities only in these gridpoints. No particles to be seen anywhere. There are mathematical techniques to try to guess what happens between these individual gridpoints, and this process is referred to as interpolation. So normally, in this grid-based approach, if we wish to simulate such a buckling effect, we'll be sorely disappointed because what we will see is that the surface details rapidly disappear due to the inaccuracies in the simulation. The reason for this is that the classical grid-based simulators utilize a technique that mathematicians like to call operator splitting. This means that we solve these fluid equations by taking care of advection, pressure, and viscosity separately. Separate quantities, separate solutions. This is great, because it eases the computational complexity of the problem, however, we have to pay a price for it in the form of newly introduced inaccuracies. For instance, some kinetic and shear forces are significantly dampened, which leads to a loss of detail for buckling effects with traditional techniques. This paper introduces a new way of efficiently solving these operators together in a way that these coupling effects are retained in the simulation. The final solution not only looks stable, but is mathematically proven to work well for a variety of cases, and it also takes into consideration collisions with other solid objects correctly. I absolutely love this, and anyone who is in the middle of creating a new movie with some fluid action going on has to be all over this new technique. And, the paper is absolutely amazing. It contains crystal clear writing, many paragraphs are so tight that I'd find it almost impossible to cut even one word from them, yet it is still digestible and absolutely beautifully written. Make sure to have a look, as always, the link is available in the video description. These amazing papers are stories that need to be told to everyone. Not only to experts. To everyone. And before creating these videos, I always try my best to be in contact with the authors of these works. And nowadays, many of them are telling me that they were really surprised by the influx of views they got after they were showcased in the series. Writing papers that are featured in Two Minute Papers takes a ridiculous amount of hard work, and after that, the researchers make them available for everyone free of charge. And now, I am so glad to see them get more and more recognition for their hard work. Absolutely amazing. Thanks for watching and for your generous support, and I'll see you next time!"
116,Designing Decorative Joinery for Furniture,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is about designing and creating furniture with pieces that are geometrically interlocked. Such pieces not only have artistic value, but such structures can also enhance the integrity and sturdiness of a piece of furniture. This piece of work takes a simple 2D drawing of this interlocking structure and assembles the required pieces for us to build a 3D model from them. This drawing can be done with one of the most user friendly modeler program out there, Google SketchUp. This can be used even by novices. From these disassembled parts, it is highly non-trivial to create a 3D printable model. For instance, it is required that these pieces can be put together with one translational motion. Basically, all we need is one nudge to put two of these pieces together. If you ever had a new really simple piece of furniture from Ikea, had a look at the final product at the shop, and thought, ""well, I only have 10 minutes to put this thing together, but anyway, how hard can it be""? And you know, 3 hours of cursing later, the damn thing is still not completely assembled. If you had any of those experiences before, this one push assembly condition is for you. And the algorithm automatically finds a sequence of motions that assembles our target 3D shape, and because we only have 2D information from the input, it also has to decide how and where to extrude, thicken, or subtract from these volumes. The search space of possible motions is immense, and we have to take into consideration that we don't even know if there is a possible solution for this puzzle at all! If this the case, the algorithm finds out and proposes changes to the model that make the construction feasible. And if this wasn't enough, we can also put this digital furniture model into a virtual world where gravitational forces are simulated to see how stable the final result is. Here, the proposed yellow regions indicate that the stability of this table could be improved via small modifications. It is remarkable to see that a novice user who has never done a minute of 3D modeling can create such a beautiful and resilient piece of furniture. Really, really nice work. Loving it. Thanks for watching and for your generous support, and I'll see you next time!"
117,Self-Illuminating Explosions,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we're going to talk about explosions. To be more precise, imagine that we already have the physics simulation data for an explosion on our computer, but we would like to visualize it on our screen. This requires a light simulation program that is able to create an image of this virtual scene that looks exactly the same as it would in reality. We have had plenty of earlier episodes on light transport, and as you know all too well, it is one of my favorite topics. I just can't get enough of it. I've put a link to these related episodes in the video description. If we wish to render a huge smoke plume, we perform something that computer graphics people call volumetric light transport. This means that a ray of light doesn't necessarily bounce off of the surface of materials, but it can penetrate their surfaces and scatter around inside of them. A technique that can deal with this is called volumetric path tracing, and if we wish to create an image of an explosion using that, well, better pack some fast food because it is likely going to take several hours. The explosion in this image took 13 hours and it is still not rendered perfectly. But this technique is able to solve this problem in 20 minutes, which is almost 40 times quicker. Unbelievable. The key idea is that this super complicated volumetric explosion data can be reimagined as a large batch of point light sources. If we solve this light transport problem between these point light sources, we get a solution that is remarkably similar to the original solution with path tracing, however, solving this new representation is much simpler. But that's only the first step. If we have a bunch of light sources, we can create a grid structure around them, and in these gridpoints, we can compute shadows and illumination in a highly efficient manner. What's more, we can create multiple of these grid representations. They all work on the very same data, but some of them are finer, and some of them are significantly sparser, more coarse. Another smart observation here is that even though sharp, high-frequency illumination details need to be computed on this fine grid, which takes quite a bit of computation time, it is sufficient to solve the coarse, low-frequency details on one of these sparser grids. The results look indistinguishable from the ground truth solutions, but the overall computation time is significantly reduced. The paper contains detailed comparisons against other techniques as well. Most of these scenes are rendered using hundreds of thousands of these point light sources, and as you can see, the results are unbelievable. If you would like to learn even more about light transport, I am holding a Master-level course on this at the Vienna University of Technology in Austria. I thought that the teachings should not only be available for those 30 people who sit in the room, who can afford a university education. It should be available for everyone. So, we made the entirety of the lecture available for everyone, free of charge, and I am so glad to see that thousands of people have watched it, and to this day I get many messages that they enjoyed it and now they see the world differently. It was recorded live with the students in the room, and it doesn't have the audio quality of Two Minute Papers. However, what it does well, is it conjures up the atmosphere of these lectures and you can almost feel like one of the students sitting there. If you're interested, have a look, the link is available in the video description! And make sure to read this paper too, it's incredible. Thanks for watching and for your generous support, and I'll see you next time!"
118,Simulating Liquid-Hair Interactions,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We already know that by using a computer, we can simulate fluids and we can simulate hair. But what about simulating both of them at the same time? This paper is about liquid-hair interaction and simulating the dynamics of wet hair. Our seasoned Fellow Scholars immediately know that this episode is going to be ample in amazing slow motion footage. I hope I didn't mess up with any of them, you will see soon if this is the case or not! Before we start talking about it, I'd like to note the following remarkable features: the authors uploaded a supplementary video in 4k resolution, executable files for their technique for all 3 major operating systems, data assets, and they also freshly revealed the full source code of the project. Hell yeah! I feel in heaven. A big, Two Minute Papers style hat tip to the authors for this premium quality presentation. If this paper were a car, it would definitely be a Maserati or a Mercedes. This technique solves the equations for liquid motion along every single hair strand, computes the cohesion effects between the hairs, and it can also simulate the effect of water dripping off the hair. Feast your eyes on these absolutely incredible results. The main issue with such an idea is that the theory of large, and small-scale simulations are inherently different, and in this case, we need both. The large-scale simulator would be a standard program that is able to compute how the velocity and pressure of the liquid evolves in time. However, we also wish to model the water droplets contained within one tiny hair strand. With a large-scale simulator, this would take a stupendously large amount of time and resources, so the key observation is that a small-scale fluid simulator program would be introduced take care of this. However, these two simulators cannot simply coexist without side effects. As they are two separate programs that work on the very same scene, we have to make sure that as we pass different quantities between them, they will still remain intact. This means that a drop of water that gets trapped in a hair strand has to disappear from the large-scale simulator, and has to be readded to it when it drips out. This is a remarkably challenging problem. But with this, we only scratched the surface. Make sure to have a look at the paper that has so much more to offer, it is impossible to even enumerate the list of contributions within in such a short video. The quality of this paper simply left me speechless and I would encourage you to take a look as well. And while this amazing footage is rolling, I would like to let you know that Two Minute Papers can exist because of your support through Patreon. Supporters of the series gain really cool perks like watching every single one of these episodes in early access. I am super happy to see how many of you decided to support the series, and in return, we are able to create better and better videos for you. Thank you again, you Fellow Scholars are the most amazing audience. Thanks for watching and for your generous support, and I'll see you next time!"
119,Real-Time Character Control With Phase-Functioned Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this piece of work, we seek to control digital characters in real-time. It happens the following way: we specify a target trajectory, and the algorithm has to synthesize a series of motions that follows that path. To make these motions as realistic as possible, this is typically accomplished by unleashing a learning algorithm on a large database that contains a ton of motion information. Previous techniques did not have a good understanding of these databases and they often synthesized motions from pieces that corresponded to different kinds of movements. This lack of understanding results in stiff, unnatural output motion. Intuitively, it is a bit like putting together a sentence from a set of letters that were cut out one by one from different newspaper articles. It is a fully formed sentence, but it lacks the smoothness and the flow of a properly aligned piece of text. This is a neural network based technique that introduces a phase function to the learning process. This phase function augments the learning with the timing information of a given motion. With this phase function, the neural network recognizes that we are not only learning periodic motions, but it knows when these motions start and when they end. The final technique takes very little memory, runs in real time, and it accomplishes smooth walking, running, jumping and climbing motions and so much more over a variety of terrains with flying colors. In a previous episode, we have discussed a different technique that accomplished something similar with a low and high level controller. One of the major selling points of this technique is that this one offers a unified solution for terrain traversal with using only one neural network. This has the potential to make it really big on computer games and real-time animation. It is absolutely amazing to witness this and be a part of the future. Make sure to have a look at the paper, which also contains the details of a terrain fitting step to make this learning algorithm capable of taking into consideration a variety of obstacles. I would also like to thank Claudio Pannacci for his amazing work in translating so many of these episodes to Italian. This makes Two Minute Papers accessible for more people around the globe, and the more people we can reach, the happier I am. Thanks for watching and for your generous support, and I'll see you next time!"
120,Digital Creatures Learn to Navigate in 3D,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Earlier, we have talked about a few amazing algorithms to teach digital creatures to walk. And this time, we're interested in controlling the joints of a digital character to not only walk properly, but take into consideration its surroundings. This new version can navigate in 3D with static and dynamic, moving obstacles, or even dribble a ball toward a target. Loving the execution and the production value of this paper. This is accomplished by an efficient system that consists of two controllers that are represented by learning algorithms. One, the low level controller is about about maintaining balance and proper limb control by manipulating the joint positions and velocities appropriately. This controller operates on a fine time scale, thirty times per second and is trained via a 4-layer neural network. Two, the high level controller can accomplish bigger overarching goals, such as following a path, or avoiding static and dynamic obstacles. We don't need to run this so often, therefore to save resources, this controller operates on a coarse time scale, only twice each second and is trained via a deep convolutional neural network. It also has support for a a small degree of transfer learning. Transfer learning means that after successfully learning to solve a problem, we don't have to start from scratch for the next one, but we can reuse some of that valuable knowledge and get a headstart. This is a heavily researched area and is likely going to be one of the major next frontiers in machine learning research. Now, make no mistake, it is not like transfer learning is suddenly to be considered a solved problem but in this particular case, it is finally a possibility. Really cool! I hope this brief exposé fired you up too. This paper is a bomb, make sure to have a look, as always, the link is available in the video description. And by the way, with your support on Patreon, we will soon be able to spend part of our budget on empowering research projects. How amazing is that? The new Two Minute Papers shirts are also flying off the shelves! Happy to hear you're enjoying them so much. If you're interested, hit up http://twominutepapers.com if you're located in the US. The EU and worldwide store's link is also available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
121,AI Learns to Synthesize Pictures of Animals,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I just finished reading this paper and I fell out of the chair. And I can almost guarantee you that the results in this work are so insane, you will have to double, or even triple check to believe what you're going to see here. This one is about image translation, which means that the input is an image, and the output is a different version of this input image that is changed according to our guidelines. Imagine that we have a Monet painting, and we'd like to create a photograph of this beautiful view. There we go. What if we'd like to change this winter landscape to an image created during the summer? There we go. If we are one of those people on the internet forums who just love to compare apples to oranges, this is now also a possibility. And have a look at this imagine that we like the background of this image, but instead of the zebras, we would like to have a couple of horses. No problem. Coming right up! This algorithm synthesizes them from scratch. The first important thing we should know about this technique, is that it uses generative adversarial networks. This means that we have two neural networks battling each other in an arms race. The generator network tries to create more and more realistic images, and these are passed to the discriminator network which tries to learn the difference between real photographs and fake, forged images. During this process, the two neural networks learn and improve together until they become experts at their own craft. However, this piece of work introduces two novel additions to this process. One, in earlier works, the training samples were typically paired. This means that the photograph of a shoe would be paired to a drawing that depicts it. This additional information helps the training process a great deal and the algorithm would be able to map drawings to photographs. However, a key difference here is that without such pairings, we don't need these labels, we can use significantly more training samples in our datasets which also helps the learning process. If this is executed well, the technique is able to pair anything to anything else, which results in a remarkably powerful algorithm. Key difference number two a cycle consistency loss function is introduced to the optimization problem. This means that if we convert a summer image to a winter image, and then back to a summer image, we should get the very same input image back. If our learning system obeys to this principle, the output quality of the translation is going to be significantly better. This cycle consistency loss is introduced as a regularization term. Our seasoned Fellow Scholars already know what it means, but in case you don't, I've put a link to our explanation in the video description. The paper contains a ton more results, and fortunately, the source code for this project is also available. Multiple implementations, in fact! Just as a side note, which is jaw dropping, by the way there is some rudimentary support for video. Amazing piece of work. Bravo! Now you can also see that the rate of progress in machine learning research is completely out of this world! No doubt that it is the best time to be a research scientist it's ever been. If you've liked this episode, make sure to subscribe to the series and have a look at our Patreon page, where you can pick up cool perks, like watching every single one of these episodes in early access. Thanks for watching and for your generous support, and I'll see you next time!"
122,An Efficient Scattering Material Representation,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Have a look at these beautiful images. Representing these materials that you see here takes more than 25 gigabytes of storage. You could store several feature-length movies on your hard drive using the same amount of storage. And this technique is able to compress all this 25 gigabytes into 45 megabytes without introducing any significant perceptible difference! That is close to a whopping 500 times more efficient representation. This improved representation not only helps easing the storage requirements of these assets, but it is also makes the rendering times, or in other words, the process of creating these images via light simulation programs typically more than twice as fast to process. That is a ton of money and time saved for the artists. An important keyword in this piece of work is anisotropic scattering. So what does that mean exactly? The scattering part means that we have to imagine these materials not as a surface, but as a volume in which rays of light bounce around and get absorbed. If we render a piece cloth made of velvet, twill, or a similar material, there are lots of microscopic differences in the surface, so much so, that it is insufficient to treat them as a solid surface, such as wood or metals. We have to think about them as volumes. This is the scattering part. The anisotropy means that light can scatter unevenly in this medium, these rays don't bounce around in all directions with equal probability. This means that there is significant forward and backward scattering in these media, making it even more difficult to create more optimized algorithms that simplify these scattering equations. If you look below here, you'll see these colorful images that researchers like to call difference images. It basically means that we create one image with the perfectly accurate technique as a reference. As expected, this reference image probably takes forever to compute, but is important to have as a yardstick. Then, we compute one image with the proposed technique that is usually significantly faster. So we have these two images, and sometimes, the differences are so difficult to see, we no way of knowing where the inaccuracies are. So what we do is subtract the two images from each other, and assign a color coding for the differences. As the error may be spatially varying, this is super useful because we can recognize exactly where the information is lost. The angrier the colors are, the higher the error is in a given region. As you can see, the proposed technique is significantly more accurate in representing this medium than a naive method using the same amount of storage. This paper is extraordinarily well written. It is one of the finest pieces of craftsmanship I've come along in long while. And yes, it is a crime not having a look at it. Also, if you liked this episode, make sure to subscribe to the series and check out our other videos. We have more than 150 episodes for you, ready to go right now! You'll love it, and there will be lots of fun to be had. Thanks for watching and for your generous support, and I'll see you next time!"
123,Deep Photo Style Transfer,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Let's have a look at this majestic technique that is about style transfer for photos. Style transfer is a magical algorithm where we have one photograph with content, and one with an interesting style. And the output is a third image with these two photos fused together. This is typically achieved by a classical machine learning technique that we call a convolutional neural network. The more layers these networks contain, the more powerful they are, and the more capable they are in building an intuitive understanding of an image. We had several earlier episodes on visualizing the inner workings of these neural networks, as always, the links are available in the video description. Don't miss out, I am sure you'll be as amazed by the results as I was when I have first seen them. These previous neural style transfer techniques work amazingly well if we're looking for a painterly result. However, for photo style transfer, the closeups here reveal that they introduce unnecessary distortions to the image. They won't look realistic anymore. But not with this new one. Have a look at these results. This is absolute insanity. They are just right in some sense. There is an elusive quality to them. And this is the challenge! We not only have to put what we're searching for into words, but we have to find a mathematical description of these words to make the computer execute it. So what would this definition be? Just think about this, this is a really challenging question. The authors decided that the photorealism of the output image is to be maximized. Well, this sounds great, but who really knows a rigorous mathematical description of photorealism? One possible solution would be to stipulate that the changes in the output color would have to preserve the ratios and distances of the input style colors. Similar rules are used in linear algebra and computer graphics to make sure shapes don't get distorted as we're tormenting them with rotations, translations and more. We like to call these operations affine transformations. So the fully scientific description would be that we add a regularization term that stipulates, that these colors only undergo affine transformations. But we've used one more new word here what does this regularization term mean? This means that there are a ton of different possible solutions for transferring the colors, and we're trying to steer the optimizer towards solutions that adhere to some additional criterion, in our case, the affine transformations. In the mathematical description of this problem, these additional stipulations appear in the form of a regularization term. I am so happy that you Fellow Scholars have been watching Two Minute Papers for so long, that we can finally talk about techniques like this. It's fantastic to have an audience that has this level of understanding of these topics. Love it. Just absolutely love it. The source code of this project is also available. Also, make sure to have a look at Distill, an absolutely amazing new science journal from the Google Brain team. But this is no ordinary journal, because what they are looking for is not necessarily novel techniques, but novel and intuitive ways of explaining already existing works. There is also an excellent write-up on research debt that can almost be understood as a manifesto for this journal. A worthy read indeed. They also created a prize for science distillation. I love this new initiative and I am sure we'll hear about this journal a lot in the near future. Make sure to have a look, there is a link to all of these in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
124,AI Creates 3D Models From Faces,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. How cool would it be to be able to place a character representing us in a digital film or a computer game? Of course, it would clearly be an extremely laborious task to digitize the 3D geometry and the albedo map of our face. This albedo map means a texture, a colored pattern that describes how our skin reflects and absorbs light. Capturing such a representation is clearly a very lengthy and expensive process. So get this completely crazy idea: this technique creates this full digital representation of any face from no more than one simple photograph. We can even get historical figures in our digital universe, all we need is one photograph of them. And now, feast your eyes on these incredible results. After taking a photograph, this technique creates two of these albedo maps: one is a complete, low frequency map, which records the entirety of the face, but only contains the rough details. The other albedo map contains finer details, but in return, is incomplete. Do you remember the texture synthesis methods that we discussed earlier in the series? The input was a tiny patch of image with a repetitive structure, and after learning the statistical properties of these structures, it was possible to continue them indefinitely. The key insight is that we can also do something akin to that here as well we take this incomplete albedo map, and try to synthesize the missing details. Pretty amazing idea indeed! The authors of the paper invoke a classical learning algorithm, a convolutional neural network to accomplish that. The deeper the neural network we use, the more high-frequency details appear on the outputs, or in other words, the crisper the image we get. In the paper, you will find a detailed description of their crowdsourced user study that was used to validate this technique, including the user interface and the questions being asked. There are also some comparisons against PatchMatch, one of the landmark techniques for texture synthesis that we have also talked about in an earlier episode. It's pretty amazing to see this Two Minute Papers knowledge base grow and get more and more intertwined. I hope you're enjoying the process as much as I do! Also, due to popular request, the Two Minute Papers T-shirts are now available! This time, we are using a different service for printing these shirts, please give us some feedback on how you liked it. I've put my e-mail address in the video description. If you attach a photo of yourself wearing some cool Two Minute Papers merch, we'll be even more delighted! Just open twominutepapers.com and you'll immediately have access to it. This link will bring you to the service that ships to the US. The link for shipping outside the US is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
125,AI Learns Geometric Descriptors From Depth Images,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, we're going to discuss a great piece of work that shows us how efficient and versatile neural network-based techniques had become recently. Here, the input is a bunch or RGB-D images, which are photographs endowed with depth information, and the output can be a full 3D reconstruction of a scene, and much, much more, which we'll see in a moment. This task is typically taken care of by handcrafting descriptors. A descriptor is a specialized representation for doing useful tasks on images and other data structures. For instance, if we seek to build an algorithm to recognize black and white images, a useful descriptor would definitely contain the number of colors that are visible in an image, and a list of these colors. Again, these descriptors have been typically handcrafted by scientists for decades. New problem, new descriptors, new papers. But not this time, because here, super effective descriptors are proposed automatically via a learning algorithm, a convolutional neural network and siamese networks. This is incredible! Creating such descriptors took extremely smart researchers and years of work on a specific problem, and were still often not as good as these ones. By the way, we have discussed siamese networks in an earlier episode, as always, the link is available in the video description. And as you can imagine, several really cool applications emerge from this. One, when combined with RANSAC, a technique used to find order in noisy measurement data, it is able to perform 3D scene reconstruction from just a few images. And it completely smokes the competition. Two, pose estimation with bounding boxes. Given a sample of an object, the algorithm is able to recognize not only the shape itself, but also its orientation when given a scene cluttered with other objects. Three, correspondance search is possible. This is really cool! This means that a semantically similar piece of geometry is recognized on different objects. For instance, the algorithm can learn the concept of a handle, and recognize the handles on a variety of objects, such as on motorcycles, carriages, chairs, and more! The source code of this project is also available. Yoohoo! Neural networks are rapidly establishing supremacy in a number of research fields, and I am so happy to be alive in this age of incredible research progress. Make sure to subscribe to the series and click the bell icon, some amazing works are coming up in the next few episodes, and there will be lots of fun to be had. Thanks for watching and for your generous support, and I'll see you next time!"
126,Semantic Scene Completion From One Depth Image,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work is an amazing application of deep neural networks, that performs semantic scene completion from only one depth image. This depth image is the colorful image that you see here, where the colors denote how far away different objects are from our camera. We can create these images inexpensively with commodity hardware, for instance, Microsoft's Kinect has a depth sensor sensor that is suitable for this task. The scene completion part means that from this highly incomplete depth information, the algorithm reconstructs the geometry for the entirety of the room. Even parts, that are completely missing from our images or things that are occluded! The output is what computer graphics researchers like to call a volumetric representation or a voxel array, which is essentially a large collection of tiny Lego pieces that build up the scene. But this is not all because the semantic part means that the algorithm actually understands what we're looking at, and thus, is able to classify different parts of the scene. These classes include walls, windows, floors, sofas, and other furniture. Previous works were able to do scene completion and geometry classification, but the coolest part of this algorithm is that it not only does these steps way better, but it does them both at the very same time. This work uses a 3D convolutional neural network to accomplish this task. The 3D part is required for this learning algorithm to be able to operate on this kind of volumetric data. As you can see, the results are excellent, and are remarkably close to the ground truth data. If you remember, not so long ago, I flipped out when I've seen the first neural network-based techniques that understood 3D geometry from 2D images. That technique used a much more complicated architecture, a generative adversarial network, which also didn't do scene completion and on top of that, the resolution of the output was way lower, which intuitively means that Lego pieces were much larger. This is insanity. The rate of progress in machine learning research is just stunning, probably even for you seasoned Fellow Scholars who watch Two Minute Papers and have high expectations. We've had plenty of previous episodes about the inner workings of different kinds of neural networks. I've put some links to them in the video description, make sure to have a look if you wish to brush up on your machine learning kung fu a bit. The authors also published a new dataset to solve these kind of problems in future research works, and, it is also super useful because the output of their technique can be compared to ground truth data. When new solutions pop up in the future, this dataset can be used as a yardstick to compare results with. The source code for this project is also available. Tinkerers rejoice! Thanks for watching and for your generous support, and I'll see you next time!"
127,Real-Time Modeling and Animation of Climbing Plants,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is about interactively modeling and editing climbing plants. This is one of my favorite kind of works: mundane sounding topic, immaculate execution. There are so many cool things about this paper, I don't even know where to start. But first, let's let's talk about the modeling part. We can, for instance, plant a seed, and we can not only have a look at how it grows as time goes by, but we can also influence the grow variability and shoot growth rates. Branches can be added and removed at will at any point in time. We can also add attractors, regions that are set to be more likely for the plant to grow towards. With these techniques, we can easily create any sort of artistic effect, be it highly artificial looking vines and branches, or some long forgotten object overgrown with climbing plants. However, a model is just 3d geometry. What truly makes these models come alive is animation, which is also executed with flying colors. The animations created with this technique are both biologically and physically plausible. So what do these terms mean exactly? Biologically plausible means that the plants grow according to the laws of nature, and physically plausible means that if we start tugging at it, branches start moving, bending and breaking according to the laws of physics. Due to its responsive and interactive nature, the applications of this technique are typically in the domain of architectural visualization, digital storytelling, or any sort of real-time application. And of course, the usual suspects, animated movies and game developers can use this to create more immersive digital environments with ease. And don't forget about me, Károly, who would happily play with this basically all day long. If you are one of our many Fellow Scholars who are completely addicted to Two Minute Papers, make sure to check out our Patreon page where you can grab cool perks, like watching these episodes in early access, or deciding the order of upcoming episodes, and more. Also, your support is extremely helpful, so much so, that even the price of a cup of coffee per month helps us to create better videos for you. We write some reports from time to time to assess the improvements we were able to make with your support. The link is in the video description, or you can just click the letter P on the ending screen in a moment. Thanks for watching and for your generous support, and I'll see you next time!"
128,Controllable Fluid and Smoke Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, we're going to talk some more about fluid simulations and fluid guiding. Hell yeah! As you know all too well, it is possible to simulate the laws of fluid motion on a computer, make a digital scene, and create absolutely beautiful videos such as the ones you see here. Newer and newer research papers show up to both extend the possible scenarios that we can simulate, and there are also other works to speed up already existing solutions. This piece of work introduces a technique that mathematicians like to call the primal-dual optimization method. This helps us accomplish two really cool things. One is fluid guiding. Fluid guiding is a problem where we're looking to exert control over the fluid while keeping the fluid flow as natural as possible. I've written my Master thesis on the very same topic and can confirm that it's one hell of a problem. The core of the problem is that if we use the laws of physics to create a fluid simulation, we get what would happen in reality as a result. However, if we wish to guide this piece of fluid towards a target shape, for instance, to form an image of our choice, we have to both retain natural fluid flows, but still, create something that would be highly unlikely to happen according to the laws of physics. For instance, a splash of water is unlikely to suddenly form a human face of our choice. The proposed technique helps this ambivalent goal of exerting a bit of control over the fluid simulation while keeping the flows as natural as possible. There are already many existing applications of fluids and smoke in movies where an actor fires a gun, and the fire and smoke plumes are added to the footage in post production. However, with a high quality fluid guiding technique, we could choose target shapes for these smoke plumes and explosions that best convey our artistic vision. And number two, it also accomplishes something that we call separating boundary conditions, which prevents imprecisions where small fluid volumes are being stuck to walls. The guiding process is also followed by an upsampling step, where we take a coarse simulation, and artificially synthesize sharp, high-frequency details onto it. Computing the more detailed simulation would often take days without such synthesizing techniques. Kind of like with Wavelet Turbulence, which is an absolutely incredible paper that was showcased in none other than the very first Two Minute Papers episode. Link is in the video description box. Don't watch it. It's quite embarrassing. And all this leads to eye-poppingly beautiful solutions. Wow. I cannot get tired of this. In the paper, you will find much more about breaking dams, tornado simulations, and applications of the primal-dual optimization method. Normally, to remain as authentic to the source materials as possible, I don't do any kind of slow motion and other similar shenanigans, but this time, I just couldn't resist it. Have a look at this, and I hope you'll like the results. Hm hm! If you feel the alluring call of fluids, I've put some resources in the video description, including a gentle description I wrote on the basics of fluid simulation and fluid control with source code both on the CPU and GPU, and a link to Doyub Kim's amazing book that I am currently reading. Highly recommended. If you also have some online tutorials and papers that helped you solidify your understanding of the topic, make sure to leave a link in the comments, I'll include the best ones in the video description. If you would like to see more episodes like this one, make sure to subscribe to Two Minute Papers, we would be more than happy to have you along on our journey of science. Thanks for watching and for your generous support, and I'll see you next time!"
129,On-the-Fly 3D Printing While Modeling,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. You are going to love this killer paper. In the classical case of 3D fabrication, we first create a 3D geometry in our modeling software on a computer. Then, after we're done, we send this model to a 3D printer to create a physical copy of it. If we don't like an aspect of the printed model, we have to go back to the computer and adjust accordingly. If there are more fundamental issues, we may even have to start over. And get this, with this piece of work, we can have a low-fidelity wireframe version printed immediately as we make changes within the modeling software. This process we can refer to as real-time or on-the-fly 3D printing. In this work, both the hardware design and the algorithm that runs the printer is described. This approach has a number of benefits, and of course, a huge set of challenges. For instance, we can immediately see the result of our decisions, and can test whether a new piece of equipment would correctly fit into the scene we're designing. Sometimes, depending on the geometry of the final object, different jobs need to be reordered to get a plan that is physically plausible to print. In this example, the bottom branch was designed by the artist first, and the top branch afterwards, but their order has to be changed, otherwise the bottom branch would block the way to the top branch. The algorithm recognizes these cases and reorders the printing jobs correctly. Quite remarkable. And, an alternative solution for rotating the object around for better reachability is also demonstrated. Because of the fact that the algorithm is capable of this sort of decisionmaking, we don't even need to wait for the printer to finish a given step, and can remain focused on the modeling process. Also, the handle of the teapot here collides with the body because of the limitations of wireframe modeling, such cases have to be detected and omitted. Connecting patches and adding differently sized holes to a model are also highly nontrivial problems that are all addressed in the paper. And this piece of work is also a testament to the potential of solutions where hardware and software is designed with each other in mind. I can only imagine how many work hours were put in this project until the final, working solution was obtained. Incredible work indeed. We really just scratched the surface in this episode, make sure to go to the video description and have a look at the paper for more details. It's definitely worth it. Also, if you enjoyed this Two Minute Papers episode, make sure to subscribe to the series and if you're subscribed, click the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!"
130,Real-Time Oil Painting on Mobile,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. It's been quite a while since we've talked about a paper on fluid simulations. Since my withdrawal symptoms are already kicking in, today, I simply have to talk to you about this amazing paint simulator program that runs in real time and on our mobile devices. These handheld devices typically have a lower image resolution compared to desktop computers, therefore it is indeed a challenge to put together a solution that artists can use to create detailed paintings with. And to accomplish this, this piece of work offers several killer features: for instance, the paint pigment concentration can be adjusted. The direction of the brush strokes is also controllable. And third, this technique is powered by a viscoelastic shallow water simulator that also supports simulating multiple layers of paint. This is particularly challenging as the inner paint layers may have already dried when adding a new wet layer on top of them. This all has to be simulated in a way that is physically plausible. But we're not done yet! With many different kinds of paint types that we're using, the overall outlook of our paintings are dramatically different depending on the lighting conditions around them. To take this effect into consideration, this technique also has an intuitive feature where the effect of virtual light sources is also simulated, and the output is changed interactively as we tilt the tablet around. And get this, gravity is also simulated, and the paint trickles down depending on the orientation of our tablet according to the laws of physics. Really cool! The paper also shows visual comparisons against similar algorithms. And clearly, artists who work with these substances all day know exactly how they should behave in reality, so the ultimate challenge is always to give it to them and ask them whether they have enjoyed the workflow and found the simulation faithful to reality. Let the artists be the judge! The user study presented in the paper revealed that the artists loved the user experience and they expressed that it's second to none for testing ideas. I am sure that with a few improvements, this could be the ultimate tool for artists to unleash their creative potential while sitting outside and getting inspired by nature. Thanks for watching and for your generous support, and I'll see you next time!"
131,Instant 3D Floorplans From Your Photos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this piece of work, we are interested in creating a 3D virtual tour for an apartment. However, for this apartment, no 3D information is available instead, the input for the algorithm is something that we can obtain easily, in this case, a 2D floorplan and a set of images that we shot in the apartment. From this information, we would create a 3D floorplan that is not only faithful to the real one in terms of geometry, but the photos with the correct viewpoints are also to be assigned to the correct walls. In order to accomplish this, one has to overcome a series of challenging problems. For instance, we have to estimate the layout of each room and find the location of the camera in each of these images. Also, to obtain high-quality solutions, the goal is to extract as much information from the inputs as possible. The authors recognized that the floorplans provide way more information than we take for granted. For instance, beyond showing the geometric relation of the rooms, it can also be used to find out the aspect ratios of the floor for each room. The window-to-wall ratios can also be approximated and matched between the photos and the floorplan. This additional information is super useful when trying to find out which room is to be assigned to which part of the 3D floorplan. Beyond just looking at the photos, we also have access to a large swath of learning algorithms that can reliably classify whether we're looking at a bathroom or a living room. There are even more constraints to adhere to in order to aggressively reduce the number of physical configurations, make sure to have a look at the paper for details, there are lots of cool tricks described there. As always, there is a link to it in the video description. For instance, since the space of possible solutions is still too vast, a branch and bound type algorithm is proposed to further decimate the number of potential solutions to evaluate. And as you can see here, the comparisons against ground truth floorplans reveal that these solutions are indeed quite faithful to reality. The authors also kindly provided a dataset with more than 200 full apartments with well over a thousand photos and annotations for future use in followup research works. Creating such a dataset and publishing it is incredibly laborious, and could easily be a paper on its own, and here, we also get an excellent solution for this problem as well. In a separate work, the authors also published a different version of this problem formulation that reconstructs the exteriors of buildings in a similar manner. There is so much to explore, the links are available in the video description, make sure to have a look! In case you're wondering, it's still considered a crime not doing that. I hope you have enjoyed this episode, and I find it so delightful to see this unbelievably rapid growth on the channel. Earlier I thought that even 2 would be amazing, but now, we have exactly 8 times as many subscribers as one year ago. Words fail me to describe the joy of showing these amazing works to such a rapidly growing audience. This is why I always say at the end of every episode... Thanks for watching and for your generous support, and I'll see you next time!"
132,Geometric Detail Transfer,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In the world of digital 3D modeling, it often occurs that we are looking for surfaces that are not perfectly smooth, but have some sort of surface detail. Wrinkles, engravings, grain on a wooden table are excellent examples of details that we can add to our models, and computer graphics people like to collectively call these things displacement maps. Artists often encounter cases where they like the displacements on one object, but the object itself is not really interesting. However, it could be that there is a different piece of geometry these details would look great on. Consider this problem solved, because in this piece of work, the input is two 3D models: one with interesting geometric details, and the other is the model onto which we transfer these surface details. The output will be our 3D geometric shape with two of these models fused together. The results look absolutely amazing. I would love to use this right away in several projects. The first key part is the usage of metric learning. Wait, technical term, so what does this mean exactly? Metric learning is a classical technique in the field of machine learning, where we're trying to learn distances between things where distance is mathematically ill-defined. Let's make it even simpler and go with an example: for instance, we have a database of human faces, and we would like to search for faces that are similar to a given input. To do this, we specify a few distances by hand, for instance, we could say that a person with a beard is a short distance from one with a moustache, and a larger distance from one with no facial hair. If we hand many examples of these distances to a learning algorithm, it will be able to find people with similar beards. And in this work, this metric learning is used to learn the relationship between objects with and without these rich surface details. This helps in the transferring process. As to creating the new displacements on the new model, there are several hurdles to overcome. One, we cannot just grab the displacements and shove them onto a different model, because it can potentially look different, have different curvatures and sizes. The solution to this would be capturing the statistical properties of the surface details and use this information to synthesize new ones on the target model. Note that we cannot just perform this texture synthesis in 2D like we do for images, because as we project the result to a 3D model, it introduces severe distortions to the displacement patterns. It is a bit like putting a rubber blanket onto a complicated object. Different regions of the blanket will be distorted differently. Make sure to have a look at the paper where the authors present quite a few more results and of course, the intricacies of this technique are also described in detail. I hope some public implementations of this method will appear soon, I would be quite excited to use this right away, and I am sure there are many artists who would love to create these wonderfully detailed models for the animated films and computer games of the future. In the meantime, we have a completely overhauled software and hardware pipeline to create these videos. We have written down our joyful and perilous story of it on Patreon if you're interested in looking a bit behind the curtain as to how these episodes are made, make sure to have a look, it is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
133,Modeling Knitted Clothing,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Not so long ago, we talked about a technique that enabled us to render stunningly high quality cloth models in real-time. It supported level of detail, self shadowing, and lots of other goodies that make computer game developers, and of course, my humble self super happy. And today, we're going to talk about a technique that is able to create these highly detailed cloth geometries for our digital characters. I have really fond memories of attending to the talk of the Oscar award winner Steve Marschner on this paper a few years ago in Switzerland, and I remember being so spellbound by it that I knew this was a day I will never forget. I am sure you'll love it too. In this piece of work, the goal is to create a digital garment model that is as detailed and realistic as possible. We start out with an input 3D geometry that shows the rough shape of the model, then, we pick a knitting pattern of our choice. After that, the points of this knitting pattern are moved so that they correctly fit this 3D geometry that we specified. And now comes the coolest part! What we created so far is an ad-hoc model that doesn't really look and behave like a real piece of cloth. To remedy this, a physics-based simulation is run that takes this ad-hoc model and the output of this process will be a realistic rest shape for these yarn curves. And here you can witness how the simulated forces pull the entire piece of garment together. We start out with dreaming up a piece of cloth geometry, and this simulator gradually transforms it into a real-world version of that. This is a step that we call yarn-level relaxation. Wow. These final results not only look magnificent, but in a physical simulation, they also behave like real garments. It's such a joy to look at results like this. Loving it. Again, I would like to note that we're not talking about the visualization of the garment, but creating a realistic piece of geometry. The most obvious drawback of this technique is its computation time it was run on a very expensive system and still took several hours of number crunching to get this done. However, I haven't seen an implementation of this on the graphics card yet, so if someone can come up with an efficient way to do it, in an ideal case, we may be able to do this in several minutes. I also have to notify you about the fact that it is considered a crime not having a look at the paper in the video description. It does not suffice to say that it is well written, it is so brilliantly presented, it's truly a one of kind work that everyone has to see. If you enjoyed this episode, make sure to subscribe to Two Minute Papers, we'd be happy to have you in our growing club of Fellow Scholars. Thanks for watching and for your generous support, and I'll see you next time!"
134,Structural Image Editing With PatchMatch,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We're currently more than 130 episodes into the series, and we still haven't talked about this algorithm. How could we go on for so long without PatchMatch? So, let's do this right now! You'll love this one. This technique helps us to make absolutely crazy modifications to previously existing photographs and it is one of the landmark papers for all kinds of photo manipulation which is still widely used to this day. Consider the following workflow: we have this image as an input. Let's mark the roofline for hole filling, or image inpainting as the literature refers to it. And the hole is now filled with quite sensible information. Now we mark some of the pillars to reshape the object. And then, we pull the roof upward. The output is a completely redesigned version of the input photograph. Wow, absolutely incredible. And the whole thing happens interactively, almost in real time, but if we consider the hardware improvement since this paper was published, it is safe to say that today it runs in real time even on a mediocre computer. And in this piece of work, the image completion part works by adding additional hints to the algorithm, for instance, marking the expected shape of an object that we wish to cut out and have it filled in with new data. Or, showing the shape of the building that we wish to edit also helps the technique considerably. These results are so stunning, I remember that when I had first seen them, I had to recheck over and over again because I could hardly believe my eyes. This technique offers not only these high quality results, but it is considerably quicker than its competitors. To accomplish image inpainting, most algorithms look for regions in the image that are similar to the one that is being removed, and borrow some information from there for the filling process. Here, one of the key ideas that speed up the process is that when good correspondances are found, if we are doing another lookup, we shouldn't restart the patch matching process, but we should try to search nearby, because that's where we are most likely to find useful information. You know what the best part is? What you see here is just the start: this technique does not use any of the modern machine learning techniques, so in the era of these incredibly powerful deep neural networks, I can only imagine the quality of solutions we'll be able to obtain in the near future. We are living amazing times indeed. Thanks for watching and for your generous support, and I'll see you next time!"
135,Shape2vec: Understanding 3D Shapes With AI,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This one is going to be absolutely amazing. This piece of work is aimed to help a machine build a better understanding of images and 3D geometry. Imagine that we have a large database with these geometries and images, and we can search and compare them with arbitrary inputs and outputs. What does this mean exactly? For instance, it can handle a text input, such as school bus and automatically retrieve 3D models, sketches and images that depict these kinds of objects. This is great, but we said that it supports arbitrary inputs and outputs, which means that we can use the 3D geometry of a chair as an input, and obtain other, similar looking chairs from the database. This technique is so crazy, it can even take a sketch as an input and provide excellent quality outputs. We can even give it a heatmap of the input and expect quite reasonable results. Typically, these images and 3D geometries contain a lot of information, and to be able to compare which is similar to which, we have to compress this information into a more concise description. This description offers a common ground for comparisons. We like to call these embedding techniques. Here, you can see an example of a 2D visualization of such an embedding of word classes. The retrieval from the database happens by compressing the user-provided input and putting it into this space, and fetching the results that are the closest to it in this embedding. Before the emergence of powerful learning algorithms, these embeddings were typically done by hand. But now, we have these deep neural networks that are able to automatically create solutions for us, that are in some sense, optimal, meaning that according to a set of rules, it will always do better than we would by hand. We get better results by going to sleep and leaving the computer on overnight than we would have working all night using the finest algorithms from ten years ago. Isn't this incredible? The interesting thing is that here, we are able to do this for several different representations: for instance, a piece of 3D geometry, or 2D color image, or a simple word, is being embedded into the very same vector space, opening up the possibility of doing these amazing comparisons between completely different representations. The results speak for themselves. This is another great testament to the power of convolutional neural networks and as you can see, the rate of progress in AI and machine learning research is absolutely stunning. Also big thumbs up for the observant Fellow Scholars out there who noticed the new outro music, and, some other minor changes in the series. If you are among those people, you can consider yourself a hardcore Two Minute Papers Scholar! High five! Thanks for watching and for your generous support, and I'll see you next time!"
136,Space-Time Video Completion,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we're going to talk about an algorithm that is capable of filling holes in space and time. Classical image inpainting, or in other words, filling holes in images is something that we've explored in earlier episodes there are several incredible techniques to take care of that. But with this piece of work, it is possible to generalize such a technique for video, and fill holes for not only one image, but a series of images. Like removing a umbrella to create an unoccluded view to the beach. Or removing a waving person from a video of us jogging. These results are truly incredible, and even though this method was published long long ago, it still enjoys a great deal of reverence among computer graphics practitioners. Not only that, but this algorithm also serves the basis of the awesome content aware fill feature introduced in Adobe Photoshop CS5. In this problem formulation, we have to make sure that our solution has spatio-temporal consistency. What does this mean exactly? This means that holes can exist through space and time, so multiple frames of a video may be missing, or there may be regions that we wish to cut out not only for one image, but for the entirety of the video. The filled in regions have to be consistent with their surroundings if they are looked at as an image, but there also has to be a consistency across the time domain, otherwise we would see a disturbing flickering effect in the results. It is a really challenging problem indeed because there are more constraints that we have to adhere to, however, a key observation is that it is also easier because in return, we have access to more information that comes from the previous and the next frames in the video. For instance, here, you can see an example of retouching old footage by removing a huge pesky artifact. And clearly, we know the fact that Charlie Chaplin is supposed to be in the middle of the image only because we have this information from the previous and next frames. All this is achieved by an optimization algorithm that takes into consideration that consistency has to be enforced through the spatial and the time domain at the same time. It can also be used to fill in completely missing frames of a video. Or, it also helps where we have parts of an image missing after being removed by an image stabilizer algorithm. Video editors do this all the time, so such a restoration technique is super useful. The source code of this technique is available, I've put a link to it in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
137,Stable Neural Style Transfer,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Neural style transfer is an incredible technique where we have two input photographs, and the output would be a combination of these two, namely, the content of one and the artistic style of the other fused together. When the first paper appeared on this topic, the news took the world by storm, and lots of speculative discussions emerged as to what this could be used for and how it would change digital arts and the video game industry. It is great fun to use these algorithms and we have also witnessed a recent proliferation of phone apps that are able to accomplish this, which is super cool because of two reasons: one, the amount of time to go from a published research paper to industry-wide application has never been so small, and, two, the first work required a powerful computer to accomplish this, and took several minutes of strenuous computation, and now, less than two years later, it's right in your pocket and can be done instantly. Talk about exponential progress in science and research, absolutely amazing. And now, while we feast our eyes upon these beautiful results, let's talk about the selling points of this extension of the original technique. The paper contains a nice formal explanation of the weak points of the existing style transfer algorithms. The intuition behind the explanation is that the neural networks think in terms of neuron activations, which may not be proportional to the color intensities in the source image styles, therefore their behavior often becomes inconsistent or different than expected. The authors propose thinking in terms of histograms, which means that the output image should rely on statistical similarities with the source images. And as we can see, the results look outstanding even when compared to the original method. It is also important to point out that this proposed technique is also more art directable, make sure to have a look at the paper for more details on that. As always, I've put a link in the video description. This extension is also capable of texture synthesis, which means that we give it a small image patch that shows some sort of repetition, and it tries to continue it indefinitely in a way that seems completely seamless. However, we have to be acutely aware of the fact that in the computer graphics community, texture synthesis is considered a subfield of its own with hundreds of papers, and one has to be extremely sure to have a clear cut selling point over the state of the art. For the more interested Fellow Scholars out there, I've put a survey paper on this in the video description, make sure to have a look! Thanks for watching and for your generous support, and I'll see you next time!"
138,Breaking DeepMind's Game AI System,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Not so long ago, Google DeepMind introduced a novel learning algorithm that was able to reach superhuman levels in playing many Atari games. It was a spectacular milestone in AI research. Interestingly, while these learning algorithms are being improved at a staggering pace, there is a parallel subfield where researchers endeavor to break these learning systems by slightly changing the information they are presented with. Fraudulent tampering with images or video feeds, if you will. Imagine a system that is designed to identify what is seen in an image. In an earlier episode, we discussed an adversarial algorithm, where in an amusing example, they added a tiny bit of barely perceptible noise to this image, to make the deep neural network misidentify a bus for an ostrich. Machine learning researchers like to call these evil forged images adversarial samples. And now, this time around, OpenAI published a super fun piece of work to fool these game learning algorithms by changing some of their input visual information. As you will see in a moment, it is so effective that by only using a tiny bit of information, it can turn a powerful learning algorithm into a blabbering idiot. The first method adds a tiny bit of noise to a large portion of the video input, where the difference is barely perceptible, but it forces the learning algorithm to choose a different action that it would have chosen otherwise. In the other one, a different modification was used, that has a smaller footprint, but is more visible. For instance, in pong, adding a tiny fake ball to the game to coerce the learner into going down when it was originally planning to go up. The algorithm is able to learn game-specific knowledge for almost any other game to fool the player. Despite the huge difference in the results, I loved the elegant mathematical formulation of the two noise types, because despite the fact that they do something radically different, their mathematical formulation is quite similar, mathematicians like to say that we're solving the same problem, while optimizing for different target norms. Beyond DeepMind's Deep Q-Learning, two other high-quality learning algorithms are also fooled by this technique. In the white box formulation, we have access to the inner workings of the algorithm. But interestingly, a black box formulation is also proposed, where we know much less about the target system, but we know the game itself, and we train our own system and look for weaknesses in that. When we've found these weak points, we use this knowledge to break other systems. I can only imagine how much fun there was to be had for the authors when they were developing these techniques. Super excited to see how this arms race of creating more powerful learning algorithms, and in response, more powerful adversarial techniques to break them develops. In the future, I feel that the robustness of a learning algorithm, or in other words, its resilience against adversarial attacks will be just as important of a design factor as how powerful it is. There are a ton of videos published on the authors' website, make sure to have a look! And also, if you wish to support the series, make sure to have a look at our Patreon page. We kindly thank you for your contribution, it definitely helps keeping the series running. Thanks for watching and for your generous support, and I'll see you next time!"
139,Automatic Creation of Sketch Tutorials,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Have a look at this magnificent idea: the input is a digital 3D model of an object and a viewpoint of our choice, and the output is an easy to follow, step by step breakdown on how to draw it. Automated drawing tutorials! I wish tools like this were available back when I was a child! Awesome! This technique offers a way to create the scaffoldings to help achieving the correct perspective and positioning for the individual elements of the 3D model, something that novice artists often struggle with. This problem is particularly challenging, because we have a bunch of competing solutions and we have to decide which one should be presented to the user. To achieve this, we have to include a sound mathematical description of how easy a drawing process is. The algorithm also makes adjustments to the individual parts of the model to make them easier to draw without introducing severe distortions to the shapes. The proposed technique uses graph theory to find a suitable ordering of the drawing steps. Beyond the scientific parts, there are a lot of usability issues to be taken into consideration: for instance, the algorithm should notify the user when a given guide is not to be used anymore and can be safely erased. Novice, apprentice, and adept users are also to be handled differently. To show the validity of this solution, the authors made a user study where they tested this new tutorial type against the most common existing solution, and found that the users were not only able to create more accurate drawings with it, but they were also enjoying the process more. I commend the authors for taking into consideration the overall experience of the drawing process, which is an incredibly important factor if the user enjoyed the process, he'll surely come back for more, and of course, the more we show up, the more we learn. Some of these tutorials are available on the website of the authors, as always, I've linked it in the video description. If you are in the mood to draw, make sure to give it a go and let us know how it went in the comments section! Hell, even I am now in the mood to give this a try! If I disappear for a while, you know where I am. Thanks for watching and for your generous support, and I'll see you next time!"
140,AI Makes Stunning Photos From Your Drawings (pix2pix),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In an earlier work, we were able to change a photo of an already existing design according to our taste. That was absolutely amazing. But now, hold onto your papers and have a look at this! Because here, we can create something out of thin air! The input in this problem formulation is an image, and the output is an image of a different kind. Let's call this process image translation. It is translation in a sense, that for instance, we can add an aerial view of a city as an input, and get the map of this city as an output. Or, we can draw the silhouette of a handbag, and have it translated to an actual, real-looking object. And we can go even crazier, for instance, day to night conversion of a photograph is also possible. What an incredible idea, and look at the quality of the execution. Ice cream for my eyes. And, as always, please don't think of this algorithm as the end of the road like all papers, this is a stepping stone, and a few more works down the line, the kinks will be fixed, and the output quality is going to be vastly improved. The technique uses a conditional adversarial network to accomplish this. This works the following way: there is a generative neural network that creates new images all day, and a discriminator network is also available all day to judge whether these images look natural or not. During this process, the generator network learns to draw more realistic images, and the discriminator network learns to tell fake images from real ones. If they train together for long enough, they will be able to reliably create these image translations for a large set of different scenarios. There are two key differences that make this piece of work stand out from the classical generative adversarial networks: One both neural networks have the opportunity to look at the before and after images. Normally we restrict the problem to only looking at the after images, the final results. And two instead of only positive, both positive and negative examples are generated. This means that the generator network is also asked to create really bad images on purpose so that the discriminator network can more reliably learn the distinction between flippant attempts and quality craftsmanship. Another great selling point here is that we don't need several different algorithms for each of the cases, the same generic approach is used for all the maps and photographs, the only thing that is different is the training data. Twitter has blown up with fun experiments, most of them include cute drawings ending up as horrifying looking cats. As the title of the video says, the results are always going to be stunning, but sometimes, a different kind of stunning than we'd expect. It's so delightful to see that people are having a great time with this technique and it is always a great choice to put out such a work for a wide audience to play with. And if you got excited for this project, there are tons, and I mean tons of links in the video description, including one to the source code of the project, so make sure to have a look and read up some more on the topic, there's going to be lots of fun to be had! You can also try it for yourself, there is a link to an online demo in the description and if you post your results in the comments section, I guarantee there will be some amusing discussions. I feel that soon, a new era of video games and movies will dawn where most of the digital models are drawn by computers. As automation and mass-producing is a standard in many industries nowadays, we'll surely be hearing people going: ""Do you remember the good old times when video games were handcrafted? Man, those were the days!"". If you enjoyed this episode, make sure to subscribe to the series, we try our best to put out two of these videos per week. We would be happy to have have join our growing club of Fellow Scholars and be a part of our journey to the world of incredible research works such as this one. Thanks for watching and for your generous support, and I'll see you next time!"
141,Real-Time Fiber-Level Cloth Rendering,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work shows us how to render a piece of cloth down to the level of fibers. This is a difficult problem, because we need to be able to handle models that are built from potentially over a hundred million fiber curves. This technique supports a variety of goodies. One level of detail is possible. This means that the closer we get to the cloth, the more details appear, and that it is possible to create a highly optimized algorithm that doesn't render these details when they are not visible. This means a huge performance boost if we are zoomed out. Two optimizations are introduced so that fiber-level self-shadows are computed in real time, which would normally be an extremely long process. Note that we're talking millions of fibers here! And three the graphical card in your computer is amazingly effective at computing hundreds of things in parallel. However, its weak point is data transfer, at which it is woefully slow, to the point that it is often worth recomputing multiple gigabytes of data right on it just to avoid uploading it to its memory again. This algorithm generates the fiber curves directly on the graphical card to minimize such data transfers and hence, it maps really effectively to the graphical card. And the result is a remarkable technique that can render a piece of cloth down to the tiniest details, with multiple different kinds of yarn models, and in real-time. What I really like about this piece of work is that this is not a stepping stone, this could be used in many state of the art systems as-is, right now. The authors also made the cloth models available for easier comparisons in followup research works. Thanks for watching and for your generous support, and I'll see you next time!"
142,Shape and Material from Video,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Imagine the following: we put an object on a robot arm, and the input is a recorded video of it. And the output would be the digital geometry and a material model for this object. This geometry and material model we can plug into a photorealistic light simulation program to have a digital copy of our real-world object. First, we have to be wary of the fact that normally, solving such a problem sounds completely hopeless. We have three variables that we have to take into consideration: the lighting in the room, the geometry of the object, and the material properties of the object. If any two of the three variables is known, the problem is relatively simple and there are already existing works to address these combinations. For instance, if we create a studio lighting setup and know the geometry of the object as well, it is not that difficult to capture the material properties. Also, if the material properties and lighting is known, there are methods to extract the geometry of the objects. However, this is a way more difficult formulation of the problem, because out of the three variables, not two, not one, but zero are known. We don't have control over the lighting, the material properties can be arbitrary, and the geometry can also be anything. Several very sensible assumptions are being made, such as that our camera has to be stationary and the rotation directions of the object should be known, and some more of these are discussed in the paper in detail. All of them are quite sensible and they really don't feel limiting. The algorithm works the following way: in the first step, we estimate the lighting and leaning on this estimation, we build a rough initial surface model. And in the second step, using this surface model, we see a bit clearer, and therefore we can refine our initial guess for the lighting and material model. However, now that we know the lighting a bit better, we can again, get back to the surface reconstruction and improve our solution there. This entire process happens iteratively, which means, that first, we obtain a very rough initial guess for the surface, and we constantly refine this piece of surface to get closer and closer to the final solution. And now, feast your eyes on these incredible results, and marvel at the fact that we know next to nothing about the input, and the geometry and material properties almost magically appear on the screen. This is an amazing piece of work, and lots and lots of details and results are discussed the paper, which is quite well written and was a joy to read. Make sure to have a look at it, the link is available in the video description. Also, thanks for all the kind comments, I've experienced a recent influx of e-mails from people all around the world expressing how they are enjoying the series and many of them telling their personal stories and their relation to science, and how these new inventions are being talked about over dinner with the family and relatives. It has been such a delight to read these messages. Thank you! And also, thanks for watching and for your generous support, and I'll see you next time!"
143,Learning to Fill Holes in Images,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is from 2007, from ten years ago and I am sure you'll be surprised by how well it is still holding up to today's standards. For me, it was one of the works that foreshadowed the incredible power of data-driven learning algorithms. So, let's grab an image, and cut a sizeable part out of it, and try to algorithmically fill it with data that makes sense. Removing a drunk photobombing friend from your wedding picture, or a building blocking a beautiful view to the sea are excellent, and honestly, painfully real examples of this. This problem we like to call image completion or image inpainting. But mathematically, this may sound like crazy talk, who really knows what information should be there in these holes, let alone a computer? The first question is, why would we have to synthesize all these missing details from scratch? Why not start looking around in an enormous database of photographs and look for something similar? For instance, let's unleash a learning algorithm on one million images. And if we do so, we could find that there may be photographs in the database that are from the same place. But then, what about the illumination? The lighting may be different! Well, this is an enormous database, so then, we pick a photo that was taken at a similar time of the day and use that information! And as we can see in the results, the technique works like magic! Awesome! It doesn't require user made annotations or any sort of manual labor. These results were way, way ahead of the competition. And sometimes, the algorithm proposes a set of solutions that we can choose from. The main challenge of this solution is finding similar images within the database, and fortunately, even a trivial technique that we call nearest neighbor search can rapidly eliminate 99.99% of the dissimilar images. The paper also discusses some of the failure cases, which arise mostly from the lack of high-level semantic information, for instance, when we have to finish people, which is clearly not what this technique is meant to do, unless it's a statue of a famous person with many photographs taken in the database. Good that we're in 2017, and we know that plenty of research groups are already working on this, and I wouldn't be surprised to see a generative adversarial network-based technique to pop up for this in the very near future. Thanks for watching and for your generous support, and I'll see you next time!"
144,AI Builds 3D Models From Images With a Twist,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. CAD stands for computer aided design, basically a digital 3D model of a scene. Here is an incredibly difficult problem: what if we give the computer a photograph of a living room, and the output would be a digital, fully modeled 3D scene. A CAD model. This is a remarkably difficult problem: just think about it! The algorithm would have to have an understanding of perspective, illumination, occlusions, and geometry. And with that in mind, have a look at these incredible results. Now, this clearly sounds impossible. Not so long ago we talked about a neural network-based technique that tried to achieve something similar and it was remarkable, but the output was a low resolution voxel array, which is kind of like an approximate model built by children from a few large lego pieces. The link to this work is available in the video description. But clearly, we can do better, so how could this be possible? Well, the most important observation is that if we take a photograph of a room, there is a high chance that the furniture within are not custom built, but mostly commercially available pieces. So who said that we have to build these models from scratch? Let's look into a database that contains the geometry for publicly available furniture pieces and find which ones are seen in the image! So here's what we do: given a large amount of training samples, neural networks are adept at recognizing objects on a photograph. That would be step number one. After the identification, the algorithm knows where the object is, now we're interested in what it looks like and how it is aligned. And then, we start to look up public furniture databases for objects that are as similar to the ones presented in the photo as possible. Finally, we put everything in its appropriate place, and create a new digital image with a light simulation program. This is an iterative algorithm, which means that it starts out with a coarse initial guess that is being refined many many times until some sort of convergence is reached. Convergence means that no matter how hard we try, only minor improvements can be made to this solution. Then, we can stop. And here, the dissimilarity between the photograph and the digitally rendered image was subject to minimization. This entire process of creating the 3D geometry of the scene takes around 5 minutes. And this technique can also estimate the layout of a room from this one photograph. Now, this algorithm is absolutely amazing, but of course, the limitations are also to be candidly discussed. While some failure cases arise from misjudging the alignment of the objects, the technique is generally quite robust. Non-cubic room shapes are also likely to introduce issues such as the omission or misplacement of an object. Also, kitchens and bathrooms are not yet supported. Note that this is not the only paper solving this problem, I've made sure to link some more related papers in the video description for your enjoyment. If you have found this interesting, make sure to subscribe and stay tuned for more Two Minute Papers episodes! Thanks for watching and for your generous support, and I'll see you next time!"
145,Digital Creatures Learn to Cooperate,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Here's a really cool computer animation problem, you'll love this one: we take a digital character, specify an initial pose and a target objective for instance, a position somewhere in space. And the algorithm has to come up with a series of smooth movements and contact interactions to obtain this goal. But these movements have to be physically meaningful, such as that self-intersection and non-natural contortions have to be avoided throughout this process. Keep an eye out for the white cross to see where the target positions are. Now, for starters, it's cool to see that humanoids are handled quite well, but here comes the super fun part: the mathematical formulation of this optimization problem does not depend on the body type at all, therefore, both humanoids and almost arbitrarily crazy, non-humanoid creatures are also supported. If we make this problem a bit more interesting, and make changes to the terrain to torment these creatures, we'll notice that they come up with sensible movements to overcome these challenges. These results are absolutely amazing. And this includes obtaining highly non-trivial target poses, such as handstands. The goal does not necessarily have to be a position, but it can be an orientation or a given pose as well. We can even add multiple characters to the environment and ask them to join forces to accomplish a task together. And here you can see that both characters take into consideration the actions of the other one and not only compensate accordingly, but they make sure that this happens in a way that brings them closer to their objective. It is truly incredible to see how these digital characters can learn such complex animations in a matter of minutes. A true testament to the power of mathematical optimization algorithms. If you wish to hear more about how optimization works, we've had a previous episode on this topic, make sure to check it out, it includes a rigorous mathematical study on how to make the perfect vegetable stew. The link is available in the video description. And, if you feel a bit addicted to Two Minute Papers, please note that these episodes are available in early access through Patreon, click on the icon with the ""p"" at the ending screen if you're interested. It also helps us a great deal in improving the quality of the series. We try to be as transparent as possible, and every now and then we write a technical memo to summarize the recent improvements we were able to make, and this is all thanks to you. If you're interested, I've put a link to the latest post in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
146,How Do Hollywood Movies Render Smoke?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. How do people create these beautiful computer generated images and videos that we see in Hollywood blockbuster movies? In the world of light simulation programs, to obtain a photorealistic image, we create a digital copy of a scene, add a camera and a light source, and simulate the paths of millions of light rays between the camera and the light sources. This technique we like to call path tracing and it may take several minutes to obtain only one image on a powerful computer. However, in these simulations, the rays of light are allowed to bounce off of the surface of objects. In reality, many objects are volumes, where the rays of light can penetrate their surface and scatter around before exiting or being absorbed. Examples include not only rendering amazingly huge smoke plumes and haze, but all kinds of translucent objects, like our skin, marble, wax, and many others. Such an extended simulation program we call not path tracing but volumetric path tracing, and we can create even more beautiful images with it, however, this comes at a steep price: if the classical path tracing took several minutes per image, this addition of complexity often bumps up the execution time to several hours. In order to save time, we have to realize that not all light paths contribute equally to our image. Many of them carry barely any contributions, and only a tiny tiny fraction of these paths carry the majority of the information that we see in these images. So what if we could create algorithms that know exactly where to look for these high value light paths and systematically focus on them? This family of techniques we call importance sampling methods. These help us finding the regions where light is concentrated, if you will. This piece of work is an excellent new way of doing importance sampling for volumetric path tracing, and it works by identifying and focusing on regions that are the most likely to scatter light. And, it beats the already existing importance sampling techniques with ease. Now, to demonstrate how simple this technique is, a few years ago, during a discussion with one of the authors, Marcos Fajado, I told him that I would implement their method in real time on the graphical card in a smaller than four kilobyte program. So we made a bet. Four kilobytes is so little, we can store only a fraction of a second of mp3 music in it. Also, this is an empty file generated with Windows Word. Apparently, in some software systems, the definition of ""nothing"" takes several times more than 4 kilobytes. And after a bit of experimentation, I was quite stunned by the results, because the final result was less than 2 kilobytes, even if support for some rudimentary animations is added. The whole computer program that executes volumetric path tracing with this equiangular importance sampling technique fits on your business card... twice. Absolute insanity. I've put a link discussing some details in the video description. Now, don't be fooled by the simplicity of the presentation here, the heart and soul of the algorithm that created the rocket launch scene in Men in Black 3 is the same as this one. Due to legal reasons it is not advisable to show it to you in this video, but this is fortunately one more excellent reason for you to have a look at the paper! As always, the link is available in the video description. This is my favorite kind of paper, where there are remarkably large gains to be had, and it can be easily added to pretty much any light simulation program out there. I often like to say that the value/complexity ratio is tending towards infinity. This work is a prime example of that. By the way, Marcos and his team recently won a technical Oscar award, not only for this, but for their decades of hard work on their Arnold renderer, which is behind many many Hollywood productions. I've put a link to their website in the video description as well, have a look! Congrats guys! Thanks for watching and for your generous support, and I'll see you next time!"
147,Fast Photorealistic Fur and Hair With Cone Tracing,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a cone-based ray tracing algorithm for rendering photorealistic images of hair and furry objects, where the number of hair strands is typically over a hundred thousand. Okay, wait. What do these term mean exactly? Ray tracing means a bona fide light simulation program where we follow the path of many millions of light rays between the light sources and our camera. This usually means that light reflections and refractions, lens blur and defocus are taken into consideration. This feature is often referred to as depth of field, or DoF in short as you can see in the video. A fully ray traced system like this for hair and fur leads to absolutely beautiful images that you also see throughout this footage. So what about the cone-based part? Earlier, we had an episode about Voxel Cone Tracing, which is an absolutely amazing technique to perform ray tracing in real time. It works by replacing these infinitely thin light rays with thicker, cone-shaped rays which reduces the execution time of the algorithm significantly, at the cost of mostly a minor, sometimes even imperceptible degradation in image quality. Since the hair strands that we're trying to hit with the rays are extremely thin, and cone tracing makes the rays thicker, extending this concept to rendering fur without nontrivial extensions is going to be a fruitless endeavor. The paper contains techniques to overcome this issue, and an efficient data structure is proposed to store and find the individual hair strands, and a way to intersect these cones with the fibers. The algorithm is also able to adapt the cone sizes to the scene we have at hand. The previous techniques typically took at least 20 to 30 minutes to render one image, and with this efficient solution, we'll be greeted by a photorealistic image at least four to six times quicker, in less than 5 minutes, while some examples were completed in less than a minute. I cannot get tired of seeing these tiny photorealistic furry animals in Pixar movies and I am super happy to see there's likely going to be much, much more of these. By the way, if you're subscribed to the channel, please click the little bell next to the subscription icon to make sure that you never miss an episode. Also, you can also follow us on twitter for updates. Thanks for watching and for your generous support, and I'll see you next time!"
148,Game AI Development With OpenAI Universe,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. OpenAI's Gym was a selection of gaming environments for reinforcement learning algorithms. This is a class of techniques that are able to learn and perform an optimal chain of actions in an environment. This environment could be playing video games, navigating a drone around, or teaching digital creatures to walk. In this system, people could create new reinforcement learning programs and decide whose AI is the best. Gym was a ton of fun, ... but have a look this one! How is this different from Gym? This new software platform, Universe works not only for reinforcement learning algorithms, but for arbitrary programs. Like a freestyle wrestling competition for AI researchers! The list of games include GTA V, Mirror's Edge, Starcraft 2, Civilization 5, Minecraft, Portal and a lot more this time around. Super exciting! You can download this framework right now and proceed testing. One can also perform different browser tasks, such as booking a plane ticket and other endeavors that require navigating around in a web browser interface. Given the current software architecture for Universe, practically any task where automation makes sense can be included in the future. And, we don't need to make any intrusive changes to the game itself, in fact, we don't even have to have access to the source code. This is huge, especially given that many of these games are proprietary software, so to make this happen, individual deals had taken place between the game development companies and OpenAI. When the company was founded by Elon Musk and Sam Altman, they have picked up so many of the most talented AI researchers around the globe. And I am so happy to see that it really, really shows. There is an excellent blog post describing the details of the system, make sure to have a look! Also, I reckon that our comments section is the absolute best I've seen on YouTube. Feel free to participate or start a discussion, there are always plenty of amazing ideas in the comments section. Thanks for watching and for your generous support, and I'll see you next time!"
149,Enhance! Super Resolution From Google,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What is super resolution? Super resolution is process where our input is a coarse, low resolution image, and the output is the same image, but now with more details and in high resolution. We'll also refer to this process as image upscaling. And in this piece of work, we are interested in performing single image super resolution, which means that no additional data is presented to the algorithm that could help the process. Despite the incredible results seen in practically any of the crime solving television shows out there, our intuition would perhaps say that this problem, for the first sight, sounds impossible. How could one mathematically fill in the details when these details are completely unknown? Well, that's only kind of true. Let's not confuse super resolution with image inpainting, where we essentially cut an entire part out of an image and try to replace it leaning on our knowledge of the surroundings of the missing part. That's a different problem. Here, the entirety of the image is known, and the details require some enhancing. This particular method is not based on neural networks, but is still a learning-based technique. The cool thing here, is that we can use a training dataset, that is, for all intents and purposes, arbitrarily large. We can just grab a high resolution image, convert it to a lower resolution and we immediately have our hands on a training example for the learning algorithm. These would be the before and after images, if you will. And here, during learning, the image is subdivided into small image patches, and buckets are created to aggregate the information between patches that share similar features. These features include brightness, textures, and the orientation of the edges. The technique looks at how the small and large resolution images relate to each other when viewed through the lens of these features. Two remarkably interesting things arose from this experiment: one, it outperforms existing neural network-based techniques, two, it only uses 10 thousand images, and one hour of training time, which is in the world of deep neural networks, is so little, it's completely unheard of. Insanity. Really, really well done. Some tricks are involved to keep the memory consumption low, the paper discusses how it is done, and there are also plenty of other details within, make sure to have a look, as always, it is linked in the video description. It can either be run directly on the low resolution image, or alternatively we can first run a cheap and naive decade-old upscaling algorithm, and run this technique on this upscaled output to improve it. Note that super resolution is a remarkably competitive field of research, there are hundreds and hundreds of papers appearing on this every year, and almost every single one of them seems to be miles ahead of the previous ones. Where in reality, the truth is that most of these methods have different weaknesses and strengths, and so far I haven't seen any technique that would be viable for universal use. To make sure that a large number of cases is covered, the authors posted a sizeable supplementary document with comparisons. This gives so much more credence to the results. I am hoping to see a more widespread adoption of this in future papers in this area. For now, when viewing websites, I feel that we are close to the point where we could choose to transmit only the lower resolution images through the network and perform super resolution on them locally on our phones and computers. This will lead to significant savings on network bandwidth. We are living amazing times indeed. If you are enjoying the series, make sure to subscribe to the channel, or you can also pick up really cool perks on our Patreon page through this icon here with the letter P. Thanks for watching and for your generous support, and I'll see you next time!"
150,Large-Scale Fluid Simulations On Your Graphics Card,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The better fluid simulation techniques out there typically run on our graphical cards, which, if we formulate the problem in a way that the many compute units within can do in parallel, if they row in unison, if you will, we'll be greeted by an incredible bump in the speed of the simulation. This leads to amazingly detailed simulations, many of which you'll see in this footage. It's going to be really good! However, sometimes we have a simulation domain that is so large, it simply cannot be loaded into the memory of our graphical card. What about those problems? Well, the solution could be subdividing the problem into independent subdomains and solving them separately on multiple devices. Slice the problem up into smaller, more manageable pieces. Divide and conquer. But wait, we would just be pretending that these subdomains are independent, because in reality, they are clearly not, because there is a large amount of fluid flowing between them, and it takes quite a bit of algebraic wizardry to make sure that the information exchange between these devices happens correctly, and on time. But if we do it correctly, we can see our reward on the screen. Let's marvel at it together! Oh, hohoho, yeah! I cannot get tired of this. Typically, the simulation in the individual subdomains are computed on one or more separate graphical cards, and the administration on the intersecting interface takes place on the processor. The challenge of such a solution is that one has to be able to show that the solution of this problem formulation is equivalent to solving the huge original problem. And, it also has to be significantly more efficient to be useful for projects in the industry. The paper is one of the finest pieces of craftsmanship I've seen lately, the link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
151,AI Makes 3D Models From Photos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What if we tried to build a generative adversarial network for 3D data? This means that this network would work not on the usual 2 dimensional images, but instead, on 3 dimensional shapes. So, the generator network generates a bunch of different 3 dimensional shapes, and the basic question for the discriminator network would be are these 3D shapes real or synthetic? The main use case of this technique can be, and now watch closely, taking a photograph from a piece of furniture, and automatically getting a digital 3D model of it. Now it is clear for both of us that this is still a coarse, low resolution model, but it is incredible to see how a machine can get a rudimentary understanding of 3D geometry in the presence of occlusions, lighting, and different camera angles. That's a stunning milestone indeed! It also supports interpolation between two shapes, which means that we consider the presumably empty space between the shapes as a continuum, and imagine new shapes that are closer to either one or the other. We can do this kind of interpolation, for instance between two chair models. But the exciting thing is that no one said it has to be two objects of the same class. So we can go even crazier, and interpolate between a car and a boat. Since the technique works on a low-dimensional representation of these shapes, we can also perform these crazy algebraic operations between them that follow some sort of intuition. We can add two chairs together or subtract different kinds of tables from each other. Absolute madness. And one of the most remarkable things about the paper is that the learning took place on a very limited amount of data, not more than 25 training examples per class. One class we can imagine as one object type, such as, chairs, tables or cars. The authors made the the source code and a pretrained network available on their website, the link is in the video description, make sure to have a look! I am so happy to see breakthroughs like this in machine learning research. One after another in quick succession. This work is surely going to spark a lot of followup papers, and we'll soon find ourselves getting extremely high quality 3D models from photographs. Also, imagine combining this with a 3D printer! You take a photograph of something, run this algorithm on it, and then print a copy of that furniture or appliance for yourself. We are living amazing times indeed! Thanks for watching and for your generous support, and I'll see you next time!"
152,Text Style Transfer,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Before we start, it is important to emphasize that this paper is not using neural networks. Not so long ago, in 2015, the news took the world by storm: researchers were able to create a novel neural network-based technique for artistic style transfer, which had quickly become a small subfield of its own within machine learning. The problem definition was the following: we provide an input image and a source photograph, and the goal is to extract the artistic style of this photo and apply it to our image. The results were absolutely stunning, but at the same time, it was difficult to control the outcome. Later, this technique was generalized for higher resolution images, and instead of waiting for hours, it now works in almost real time and is used in several commercial products. Wow, it is rare to see a new piece of technology introduced to the markets so quickly! Really cool! However, this piece of work showcases a handcrafted algorithm that only works on a specialized case of inputs, text-based effects, but in this domain, it smokes the competition. And here, the style transfer happens not with any kind of neural network or other popular learning algorithm, but in terms of statistics. In this formulation, we know about the source text as well, and because of that we know exactly the kind of effects that are applied to it. Kind of like a before and after image for some beauty product, if you will. This opens up the possibility of analyzing its statistical properties and applying a similar effect to practically any kind of text input. The term statistically means that we are not interested in one isolated case, but we describe general rules, namely, in what distance from the text, what is likely to happen to it. The resulting technique is remarkably robust and works on a variety of input output pairs, and is head and shoulders beyond the competition, including the state of the art neural network-based techniques. That is indeed quite remarkable. I expect graphic designers to be all over this technique in the very near future. This is an excellent, really well-written paper and the evaluation is also of high quality. If you wish to see how one can do this kind of magic by hand without resorting to neural networks, don't miss out on this one and make sure to have a look! There is also a possibility of having a small degree of artistic control over the outputs, and who knows, some variant of this could open up the possibility of a fully animated style transfer from one image. Wow! And before we go, we'd like to send a huge shoutout to our Fellow Scholars who contributed translations of our episodes for a variety of languages. Please note that the names of the contributors are always available in the video description. It is really great to see how the series is becoming more and more available for people around the globe. If you wish to contribute, click on the cogwheel icon in the lower right and the substitles/cc text. Thank you so much! Thanks for watching and for your generous support, and I'll see you next time!"
153,Deep Learning Program Hallucinates Videos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Ever thought about the fact that we have a stupendously large amount of unlabeled videos on the internet? And, of course, with the ascendancy of machine learning algorithms that can learn by themselves, it would be a huge missed opportunity to not make any use of all this free data. This is a crazy piece of work where the idea is to unleash a neural network on a large number of publicly uploaded videos on the internet, and see how well it does if we ask it to generate new videos from scratch. Here, unlabeled means there is no information as to what we see in these videos, they are just provided as-is. Machine learning methods that work on this kind of data, we like to call unsupervised learning techniques. This work is based on a generative adversarial network. Wait, what does this mean exactly? This means that we have two neural networks that race each other, where one tries to generate more and more real-looking animations, and passes it over to the other that learns to tell real footage from fake ones. The first we call the generator network, and the second is the discriminator network. They try to outperform each other, and this rivalry goes on for quite a while and improves the quality of output for both neural networks, hence the name, generative adversarial networks. At first, we have covered this concept that was used to generate images from written text descriptions. The shortcoming of this approach was the slow training time that led to extremely tiny, low resolution output images. This was remedied by a followup work which proposed a two-stage version of this architecture. We have covered this in an earlier Two Minute Papers episode, as always, the link is available in the video description. It would not be an understatement to say that I nearly fell off the chair when seeing these incredible results. So, where do we go from here? What shall be the next step? Well, of course, video! However, the implementation of such a technique is far from trivial. In this piece of work, the generator network learns not on the original representation of the videos, but on the foreground and background video streams separately, and it also has to learn what combination of these yields realistic footage. This two-stream architecture is particularly useful in modeling real-world videos where the background is mostly stationary and there is an animated movement in the foreground. A train passing the station or people playing golf on the field are excellent examples of this kind of separation. We definitely need a high quality discriminator network as well, as in the final synthesized footage, not only the foreground and background must go well together, but the synthesized animations also have to be believable for human beings. This human being is, in our case, is represented by the discriminator network. Needless to say, this problem is extremely difficult and the quality of the discriminator network makes or breaks this magic trick. And of course, the all important question immediately arises: if there are multiple algorithms performing this action, how do we decide which one is the best? Generally, we get a few people, and show them a piece of synthesized footage with this algorithm and previous works, and have them decide which they deem more realistic. This is still the first step I expect these techniques to improve so rapidly that we'll soon find ourselves testing against real-world footage. And who knows, sometimes perhaps failing to recognize which is which. The results in the paper show that this new technique beats the previous techniques by a significant margin, and that users have a strong preference towards the two-stream architecture. The previous technique they compare against is an autoencoder, which we have discussed in a previous Two Minute Papers episode, check it out, it is available in the video description! The disadvantages of this approach are quite easy to identify this time around: we have a very limited resolution for these output video streams, that is, 64x64 pixels for 32 frames, which, even at modest framerate, is just slightly over one second of footage. The synthesized results vary greatly in quality, but it is remarkable to see that the machine can have a rough understanding of the concept of a large variety of movement and animation types. It is really incredible to see that the neural network learns about the representations of these objects and how they move, even when it wasn't explicitly instructed to do so. We can also visualize what the neural network has learned. This is done by finding different image inputs that make a particular neuron extremely excited. Here, we see a collection of inputs including these activations for images of people and trains. The authors' website is definitely worthy of checking out as some of the submenus are quite ample in results. Some amazing, some, well, a bit horrifying, but what is sure is that all of them are quite interesting. And before we go, a huge shoutout to László Csöndes, who helped us quite a bit in sorting out a number of technical issues with the series. Thanks for watching and for your generous support, and I'll see you next time!"
154,Amazing Slow Motion Videos With Optical Flow,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I am really excited to show this to you as I was looking to make this episode for quite a while. You'll see lots of beautiful slow-motion footage during the narration. And at first, it may seem disconnected from the narrative, but by the end of video, you'll understand why they look the way they do. Now, before we proceed, let's talk about the difference between interpolation and extrapolation. Interpolation means that we have measurement points for a given quantity, and we'd like to know what happened between these points. For instance, we have two samples of a person's location at four and at five o'clock, and we'd like to know where the guy was at four thirty. However, if we're doing extrapolation, we're interested in guessing a quantity beyond the reach of our sample points. For instance, extrapolation would be predicting what happens after the very last frame of the video. In our earlier episode, we talked about financial extrapolation, make sure to have a look, it was super fun. The link is in the video description. Optical flow is really useful because it can do this kind of interpolation and extrapolation for images. So let's do one of them right now. You'll see which it will be in a second. So this is a classical scenario that we often encounter when producing a new Two Minute Papers episode here, we have a 25 or 30 frames per second video on a 60 frames per second timeline. This means that roughly only every other frame is duplicated, and offers no new information. You can see this as I step through these individual frames. The more astute Fellow Scholars immediately would point out that wait, we have a lot of before and after image pairs, so we could do a lot better! Why don't we try to estimate what happened between these images? And that is exactly what we call frame interpolation. Interpolation because it is something between two known measurement points. And if we run the optical flow algorithm that can accomplish this, we can fill in these doubled frames with new ones that actually carry new information. So the ratio here was roughly two to one. Roughly every other frame provides new information. Super cool! So, what are the limits of this technique? What if we artificially slow the video down, so that it's much longer, so not only every other, but most of the frames are just duplicates? This results in a boring and choppy animation. Can we fill those in too? Note that the basic optical flow equations are written for tiny changes in position, so we shouldn't expect it to be able to extrapolate or interpolate any quantity over a longer period of time. But of course, it always depends on the type of motion we have at hand, so let's give it a try! As you can see, with optical flow, the algorithm has an understanding of the motions that take place in the footage, and because of that, we can get some smooth, buttery, slow motion footage that is absolutely mesmerizing. Almost like shooting with a slow motion camera. And note that the majority of these frames were not containing any information, and this motion was synthesized from these distant sample points that are miles and miles away from each other. However, it is also important to point out that optical flow is not a silver bullet and it should be used with moderation and special care as it can also introduce nasty artifacts like the one that you see here. This is due to an abrupt, high frequency change that is more difficult to predict than a slow and steady translation or rotation motion. To avoid these cases, we can use a much simpler frame interpolation technique that we call frame blending. This is a more naive technique that doesn't do any meaningful guesswork and computes the average of the two results. Why don't we give this one a try too? Or even better, let's have a look at the difference between the original choppy footage, and the interpolated versions with frame blending and optical flow. If we do that, we see that frame blending is unlikely to give us nasty artifacts, but in return, the results are significantly more limited compared to optical flow because it doesn't have an understanding of the motion taking place in the footage. So, the question is, when to use which? Well, until we get an algorithm that is able to adaptively decide when to use what, it still comes down to individual judgement and sometimes quite a bit of trial and error. I'd like to make it extremely sure that you don't leave this video thinking that this is the only application of optical flows. It's just one of the coolest ones! But this motion estimation technique also has many other uses. For instance, if we have an unmanned aerial vehicle, it's really great if we can endow it with an optical flow sensor, because then, it will be able to know in which direction it needs to rotate to avoid a tree, or whether it is stable or not at a given point in time. And, with your support on Patreon, we were not only able to bump up the resolution of future Two Minute Papers episodes to 4K, but we're also running them at true 60 frames per second, which means that every footage can undergo either a frame blending or optical flow step to make the animations smoother and more enjoyable for you. This takes a bit of human labor and is computationally expensive, but our new Two Minute Papers rig is now capable of handling this. It is fantastic to see that you Fellow Scholars are willing to support the series, and through this, we can introduce highly desirable improvements to the production pipeline. This is why we thank you at the end of every episode for your generous support. You Fellow Scholars are the best YouTube audience anywhere. And who knows, maybe one day, we'll be at a point where Two Minute Papers can be a full time endeavor, and we'll be able to make even more elaborate episodes. As I am tremendously enjoying making these videos, that would be absolutely amazing. Have you found any of these disturbing optical flow artifacts during this episode? Have you spotted some of these in other videos on YouTube? Let us know in the comments section so we can learn from each other. Thanks for watching and for your generous support, and I'll see you next time!"
155,Neural Network Learns The Physics of Fluids and Smoke,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work is still in progress done by one of the members of the Google Brain research team and several researchers from the amazing New York University. The goal was to show a neural network video footage of lots and lots of fluid and smoke simulations, and have it learn how the dynamics work, to the point that it can continue and guess how the behavior of a smoke puff would change in time. We stop the video and it would learn how to continue it, if you will. Now that is a tall order if I've ever seen one. Most of this episode will not be about the technical details of this method, but about the importance and ramifications of such a technique. And since almost all the time, our episodes are about already published works, it also makes a great case study on how to evaluate and think about the merits and shortcomings of a research project that is still in the works. This definitely is an interesting take as normally, we use neural networks to solve problems that are otherwise close to impossible to tackle. Here, the neural networks are applied to solve something that we already know how to solve. And the question immediately comes to mind, why would anyone bother to do that? We've had the very least 20 episodes on different kinds of incredible fluid simulation techniques, so it is abundantly clear that this is a problem that we can solve. However, the neural networks does not only solve it correctly in a sense that the results are easily confused with real footage, but what's more, the execution time of the algorithm is in the order of a few milliseconds for a reasonably sized simulation. This normally takes several minutes with traditional techniques. It does something that we already know quite well how to do, but it does it better in many regards. Loving the idea behind this work. Training is a pre-processing step that is a long and arduous process that only has to be done once, and afterwards, querying the neural network, that is, predicting what happens next in the simulation runs almost immediately. In any case, in way less time than calculating all the forces and pressures in the simulation while retaining high quality results. It is like the preparation for an exam that may take weeks, but when we're finally there in the examination room, if we're well prepared, we make short work of the puny questions the professor has presented us with. I am quietly noting that during my college years, I was also studying the beautiful Navier-Stokes equations and even as a highly motivated student, it took several months to understand the theory and write my first fluid simulator. This neural network can learn something very similar in a matter of days. What a stunning and, may I say humiliating revelation. Note that this piece of work has not yet been peer-reviewed, there are some side by side comparisons with real simulations to validate the accuracy of the algorithm, but more rigorous analysis is required before publishing. The failure cases for classical hand-crafted techniques are easier to identify because of the fact that their mathematical description is available for scrutiny. In the case of a neural network, this piece of mathematics is also there, but it's not intuitive for human beings, therefore it is harder to assess when it works well and when it is expected to break down. We should be particularly vigilant about this fact when evaluating a task performed by any kind of neural network-based learning algorithm. For now, the results look quite reassuring, even the phenomenon of a smoke puff bouncing back from an object is modeled with high fidelity. There was a loosely related work from the ETH Zürich and Disney Research in Switzerland, and enumerating the differences is a bit too technical for such a short video, but I have included it in the video description box for the more curious Fellow Scholars out there. Now you might have noticed the lack of the usual disclaimer in the thumbnail image, stating that I did not take any part in the project, which was not the case this time. I feel that it is important to mention my affiliation, even though my role in this project has been extremely tiny. You can read about this in the acknowledgements section of the paper. Needless to say, all the credit goes to the authors of the paper for this amazing idea. I envision all kinds of interactive digital media, including the video games of the future being infused with such neural networks for real-time fluid and smoke simulations. And let's not forget that this is only the first step: we haven't even talked about other kinds of perhaps learnable physical simulations with collision detection, shattering glassy objects, and gooey soft body simulations. And we also have seen the very first results with light simulation pipelines that are augmented with neural networks. I think it is now a thinly veiled fact that I am extremely excited for this. And this piece of work is not the destination, but a stepping stone towards something truly remarkable. Thanks for watching and for your generous support, and I'll see you next time!"
156,Stunning Video Game Graphics With Voxel Cone Tracing (VXGI),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I consider this one to be one of the most influential papers in the field of light transport. Normally, to create a photorealistic image, we have to create a digital copy of a scene, and simulate the paths of millions of light rays between the camera and the light sources. This is a very laborious task that usually takes from minutes to hours on a complex scene, noting that there are many well-known corner cases that can take up to days as well. As the rays of light can bounce around potentially indefinitely, and if we add that realistic materials and detailed scene geometry descriptions are not easy to handle mathematically, it is easy to see why this is a notoriously difficult problem. Simulating light transport in real time has been an enduring problem and is still not solved completely for every possible material model and light transport effect. However, Voxel Cone Tracing is as good of a solution as one can wish for at the moment. The original formulation of the problem is continuous, which means that rays of light can bounce around in infinitely many directions and the entirety of this digital world is considered to be a continuum. If we look at the mathematical formulation, we see infinities everywhere we look. If you would like to learn more about this, I am holding a Master-level course at the Technical University of Vienna, the entirety of which is available on YouTube. As always, the link is available in the video description for the more curious Fellow Scholars out there. If we try to approximate this continuous representation with tiny tiny cubes, we get a version of the problem that is much less complex, and easier to tackle. If we do this well, we can make it adaptive, which means that these cubes are smaller where there is a lot of information so we don't lose out on many details. This data structure we call a sparse voxel octree. For such a solution, mathematicians like to say that this technique works on a discretized version of the continuous problem. And since we're solving a vastly simplified version of the problem, the question is always whether this way we can remain true to the original solution. And the results show beauty unlike anything we've seen in computer game graphics. Just look at this absolutely amazing footage. Ice cream for my eyes. And all this runs in real time on your consumer graphics card. Imagine this in the virtual reality applications of the future. My goodness. I've chosen the absolute best profession. Also, this technique maps really well to the graphical card and is already implemented in Unreal Engine 4 and NVIDIA has a framework, GameWorks, where they are experimenting with this in their project by the name VXGI. I had a very pleasant visit at NVIDIA's GameWorks lab in Switzerland not so long ago, friendly greetings to all the great and fun people in the team! Some kinks still have to be worked out. For instance, there are still issues with light leaking through thin objects. Beyond that, the implementation of this algorithm contains a multitude of tiny little distinct elements. It is indeed true that many of the elements are puzzle pieces that are interchangeable and can be implemented in a number of different ways, and that's likely one of the reasons why NVIDIA and others are still preparing their implementation for widespread industry use. Soon, we'll be able to solidify the details some more and see what the best practices are. I cannot wait to see this technique appear in the video games of the future. Note that this, and future episodes will be available in 4K resolution for a significant bump in the visual quality of the series. It takes a ton of resources to produce these videos, but it's now possible through the support of you Fellow Scholars on Patreon. There is a more detailed write-up on that, I've included it in the video description. Thank you so much for supporting the show throughout 2016, and looking forward to continuing our journey together in 2017! Thanks for watching and for your generous support, and I'll see you next time!"
157,Image Synthesis From Text With Deep Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is what we have been waiting for. Earlier, we talked about a neural network that was able to describe in a full sentence what we can see on an image. And it had done a damn good job at that. Then, we have talked about a technique that did something really crazy, the exact opposite: we wrote a sentence, and it created new images according to that. This is already incredible. And we can create an algorithm like this by training not one, but two neural networks: The first is the generative network, that creates millions of new images, and the discriminator network judges whether these are real or fake images. The generative network can improve its game based on the feedback and will create more and more realistic looking images, while the discriminator network gets better and better at telling real images from fake ones. Like humans, this rivalry drives both neural networks towards perfecting their crafts. This architecture is called a generative adversarial network. It is also like the classical, evergoing arms race between criminals who create counterfeit money and the government, which seeks to implement newer and newer measures to tell a real hundred dollar bill from a fake one. The previous generative adversarial networks were adept at creating new images, but due to their limitations, their image outputs were the size of a stamp at best. And we were wondering, how long until we get much higher resolution images from such a system? Well, I am delighted to say that apparently, within the same year. In this work, a two-stage version of this architecture is proposed. The stage 1 network is close to the generative adversarial network we described. And most of the fun happens in the stage 2 network, that takes this rough, low resolution image and the text description and is told to correct the defects of the previous output and create a higher resolution version of it. In the video, the input text description and the stage-1 results are shown, and building on that, the higher resolution stage-2 images are presented. And the results are... unreal. There was a previous article and Two Minute Papers episode on the unreasonable effectiveness of recurrent neural networks. If that is unreasonable effectiveness, then what is this? The rate of progress in machine learning research is unlike any other field I have ever seen. I honestly can't believe what I am seeing here. Dear Fellow Scholars, what you see might very well be history in the making. Are there still faults in the results? Of course there are. Are they perfect? No, they certainly aren't. However, research is all about progress and it's almost never possible to go from 0 to a 100% with one new revolutionary idea. However, I am sure that in 2017, researchers will start working on generating full HD animations with an improved version of this architecture. Make sure to have a look at the paper, where the ideas, challenges, and possible solutions are very clearly presented. And for now, I need some time to digest these results. Currently I feel like being dropped into the middle of a science-fiction movie. And, this one will be our last video for this year. We have had an amazing year with some incredible growth on the channel, way more of you Fellow Scholars decided to come with us on our journey than I would have imagined. Thank you so much for being a part of Two Minute Papers, we'll be continuing full steam ahead next year, and for now, I wish you a Merry Christmas and happy holidays. 2016 was an amazing year for research, and 2017 will be even better. Stay tuned! Thanks for watching and for your generous support, and I'll see you next time!"
158,Crumpling Sound Synthesis,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, we're going to crush some soda cans. In the footage that you see here, the animations are performed by an already existing algorithm for thin shell deformations, and for a complete sensorial experience, this piece of work aims synthesize sound for these phenomena. Sounds for crumpling up all kinds of candy wraps, foils, and plastic bags. A lofty, noble goal, loving it. However, this problem is extraordinarily difficult. The reason is that these crumpling simulations are amazingly detailed, and even if we knew all the physical laws for the sound synthesis, which is already pretty crazy, it would still be a fruitless endeavor to take into consideration every single thing that takes place in the simulation. We have to come up with ways to cut corners to decrease the execution time of our algorithm. Running a naive, exhaustive search would take tens of hours for only several seconds of footage. And the big question is, of course, what can we do about it? And before we proceed, just a quick reminder that the geometry of these models are given by a lot of connected points that people in computer graphics like to call vertices. The sound synthesis takes place by observing the changes in the stiffness of these models, which is the source of the crumpling noise. Normally, our sound simulation scales with the number of vertices, and it is abundantly clear that there are simply too many of them to go through one by one. To this end, we should strive to reduce the complexity of this problem. First, we start with identifying and discarding the less significant vibration modes. Beyond that if in one of these vertices, we observe that a similar kind of buckling behavior is present in its neighborhood, we group up these vertices into a patch, and we then forget about the vertices and run the sound synthesis on these patches. And of course, the number of patches is significantly less than the number of vertices in the original model. In this footage, you can see some of these patches. And it turns out that the execution time can be significantly decreased by these optimizations. With these techniques, we can expect results in at least 5 times quicker, but if we're willing to introduce slight degradations to the quality of the sounds, we can even go 10 times quicker with barely perceptible changes. To evaluate the quality of the solutions, there is a user study presented in the paper. And the pinnacle of all tests is of course, when we let reality be our judge. Everything so far sounds great on paper, but how does it compare to what we experience in reality? Wow. Truly excellent results. Suffice to say, they are absolutely crushing it. And we haven't even talked about stochastic enrichment and how one of these problems can be solved optimally via dynamic programming. If you're interested, make sure to have a look at the paper! Thanks for watching and for your generous support, and I'll see you next time!"
159,3D Printing Flexible Shells For Molding,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about 3D printing flexible molds for objects with detailed geometry. The main observation is that the final object not only has to be cast, but also has to be removed conveniently from the mold. Finding an appropriate layout for the cuts is a non-trivial problem. The technique endeavors to have the least amount of cuts, and the length of the cuts is also subject to minimization. I see the light bulb lighting up in the heads of our seasoned Fellow Scholars, immediately noticing that this sounds like an optimization problem. And in this problem, we start out from a dense cut layout, and iteratively remove as many of these cuts as possible until some prescribed threshold is met. However, we have to be vigilant about the fact that these cuts will result in deformations during the removal process. We mentioned before that we're interested in shapes that have geometry that is rich in details, therefore this distortion effect is to be minimized aggressively. Also, we cannot remove these cuts indefinitely, because sometimes, more cuts have to be added to reduce the stress induced by the removal process. This is a cunning plan, however, a plan that only works if we can predict where and how these deformations will happen, therefore we have to simulate this process on our computer. During removal, forces are applied to the mold, which we also have to take into consideration. To this end, there is an actual simulation of the entirety of the extraction process to make sure that the material can be removed from the mold in a non-destructive manner. Wow! The paper discusses tons of issues that arise from this problem formulation, for instance, what one should do with the tiny air bubbles stuck in the resin. Or, the optimization part is also non-trivial, to which a highly effective homebrew solution is presented. And there's a lot more, make sure to have a look! Of course, as always, we would love to hear your ideas about possible applications of this technique. Leave your thoughts in the comments section! Thanks for watching and for your generous support, and I'll see you next time!"
160,Multiphase Fluid Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What a wonderful day to talk about fluid simulations! This technique is an extension to Smoothed Particle Hydrodynamics, or SPH in short, which is a widely used particle-based simulation technique where the visual quality scales with the number of simulated particles. The more particles we use in the simulation, the more eye candy we can expect. And the goal here is to create an extension of these SPH-based simulations to include deformable bodies and granular materials to the computations. This way, it is possible to create a scene where we have instant coffee and soft candy dissolving in water. Which not only looks beautiful, but sounds like a great way to get your day started. Normally, we have to solve a separate set of equations for each of the phases or material types present, but because this proposed method is able to put them in one unified equation, it scales well with the number of materials within the simulation. This is not only convenient from a theoretical standpoint, but it also maps well to parallel architectures and the results shown in the video were run on a relatively high-end consumer NVIDIA card. This is remarkable, as it should not be taken for granted that a new fluid simulation technique runs well on the GPU. The results indeed indicate that the number of phases only have a mild effect on the execution time of the algorithm. A nice and general framework for fluid-solid interactions, dissolution, elastoplastic solids, and deformable bodies. What a fantastic value proposition. I could watch and play with these all day. I'll try my best to resist, but in case the next episode is coming late, you know where I am. The quality of the paper is absolutely top tier and if you like physics, you're going to have lots of fun reading it. Thanks for watching and for your generous support, and I'll see you next time!"
161,Precomputed Deformation Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work is about reducing the time needed to simulate elastic deformations by means of precomputation. Okay, so what does the term precomputation mean? If we are at an open book exam and we are short on time, which is, basically, every time, it would be much better to do a precomputation step, namely studying at home for a few days before, and then, when we are there, we are endowed with quite a bit of knowledge, and are guaranteed to do much better than trying to grasp the simplest concepts on the spot. This precomputation step we only have to do once, and it almost doesn't matter how lengthy it is, because after that, we can answer any question in this topic in the future. Well, sometimes passing exams is not as easy as described here, but a fair bit of precomputation often goes a long way. Just saying. The authors have identified 3 major bottlenecks in already existing precomputation techniques, and proposed optimizations to speed them up considerably at the cost of higher memory consumption. For instance, the algorithm is trained on a relatively large set of training pose examples. If we don't have enough of these training examples, the quality of the animations will be unsatisfactory, but if we use too many, that's too resource intensive. We have to choose just the right amount, and the right kinds of poses, which is a highly non-trivial process. Note that this training is not the same kind of training we are used to see with neural networks. This work doesn't have anything to do with neural networks at all! The results of the new technique are clearly very close to the results we would obtain with standard methods, however, the computation time is 20 to 2000 times less. In the more favorable cases, computing deformations that would take several hours can take less than a second. That is one jaw-dropping result and a hefty value proposition indeed. This example shows that after a short precomputation step, we can start torturing this poor armadillo and expect high-quality elastic deformations. And there is a lot of other things to be learned from the paper. Gram-Schmidt orthogonalization, augmented Krylov iterations, Newton-PCG solvers. Essentially, if you pick up a dry textbook on linear algebra, and for every technique you see there, you ask what on Earth this is useful for, you wouldn't have to go through hundreds of works, you would find a ton of answers in just this one absolutely amazing paper. Also, please don't forget that you Fellow Scholars make Two Minute Papers happen. If you wish to support the show and get access to cool perks, like an exclusive early access program where you can watch these episodes 16-24 hours in advance, check out our page on Patreon. Just click on the icon with the letter P at the end of this video or just have a look at the video description. Thanks for watching and for your generous support, and I'll see you next time!"
162,Sound Propagation With Bidirectional Path Tracing,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Imagine if we had an accurate algorithm to simulate how different sound effects would propagate in a virtual world. We would find computer games exhibiting gunfire in open areas, or a pianist inside a castle courtyard to be way more immersive, and we've been waiting for efficient techniques for this for quite a while now. This is a research field where convolutions enjoy quite a bit of attention due to the fact that they are a reliable and efficient way to approximate how a given signal would sound in a room with given geometry and material properties. However, the keyword is approximate this, however, is one of those path sampling techniques that gives us the real deal, so quite excited for that! So what about this path sampling thing? This means an actual simulation of sound waves. We have a vast literature and decades of experience in simulating how rays of light bounce and reflect around in a scene, and leaning on this knowledge, we can create beautiful photorealistic images. The first idea is to adapt the mathematical framework of light simulations to be able to do the very same with sound waves. Path tracing is a technique where we build light paths from the camera, bounce them around in a scene, and hope that we hit a light source with these rays. If this happens, then we compute the amount of energy that is transferred from the light source to the camera. Note that energy is a more popular and journalistic term here, what researchers actually measure here is a quantity called radiance. The main contribution of this work is adapting bidirectional path tracing to sound. This is a technique originally designed for light simulations that builds light paths from both the light source and the camera at the same time, and it is significantly more efficient than the classical path tracer on difficult indoors scenes. And of course, the main issue with these methods is that they have to simulate a large number of rays to obtain a satisfactory result, and many of these rays don't really contribute anything to the final result, only a small subset of them are responsible for most of the image we see or sound we hear. It is a bit like the Pareto principle or the 80/20 rule  on steroids. This is ice cream for my ears. Love it! This work also introduces a metric to not only be able to compare similar sound synthesis techniques in the future, but the proposed technique is built around minimizing this metric, which leads us to an idea on which rays carry important information and which ones we are better off discarding. I also like this minimap on the upper left that actually shows what we hear in this footage, exactly where the sound sources are and how they change their positions. Looking forward to seeing and listening to similar presentations in future papers in this area! A typical number for the execution time of the algorithm is between 15-20 milliseconds per frame on a consumer-grade processor. That is about 50-65 frames per second. The position of the sound sources makes a great deal of difference for the classical path tracer. The bidirectional path tracer, however, is not only more effective, but offers significantly more consistent results as well. This new method is especially useful in these cases. There are way more details explained in the paper, for instance, it also supports path caching and also borrows the all-powerful multiple importance sampling from photorealistic rendering research. Have a look! Thanks for watching and for your generous support, and I'll see you next time!"
163,Water Wave Simulation with Dispersion Kernels,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this piece of work, we are interested in simulating the dynamics of water waves. There are quite a few forces acting on a bucket of water, such as surface tension, internal pressure, external force fields (such as wind, for instance), and gravity. Therefore, it is not a surprise that these waves can become quite complex with a lot of high-frequency details that are difficult to simulate. Accurately modeling wave reflections after colliding with solids is also an important and highly sought-after detail to capture. This piece of work simulates Sir George Biddell Airy's dispersion model. Now what does this mean exactly? The Airy model describes many common wave phenomena accurately, such as how longer waves are dominated by gravitational forces and how shorter waves dance mostly according the will of surface tension. However, as amazing this theory is, it does not formulate these quantities in a way that would be directly applicable to a computer simulation. The main contribution of this paper is a new convolution formulation of this model and some more optimizations that can be directly added into a simulation, and not only that, but the resulting algorithm parallelizes and maps well to the graphical card in our computers. We have earlier discussed what a convolution is. Essentially, it is a mathematical operation that can add reverberation to the sound of our guitar, or accurately simulate how light bounces around under our skin. Links to these episodes are available in the video description and at the end of the video, check them out, I am sure you'll have a lot of fun with them! Regarding applications as the technique obeys Airy's classical dispersion model, I expect and hope this to be useful for ocean and coastal engineering and in simulating huge tidal waves. Note that limitations apply, for instance, the original linear theory is mostly good for shallow water simulations and larger waves in deeper waters. The proposed approximation itself also has inherent limitations, such as the possibility of waves going through thinner objects. The resulting algorithm is, however, very accurate and honestly, a joy to watch. It is also shown to support larger-scale scenes here you see how beautifully it can simulate the capillary waves produced by these rain drops and of course, the waves around the swans in the pond. This example took roughly one and a half second per frame to compute. You know the drill a couple more followup papers down the line, and it will surely run in real time. Can't wait! Also, please let me know in the comments section whether you have found this episode understandable. Was it easy to follow? Too much? Your feedback is, as always, highly appreciated. Thanks for watching and for your generous support, and I'll see you next time!"
164,3D Printing Acoustic Filters,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What is an acoustic filter? Well, it is an arbitrarily shaped object that takes a sound as an input, and outputs a different sound. These filters have some really amazing applications that you'll hear about in a minute. In this work, a novel technique is proposed to automatically design such filters. It works by building an arbitrarily shaped object as a set of connected tiny resonators, and chooses appropriate sizes and setups for each of these elements to satisfy a prescribed set of acoustic properties. Instead of resorting to a lengthy and flimsy trial and error phase, we can use physics to simulate what would happen if we were to use a given arrangement in reality. Again, one of those works that have a deep connection to the real world around us. Absolutely amazing. The goal can be to eliminate the peaks of the sound of a car horn or an airplane engine, and we can achieve this objective by means of optimization. The proposed applications we can divide into three main categories: The first is identifying and filtering noise attenuation components for a prescribed application, to which we can also refer to as muffler design. In simpler words, we are interested in filtering or muffling the peaks of a known signal. Designing such objects is typically up to trial and error and in this case, it is even harder because we're interested in a wider variety of shape choices other than exhaust pipes and tubes that are typically used in the industry. Second, designing musical instruments is hard, and unless we design them around achieving a given acoustic response, we'll likely end up with inharmonious gibberish. And this method also supports designing musical instruments with, hmm, well, non-conventional shapes. Well, this is as non-conventional as it gets I'm afraid. And also, that is about the most harmonious sound I've heard coming out of the rear end of a hippo. And third, this work opens up the possibility of making hollow objects that are easy to identify by means of acoustic tagging. Check out this awesome example that involves smacking these 3d printed piggies. If you feel like improving your kung fu in math, there are tons of goodies such as transmission matrices, the Helmholtz equation, oh my! The paper and the talk slides are amazingly well written, and yes, you should definitely have a look at them. Let us know in the comments section if you have some ideas for possible applications beyond these ones, we love to read your take on these works. Thanks for watching and for your generous support, and I'll see you next time!"
165,Synchronizing Animations To Sound,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is going to be absolutely amazing. Earlier, we had some delightful discussions on synthesizing sound from animations. The input would be a sequence, for instance, a video depicting the complete and utter destruction of plates, wooden bunnies, or footage of bubbling water. And the output should be a physical simulation that yields appropriate sound effects for the observed phenomenon. In short: input, animation, output, synthesized sound effects for this animation. And, get this! What if we would turn the problem around, where we have sound as an input, and we try to synthesize an animation that could create such a sound. Mmm, I like it, a very spicy project indeed. And however crazy this idea may sound, given the richness of sound effects in nature, it may actually be easier to generate a believable animation than a perfect sound effect. It is extremely difficult to be able to match the amazingly detailed real-world sound of, for instance a sliding, rolling or bouncing bolt with a simulation. The more I think about this, the more I realize that this direction actually makes perfect sense. And, no machine learning is used here, if we look under the hood, we'll see a pre-generated database of rigid body simulations with dozens of different objects, and a big graph that tries to group different events and motions together and encode the order of execution of these events and motions. Now hold on to your papers, and let's check out the first round of results together. Wow. I think it would be an understatement to say that they nailed it. And what's more, we can also add additional constraints, like a prescribed landing location to the object to make sure that the animations are not too arbitrary, but are more in line with our artistic vision. Crazy. As I am looking through the results, I am still in complete disbelief. This shouldn't be possible. Also, please don't get the impression that this is all there is to this technique. There are a lot more important details that we haven't discussed here that the more curious Fellow Scholars could be interested in... discrete and continuous time contact events, time warping and motion connections. There are tons of goodies like these in the paper, please have a look to be able to better gauge and appreciate the merits of this work. Some limitations apply, such as the environment is constrained to be this plane that we've seen in these animations, and as always with works that are inventing something completely new it currently takes several minutes, which is not too bad, but of course, there is plenty of room to accelerate the execution times. And some bonus footage! This will be just spectacular for creating music videos, animated movies, and I am convinced that professional artists will be able to do incredible things with such a tool. Thanks for watching, and for your generous support, and I'll see you next time!"
166,Deep Learning Program Simplifies Your Drawings,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. First, let's talk about the raster and vector graphics. What do these terms mean exactly? A raster image is a grid made up of pixels, and for each of these pixels, and for each of the pixels, we specify a color. That's all there is in an image it is nothing but a collection of pixels. All photographs on your phone, and generally most images you encounter are raster images. It is easy to see that the quality of such images greatly depends on the resolution of this grid of course, the more grid points, the finer the grid is, the more details we can see. However, in return, if we disregard compression techniques, the file size grows proportionally to the number of pixels, and if we zoom in too close, we shall witness these classic staircase effects that we like to call aliasing. However, if we are designing a website, or a logo for a company, which should look sharp on all possible devices and zoom levels, vector graphics is a useful alternative. Vector images are inherently different from raster images, as the base elements of the image are not pixels, but vectors and control points. The difference is like storing the shape of a circle on a lot of pixels point by point, which would be a raster image, or just saying that I want a circle on these coorindates with a given radius. And as you can see in this example, the point of this is to have razor sharp images at higher zoom levels as well. Unless we go too crazy with fine details, file sizes are also often remarkably small for vector images, because we're not storing the colors of millions of pixels. We are only storing shapes. If we want to sound a bit more journalistic we can kind of say that vector images have infinite resolution. We can zoom in as much as we wish, and we won't lose any detail during this process. Vectorization is the process where we try to convert a raster image to a vector image. Some also like to call this process image tracing. The immediate question arises why are we not using vector graphics everywhere? Well, one, the smoother the color transitions and the more detail we have in our images, the quicker the advantage of vectorization evaporates. And two, also note that this procedure is not trivial and we are also often at the mercy of the vectorization algorithm in terms of output quality. It is often unclear in advance whether it will work well on a given input. So now we know everything we need to know to be able to understand and appreciate this amazing piece of work. The input is a rough sketch, that is a raster image, and the output is a simplified, cleaned-up and vectorized version of it. We're not only doing vectorization, but simplification as well. This is a game changer, because this way, we can lean on the additional knowledge that these input raster images are sketches, hand-drawn images, therefore there is a lot of extra fluff in them that would be undesirable to retain in the vectorized output, therefore the name, sketch simplification. In each of these cases, it is absolute insanity how well it works. Just look at these results! The next question is obviously, how does this wizardry happen? It happens by using a classic deep learning technique, a convolutional neural network, of course, that was trained on a large number of input and output pairs. However, this is no ordinary convolutional neural network! This particular variant differs from the standard well-known architecture as it is augmented with a series of upsampling convolution steps. Intuitively, the algorithm learns a sparse and concise representation of these input sketches, this means that it focuses on the most defining features and throws away all the unneeded fluff. And the upsampling convolution steps make it able to not only understand, but synthesize new, simplified, and high-resolution images that we can easily vectorize using standard algorithms. It is fully automatic and requires no user intervention. In case you are scratching your head about these convolutions, we have had plenty of discussions about this peculiar term before, I have linked the appropriate episodes in the video description box. I think you'll find them a lot of fun in one of them, I pulled out a guitar and added reverberation to it using convolution. It is clear that there is a ton of untapped potential in using different convolution variations in deep neural networks. We've seen in a DeepMind paper earlier that used dilated convolutions for state of the art speech synthesis, that is a novel convolution variant and this piece of work is no exception either. There is also a cool online demo of this technique that anyone can try. Make sure to post your results in the comments section! We'd love to have a look at your findings. Also, have a look at this Two Minute Papers fan art. A nice little logo one of our kind Fellow Scholars sent in. It's really great you see that you've taken your time to help out the series, that's very kind of you. Thank you! Thanks for watching, and for your generous support, and I'll see you next time!"
167,Human Pose Estimation With Deep Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Pose estimation is an interesting area of research where we typically have a few images or video footage of humans, and we try to automatically extract the pose this person was taking. In short, the input is mostly a 2D image, and the output is typically a skeleton of the person. Applications of pose estimation include automatic creation of assets for computer games and digital media, analyzing and coaching the techniques of athletes, or helping computers understand what they see for the betterment of robotics and machine learning techniques. And this is just a taste, the list was by no means exhaustive. Beyond the obvious challenge of trying to reconstruct 3D information from a simple 2D image, this problem is fraught with difficulties as one has to be able to overcome the ambiguity of lighting, occlusions, and clothing covering the body. A tough problem, no question about that. An ideal technique would do this automatically without any user intervention, which sounds like wishful thinking. Or does it? In this paper, a previously proposed convolutional neural network is used to predict the position of the individual joints, and curiously, it turns out that we can create a faithful representation of the 3D human body from that by means of optimization. We have had a previous episode on mathematical optimization, you know the drill, the link is available in the video description box. What is remarkable here is that not only the pose, but the body type is also inferred, therefore the output of the process is not just a skeleton, but full 3D geometry. It is coarse geometry, so don't expect a ton of details, but it's 3D geometry, more than what most other competing techniques can offer. To ease the computational burden of this problem, in this optimization formulation, healthy constraints are assumed that apply to the human body, such as avoiding unnatural knee and elbow bends, and self-intersections. If we use these constraints, the space in which we have to look for possible solutions shrinks considerably. The results show that this algorithm outperforms several other state of the art techniques by a significant margin. It is an auspicious opportunity to preserve and recreate a lot of historic events in digital form, maybe even use them in computer games, and I am sure that artists will make great use of such techniques. Really well done, the paper is extremely well written, the mathematics and the optimization formulations are beautiful, it was such a joy to read. Regarding the future, I am pretty sure we're soon going to see some pose and skeleton transfer applications via machine learning. The input would be a real-world video with a person doing something, and we could essentially edit the video and bend these characters to our will. There are some exploratory works in this area already, the Disney guys for instance are doing quite well. There will be lots of fun to be had indeed! Also, make sure to check out the YouTube channel of Welch Labs, who has a great introductory series for neural networks, which is in my opinion, second to none. He also has a new series called ""Learning to see"", where he codes up a machine learning technique for a computer vision application. It is about counting the number of fingers on an image. Really cool, right? The quality of these videos is through the roof, the link for both of these series are available in the description box, make sure to check them out! Thanks for watching, and for your generous support, and I'll see you next time!"
168,Computer Games Empower Deep Learning Research,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What are datasets? A dataset is typically a big bunch of data, for instance, a database of written letters, digits, images of human faces, stock market data that scientists can use to test their algorithms on. If two research groups wish to find out whose algorithm performs better at recognizing traffic signs, they run their techniques on one of these datasets and test their methods on equal footings. For instance, the CamVid dataset stands for Cambridge-driving Labeled Video Database, and it offers several hundreds of images depicting a variety of driving scenarios. It is meant to be used to test classification techniques: the input is an image, and the question is for each of the pixels, which one of them belongs to what class. Classes include roads, vegetation, vehicles, pedestrians, buildings, trees and more. These regions are labeled with all the different colors that you see on these images. To have a usable dataset, we have to label tens of thousands of these images, and as you may imagine, creating such labeled images requires a ton of human labor. The first guy has to accurately trace the edges of each of the individual objects seen on every image, and there should be a second guy to cross-check and make sure everything is in order. That's quite a chore. And we haven't even talked about all the other problems that arise from processing footage created with handheld cameras, so this takes quite a bit of time and effort with stabilization and calibration as well. So how do we create huge and accurate datasets without investing a remarkable amount of human labor? Well, hear out this incredible idea. What if we would record a video of us wandering about in an open-world computer game, and annotate those images. This way, we enjoy several advantages: 1. Since we have recorded continuous videos, after annotating the very first image, we will have information from the next frames, therefore if we do it well, we can propagate a labeling from one image to the next one. That's a huge time saver. 2. In a computer game, one can stage and record animations of important, but rare situations that would otherwise be extremely difficult to film. Adding rain or day and night cycles to a set of images is also trivial, because we simply can query the game engine to do this for us. 3. Not only that, but the algorithm also has some knowledge about the rendering process itself. This means that it looks at how the game communicates with the software drivers and the video card, tracks when the geometry and textures for a given type of car are being loaded or discarded, and uses this information to further help the label propagation process. 4. We don't have any of the problems that stem from using handheld cameras. Noise, blurriness, problems with the lens, and so on are all non-issues. Using this previous CamVid dataset, the annotation of one image takes around 60 minutes, while with this dataset, 7 seconds. Thus, the authors have published almost 25000 high-quality images and their annotations to aid computer vision and machine learning research in the future. That's a lot of images, but of course, the ultimate question arises: how do we know if these are really high-quality training samples? They were only taken from a computer game after all! Well, the results show that using this dataset, we can achieve an equivalent quality of learning compared to the CamVid dataset by using one third as many images. Excellent piece of work, absolutely loving the idea of using video game footage as a surrogate for real-world data. Fantastic. And in the meantime, while we're discussing computer graphics, here's a nice computer graphics challenge grant from Experiment. Basically, if you start a new research project through their crowdfunded system, you may win additional funding that comes straight for them. Free money. If you are interested in doing any kind of research in this area, or if you are a long-time practitioner, make sure to have a look. The link is available in the video description box. Thanks for watching, and for your generous support, and I'll see you next time!"
169,Building a Community Around Two Minute Papers,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a quick update on our plans with Two Minute Papers. The more I look at our incredibly high quality comments sections on YouTube or the community efforts that started out on our subreddit, the more I get the feeling that we should strive to create a community platform around the series. A platform for people who wish to learn, exchange ideas, collaborate, and help each other. For instance, the Two Minute Papers Data project has recently started. The initial idea is that there would be a public crowdsourced database with episode-related metadata that anyone can add information to. This would be useful for several different reasons. We can, for instance, include implementation links for each of the papers, for the ones that have source code either from the authors or some independent implementations from the community. We can also include keywords on the topic of the episode, such as machine learning, recurrent neural networks, cloth simulations for an easy way to search for a topic of interest. If we are interested in any of these, we can just easily search for a keyword and immediately know what episode it was referenced in. The very same could be done with some of the technical words used in the series, such as Metropolis sampling, backpropagation and overfitting. This way, we could maybe build an intuitive technical dictionary with definitions and links pointing to the appropriate episodes where they were explained. I think that would be the ultimate learning resource and having such a dictionary would be super useful I was playing with the thought of having this for quite a while now. So this data project has just started out on the Two Minute Papers subreddit, but it can only happen with the help of the community and this means you. If you're interested in helping, please drop by and leave us a note, I've put a link in the description box. Looking forward to seeing you there! It seems that there is interest in doing such collaborations and experiments together between you Fellow Scholars. Imagine how cool it would be to see students and researchers forming an open and helpful community, brainstorming, running the implementations, and sharing their ideas and findings with each other. Who knows, maybe one day, a science paper will be written as a result of such endeavors. Please let us know in the comments section what you think about these things. Would you participate in such an endeavor? As always, we love reading your awesome feedback! Also, in the meantime, a huge thank you for the following Fellow Scholars who translated our episodes to several languages! If you wish to contribute too, click the cogwheel button in any video on the lower right, click subtitles/CC, then add subtitles/CC. I am trying my very best to credit every contributor, if I have forgotten anyone, that's not intended, please let me know and I'll fix it. Meanwhile, on Patreon, we are currently oscillating around our current milestone. Reaching this one means that all of our software and hardware costs are covered, which is quite amazing. Because creating YouTube videos with high information density and short duration is the very definition of financial suicide, it's very helpful for us to have such a safety net with Patreon. Also, there was an episode not so long ago about the new Two Minute Papers machine which was bought solely from your support, and I am still stunned by this. I tried to explain this to close members of the family that we were able to buy this new machine with the support of complete strangers from the internet whom I've never met. They said that this is clearly impossible. Apparently, it's not impossible and so many kind people are watching the series, really, thank you so much! When we reach our milestone after that, we will be able to spend 1% of these funds to directly help other research projects and conferences. Please remember that your support makes Two Minute Papers possible, and you can cancel these pledges at any time. Also, if you don't feel like using Patreon or don't have any disposable income, that is completely fine. I'd like to emphasize that none of this is required, it's just an option to help, and we completely understand that it's not easy to make ends meet for many many of you, even with lots of overtime at a tiring and difficult job. Two Minute Papers is always going to be here and will always be free for everyone. We would like to spread the word so even more of us can marvel at the wonders of research. Just watching the series and sharing these episodes is also a great deal of help and we are super grateful for it! It is really incredible to see how the series has grown and how many of you Fellow Scholars are interested in research, and the inventions of the future. Let's continue our journey of science together! In the meantime, Google DeepMind and Blizzard has announced a joint effort to make it possible for computer programs to play Starcraft 2, a famous real-time strategy game from the start of 2017. Whoa! There was a question about the first thing I will do when this project takes off. Well, of course, I'll take one week of vacation and write an AI that can play against itself and watch with tremendous enjoyment as they beat the living hell out of each other and other real players. If I heard it correctly, in one of the upcoming championships, computer algorithms will also be allowed to play. And who knows, maybe defeat the reigning player champions, you know, like in Chess and Go. We're living amazing times indeed. I am counting the days. Super excited. Thanks for watching, and for your generous support, and I'll see you next time!"
170,How To Steal a Lost Election With Gerrymandering,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Let's talk about the mathematical intricacies of the elections! Here you can see the shape of the twelfth congressional district in North Carolina in the 90's. This is not a naturally shaped electoral district, is it? One might say this is more of an abomination. If we try to understand why it has this peculiar shape, we shall find a remarkable mathematical mischief. Have a look at this example of 50 electoral precincts. The distribution is 60% percent blue, and 40% red. So this means that the blue party should win the elections and gain seats with the ratio of 60 to 40, right? Well, this is not exactly how it works. There is a majority decision district by district, regardless of the vote ratios. If the electoral districts are shaped like this, then the blue party wins 5 seats to zero. However, if they are, instead, shaped like this, the red party wins 3 to 2. Which is kind of mind blowing, because the votes are the very same. And this is known as the wasted vote effect. This term doesn't refer to someone who enters the voting booth intoxicated, this means that one can think of pretty much every vote beyond 50% + 1 to a party in a district, to be irrelevant. It doesn't matter if the district is won by 99% of the votes or just by 50% + 1 vote. So, the cunning plan is now laid out. What if, instead, we could regroup all these extra votes to win in a different district where we were losing? And now, we have ceremoniously arrived to the definition of Gerrymandering, which is the process of manipulating electoral district boundaries to turn the tide of an election. The term originates from one of the elections in the USA in the 1800s, where Governor Elbridge Gerry signed a bill to reshape the districts of Massachusetts in order to favor his party. And at that time, understanably, all the papers and comic artists were up in arms about this bill. So how does one perform gerrymandering? Gerrymandering is actually a mathematical problem of the purest form where we are trying to maximize the number of seats that we can win by manipulating the district boundaries appropriately. It is important to note that the entire process relies on a relatively faithful prediction of the vote distributions per region, which in many countries, is not really changing all that much in time. This is a problem that we can solve via standard optimization techniques. Now, hold on to your papers, and get this: for instance, we can use Metropolis sampling to solve this problem, which is, absolutely stunning. So far, in an earlier episode, we have used Metropolis sampling to develop a super efficient light simulation program to create beautiful images of virtual scenes, and the very same technique can also be used to steal an election. In fact, Metropolis sampling was developed and used during the Manhattan project, where the first atomic bomb was created in Los Alamos. I think it is completely understandable that the power of mathematics and research still give many of us sleepless nights, sometimes delightful, sometimes perilous. It is also important to note that in order to retain the fairness of the elections in a district-based system, it is of utmost importance that these district boundaries are drawn by an independent organization and that the process is as transparent as possible. I decided not to cite a concrete paper in this episode. If you would like to read up on this topic, I recommend searching for keywords, like redistricting and gerrymandering on Google Scholar. Please feel free to post the more interesting finding of yours in the comments section, we always have excellent discussions therein. Thanks for watching, and for your generous support, and I'll see you next time!"
171,Real-Time Soft Body Dynamics for Video Games,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have had plenty of episodes about fluid simulations, so how about some tasty soft body dynamics for today? Soft body dynamics basically means computing what happens when we smash together different deformable objects. Examples include folding sheets, playing around with noodles, or torturing armadillos. I think this is a nice and representative showcase of the immense joys of computer graphics research! The key to real-time physically based simulations is parallelism. Parallelism means that we have many of the same units working together in harmony. Imagine if we had to assign 50 people to work together to make a coffee in the same kitchen. As you may imagine, they would trip over each other, and the result would be chaos, not productivity. Such a process would not scale favorably, because as we would add more people after around 3 or 4, the productivity would not increase, but drop significantly. You can often hear a similar example of 9 pregnant women not being able to give birth to a baby in one month. For better scaling, we have to subdivide a bigger task into small tasks in a way that these people can work independently. The more independently they can work, the better the productivity will scale as we add more people. In software engineering, these virtual people we like call threads, or compute units. As of 2016, mid-tier processors are equipped with 4-8 logical cores, and for a video card, we typically have compute units in the order of hundreds. So if we wish to develop efficient algorithms, we have to make sure that these big simulation tasks are subdivided in a way so that these threads are not tripping over each other. And the big contribution of this piece of work is a technique to distribute the computation tasks to these compute units in a way that they are working on independent chunks of the problem. This is achieved via using graph coloring, which is a technique typically used for designing seating plans, exam timetabling, solving sudoku puzzles and similar assignment tasks. It not only works in an absolutely spectacular manner, but graph theory is an immensely beautiful subfield of mathematics, so additional style points to the authors! The technique produces remarkably realistic animations and requires only 15 milliseconds per frame, which means that this technique can render over 60 frames per second comfortably. And the other most important factor is that this technique is also stable, meaning that it offers an appropriate solution, even when many other techniques fail to deliver. Thanks for watching, and for your generous support, and I'll see you next time!"
172,Generating Tangle Patterns With Grammars,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A tangle pattern is a beautiful, intervowen tapestry of basic stroke patterns, like dots, straight lines, and simple curves. If we look at some of these works, we see that many of these are highly structured, and maybe, we could automatically create such beautiful structures with a computer. And, now, hold on to your papers, because this piece of work is about generating tangle patterns with grammars. Okay, now, stop right there. How on Earth do grammars have anything to do with computer graphics or tangle patterns? The idea of this sounds as outlandish as it gets. Grammars are a set of rules that tell us how to build up a structure, such as a sentence properly from small elements, like nouns, adjectives, pronouns, and so on. Math nerds also study grammars extensively and set up rules that enforce that every mathematical expression satisfies a number of desirable constraints. It's not a surprise that when mathematicians talk about grammars, they will use these mathematical hieroglyphs like the ones you see on the screen. It is a beautiful subfield of mathematics that I have studied myself before, and am still hooked. Especially given the fact that from grammars, we can build not only sentences, but buildings. For instance, a shape grammar for buildings can describe rules like a wall can contain several windows, below a window goes a window sill, one wall may have at most two doors attached, and so on. My friend Martin Ilcik is working on defining such shape grammars for buildings, and using these grammars, he can generate a huge amount of different skyscrapers, facades, and all kinds of cool buildings. In this piece of work, we start out with an input shape, subdivide it into multiple other shapes, assign these smaller shapes into groups. And the final tangle is obtained by choosing patterns and assigning them to all of these groups. This yields a very expressive, powerful, tool that anyone can use to create beautiful tangle patterns. And all this, through the power of grammars. Thanks for watching, and for your generous support, and I'll see you next time!"
173,3D Printing Materials With Subsurface Scattering,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Subsurface scattering means that not every ray of light is reflected or absorbed on the surface of a material, but some of it may get inside somewhere, and come out somewhere else. For instance, our skin is a great and fairly unknown example of that. We can witness this beautiful effect if we place a strong light source behind our ears. Note that many other materials, such as plant leaves, many fruits such as apples and oranges, wax, marble also have subsurface scattering. The more we look at objects like these, the more we recognize how beautiful and ubiquitous subsurface scattering and translucency is in mother nature. And today, our main question is whether we can reproduce this kind of effect with 3D printed materials. The input would be a real material, such as these slabs, and the output would be an arbitrary shaped 3d printed material with similar scattering properties. Something that looks similar. What you see here is already the result of the 3D printing process, and wow, they look very tasty indeed. The process starts with a measurement apparatus where we grab a real material, and create a diffusion profile from it that describes how light scatters inside of this material. We have talked quite a bit about diffusion profiles before, I've put some links to earlier episodes in the video description box. If you check it out, you'll see how we can add subsurface scattering to an already existing image by ""kind of"" multiplying it with an other image. This is another one of those amazing inventions of mankind. Now, onto 3D printing. When we would like to 3d print something, we basically have a few different materials to work with, and we have to specify a shape. This shape is approximated with a three-dimensional grid. Each of these tiny grid elements typically have the thickness of several microns, which basically means a tiny fraction of the diameter of one hair strand, and we like to call these elements voxels. Now, before printing, we have to specify what kind of material we'd like to fill each of these voxels with. This is the general workflow for most 3D printers. What is specific to this work is that, after that, we have to take one column of this material, and look at the scattering properties of it. Let's call this column one stacking. We could measure that stacking by hand and see how it relates to the original target material, and we are trying to minimize the difference between the two. However, it would take millions of tries and would likely take a lifetime to print just one high-quality reproduction. So basically, we have an optimization problem where we're looking for a stacking that will appear similar to the chosen diffusion profiles. The difference between the appearance of the two is to be minimized. However, we have to realize, that in physics, the laws of light scattering are well understood, and the wonderful thing is that instead of printing a real object, we could just use a light simulation program to tell us how close the results should be. Now, this would work great, but it would still take an eternity because simulating light scattering through a stack of materials would take the very least, several seconds. And we have to try up to millions of stackings for each column, and there is a lot of columns to compute. Why a lot of different columns? Well, it's because we have a heterogeneous problem, which means that the whole material can contain variations in color and scattering properties. The geometry may also be uneven, so this is a vastly more difficult formulation of the initial problem. A classical light simulation program would be able to solve this, well, in a matter of years. However, there is a wonderful tool that is able to almost immediately tell us how much light is scattering inside of a stack of a ton of different materials. An almost instant multi-layer scattering tool, if you will. It really is a miracle that we can get the results for something so quickly that would otherwise require following the paths of millions of light rays. We call this technique the Hankel transform. The mathematical description of it is absolutely beautiful, but I personally think the best way of motivating these techniques is through application. Like this one. Imagine that many mathematicians have to study this transform without ever hearing what it can be used for. These are not some dry and tedious materials that one has to memorize we can do miracles with these inventions, and I feel that people need to know about that! With the use of the Hankel transform and some additional optimizations, one can efficiently find solutions that lead to high-quality reproductions of the input material. Excellent piece of work, definitely one of my favorites in 3D fabrication. As always, we'd love to read your feedback on this episode, let us know whether you have found it understandable! I hope you did! Also, a quick shoutout to BetterExplained.com. Please note that this is not a sponsored message. It has multiple slogans, such as ""math lessons for lasting insight"" or ""math without endless memorization"". This webpage is run by Kalid Azad, and contains tons of intuitive math lessons I wish I had access to during my years at the university. For instance, here is his guide on Fourier Transforms, which is a staple technique in every mathematician's and engineer's skill set, and is a prerequisite to understanding the Hankel transform. If you wish to learn mathematics, definitely check this website out, and if you don't wish to learn mathematics, then also definitely check this website out. Thanks for watching, and for your generous  support, and I'll see you next time!"
174,Sound Synthesis for Fluids With Bubbles,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have had quite a few episodes on simulating the motion of fluids and creating beautiful footages from the results. In this work, the authors created a simulator, that shows us not only the motion of a piece of fluid, but the physics of bubbles within as well. This sounds great, but there are two huge problems: one, there are a lot of them, and two, they can undergo all kinds of deformations and topology changes. To conjure up video footage that is realistic, and relates to the real world, several bubble-related effects, such as entrainment, splitting, merging, advection and collapsing, all have to be simulated faithfully. However, there is a large body of research out there to simulate bubbles, and here, we are not only interested in the footage of this piece of fluid, but also what kind of sounds it would emit when we interact with it. The result is something like this. The vibrations of a bubble is simulated by borrowing the equations that govern the movement of springs in physics. However, this, by itself would only be a forlorn attempt at creating a faithful sound simulation, as there are other important factors to take into consideration. For instance, the position of the bubble matters a great deal. This example shows that the pitch of the sound is expected to be lower near solid walls, as you can see it marked with dark blue on the left, right side and below, and have a higher pitch near the surface, which is marked with red. You can also see that there are significant differences in the frequencies depending on the position, the highest frequency being twice as high as the lowest. So this is definitely an important part of the simulation. Furthermore, taking into consideration the shape of the bubbles is also of utmost importance. As the shape of the bubble goes from an ellipsoid to something close to a sphere, the emitted sound frequency can drop by as much as 30%. Beyond these effects, there were still blind spots even in state of the art simulations. With previous techniques, a chirp-like sound was missing, which is now possible to simulate with a novel frequency extension model. Additional extensions include a technique that models the phenomenon of the bubbles popping at the surface. The paper discusses what cases are likely to emphasize which of these extension's effects. Putting it all together, it sounds magnificent, check it out! But still, however great these sounds are, without proper validation, these are still just numbers on a paper. And of course, as always, the best way of testing these kinds of works if we let reality be our judge, and compare the results to real world footage. So I think you can guess what the next test is going to be about! The  authors also put up a clinic on physics and math, and the entirety of the paper is absolutely beautifully written. I definitely recommend having a look, as always, the link is available in the description box. Also, you'll find one more link to a playlist with all of our previous episodes on fluid simulations. Lots of goodies there. As always, we'd love to read your feedback on this episode, let us know whether you have found it understandable! I hope you did. Thanks for watching, and for your generous support, and I'll see you next time!"
175,3D Printing Auxetic Materials,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We are back! And in this episode, we shall talk about auxetic materials. Auxetic materials are materials that when stretched, thicken perpendicular to the direction we're stretching them. In other words, instead of thinning, they get fatter when stretched. Really boggles the mind, right? They are excellent at energy absorption and resisting fracture, and are therefore widely used in body armor design, and I've read a research paper stating that even our tendons also show auxetic behavior. These auxetic patterns can be cut out from a number of different materials, and are also used in footwear design and actuated electronic materials. However, all of these applications are restricted to rather limited shapes. Furthermore, even the simplest objects, like this sphere cannot be always approximated by inextensible materials. However, if we remove parts of this surface in a smart way, this inextensible material becomes auxetic, and can approximate not only these rudimentary objects, but much more complicated shapes as well. However, achieving this is not trivial. If we try the simplest possible solution, which would basically be shoving the material onto a human head like a paperbag, but as it is aptly demonstrated in these images, it would be a fruitless endeavor. This method tries to solve this problem by flattening the target surface with an operation that mathematicians like to call a conformal mapping. For instance, the world map in our geography textbooks is also a very astutely designed conformal mapping from a geoid object, the Earth, to a 2D plane which can be shown on a sheet of paper. However, this mapping has to make sense so that the information seen on this sheet of paper actually makes sense in the original 3D domain as well. This is not trivial to do. After this mapping, our question is where the individual points would have to be located so that they satisfy three conditions: one: the resulting shape has to approximate the target shape, for instance, the human head, as faithfully as possible two: the construction has to be rigid three: when we stretch the material, the triangle cuts have to make sense and not intersect each other, so huge chasms and degenerate shapes are to be avoided. This work is using optimization to obtain a formidable solution that satisfies these constraints. If you remember our earlier episode about optimization, I said there will be a ton of examples of that in the series. This is one fine example of that! And the results are absolutely amazing the possibility of creating a much richer set of auxetic material designs is now within the realm of possibility, and I expect that it will have applications from designing microscopic materials, to designing better footwear and leather garments. And we are definitely just scratching the surface! The method supports copper, aluminum, plastic and leather designs, and I am sure there will be mind blowing applications that we cannot even fathom so early in the process. As an additional selling point, the materials are also reconfigurable, meaning that from the same piece of material, we can create a number of different shapes. Even non-trivial shapes with holes, such as a torus, can be created. Note that in mathematics, the torus is basically a fancy name for a donut. A truly fantastic piece of work, definitely have a look at the paper, it has a lot of topological calculations, which is an awesome subfield of mathematics. And, the authors' presentation video is excellent, make sure to have a look at that. Let me know if you have found this episode understandable, we always get a lot of awesome feedback and we love reading your comments. Thanks for watching, and for your generous support, and I'll see you next time!"
176,Patreon Update - New Machine!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I apologize for the delays during last week, I always put out notifications about such events on Twitter and Facebook, make sure to follow us there so you Fellow Scholars know about these well in advance. And I have to say, during this time, I really missed you and making videos so much! This video is a quick update on what has happened since, and I'd also like to assure you that the next Two Minute Papers episode is already in the works and is going to arrive soon. Very soon. Patreon is a platform where you can support your favorite creators with monthly recurring tips and get cool perks in return. We have quite a few supporters who are really passionate about the show, and during the making of the last few episodes, we have encountered severe hardware issues. There were freezes, random restarts, blue screens of death constantly, and the computer was just too unstable to record and edit Two Minute Papers. The technicians checked it out and found that quite a few parts would have to be replaced, and I figured that this would be a great time to replace this old configuration. So, we have ordered the new Two Minute Papers rig, and the entire purchase happened with the help of you, our Patreon supporters! We were able to replace it effortlessly, which is just amazing. Words fail me to describe how grateful I am for your generous support, and I am still stunned by this. It is just unfathomable to me that I am just sitting here in a room with a microphone, having way too much fun with research papers, and many of you enjoy this series enough to support it, and it had come to this. You Fellow Scholars make Two Minute Papers happen. Thanks so much. I'll try to squeeze in a bit of footage of the new rig, and transparency above all, will post the configuration in the video description box for the more curious minds out there. What I can say at this point, is that this new rig renders videos three times as quickly as the previous one. And even though these are just short videos, the rendering times for something in full HD and 60 fps are surprisingly long even when run on the graphical card. Well, not anymore. So, one more time, thank you so much for supporting the series. This is absolutely amazing, it really is. This was a quick update video, the next episode is coming soon, and I'll try my very best so we can be back at our regular schedule of two videos per week. Lots of spectacular works are on our list, stay tuned! Oh, and by the way, we have Fellow Scholars watching from all around the world. And in the meantime, some of them have started translating our episodes to German, Portuguese, Spanish and Italian. For some reason, I cannot see the names of the kind people who took their time to contribute, so I'd like to kindly thank you for your work, it makes Two Minute Papers accessible to even more people, which is amazing. Thanks for watching, and for your generous support, and I'll see you next time!"
177,Sound Propagation With Adaptive Impulse Responses,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Have you ever wondered how your voice, or your guitar would sound in the middle of a space station? A realistic simulation of sounds within virtual environments dramatically improves the immersion of the user in computer games and virtual reality applications. To be able to simulate these effects, we need to compute the interaction between sound waves and the geometry and materials within the scene. If you remember, we also had quite a few episodes about light simulations, where we simulated the interaction of light rays or waves and the scene we have at hand. Sounds quite similar, right? Well, kind of, and the great thing is that we can reuse quite a bit of this knowledge and some of the equations for light transport for sound. This technique we call path tracing, and it is one of the many well-known techniques used for sound simulation. We can use path tracing to simulate the path of many waves to obtain an impulse response, which is a simple mathematical function that describes the reverberation that we hear if we shoot a gun in a given scene, such as a space station or a church. After we obtained these impulse reponses, we can use an operation called the convolution with our input signal, like our voice to get a really convincing result. We have talked about this in more detail in earlier episodes, I've put a link for them in the video description box. It is important to know that the impulse reponse depends on the scene and where we, the listeners are exactly in the scene. In pretty much every concert ever, we find that sound reverberations are quite different in the middle of the arena versus standing at the back. One of the main contributions of this work is that it exploits temporal coherence. This means that even though the impulse response is different if we stand at different places, but these locations don't change arbitrarily and we can reuse a lot of information from the previous few impulse reponses that we worked so hard to compute. This way, we can get away with tracing much fewer rays and still get high-quality results. In the best cases, the algorithm executes five times as quickly as previous techniques, and the memory requirements are significantly more favorable. The paper also contains a user study. Limitations include a bit overly smooth audio signals and some fidelity loss in the lower frequency domains. Some of these scenes in the footage showcase up to 24 distinct sound sources, and all of them are simulated against the geometry and the materials found in the scene. So let's listen together and delight in these magnificent results. Thanks for watching, and for your generous support, and I'll see you next time!"
178,Estimating Matrix Rank With Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work is not meant to be a highly useful application, only a tongue in cheek jab at the rising trend of trying to solve simple problems using deep learning without carefully examining the problem at hand. As always, we note that all intuitive explanations are wrong, but some are helpful, and the most precise way to express these thoughts can be done by using mathematics. However, we shall leave that to the textbooks and will try to understand these concepts by floating about on the wings of intuition. In mathematics, a matrix is a rectangular array in which we can store numbers and symbols. Matrices can be interpreted in many ways, for instance, we can think of them as transformations. Multiplying a matrix with a vector means applying this transform to the vector, such as scaling, rotation or shearing. The rank of a matrix can be intuitively explained in many ways. My favorite intuition is that the rank encodes the information content of the matrix. For instance, in an earlier work on Separable Subsurface Scattering, we recognized that many of these matrices that encode light scattering inside translucent materials, are of relatively low rank. This means that the information within is highly structured and it is not random noise. And from this low rank property follows that we can compress and represent this phenomenon using simpler data structures, leading to an extremely efficient algorithm to simulate light scattering within our skin. However, the main point is that finding out the rank of a large matrix is an expensive operation. It is also important to note that we can also visualize these matrices by mapping the numbers within to different colors. As a fun sidenote, the paper finds, that the uglier the colorscheme is, the better suited it is for learning. This way, after computing the ranks of many matrices, we can create a lot of input images and output ranks for the neural network to learn on. After that, the goal is that we feed in an unknown matrix in the form of an image, and the network would have to guess what the rank is. It is almost like having an expert scientist unleash his intuition on such a matrix, much like a fun guessing game for intoxicated mathematicians. And the ultimate question, as always is, how does this knowledge learned by the neural network generalize? The results are decent, but not spectacular, but they also offer some insights as to which matrices have surprising ranks. We can also try computing the products of matrices, which intuitively translates to guessing the result after we have done one transformation after the other. Like the output of scaling after a rotation operation. They also tried to compute the inverse of matrices, for which the intuition can be undoing the transformation. If it is a rotation to a given direction, the inverse would be rotating back the exact same amount, or if we scaled something up, then scaling it back down would be its inverse. Of course, these are not the only operations that we can do with matrices, we only used these for the sake of demonstration. The lead author states on his website that this paper shows that ""linear algebra can be replaced with machine learning"". Talk about being funny and tongue in cheek. Also, I have linked the website of David in the description box, he has a lot of great works and I am surely not doing him justice by of all those great works, covering this one. Rufus von Woofles, graduate of the prestigious Muddy Paws University was the third author of the paper, overlooking the entirety of the work and making sure that the quality of the results is impeccable. As future work, I would propose replacing the basic mathematical operators such as addition and multiplication by machine learning. Except that it is already done and is hilariously fun, and it even supports division by zero. Talk about the almighty powers of deep learning. Thanks for watching, and for your generous support, and I'll see you next time!"
179,WaveNet by Google DeepMind,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When I opened my inbox today, I was greeted by a huge deluge of messages about WaveNet. Well, first, it's great to see that so many people are excited about these inventions, and second, may all your wishes come true as quickly as this one! So here we go. This piece of work is about generating audio waveforms for Text To Speech and more. Text To Speech basically means that we have a voice reading whatever we have written down. The difference in this work, is, however that it can synthesize these samples in someone's voice provided that we have training samples of this person speaking. It also generates waveforms sample by sample, which is particularly perilous because we typically need to produce these at the rate of 16 or 24 thousand samples per second, and as we listen to the TV, radio and talk to each other several hours a day, the human ear and brain is particularly suited to processing this kind of signal. If the result is off by only the slightest amount, we immediately recognize it. It is not using a recurrent neural network, which is typically suited to learn sequences of things, and is widely used for sound synthesis. It is using a convolutional neural network, which is quite surprising because it is not meant to process sequences of data that change in time. However, this variant contains an extension that is able to do that. They call this extension dilated convolutions and they open up the possibility of making large skips in the input data so we have a better global view of it. If we were working in computer vision, it would be like increasing the receptive field of the eye so we can see the entire landscape, and not only a tree on a photograph. It is also a bit like the temporal coherence problem we've talked about earlier. Taking all this into consideration results in more consistent outputs over larger time scales, so the technique knows what it had done several seconds ago. Also, training a convolutional neural network is a walk in the park compared to a recurrent neural network. Really cool! And the results beat all existing widely used techniques by a large margin. One of these is the concatenative technique, which builds sentences from a huge amount of small speech fragments. These have seen a ton of improvements during the years, but the outputs are still robotic and it is noticeable that we're not listening to a human but a computer. The DeepMind guys also report that: ""Notice that non-speech sounds, such as breathing and mouth movements, are also sometimes generated by WaveNet; this reflects the greater flexibility of a raw-audio model."" At the same time, I'd like to note that in the next few episodes, it may be that my voice is a bit different, but don't worry about that. It may also happen that I am on a vacation but new episodes and voice samples pop up on the channel, please don't worry about that either. Everything is working as intended! They also experimented with music generation, and the results are just stunning. I don't know what to say. These difficult problems, these impenetrable walls crumble one after another as DeepMind takes on them. Insanity. Their blog post and the paper are both really well written, make sure to check them out, they are both linked in the video description box. I wager that artistic style transfer for sound and instruments is not only coming, but it'll be here soon. I imagine that we'll play a guitar and it will sound like a harp, and we'll be able to sing something in Lady Gaga's voice and intonation. I've also seen someone pitching the idea of creating audiobooks automatically with such a technique. Wow. I travel a lot and am almost always on the go, so I personally would love to have such audiobooks! I have linked the mentioned machine learning reddit thread in the description box, as always, there's lots of great discussion and ideas there. It was also reported that the algorithm currently takes 90 minutes to synthesize one second of sound waveforms. You know the drill, one followup paper down the line, it will take only a few minutes, a few more papers down the line, it'll be real time. Just think about all these advancements. What a time we're living in! And I am extremely excited to present them all to you Fellow Scholars in Two Minute Papers. Make sure to leave your thoughts and ideas in the comments section, we love reading them! Thanks for watching, and for your generous support, and I'll see you next time!"
180,Automatic Hair Modeling from One Image,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A couple episodes ago, we finally set sail in the wonderful world of hair simulations. And today we shall continue our journey in this domain. But this time, we are going to talk about hair modeling. So first, what is the difference between hair simulation and modeling? Well, simulation is about trying to compute the physical forces that act on hair strands, thereby showing to the user, how they would move about in reality. Modeling, however, is about obtaining geometry information from a photograph. This geometry information we can use in our movies and computer games. We can also run simulations on them and see how they look on a digital character. Just think about it, the input is one photograph and the output is a digital 3D model. This sounds like a remarkably difficult problem. Typically something that a human would do quite well at, but it would be too labor-intensive to do so for a large number of hairstyles. Therefore, as usual, neural networks enter the fray by looking at the photograph, and trying to estimate the densities and distributions of the hair strands. The predicted results are then matched with the hairstyles found in public data repositories, and the closest match is presented to the user. You can see some possible distribution classes here. Not only that, but the method is fully automatic, which means that unlike most previous works, it doesn't need any guidance from the user to accomplish this task. As a result, the authors created an enormous dataset with 50 thousand photographs and their reconstructions that they made freely available for everyone to use. The output results are so spectacular, it's almost as if we were seeing magic unfold before our eyes. The fact that we have so many hairstyles in this dataset also opens up the possibility of editing, which is always quite a treat for artists working in the industry. The main limitation is a poorer reconstruction of regions that are not visible in the input photograph, but I think that goes without saying. It is slightly ameliorated by the fact that the public repositories contain hairstyles that make sense, so we can expect results of reasonable quality even for the regions we haven't seen in the input photograph. As always, please let me know below in the comments section whether you have found everything understandable in this episode. Was it easy to digest? Was there something that was not easy to follow? Your feedback, as always, is greatly appreciated. Thanks for watching, and for your generous support, and I'll see you next time!"
181,"StyLit, Illumination-Guided Artistic Style Transfer","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Earlier, we have talked quite a bit about a fantastic new tool that we called artistic style transfer. This means that we have an input photograph that we'd like to modify, and another image from which we'd like to extract the artistic style. This way, we can, for instance, change our photo to look in the style of famous artists. Now, artists in the visual effects industry spend a lot of time designing the lighting and the illumination of their scenes, which is a long and arduous process. This is typically done in some kind of light simulation program, and if anyone thinks this is an easy and straightforward thing to do, I would definitely recommend trying it. After this lighting and illumination step is done, we can apply some kind of artistic style transfer, but we shall quickly see that there is an insidious side effect to this process: it disregards, or even worse, destroys our illumination setup, leading to results that look physically incorrect. Today, we're going to talk about a flamboyant little technique that is able to perform artistic style transfer in a way that preserves the illumination of the scene. These kinds of works are super important, because they enable us to take the wheel from the hands of the neural networks that perform these operations, and force our will on them. This way, we can have a greater control over what these neural networks do. Previous techniques take into consideration mostly color and normal information. Normals basically encode the shape of an object. However, these techniques don't really have a notion of illumination. They don't know that a reflection on an object should remain intact, and they have no idea about the existence of shadows either. For instance, we have recently talked about diffuse and specular material models, and setting up this kind of illumination is something that artists in the industry are quite familiar with. The goal is that we can retain these features throughout the process of style transfer. In this work, the artist is given a printed image of a simple object, like a sphere. This is no ordinary printed image, because this image comes from a photorealistic rendering program, which is augmented by additional information, like what part of the image is a shadowed region, and where the reflections are. And then, when the artist starts to add her own style to it, we know exactly what has been changed and how. This leads to a much more elaborate style transfer pipeline where the illumination stays intact. And the results are phenomenal. What is even more important, the usability of the solution is also beyond amazing. For instance, here, the artist can do the stylization on a simple sphere and get the artistic style to carry over to a complicated piece of geometry almost immediately. Temporal coherence is still to be improved, which means that if we try this on an animated sequence, it will be contaminated with flickering noise. We have talked about a work that does something similar for the old kind of style transfer, I've put a link in the video description box for that. I am sure that this kink will be worked out in no time. It's also interesting to note that the first style transfer paper was also published just a few months ago this year, and we're already lavishing in excellent followup papers. I think this demonstrates the excitement of research quite aptly the rate of progress in technology and algorithms are completely unmatched. Fresh, new ideas pop up every day, and we can only frown and wonder at their ingenuity. As usual, please let me know in the comments section whether you have found this episode interesting and understandable. If you felt that everything is fine here, that is also valuable feedback. Thank you! And by the way, if you wish to express your scholarly wisdom, our store is open with some amazing quality merch. Have a look! We also have a huge influx of people who became Patrons recently, welcome, thank you so much for supporting Two Minute Papers. We love you too. Thanks for watching, and for your generous support, and I'll see you next time!"
182,Interactive Hair-Solid Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have talked about fluid and cloth simulations earlier, but we never really set foot in the domain of hair simulations in the series. To obtain some footage of virtual hair movement, simulating the dynamics of hundreds of thousands of hair strands is clearly too time consuming and would be a flippant attempt to do so. If we do not wish to watch a simulation unfold with increasing dismay as it would take hours of computation time to obtain just one second of footage, we have to come up with a cunning plan. A popular method to obtain detailed real-time hair simulations is not to compute the trajectory of every single hair strand, but to have a small set of strands that we call guide hairs. For these guide hairs, we compute everything. However, since this is a sparse set of elements, we have to fill the gaps with a large number of hair strands, and we essentially try to guess how these should move based on the behavior of guide hairs near them. Essentially, one guide hair is responsible in guiding an entire batch, or an entire braid of hair, if you will. This technique we like to call a reduced hair simulation, and the guessing part is often referred to as interpolation. And the question immediately arises: how many guide hairs do we use and how many total hair strands can we simulate with them without our customers finding out that we're essentially cheating? The selling point of this piece of work, is that it uses only 400 guide hairs, and leaning on them, and it can simulate up to a total number of 150 thousand strands in real time. This leads to amazingly detailed hair simulations. My goodness, look at how beautiful these results are! Not only that, but as it is demonstrated quite aptly here, it can also faithfully handle rich interactions and collisions with other solid objects. For instance, we can simulate all kinds of combing, or pulling our hair out, which is what most researchers do in their moments of great peril just before finding the solution to a difficult problem. Not only hair, but a researcher simulator, if you will. The results are compared to a full space simulation, which means simulating every single hair strand, and that is exactly as time consuming as it sounds. The results are very close to being indistinguishable, which was not the case for previous works that created false intersections where hair strands would erroneously go through solid objects. We can also stroke bunnies with our hair models in this truly amazing piece of work. These episodes are also available in early access for our Patreon supporters. We also have plenty of other really neat perks, I've put a link in the description box, make sure to have a look! Thanks for watching, and for your generous support, and I'll see you next time!"
183,3D Printing With Filigree Patterns,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Filigrees are detailed, thin patterns typically found in jewelry, fabrics and ornaments, and as you may imagine, crafting such motifs on objects is incredibly laborious. This project is about leaving out the craftsmen from the equation by choosing a set of target filigree patterns and creating a complex shape out of them that can be easily 3D printed. The challenge lies in grouping and packing up these patterns to fill a surface evenly. We start out with a base model with a poor structure, which is not completely random, but as you can see, is quite a forlorn effort. In several subsequent steps, we try to adjust the positions and shapes of the filigree elements to achieve more pleasing results. The more pleasing results we define as one that minimizes the amount of overlapping, and maximizes the connectivity of the final shape. Sounds like an optimization problem from earlier. And, that is exactly what it is. Really cool, right? The optimization procedure itself is far from trivial, and the paper discusses possible challenges and their solutions in detail. For instance, the fact that we can also add control fields to describe our vision regarding the size and orientation of the filigree patterns is an additional burden that the optimizer has to deal with. We can also specify the ratio of the different input filigree elements that we'd like to see added to the model. The results are compared to previous work, and the difference speaks for itself. However, it's important to point out that even this thing that we call previous work was still published this year. Talk about rapid progress in research! Absolutely phenomenal work. The evaluation and the execution of the solution, as described in the paper is also second to none. Make sure to have a look. And thank so much for taking the time to comment on our earlier video about the complexity of the series. I'd like to assure you we read every single comment and found a ton of super helpful feedback there. It seems to me that a vast majority of you agree that a simple overlay text does the job, and while it is there, it's even better to make it clickable so it leads to a video that explains the concept in a bit more detail for the more curious minds out there. I'll try to make sure that everything is available in mobile as well. You Fellow Scholars are the best and thanks so much for everyone for leaving a comment. Also, please let me know in the comments section if you have found this episode to be understandable or if there were any terms that you've never heard of. If everything was in order, that's also valuable information, so make sure to leave a comment. Thanks for watching, and for your generous support, and I'll see you next time!"
184,Neural Material Synthesis,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If you are new here, this is a series about research with the name Two Minute Papers, but let's be honest here. It's never two minutes. We are going to talk about two really cool papers that help us create physically based material models from photographs that we can use in our light simulation programs. Just as a note, these authors, Miika and Jaakko have been on a rampage for years now and have popped so many fantastic papers each of which I was blown away by. For instance, earlier, we talked about their work on Gradient Domain Light Transport, brilliant piece of work, I've put a link in the description box, make sure to check it out! So the problem we're trying to solve is very simple to understand: the input is a photograph of a given material somewhere in our vicinity, and the output is a bona fide physical material model that we can use in our photorealistic rendering program. We can import real world materials in our virtual worlds, if you will. Before we proceed, let's define a few mandatory terms: A material is diffuse if incoming light from one direction is reflected equally in all directions. This means that they look the same from all directions. White walls and matte surfaces are excellent examples of that. A material we shall consider specular if incoming light from one direction is reflected back to one direction. This means that if we turn our head a bit, we will see something different. For instance, the windshield of a car, water and reflections in a mirror can be visualized with a specular material model. Of course, materials can also be a combination of both. For instance, car paint, our hair and skin are all combinations of these material models. Glossy materials are midway between the two where the incoming light from one direction is reflected to not everywhere equally, and not in one direction, but a small selected set of directions. They change a bit when we move our head, but not that much. In the Two-Shot Capture paper, a material model is given by how much light is reflected and absorbed by the diffuse and specular components of the material, and something that we call a normal map, which captures the bumpiness of the material. Other factors like glossiness and anisotropy are also recorded, but we shall focus on the diffuse and the specular parts. The authors ask us to grab our phone for two photographs of a material to ensure a high-quality reconstruction procedure: one with flash, and one without. And the question immediately arises: why two images? Well, the image without flash can capture the component that looks the same from all directions, this is the diffuse component, and the photograph with flash can capture the specular component because we can see how the material handles specular reflections. And it is needless to say, the presented results are absolutely fantastic. So, first paper, two images, one material model. And therein lies the problem, which they tried to address in the second paper. If a computer looks at such an image, it doesn't know which part of one photograph is the diffuse and which is the specular reflection. However, I remember sitting in the waiting room of a hospital while reading the first paper, and this waiting room had a tiled glossy wall, and I was thinking that one image should be enough, because if I look at something, I can easily discern what the diffuse colors are, and which part is the specular reflection of something else. I don't need multiple photographs for that. I can also immediately see how bumpy it is, even from one photograph, I don't need to turn my head around. This is because we, humans have not a mathematical, but an intuitive understanding of the materials we see around us. So can we explain the same kind of understanding of materials to a computer somehow? Can we do it with only one image? And the answer is, yes we can, and, hopefully, we already feel the alluring call of neural networks. We can get a neural network that was trained on a lot of different images to try to guess what these material reflectance parameters should look like. However, the output should not be one image, but multiple images with the diffuse and specular reflectance informations, and a normal map to describe the bumpiness of this surface. Merely throwing a neural network at this problem, is however, not sufficient. There needs to be some kind of conspiracy between these images, because real materials are not arbitrarily put together. If one of these images is smooth, or has interesting features somewhere, the others have to follow it in some way. This ""some way"" is mathematically quite challenging to formulate, which is a really cool part of the paper. This conspiracy part is a bit like if we had 4 criminals testifying at a trial, where they try to sell their lies, and to maintain the credibility of their made up story, they have previously had to synchronize their lies so they line up correctly. The paper contains neat tricks to control the output of the neural network and create these conspiracies across these multiple image outputs that yield a valid and believable material model. And the results, are again, just fantastic. Second paper, one image, one material model. It doesn't get any better than that. Spectacular, not specular...spectacular piece of work. The first paper is great, but the second is smoking hot, by all that is holy, I am getting goosebumps. If you are interested in hearing a bit more about light transport and are not afraid of some mathematics, we recently recorded my full course on this at the Technical University of Vienna, the entirety of which is freely available for everyone. There is a link for it in the video description box, make sure to check it out! Thanks for watching, and for your generous support, and I'll see you next time!"
185,On the Complexity of Two Minute Papers,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is not an episode about a paper, but it's about the series itself. There are some minor changes coming, and I am trying my very best to make it as enjoyable as possible to you, so I would really like to hear your opinion on an issue. Many of our episodes are on new topics where I am trying my best to cover the basics so that the scope of a new research work can be understood clearly. However, as we are continuing our journey deeper into the depths of state of the art research, it inevitably happens that we have to build on already existing knowledge from earlier episodes. The big question is, how we should handle such cases. For instance, in the case of a neural network paper, the solution we went for so far was having a quick recap for what a neural network is. We can either have this recap in every episode about for instance, neural networks, fluid simulations or photorealistic rendering and be insidiously annoying to our seasoned Fellow Scholars who know it all. Or, we don't talk about the preliminaries to cater to the more seasoned Fellow Scholars out there, at the expense new people who are locked out of the conversation, as they may be watching their very first episode of Two Minute Papers. So the goal is clear, I'd like the episodes to be as easily understandable as possible, but while keeping the narrative intact so that every term I use is explained in the episode. First, I was thinking about handing out a so called ""dictionary"" in the video description box where all of these terms would be explained briefly. At first, this sounded like a good idea, but most people new to the series would likely not know about it, and for them, the fact that these episodes are not self-contained anymore would perhaps be confusing, or even worse, repulsive. The next idea was that, perhaps, instead of re-explaining these terms over and over again, we could add an overlay text in the video for them. The more seasoned Fellow Scholars won't be held up because they know what a Lagrangian fluid simulation is, but someone new to the series could also catch up easily just by reading a line of text that pops up. I think this one would be a formidable solution. I would love to know your opinion on these possible solutions, I personally think that the overlay text is the best, but who knows, maybe a better idea gets raised. Please make sure to let me know below in the comments section whether you have started watching Two Minute Papers recently or maybe you're a seasoned Fellow Scholar, and how you feel about the issue. Have you ever encountered terms that you didn't understand? Or was it the opposite, am I beating a dead horse with re-explaining all this simple stuff? I'd like to make these episodes the best I possibly can so that seasoned Fellow Scholars and people new to the show alike can marvel at the wonders of research. All feedback is welcome and please make sure to leave a comment so I can better understand how you feel about this issue and what would make you happier. Thanks for watching, and for your generous support, and I'll see you next time!"
186,What is an Autoencoder?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As we have seen in earlier episodes of the series, neural networks are remarkably efficient tools to solve a number of really difficult problems. The first applications of neural networks usually revolved around classification problems. Classification means that we have an image as an input, and the output is, let's say a simple decision whether it depicts a cat or a dog. The input will have as many nodes as there are pixels in the input image, and the output will have 2 units, and we look at the one of these two that fires the most to decide whether it thinks it is a dog or a cat. Between these two, there are hidden layers where the neural network is asked to build an inner representation of the problem that is efficient at recognizing these animals. So what is an autoencoder? An autoencoder is an interesting variant with two important changes: first, the number of neurons is the same in the input and the output, therefore we can expect that the output is an image that is not only the same size as the input, but actually is the same image. Now, this normally wouldn't make any sense, why would we want to invent a neural network to do the job of a copying machine? So here goes the second part: we have a bottleneck in one of these layers. This means that the number of neurons in that layer is much less than we would normally see, therefore it has to find a way to represent this kind of data the best it can with a much smaller number of neurons. If you have a smaller budget, you have to let go of all the fluff and concentrate on the bare essentials, therefore we can't expect the image to be the same, but they are hopefully quite close. These autoencoders are capable of creating sparse representations of the input data and can therefore be used for image compression. I consciously avoid saying ""they are useful for image compression"". Autoencoders, offer no tangible advantage over classical image compression algorithms like JPEG. However, as a crumb of comfort, many different variants exist that are useful for different tasks other than compression. There are denoising autoencoders that after learning these sparse representations, can be presented with noisy images. As they more or less know how this kind of data should look like, they can help in denoising these images. That's pretty cool for starters! What is even better is a variant that is called the variational autoencoder that not only learns these sparse representations, but can also draw new images as well. We can, for instance, ask it to create new handwritten digits and we can actually expect the results to make sense! There is an excellent blog post from Francois Cholle, the creator of the amazing Keras library for building and training neural networks, make sure to have a look! With these examples, we were really only scratching the surface, and I expect quite a few exciting autoencoder applications to pop up in the near future as well. I cannot wait to get my paws on those papers. Hopefully you Fellow Scholars are also excited! If you are interested in programming, especially in python, make sure to check out the channel of Sentdex for tons of machine learning programming videos and more. Thanks for watching, and for your generous support, and I'll see you next time!"
187,The Science of Medal Predictions (2016 Rio Olympics Edition),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The 2016 Rio Olympic Games is right around the corner, so it is the perfect time to talk a bit about how we can use science to predict the results. Before we start, I'd like to mention that we won't be showcasing any predictions for this year's Olympics, instead, we are going to talk about a model that was used to predict the results of previous Olympic Games events. So, the following, very simple question arises: can we predict the future? The answer is very simple: no, we can't. End of video, thanks for watching! Well, jokes aside, we cannot predict the future itself, but we can predict what is likely to happen based on our experience of what happened so far. In mathematics, this is what we call extrapolation. There is also a big difference between trying to extrapolate the results of one athlete, or the aggregated number of medals for many athletes, usually an entire nation. To bring up an example about traffic, if we were to predict where one individual car is heading, we would obviously fail most of the time. However, whichever city we live in, we know exactly the hotspots where there are traffic jams every single morning of the year. We cannot accurately predict the behavior of one individual, but if we increase the size of the problem and predict for a group of people, it suddenly gets easier. Going back to the Olympics, will Usain Bolt win the gold on the 100 meter sprint this year? Predicting the results of one athlete is usually hopeless, and we bravely call such endeavors to be mere speculation. The guy whose results we're trying to predict may not even show up this year as many of you have heard, many of the Russian athletes have been banned from the Olympic Games. Our model would sure as hell not be able to predict this. Or would it? We'll see in a second, but hopefully it is easy to see that macro-level predictions are much more feasible than predicting on an individual level. In fact, to demonstrate how much of an understatement it is to say feasible, hold onto your seatbelts, because Daniel Johnson, a professor of microeconomics at the Colorado College created a simple prediction model, that, over the past 5 Olympic Games, was able to achieve 94% agreement between the predicted and actual medal counts per nation. What is even more amazing is that the model doesn't even take into consideration the athletic abilities of any of these contenders. Wow! Media articles report that his model uses only 5 simple variables: a country's per-capita income, population, political structure, climate, and host-nation advantage. Now first, I'd first like to mention that GDP per capita means the Gross Domestic Product of one person in a given country, therefore it is independent of the population of the country. If we sit down and read the paper, which is a great and very easy read and you should definitely have a look, it's in the video description box. So, upon reading the paper, we realize there are more variables that are subject to scrutiny: for instance, a proximity factor, which encodes the distance from the hosting nation. Not only the hosting nation itself, but its neighbors are also enjoying significant advantages in the form of lower transportation costs and being used to the climate of the venue. Unfortunately I haven't found his predictions for this year's Olympics, but based on the simplicity of the model, it should be quite easy to run the predictions provided that the sufficient data is available. The take home message is that usually the bigger the group we're trying to predict results for, the lesser the number of variables that are enough to explain their behavior. If we are talking about the Olympics, 5 or 6 variables are enough to faithfully predict nationwide medal counts. These are amazing results that are also a nice testament to the power of mathematics. I also really like how the citation count of the paper gets a big bump every four years. I wonder why? If you are interested in how the Olympic Games unfold, make sure to have a look at the Olympics reddit, I found it to be second to none. As always, the link is available in the description box. Thanks for watching and for your generous support, and I'll see you next time!"
188,Peer Review and the NeurIPS Experiment,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We are here to answer a simple question: what is peer review? Well, in science, making sure that the validity of published results is beyond doubt is of utmost importance. To this end, many scientific journals and conferences exist where researchers can submit their findings in the form of a science paper. As a condition of acceptance, these papers shall undergo extensive scrutiny by typically 2 to 5 other scientists. This refereeing process we call peer review. Single blind reviewing means that the names of the reviewers are shrouded in mystery, but the authors of the paper are known to them. In double blind reviews, however, the papers are anonymized, and none of the parties know the names of each other. These different kinds of blind reviews were made to eliminate possible people-related biases. There is a lot of discussion whether they do a good job at that or not, but this is what they are for. After the review, if the results are found to be correct, and the reviews are favorable enough, the paper is accepted and subsequently published in a journal and/or presented at a conference. Usually, the higher the prestige of a publication venue is, the higher the likelihood of rejection, which inevitably raises a big question: how to choose the papers that are to be accepted? As we are scientists, we have to try to ensure that the peer review is a fair and consistent process. To measure if this is the case, the NIPS experiment was born. NIPS is one of the highest quality conferences in machine learning with a remarkably low acceptance ratio, which typically hovers below 25%. This is indeed remarkably low considering the fact that many of the best research groups in the world submit their finest works here. So here is the astute idea behind the NIPS experiment: a large amount of papers would be secretly disseminated to multiple committees, they would review it without knowing about each other, and we would have a look whether they would accept or reject the same papers. Re-reviewing papers and see if the results is the same, if you will. At a given prescribed acceptance ratio, there was a disagreement for 57% of the papers. This means that one of the committees would accept the paper and the other wouldn't, and vice versa. Now, to put this number into perspective, the mathematical model of a random committee was put together. This means that the members of this committee have no idea what they are doing, and as a review, they basically toss up a coin and accept or reject the paper based on the result. The calculations conclude that this random committee would have this disagreement ratio of about 77%. This is hardly something to be proud of: the consistency of expert reviewers is significantly closer to a coinflip than to a hypothetical perfect review process. So, experts, 57% disagreement, Coinflip Committee, 77% disagreement. It is not as bad as the Coinflip Committee, so the question naturally arises: where are the differences? Well, it seems that the top 10% of the papers are clearly accepted by both committees, the bottom 25% of the papers are clearly rejected, this is the good news, and the bad news is that anything between might as well be decided with a cointoss. If the consistency of peer review is subject to maximization, we clearly have to do something different. Huge respect for the NIPS organizers for doing this laborious experiment, for the reviewers who did a ton of extra work, and kudos for the fact that the organizers were willing to release such uncomfortable results. This is very important, and is the only way of improving our processes. Hopefully, someday we shall have our revenge over the Coinflip Committee. Can we do something about this? What is a possible solution? Well, of course, this is a large and difficult problem for which I don't pretend to have any perfect solutions, but there is a really interesting idea by a renowned professor about crowdsourcing reviews that I found to be spectacular. I'll leave the blog post in the comments section both for this and the NIPS experiment, and we shall have an entire episode about this soon. Stay tuned! Thanks for watching, and for your generous support, and I'll see you next time!"
189,Task-based Animation of Virtual Characters,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this piece of work, you'll see wondrous little sequences of animations where a virtual character is asked to write on a whiteboard, move boxes, and perform different kinds of sitting behaviors. The emphasis is on synthesizing believable footstep patterns for this character. This sounds a bit mundane, but we'll quickly realize that the combination and blending of different footstep styles is absolutely essential for a realistic animation of these tasks. Beyond simple locomotion (walking if you will), for instance, sidestepping, using toe and heel pivots, or partial turns and steps every now and then are essential in obtaining a proper posture for a number of different tasks. A rich vocabulary of these movement types and proper transitions between them lead to really amazing animation sequences that you can see in the video. For instance, one of the most heinous examples of the lack of animating proper locomotion we can witness in older computer games, and sometimes even today, is when a character is writing on a whiteboard, who suddenly runs out of space, turns away from it, walks a bit, turns back towards the whiteboard and continues writing there. Even if we have impeccable looking photorealistically rendered characters, such robotic behaviors really ruin the immersion. In reality, a simple sidestepping would do the job, and this is exactly what the algorithm tells the character to perform. Very simple and smooth. This technique works by decomposing a given task to several subtasks, like starting to sit on a box, or getting up, and choosing the appropriate footstep types and transitions for them. One can also mark different tasks as being low or high-effort that are marked with green and blue. A low effort task could mean fixing a minor error on the whiteboard nearby without moving there, and a high effort task that we see marked with blue would be continuing our writing on a different part of the whiteboard. For these tasks, the footsteps are planned accordingly. Really cool. This piece of work is a fine example of the depth and complexity of computer graphics and animation research, and how even the slightest failure in capturing fine scale details is enough to break the immersion of reality. It is also really amazing that we have so many people who are interested in watching these videos about research, and quite a few of you decided to also support us on Patreon. I feel really privileged to have such amazing supporters like you Fellow Scholars. As always, I kindly thank you for this at the end of these videos, so here goes... Thanks for watching, and for your generous support, and I'll see you next time!"
190,What is Optimization? + Learning Gradient Descent,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, we're not going to have the usual visual fireworks that we had with most topics in computer graphics, but I really hope you'll still find this episode enjoyable and stimulating. This episode is also going to be a bit heavy on what optimization is and we'll talk a little bit at the end about the intuition of the paper itself. We are going to talk about mathematical optimization. This term is not to be confused with the word ""optimization"" that we use in our everyday lives for, for instance, improving the efficiency of a computer code or a workflow. This kind of optimization means finding one, hopefully optimal solution from a set of possible candidate solutions. An optimization problem is given the following way: one, there is a set of variables we can play with, and two, there is an objective function that we wish to minimize or maximize. Well, this probably sounds great for mathematicians, but for everyone else, maybe this is a bit confusing. Let's build a better understanding of this concept through an example! For instance, let's imagine that we have to cook a meal for our friends from a given set of ingredients. The question is, how much salt, vegetables and meat goes into the pan. These are our variables that we can play with, and the goal is to choose the optimal amount of these ingredients to maximize the tastiness of the meal. Tastiness will be our objective function, and for a moment, we shall pretend that tastiness is an objective measure of a meal. This was just one toy example, but the list of applications is endless. In fact, optimization is so incredibly ubiquitous, there is hardly any field of science where some form of it is not used to solve difficult problems. For instance, if we have the plan of a bridge, we can ask it to tell us the minimal amount of building materials we need to build it in a way that it remains stable. We can also optimize the layout of the bridge itself to make sure the inner tension and compression forces line up well. A big part of deep learning is actually also an optimization problem. There are a given set of neurons, and the variables are when they should be activated, and we're fiddling with these variables to minimize the output error, which can be, for instance, our accuracy in guessing whether a picture depicts a muffin or a chihuahua. The question for almost any problem is usually not whether it can be formulated as an optimization problem, but whether it is worth it. And by worth it I mean the question whether we can solve it quickly and reliably. An optimizer is a technique that is able to solve these optimization problems and offer us a hopefully satisfactory solution to them. There are many algorithms that excel at solving problems of different complexities, but what ties them together is that they are usually handcrafted techniques written by really smart mathematicians. Gradient descent is one of the simplest optimization algorithms where we change each of the variables around a bit, and as a result, see if the objective function changes favorably. After finding a direction that leads to the most favorable changes, we shall continue our journey in that direction. What does this mean in practice? Intuitively, in our cooking example, after making several meals, we would ask our guests about the tastiness of these meals. From their responses, we would recognize that adding a bit more salt led to very favorable results, and since these people are notorious meat eaters, decreasing the amount of vegetables and increasing the meat content also led to favorable reviews. And we, of course, on the back of this newfound knowledge, will cook more with these variable changes in pursuit of the best possible meal in the history of mankind. This is something that is reasonably close to what gradient descent is in mathematics. A slightly more sophisticated version of gradient descent is also a very popular way of training neural networks. If you have any questions regarding the gradient part, we had an extended Two Minute Papers episode on what gradients are and how to use them to build an awesome algorithm for light transport. It is available, where? Well, of course, in the video description box, Károly, why are you even asking. So what about the paper part? This incredible new work of Google DeepMind shows that an optimization algorithm itself can emerge as a result of learning. An algorithm itself is not considered the same one thing as deciding what an image depicts or how we should grade a student essay, it is an algorithm, a sequence of steps we have to take. If we're talking about outputting sequences, we'll definitely need to use a recurrent neural network for that. Their proposed learning algorithm can create new optimization techniques that outperform previously existing methods not everywhere, but on a set of specialized problems. I hope you've enjoyed the journey, we'll talk quite a bit about optimization in the future, you'll love it. Thanks for watching, and for your generous support, and I'll see you next time!"
191,Bundlefusion: 3D Scenes from 2D Videos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work enables us to walk around in a room with a camera, and create a complete 3D computer model from the video footage. The technique has a really cool effect where the 3D model is continuously refined as we obtain more and more data by walking around with our camera. This is a very difficult problem, and a good solution to this offers a set of cool potential applications. If we have a 3D model of a scene, what can we do with it? Well, of course, assign different materials to them and run a light simulation program for architectural visualization applications, animation movies, and so on. We can also easily scan a lot of different furnitures and create a useful database out of them. There are tons of more applications, but I think these should do for starters. Normally, if one has to create a 3D model of a room or a building, the bottom line is that it requires several days or weeks of labor. Fortunately, with this technique, we'll obtain a 3D model in real time and we won't have to go through these tribulations. However, I'd like to note that the models are still by far not perfect, if we are interested in the many small, intricate details, we have add them back by hand. Previous methods were able to achieve similar results, but they suffer from a number of different drawbacks, for instance, most of them don't support traditional consumer cameras or take minutes to hours to perform the reconstruction. To produce the results presented in the paper, an NVIDIA TITAN X video card was used, which is currently one of the pricier pieces of equipment for consumers, but not so much for companies who are typically interested in these applications. If we take into consideration the rate at which graphical hardware is improving, anyone will be able to run this at home in real time in a few years time. The comparisons to previous works reveal that this technique is not only real time, but the quality of the results is mostly comparable, and in some cases, it surpasses previous methods. Thanks for watching, and for your generous support, and I'll see you next time!"
192,Photorealistic Images from Drawings,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When we were children, every single one of us dreamed about having a magic pencil that would make our adorable little drawings come true. With the power of machine learning, the authors of this paper just made our dreams come true. Here's the workflow: we provide a crude drawing of something, and the algorithm fetches a photograph from a database that depicts something similar to it. It's not synthesizing new images from scratch from a written description like one of the previous works, it fetches an already existing image from a database. The learning happens by showing a deep convolutional neural network pairs of photographs and sketches. If you are not familiar with these networks, we have some links for you in the video description box! It is also important to note that this piece of work does not showcase a new learning technique, it is using existing techniques on a newly created database that the authors kindly provided free of charge to encourage future research in this area. What we need to teach these networks is the relation of a photograph and a sketch. For instance, in an earlier work by the name Siamese networks, the photo and the sketch would be fed to two convolutional neural networks with the additional information whether this pair is considered similar or dissimilar. This idea of Siamese networks was initially applied to signature verification more than 20 years ago. Later, Triplet networks were used provide the relation of multiple pairs, like ""this sketch is closer to this photo than this other one"". There is one more technique referred to in the paper that they used, which is quite a delightful read, make sure to have a look! We need lots and lots of these pairs so the learning algorithm can learn what it means that a sketch is similar to a photo, and as a result, fetch meaningful images for us. So, if we train these networks on this new database, this magic pencil dream of ours can come true. What's even better, anyone can try it online! This is going to be a very rigorous and scholarly scientific experiment I don't know what this should be, but I hope the algorithm does. Well, that kinda makes sense. Thanks, algorithm! For those Fellow Scholars out there who are endowed with better drawing skills than I am, well, basically all of you if you have tried it and got some amazing, or maybe not so amazing results, please post them in the comments section! Or, as we now have our very own subreddit, make sure to drop by and post some of your results there so we can marvel at them, or have a good laugh at possible failure cases. I am looking forward to meeting you Fellow Scholars at the subreddit. Flairs are also available. Thanks for watching, and for your generous support, and I'll see you next time!"
193,Visually Indicated Sounds,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This name is not getting any easier, is it? It used to be Károly Zsolnai, which was hard enough, and now this... haha. Anyway, let's get started. This technique simulates how different objects in a video sound when struck. We have showcased some marvelous previous techniques that were mostly limited to wooden and plastic materials. Needless to say, there are links to these episodes in the video description box. A convolutional neural network takes care of understanding what is seen in the video. This technique is known to be particularly suited to processing image and video content. And it works by looking at the silent video directly and trying to understand what is going on, just like a human would. We train these networks with input and output pairs the input is a video of us beating the hell out of some object with a drumstick. The joys of research! And the output is the sound this object emits. However, the output sound is something that changes in time. It is a sequence, therefore it cannot be handled by a simple classical neural network. It is learned by a recurrent neural network that can take care of learning such sequences. If you haven't heard these terms before, no worries, we have previous episodes on all of them in the video description box, make sure to check them out! This piece of work is a nice showcase of combining two quite powerful techniques: the convolutional neural network tries to understand what happens in the input video, and the recurrent neural network seals the deal by learning and guessing the correct sound that the objects shown in the video would emit when struck. The synthesized outputs were compared to the real world results both mathematically and by asking humans to try to tell from the two samples which one the real deal is. These people were fooled by the algorithm around 40% of the time, which I find to be a really amazing result, considering two things: first, the baseline is not 50%, but 0% because people don't pick choices at random we cannot reasonably expect a synthesized sound to fool humans at any time. Like nice little neural networks, we've been trained to recognize these sounds all our lives, after all. And second, this is one of the first papers from a machine learning angle on sound synthesis. Before reading the paper, I expected at most 10 or 20 percent, if that. The tidal wave of machine learning runs through a number of different scientific fields. Will deep learning techniques establish supremacy in these areas? Hard to say yet, but what we know for sure is that great strides are made literally every week. There are so many works out there, sometimes I don't even know where to start. Good times indeed! Before we go, some delightful news for you Fellow Scholars! The Scholarly Two Minute Papers store is now open! There are two different kinds of men's T-shirts available, and a nice sleek design version that we made for the Fellow Scholar ladies out there! We also have The Scholarly Mug to get your day started in the most scientific way possible. We have tested the quality of these products and were really happy with what we got. If you ordered anything, please provide us feedback on how you liked the quality of the delivery and the products themselves. If you can send us an image of yourself wearing or using any of these, we'd love to have a look. Just leave them in the comments section or tweet at us! If you don't like what you got, within 30 days, you can exchange it or get your product cost refunded. Thanks for watching, and for your generous support, and I'll see you next time!"
194,Time Varying Textures,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This research group is known for their extraordinary ideas, and this piece of work is, of course, no exception. This paper is about time varying textures. Have a look at these photographs that were taken at a different time. And the million dollar question is, can we simulate how this texture would look if we were to go forward in time? A texture weathering simulation, if you will. The immediate answer is that of course not. However, in this piece of work, a single input image is taken, and without any user interaction, the algorithm attempts to understand how this texture might have looked in the past. Now, let's start out by addressing the elephant in the room: this problem can obviously not be solved in the general case for any image. However, if we restrict our assumptions to textures that contain a repetitive pattern, then it is much more feasible to identify the weathering patterns. To achieve this, an age map is built where the red regions show the parts that are assumed to be weathered. You can see on the image how these weathering patterns break up the regularity. Leaning on the assumption that if we go back in time, the regions marked with red will recede, and if we go forward in time, they will grow, we can write a really cool weathering simulator that creates results that look like wizardry. Broken glass, cracks, age rings on a wooden surface, you name it. But we can also use this technique to transfer weathering patterns from one image onto another. Textures with multiple layers are also supported, which means that it can handle images that are given as a sum of a regular and irregular patterns. The blue background is regular and quite symmetric, but the ""no parking"" text is lacking these regularities. And the amazing thing is that the technique still works on such cases. The results are also demonstrated by putting these weathered textures on 3D models so we can see them all in their glory in our own application. Thanks for watching, and for your generous support, and I'll see you next time!"
195,Fermat Spirals for Layered 3D Printing,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What are Hilbert curves? Hilbert curves are repeating lines that are used to fill a square. Such curves, so far, have enjoyed applications like drawing zigzag patterns to prevent biting in our tail in a snake game. Or, jokes aside, it is also useful in, for instance, choosing the right pixels to start tracing rays of light in light simulations, or to create good strategies in assigning numbers to different computers in a network. These numbers, by the way, we call IP addresses. These are just a few examples, and they show quite well how a seemingly innocuous mathematical structure can see applications in the most mind bending ways imaginable. So here is one more. Actually, two more. Fermat's spiral is essentially a long line as a collection of low curvature spirals. These are generated by a remarkably simple mathematical expression and we can also observe such shapes in mother nature, for instance, in a sunflower. And the most natural question emerges in the head of every seasoned Fellow Scholar. Why is that? Why would nature be following mathematics, or anything to do with what Fermat wrote on a piece of paper once? It has only been relatively recently shown that as the seeds are growing in the sunflower, they exert forces on each other, therefore they cannot be arranged in an arbitrary way. We can write up the mathematical equations to look for a way to maximize the concentration of growth hormones within the plant to make it as resilient as possible. In the meantime, this force exertion constraint has to be taken into consideration. If we solve this equation with blood sweat and tears, we may experience some moments of great peril, but it will be all washed away by the beautiful sight of this arrangement. This is exactly what we see in nature. And, which happens to be almost exactly the same as a mind-bendingly simple Fermat spiral pattern. Words fail me to describe how amazing it is that mother nature is essentially able to find these solutions by herself. Really cool, isn't it? If our mind wasn't blown enough yet, Fermat spirals can also be used to approximate a number of different shapes with the added constraint that we start from a given point, take an enormously long journey of low curvature shapes, and get back to almost exactly where we started. This, again, sounds like an innocuous little game evoking ill-concealed laughter in the audience as it is presented by as excited as underpaid mathematicians. However, as always, this is not the case at all. Researchers have found that if we get a 3D printing machine and create a layered material exactly like this, the surface will have a higher degree of fairness, be quicker to print, and will be generally of higher quality than other possible shapes. If we think about it, if we wish to print a prescribed object, like this cat, there is a stupendously large number of ways to fill this space with curves that eventually form a cat. And if we do it with Fermat spirals, it will yield the highest quality print one can do at this point in time. In the paper, this is demonstrated for a number of shapes of varying complexities. And this is what research is all about finding interesting connections between different fields that are not only beautiful, but also enrich our everyday lives with useful inventions. In the meantime, we have reached our first milestone on Patreon, and I am really grateful to you Fellow Scholars who are really passionate about supporting the show. We are growing at an extremely rapid pace and I am really excited to make even more episodes about these amazing research works. Thanks for watching, and for your generous support, and I'll see you next time!"
196,Procedural Yarn Models for Cloth Rendering,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we're going to talk about a procedural algorithm. But first of all, what does procedural mean? Procedural graphics is an exciting subfield of computer graphics where instead of storing a lot of stuff, information is generated on the fly. For instance, in photorealistic rendering, we're trying to simulate how digital objects would look like in real life. We usually seek to involve some scratches on our digital models, and perhaps add some pieces of bump or dirt on the surface of the model. To obtain this, we can just ask the computer to not only generate them on the fly, but we can also edit them as we desire. We can also generate cloudy skies and many other things where only some statistical properties have to be satisfied, like how many clouds we wish to see and how puffy they should be, which would otherwise be too laborious to draw by hand. We are scholars after all, we don't have time for that. There are also computer games where the levels we can play through are not predetermined, but also generated on the fly according to some given logical constraints. This can mean that a labyrinth should be solvable, or the level shouldn't contain too many enemies that would be impossible to defeat. The main selling point is that such a computer game has potentially an infinite amount of levels. In this paper, a technique is proposed to automatically generate procedural yarn geometry. A yarn is a piece of thread from which we can sew garments. The authors extensively studied parameters in physical pieces of yarns such as twisting and hairyness and tried to match them with a procedural technique. So, for instance, if in a sudden trepidation we wish to obtain a realistic looking piece of cotton, rayon or silk in our light simulation programs, we can easily get a unique sample of a chosen material, which will be very close to the real deal in terms of these intuitive parameters like hairyness. And we can not only get as long or as many of these as we desire, but can also edit them according to our artistic vision. The solutions are validated against photographs and even CT scans. I always emphasize that I really like these papers where the solutions have some connection to real world around us. This one is super fun indeed! The paper is a majestic combination of beautifully written mathematics and amazing looking results. Make sure to have a look! And you know, we always hear these news where other YouTubers have problems with what is going on in their comments section. Well, not here with our Fellow Scholars. Have a look at the comments section of our previous episode. Just absolutely beautiful. I don't even know what to say, it feels like a secret hideout of respectful and scholarly conversations. It's really amazing that we are building a community of Fellow Scholars, humble people who wish nothing else than to learn more. Thanks for watching, and for your generous support, and I'll see you next time!"
197,What Can We Learn From Deep Learning Programs?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I have recently been witnessing a few heated conversations regarding the submission of deep learning papers to computer vision conferences. The forums are up in arms about the fact that despite some of these papers showcased remarkably good results, they were rejected on the basis of, from what I have heard, not adding too much to the tree of knowledge. They argue that we don't understand what is going on in these neural networks and cannot really learn anything new from them. I'll try to understand and rephrase their argument differently. We know exactly how to train a neural network, it's just that as an output of this process, we get a model of something that resembles a brain as a collection of neurons and circumstances under which these neurons are activated. We store these in a file that can take up to several gigabytes, and the best solutions are often not intuitively understandable for us. For instance, in this video, we're training a neural network to classify these points correctly, but what exactly can we learn if we look into these neurons? Now imagine that in practice, we don't have a handful of these boxes, but millions of them, and more complex than the ones you see here. Let's start with a simple example that hopefully helps getting a better grip of this argument. Now, I'll be damned if this video won't be more than a couple minutes, so this is going to be one of those slightly extended Two Minute Paper episodes. I hope you don't mind! The grammatical rules of my native language, a lot of them, are contained in enormous tomes that everyone has to go through during their school years. Rules are important. They give the scaffoldings for constructing sentences that are grammatically correct. Can we explain or even enumerate these rules? Well, unless you are a linguist, the answer is no. Almost no one really remembers more than a few rules, but every native speaker knows how their language should be spoken. And it is because we've heard a lot of sentences that are correct and learned it by heart what makes a proper sentence and what is gibberish. This is exactly what neural networks do they are trained in a very similar way. In fact, they are so effective at it that if we you try to forcefully insert some of our knowledge in there, the solutions are going to get worse. It is therefore an appropriate time to ask questions like what merits a paper and what do we define as scientific progress. What if we have extremely accurate algorithms where we don't know what is going on under the hood, or simpler, more intuitive algorithms that may be subpar in accuracy. If we have a top tier scientific conference where only a very limited number of papers get accepted, which ones shall we accept? I hope that this question will spark a productive discussion, and hopefully scientific research venues will be more vigilant about this question in the future. Okay, so the question is crystal clear: knowledge or efficiency? How about possible solutions? Can we extract scientific insights out of these neural networks? Model compression is a way to essentially compress the information in this brain-ish thing, this collection of neurons we described earlier. To demonstrate why this is such a cool idea, let's quickly jump to this program by DeepMind that plays Atari games at an amazingly high level. In breakout, the solution program that you see here is essentially an enormous table that describes what the program should do when it sees different inputs. It is so enormous that it has many millions of records in there. A manual of many thousand pages, if you will. It is easy to execute for a computer, but completely impossible for us to understand why and how it works. However, if we intuitively think about the game itself, we could actually write a super simple program in one line of code that would almost be as good as this. All we need to do is try to follow the ball with the paddle. One line of code, and pretty decent results. Not optimal, but quite decent. From such a program, we can actually learn something about the game. Essentially what we could do with these enormous tables, is compressing them into much much smaller ones. Ones that are so tiny, that we can actually build an intuition from them. This way, the output of a machine learning technique wouldn't only be an extremely efficient program, but finally, the output of the procedure would be knowledge. Insight. If you think about it, such an algorithm would essentially do research by itself. At first, it would randomly try experimenting, and after a large amount of observations are collected, these observations would be explained by a small number of rules. That is exactly the definition of research. And perhaps, this is one of the more interesting future frontiers of machine learning research. And by the way, earlier we have talked about a fantastic paper on Neural Programmer Interpreters that also aimed to output complete algorithms that can be directly used and understood. The link is available in the description box. Thanks for watching, and for your generous support, and I'll see you next time!"
198,Hallucinating Images With Deep Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In an earlier episode, we showcased a technique for summarizing images not in a word, but an entire sentence that actually makes sense. If you were spellbound by those results, you'll be out of your mind when you hear this one: let's turn it around, and ask the neural network to have a sentence as an input, and ask it to generate images according to it. Not fetching already existing images from somewhere, generating new images according to these sentences. Create new images according to sentences. Is this for real? This is an idea, that is completely out of this world. A few years ago, if someone proposed such an idea and hoped that any useful result can come out of this, that person would have immediately been transported to an asylum. An important keyword here is ""zero shot"" recognition. Before we go to the zero part, let's talk about one shot learning. One shot learning means a class of techniques that can learn something from one, or at most a handful of examples. Deep neural networks typically require to see hundreds of thousands of mugs before they can learn the concept of a mug. However, if I show one mug to any of you Fellow Scholars, you will, of course, immediately get the concept of a mug. At this point, it is amazing what these deep neural networks can do, but with the current progress in this area, I am convinced that in a few years, feeding millions of examples to a deep neural network to learn such a simple concept will be considered a crime. Onto zero shot recognition! The zero shot is pretty simple it means zero training samples. But this sounds preposterous! What it actually means is that we can train our network to recognize birds, tiny things, what the concept of blue is, what a crown is, but then we ask it to show us an image of ""a tiny bird with a blue crown"". Essentially, the neural network learns to combine these concepts together and generate new images leaning on these learned concepts. I think this paper is a wonderful testament as to why Two Minute Papers is such a strident advocate of deep learning and why more people should know about these extraordinary works. About the paper it is really well written, there are quite a few treats in there for scientists: game theory and minimax optimization, among other things. Cupcakes for my brain. We will definitely talk about these topics in later Two Minute Papers episodes, stay tuned! But for now, you shouldn't only read the paper you should devour it. And before we go, let's address the elephant in the room: the output images are tiny because this technique is very expensive to compute. Prediction: two papers down the line, it will be done in a matter of seconds, two even more papers down the line, it will do animations in full HD. Until then, I'll sit here stunned by the results, and just frown and wonder. Thanks for watching, and for your generous support, and I'll see you next time!"
199,Rocking Out With Convolutions,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. And today we're here to answer one question: what is a convolution? I have heard many university students crying in despair over their perilous journeys of understanding what convolutions are, and why they are useful. Let me give a helping hand here! A convolution is a mathematical technique to mix together two signals. A lot of really useful tasks can be accomplished through this operation. For instance, convolutions can be used to add reverberation to a recorded instrument. So I play my guitar here in my room, and it can sound like as if it were recorded in a large concert hall. Now, dear Fellow Scholars, put on a pair of headphones, and let me rock out on my guitar to show it to you! First, you'll hear the dry guitar signal. And this is the same signal with the added reverberation. It sounds much more convincing, right? In simple words, a convolution is a bit like saying guitar plus concert hall equals a guitar sound that was played in a concert hall. The only difference is that we don't say guitar ""plus"" concert hall, we say guitar ""convolved"" with the concert hall. If we want to be a bit more accurate, we could say the impulse response of the hall, which records how this place reacts to a person who starts to play the guitar in there. People use the living hell out of convolution reverberation plugins in the music industry. Convolutions can also be used to blur or sharpen an image. We also had many examples of convolutional neural networks that provide efficient means to, for instance, get machines to recognize traffic signs. We can also use them to add sophisticated light transport effects, such as subsurface scattering to images. This way, we can conjure up digital characters with stunningly high quality skin and other translucent materials in our animations and computer games. We have had a previous episode on this, and it is available in the video description box, make sure to have a look! As we said before, computing a convolution is not at all like addition. Not even close. For instance, the convolution of two boxes is ... a triangle. Wow, what? What kind of witchcraft is this? It doesn't sound intuitive at all! The computation of the convolution means that we start to push this box over the other one and at every point in time, we take a look at the intersection between the two signals. As you can see, at first, they don't touch at all. Then, as they start to overlap, we have highlighted the intersected area with green, and as they get closer to each other, this area increases. When they are completely overlapped, we get the maximum intersection area, which then starts to dwindle as they separate. So there you have it. It is the miracle of mathematics that by computing things like this, we can rock out in a virtual chuch or a stadium which sounds very close to the real deal. And before we go, a quick shoutout to immersive math, a really intuitive resource for learning linear algebra. If you are into math, you simply have to check this one out, it's really cool. Thanks for watching, and for your generous support, and I'll see you next time!"
200,Reinforcement Learning with OpenAI's Gym,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is a technique in the field of machine learning to learn how to navigate in a labyrinth, play a video game, or to teach a digital creature to walk. Usually, we are interested in a series of actions that are in some sense, optimal in a given environment. Despite the fact that many enormous tomes exist to discuss the mathematical details, the intuition behind the algorithm itself is incredibly simple. Choose an action, and if you get rewarded for it, keep doing it. If the rewards are not coming, try something else. The reward can be, for instance, our score in a computer game or how far our digital creature could walk. It is usually quite difficult to learn things where the reward comes long after our action because we don't know when exactly the point was when we did something well. This is one of the reasons why Google DeepMind will try to conquer strategy games in the future, because this is a genre where good plays usually include long-term planning that reinforcement learning techniques don't really excel at. By the way, this just in: they the have just published an excellent paper on including curiosity in this equation in a way that helps long term planning remarkably. As more techniques pop up in this direction, it is getting abundantly clear that we need a framework where they can undergo stringent testing. This means that the amount of collected rewards and scores should be computed the same way, and in the same physical framework. OpenAI is a non-profit company boasting an impressive roster of top-tier researchers who embarked on the quest to develop open and ethical artificial intelligence techniques. We've had a previous episode on this when the company was freshly founded, and as you might have guessed, the link is available in the description box. They have recently published their first major project that goes by the name Gym. Gym is a unified framework that puts reinforcement learning techniques on an equal footing. Anyone can submit their solutions which are run on the same problems, and as a nice bit of gamification, leaderboards are established to see which technique emerges victorious. These environments range from a variety of computer games to different balancing tasks. Some simpler reference solutions are also provided for many of them as a starting point. This place is like Disneyworld for someone who is excited about the the field of reinforcement learning. With more and more techniques, the subfield gets more saturated, it gets more and more difficult to be the first at something. That's a great challenge for researchers. From a consumer point of view, this means that better techniques will pop up day by day. And, as I like to say quite often, we have really exciting times ahead of us. A quick shoutout to Experiment, a startup to help research projects come to fruition by crowdsourcing them. Current experiments include really cool projects like how we could implement better anti-doping policies for professional sports, or to show on a computer screen how our visual imagination works. Thanks for watching, and for your generous support, and I'll see you next time!"
201,Image Colorization With Deep Learning and Classification,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about adding color to black and white images. There were some previous works that tackled this problem, and many of them worked quite well, but there were cases when the results simply didn't make too much sense. For instance, the algorithm often didn't guess what color the fur of a dog should be. If we would give the same task to a human, we could usually expect better results because the human knows what breed the dog is, and what colors are appropriate for that breed. In short, we know what is actually seen on the image, but the algorithm doesn't it just trains on black and white and colored image pairs and learns how it is usually done without any concept of what is seen on the image. So here is the idea let's try to get the neural network not only to colorize the image, but classify what is seen on the image before doing that. If we see a dog in an image, it is not that likely to be pink, is it? If we know that we have to deal with a golf course, we immediately know to reach out for those green crayons. This is a novel fusion-based technique. This means that we have a separate neural network for classifying the images and one for colorizing them. The fusion part is when we unify the information in these neural networks so we can create an output that aggregates all this information. And the results, are just spectacular, the additional information on what these images are about really make a huge impact on the quality of the results. Please note that this is by far not the first work on fusion, I've also linked an earlier paper for recognizing objects in videos, but I think this is a really creative application of the same train of thought that is really worthy of our attention. To delight the fellow tinkerers out there, the source code of the project is also available. The supplementary video reveals that temporal coherence is still a problem. This means that every image is colorized separately with no communication. It is a bit like giving the images to colorize one by one to different people, with no overarching artistic direction. The result we'll get this way is a flickery animation. This problem has been solved for artistic style transfer, which we have discussed in an earlier episode, the link is in the description box. There was one future episode planned about plastic deformations. I have read the paper several times, and it is excellent, but I felt that the quality of my presentation was not up there to put it in front of you Fellow Scholars. It may happen in the future, but I had to shelf this one for now. Please accept my apologies for that. In the next episode, we'll continue with OpenAI's great new invention for reinforcement learning. Thanks for watching, and for your generous support, and I'll see you next time!"
202,Schrödinger's Smoke,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There are two main branches of efficient smoke and fluid simulator programs: Eulerian and Lagrangian techniques. Before we dive into what these terms mean, I'd like to note that we have closed captions available for this series that you can turn on by clicking the cc button at the bottom of the player. With that out of the way, the Eulerian technique means that we have a fixed grid, and the measurement happens in the gridpoints only. We have no idea what happens between these gridpoints. It may sound counterintuitive at first because it has no notion of particles at all. With the Lagrangian technique, we have particles that move around in space, and we measure important quantities like velocity and pressure with these particles. In short, Eulerian grids, Lagrangian particles. Normally, the problem with Eulerian simulations is that we don't know what exactly happens between the gridpoints, causing information to disappear in these regions. To alleviate this, they are usually combined with Lagrangian techniques, because if we also track all these particles individually, we cannot lose any of them. The drawback is, of course, that we need to simulate millions of particles, which will take at least a few minutes for every frame we wish to compute. By formulating his famous equation, the Austrian physicist Erwin Schrödinger won the Nobel prize in 1933. In case you're wondering, yes, this is the guy who forgot to feed his cat. There is two important things you should know about the Schrödinger equation: one is that it is used to describe how subatomic particles behave in time, and two, it has absolutely nothing to do with large-scale fluid simulations whatsoever. The point of this work is to reformulate Schödinger's equation in a way that it tracks the density and the velocity of the fluid in time. This way, it can be integrated in a purely grid-based, Eulerian fluid simulator, and we don't need to track all these individual particles one by one, but we can still keep these fine, small-scale details in a way that rivals Lagrangian simulations, but without the huge additional costs. So, the idea is absolutely bonkers, just the thought of doing this sounds so outlandish to me. And it works! Obstacles are also supported by this technique. Many questions still remain, such as how to mix different fluid interfaces together, how to model the forces between them. I do not have the prescience to see the limits of the approach, but I am quite convinced that this direction holds a lot of promise for the future. I cannot wait to play with the code and see some followup works on this! As always, everything is linked in the video description box. The paper is not only absolutely beautifully written, but it is also a really fun paper to read. As I read it, I really loved how a jolt of epiphany ran through me. It is a fantastic feeling when a lightbulb lights up in my mind as I suddenly get to understand something. I think it is the scientist's equivalent of obtaining englightenment. May it happen to you Fellow Scholars often during your journeys! And I get to spend quite a bit of time every day reading fine works like this. It's a good life. I'd like to give a quick shoutout to this really cool website called Short Science, which is a collection of crowdsourced short summaries for scientific papers. Really cool stuff, make sure to have a look! Thanks for watching, and for your generous support, and I'll see you next time!"
203,Storytime Reading Comments,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have reached a lot of milestones lately. For instance, we now have over one million views on the channel. In the next few days we're also hoping to hit ten thousand subscribers. To put it in perspective, in 2015 August 20th, we have had a 250 subscriber special episode where my mind was pretty much blown. That was only about 9 months ago. Whoa. For the record, here is an image that supposedly contains ten thousand people. Just imagine that they all have doctoral hats and you immediately have a mental image of ten thousand of our Fellow Scholars. It's insanity. And I must say that I am completely blown away by your loyal support. Thanks so much for everyone for the many kind messages, comments and e-mails, of which I'll read quite a few in a second. It just boggles the mind to see that so many people are interested in learning more about awesome new research inventions, and I hope that it is as addictive to you as it was for me when I first got to see some of these results. A huge thank you also to our generous supporters on Patreon, I find it really amazing that we have quite a few Fellow Scholars out there who love the series so much that they are willing to financially help our cause. Just think about it. Especially given that fact that it's not easy to make ends meet these days, I know this all too well being a full time researcher, doing Two Minute Papers and having a baby is extremely taxing, but I love it. I really do. And I know there are people who have it way worse, and here we have these Fellow Scholars who believe in this cause and are willing to help out. Thanks so much for each one of you, I am honored to have loyal supporters like you. We are more than halfway there towards our first milestone on Patreon, which means that our hardware and software costs can be completely covered with your help. As you know, we have really cool perks for our Patrons. One of these perks is that, for instance, our Professors can decide the order of the next episodes, and I was really surprised to see that this episode won by a record number of points. Looks like you Fellow Scholars are really yearning for some story time, so let's bring it on! The focus of Two Minute Papers has always been what a work is about and why it is important. It has always been more about intuition than specific details. That is the most important reason why Two Minute Papers exists. But this sounds a bit cryptic, so let me explain myself. Most materials on any scientific topic on YouTube are about details. When this big deep learning rage started, and it was quite fresh, I wanted to find out what deep learning was. I haven't found a single video that was not at least 30-90 minutes long, and all of them talked about partial derivatives and the chain rule, but never explained what we're exactly doing and why we are doing it. It took me four days to get a good grasp, a good high-level intuition of what is going on. I would have loved to have a resource that explains this four days worth of research work in just two minutes in a way that anyone can understand. Anyone. I haven't found anything, and this was just one example of many. Gandhi said ""Be the change that you wish to see in the world"", and this is how Two Minute Papers was born. It has never been about the details, it has always been about intuition"
204,Surface-Only Liquids,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Most of the techniques we've seen in previous fluid papers run the simulation inside the entire volume of the fluids. These traditional techniques scale poorly with the size of our simulation. But wait, as we haven't talked about scaling before, what does this scaling thing really mean? Favorable scaling means that if we have a bigger simulation, we don't have to wait longer for it. Our scaling is fairly normal if we have a simulation twice as big and we need to wait about twice as much. Poor scaling can give us extraordinarily bad deals, such as waiting ten or more times as much for a simulation that is only twice as big. Fortunately, a new class of algorithms is slowly emerging that try to focus more resources on computing what happens near the surface of the liquid, and try to get away with as little as possible inside of the volume. This piece of work shows that most of the time, we can get away with not doing computations inside the volume of the fluid, but only on the surface. This surface-only technique scales extremely well compared to traditional techniques that simulate the entire volume. If a piece of fluid were an apple, we'd only have to eat the peel, and not the whole apple. It's a lot less chewing, right? As a result, the chewing, or the computation, if you will, typically takes seconds per image instead of minutes. A previous technique on narrow band fluid simulations computed the important physical properties near the surface, but in this case, we compute not near the surface, but only on the surface. The difference sounds subtle, but it makes a completely different mathematical background. To make such a technique work, we have to make simplifications to the problem. For instance, one of the simplifications is to make the fluids incompressible. This means that the density of the fluid is not allowed to change. The resulting technique supports simulating a variety of cases such as dripping water, droplet and crown splashes. fluid chains and sheet flapping. I was spellbound by the mathematics written in the paper that is both crystal clear and beautiful in its flamboyancy. This one is such a spectacular paper. It is so good, I had it on my tablet and couldn't wait to get on the train so I could finally read it. The main limitation of the technique is that it is not that useful if we have a large surface to volume ratio, simply because the peel is still a large amount compared to the volume of our apple. We need it the other way around for this technique to be useful, which is true in many cases. Thanks for watching, and for your generous support, and I'll see you next time!"
205,Artistic Style Transfer For Videos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have previously talked about a technique that used a deep neural network to transfer the artistic style of a painting to an arbitrary image, for instance, to a photograph. As always, if you're not familiar with some of these terms, we have discussed them in previous episodes, and links are available in the description box, make sure to check them out! Style transfer is possible on still images. As there is currently no technique to apply this to videos, it is hopefully abundantly clear that a lot of potential still lies dormant inside. But can we apply this artistic style transfer to videos? Would it work if we would simply try? For an experienced researcher, it is flagrantly obvious that it's an understatement to say that that it wouldn't work. It would fail in a spectacular manner, as you can see here. But with this technique, it apparently works quite well. To be frank, the results look gorgeous. So how does it work? Now, don't be afraid, you'll be presented with a concise, but deliberately obscure statement: This technique preserves temporal coherence when applying the artistic style by incorporating the optical flow of the input video. Now, the only question is what temporal coherence and optical flow means. Temporal coherence is a term that was used by physicists to describe, for instance, how the behavior of a wave of light changes, or stays the same if we observe it at different times. In computer graphics, it is also an important term because oftentimes, we have techniques that we can apply to one image, but not necessarily to a video, because the behavior of the technique changes drastically from frame to frame, introducing a disturbing flickering effect that you can see in this video here. We have the same if we do the artistic style transfer, because there is no communication between the individual images of the video. The technique has no idea that most of the time we're looking at the same things, and if so, the artistic style would have to be applied the same way over and over to these regions. We are clearly lacking temporal coherence. Now, onto optical flows. Imagine a flying drone that takes a series of photographs while hovering and looking around above us. To write sophisticated navigation algorithms, the drone would have to know which object is which across many of these photographs. If we have slightly turned, most of what we see is the same, and only a small part of the new image is new information. But the computer doesn't know that, as all it sees is a bunch of pixels. Optical flow algorithms help us achieving this by describing the possible motions that give us photograph B from photograph A. In this application, what this means is that there is some inter-frame communication, the algorithm will know that if I colored this person this way a moment ago, I cannot drastically change the style of that region on a whim. It is now easy to see why naively applying such techniques to many individual frames would be a flippant attempt to create beautiful, smooth looking videos. So now, it hopefully makes a bit more sense: This technique preserves temporal coherence when applying the artistic style by incorporating the optical flow of the input video. Such great progress in so little time. Loving it. Last time I've mentioned Kram from the comments section, and this time, I'd like to commend Relatedgiraffe for his insightful comments. Thanks for being around and I've definitely learned from you Fellow Scholars! I am really loving the respectful and quality discussions that take place in the comments section, and it is really cool that we can both learn from each other. Thanks for watching, and for your generous support, and I'll see you next time!"
206,Deep Reinforcement Terrain Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a followup work to a technique we have talked about earlier. We have seen how different creatures learned to walk, and their movement patterns happened to be robust to slight variations in the terrain. In this work, we imagine these creatures as a collection of joints and links, typically around 20 links. Depending on what actions we choose for these individual body parts in time, we can construct movements such as walking, or leaping forward. However, this time, these creatures not only learn to walk, but they also monitor their surroundings and are also taught to cope with immense difficulties that arise from larger terrain differences. This means that they learn both on character features, like where the center of mass is and what the velocity of different body parts are, and terrain features, such as, what the displacement of the slope we're walking up on is or if there's a wall ahead of us. The used machinery to achieve this is deep reinforcement learning. It is therefore a combination of a deep neural network and a reinforcement learning algorithm. The neural network learns the correspondence between these states and output actions, and the reinforcement learner tries to guess which action will lead to a positive reward, which is typically measured as our progress on how far we got through the level. In this footage we can witness how a simple learning algorithm built from these two puzzle pieces can teach these creatures to modify their center of mass and adapt their movement to overcome more sophisticated obstacles, and, other kinds of advertisites. And please note that the technique still supports a variety of different creature setups. One important limitation of this technique is that it is restricted to 2D. This means that the characters can walk around not in a 3D world, but on a plane. A question whether we're shackled by the 2D-ness of the technique or if the results can be applied to 3D remains to be seen. I'd like to note that candidly discussing limitations is immensely important in research, and the most important thing is often not we can do at this moment, but the long-term potential of the technique, which, I think this work has in abundance. It's very clear that in this research area, enormous leaps are made year by year, and there's lots to be excited about. As more papers are published on this locomotion problem, the authors also discuss that it would be great to have a unified physics system and some error metrics so that we can measure these techniques against each other on equal footings. I feel that such a work would provide fertile grounds for more exploration in this area, and if I see more papers akin to this one, I'll be a happy man. Thanks for watching, and for your generous support, and I'll see you next time!"
207,Separable Subsurface Scattering,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Subsurface scattering means that a portion of incoming light penetrates the surface of a material. Our skin is a little known, but nonetheless great example of that, but so are plant leaves, marble, milk, or snails, to have a wackier example. Subsurface scattering looks unbelievably beautiful, but at the same time, it is very expensive to compute because we have to simulate up to thousands and thousands of light scattering events for every ray of light. And we have to do this for millions of rays. It really takes forever. The lack of subsurface scattering is the reason why we've seen so many lifeless, rubber-looking human characters in video games and animated movies for decades now. This technique is a collaboration between the Activision Blizzard game development company, the University of Zaragoza in Spain, and the Technical University of Vienna in Austria. And, it can simulate this kind of subsurface light transport in half a millisecond per image. Let's stop for a minute and think about this. Earlier, we talked about subsurface scattering techniques that were really awesome, but still took at least let's say four hours on a scene before they became useful. This one is half a millisecond per image. Almost nothing. In one second, it can do this calculation two thousand times. Now, this has to be a completely different approach than just simulating many millions of rays of light, right? We can't take a four hour long algorithm, do some magic and get something like this. The first key thought is that we can set up some cool experiment where we play around with light sources and big blocks of translucent materials, and record how light bounces off of these materials. Cool thing number one: we only need to do it once per material. Number two: the results can be stored in an image. This is what we call a diffusion profile and this is how it looks like. So we have an image of the diffusion profile, and one image of the material that we would like to add subsurface scattering to. This is a convolution-based technique, which means that it enables us not to add these two images together, but to mix them together in a way that the optical properties of the diffusion profiles are carried to the image. If we add the optical properties of an apple to a human face, it will look more like a face that has been carved out of a giant apple. A less asinine application is, of course, if we mix it with the appropriate skin profile image, then we'll get photorealistic looking faces, as it is demonstrated quite aptly by this animation. This apple to skin example, by the way, you can actually try for yourself, as the source code and an executable demo is also freely available for everyone to experiment with. Convolutions have so many cool applications, I don't even know where to start. In fact, I think we should have an episode solely on that. Can't wait, it's going to be a lot of fun! These convolution computations are great, but they are still too expensive for real-time video games. What this work gives us, is a set of techniques that are able to compute this convolution not on these original images, but much smaller, tiny-tiny strips which are much cheaper, but the result of the computations look barely distinguishable. Another cool thing is that the quality of the results is not only scientifically provable, but this technique also opens up the possibility of artistic manipulation. It is done in a way that we can start out with a physically plausible result and tailor it to our liking. You can see some exaggerated examples of that. The entire technique is so simple, a computer program that executes it can fit on your business card. It also seems to have appeared in Blender recently. Also, a big hello and a shoutout for the awesome people at Intel who recently invited my humble self to chat a bit about this technique. If you would like to hear more about the details on how this algorithm works, I have put some videos in the description box. The most important take home message from this project, at least for me, is that it is possible to conduct academic research projects together with companies, and create results that can make it to multi-million dollar computer games, but also having proven results that are useful for the scientific community. Thanks for watching, and for your generous support, and I'll see you next time!"
208,Real-Time Shading With Area Light Sources,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In computer graphics, we use the term shading to describe the process of calculating the appearance of a material. This gives the heart and soul of most graphical systems that visualize something on our screen. Let the blue sphere be the object to be shaded and the red patch be the light source illuminating it. The question is, in this configuration, how should the blue sphere look in reality? In order to obtain high-quality images, we need to calculate how much of the red patch is visible from the blue sphere. This describes the object's relation to the light source. It it close or is it nearby? Is it facing the object or not? What shape is the light source? These factors determine how much light will arrive to the surface of the blue sphere. This is what mathematicians like to call an integration problem. However, beyond this calculation we also have to take into consideration the reflectance of the material that the blue sphere is made of. Whether we have a white wall surface or an orange makes a great deal of difference and throws a wrench in our already complex calculations. The final shading is the product of this visibility situation and the material properties of the sphere. Needless to say that the mathematical description of many materials can get extremely complex, which makes our calculations really time consuming. In this piece of work, a technique is proposed that can approximate these two factors in real time. The paper contains a very detailed demonstration of the difference between this and the analytical computations that give us the perfect results but take extremely long. In short, this technique is very closely matching the analytic results, but it is doing it in real time. I really don't know what to say. We're used to wait for hours to obtain images like this, and now, 15 milliseconds per frame. What a hefty value proposition for a paper. Absolutely spectacular. Some of the results really remind me of topological calculations. Topology is a subfield of mathematics that studies what properties of different shapes are preserved when these shapes are undergoing deformations. It's super useful because, for instance, if we can prove that light behaves in some way when the light source has the shape of a disk, then if we're interested in other shapes, topology can help us determine whether all these enormous books full of theorems on other shapes are going to apply to this shape or not. It may be that we don't need to invent anything and can just use this vast existing knowledge base. Some of the authors of this paper work at Unity, which means that we can expect these awesome results to appear in the video games of the future. Some code and demos are also available on their website which I've linked in the description box, make sure to check them out! Thanks for watching, and for your generous support, and I'll see you next time!"
209,Deep Learning and Cancer Research,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Let's try to assess the workflow of this piece of work in the shortest possible form. The input is images of cells, and the output of the algorithm is a decision that tells us which one of these are cancer cells. As the pipeline of the entire experiment is quite elaborate, we'll confine ourselves to discuss the deep learning-related step at the very end. Techniques prior to this one involved adding chemicals to blood samples. The problem is that these techniques were not so reliable, and that they also destroyed the cells, so it was not possible to check the samples later. As the title of the paper says, it is a label-free technique, therefore it can recognize cancer cells without any intrusive changes to the samples. The analysis happens by simply looking at them. To even have a chance at saying anything about these cells, domain experts have designed a number of features that help us making an educated decision. For instance, they like to look at refractive indices, that tell us how much light slows down when passing through cells. Light absorption and scattering properties are also recognized by the algorithm. Morphological features are also quite important as they describe the shape of the cells and they are among the most useful features for the detection procedure. So, the input is an image, then come the high level features, and the neural networks help locating the cancer cells by learning the relation of exactly what values for these high-level features lead to cancer cells. The proposed technique is significantly more accurate and consistent in the detection than previous techniques. It is of utmost importance that we are able to do something like this on a mass scale because the probability of curing cancer depends greatly on which phase we can identify it. One of the most important factors is early detection and this is exactly how deep learning can aid us. To demonstrate how important early detection is, have a look at this chart of the ovarian cancer survival rates as a function of how early the detection takes place. I think the numbers speak for themselves, but let's bluntly state the obvious: it goes from almost surely surviving to almost surely dying. By the way, they were using L2 regularization to prevent overfitting in the network. We have talked about what each of these terms mean in a previous episode, I've put a link for that in the description box. 95% success rate with the throughput of millions of cells per second. Wow, bravo. A real, Two Minute Papers style hat tip to the authors of the paper. It is really amazing to see different people from so many areas working together to defeat this terrible disease. Engineers create instruments to be able to analyze blood samples, doctors choose the most important features, and computer scientists try to find out the relation between the features and illnesses. Great strides have been made in the last few years, and I am super happy to see that even if you're not a doctor and you haven't studied medicine, you can still help in this process. That's quite amazing. A big shoutout to Kram who has been watching Two Minute Papers since the very first episodes and his presence has always been ample with insightful comments. Thanks for being around! And also, thanks for watching, and for your generous support, and I'll see you next time!"
210,Face2Face: Real-Time Facial Reenactment,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There was a previous episode on a technique where the inputs were a source video of ourselves, and a target actor. And the output was a video of this target actor with our facial gestures. With such an algorithm, one can edit pre-recorded videos in real time, and the current version only needs a consumer webcam to do that. This new version addresses two major shortcomings: One: the previous work relied on depth information, which means that we needed to know how far different parts of the image were from the camera. This newer version only relies on color information and does not need anything beyond that. Whoa! Two: Previous techniques often resorted to copying the footage from the mouth and adding synthetic proxies for teeth. Not anymore with this one! I tip my hat to the authors, who came up with a vastly improved version of their previous method so quickly, and it is probably needless to say that the ramifications of such an existing technique are far reaching, and are hopefully pointed in a positive direction. However, we should bear in mind that from now on, we may be one step closer to an era where a video of something happening won't be taken as proper evidence. I wonder how this will affect legal decision-making in the future. Thanks for watching, and for your generous support, and I'll see you next time!"
211,Training Deep Neural Networks With Dropout,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A quick recap for the Fellow Scholars out there who missed some of our earlier episodes. A neural network is a machine learning technique that was inspired by the human brain. It is not a brain simulation by any stretch of the imagination, but it was inspired by the inner workings of the human brain. We can train it on input and output pairs like images, and descriptions, whether the images depict a mug or a bus. The goal is that after training, we would give unknown images to the network and expect it to recognize whether there is a mug or a bus on them. It may happen that during training, it seems that the neural network is doing quite well, but when we provide the unknown images, it falters and almost never gets the answer right. This is the problem of overfitting, and intuitively, it is a bit like students who are not preparing for an exam by obtaining useful knowledge, but students who prepare by memorizing answers from the textbook instead. No wonder their results will be rubbish on a real exam! But no worries, because we have dropout, which is a spectacular way of creating diligent students. This is a technique where we create a network where each of the neurons have a chance to be activated or disabled. A network that is filled with unrealiable units. And I really want you to think about this. If we could have a system with perfectly reliable units, we should probably never go for one that is built from less reliable units instead. What is even more, this piece of work proposes that we should cripple our systems, and seemingly make them worse on purpose. This sounds like a travesty. Why would anyone want to try anything like this? And what is really amazing is that these unreliable units can potentially build a much more useful system that is less prone to overfitting. If we want to win competitions, we have to train many models and average them, as we have seen with the Netflix prize winning algorithm in an earlier episode. It also relates back to the committee of doctors example that is usually more useful than just asking one doctor. And the absolutely amazing thing is that this is exactly what dropout gives us. It gives the average of a very large number of possible neural networks, and we only have to train one network that we cripple here and there to obtain that. This procedure, without dropout, would normally take years and such exorbitant timeframes to compute, and would also raise all kinds of pesky problems we really don't want to deal with. To engage in modesty, let's say that if we are struggling with overfitting, we could do a lot worse than using dropout. It indeed teaches slacking students how to do their homework properly. Please keep in mind using dropout also leads to longer training times, my experience has been between 2 to 10x, but of course, it heavily depends on other external factors. So it is indeed true that dropout is slow compared to training one network, but it is blazing fast at what it actually approximates, which is training an exponential number of models. I think dropout is one of the greatest examples of the beauty and the perils of research, where sometimes the most counterintuitive ideas give us the best solutions. Thanks for watching, and for your generous support, and I'll see you next time!"
212,Narrow Band Liquid Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Our endeavors in creating amazingly detailed fluid simulations is often hamstrung by the fact that we need to simulate the motion of tens of millions of particles. Needless to say this means excruciatingly long computation times and large memory consumption. This piece of work tries to alleviate the problem by confining the usage of particles to a narrow band close to the liquid surface and thus, decimating the number of particles used in the simulation. The rest of the simulation is computed on a very coarse grid, where we compute quantities of the fluid like velocity and pressure in gridpoints, and instead of computing them everywhere, we try to guess what is happening between these gridpoints. The drawback of this is that we may miss a lot of details because of that. And the brilliant part of this new technique is that we only use a cheap, sparse grid where there is not a lot of things happening, and use the expensive particles only near the surface, where there are a lot of details we can capture well. The FLIP term that you see in the video means Fluid Implicit Particle, a popular way of simulating fluids that uses both grids and particles. In this scene, the old method uses 24 million particles, while the new technique uses only one million, and creates closely matching results. You can see a lot of excess particles in the footage with the classical simulation technique, and the foamish-looking version is the proposed new, more efficient algorithm. Creating such a technique is anything but trivial. Unless special measures are taken, the simulation may have robustness issues, which means that there are situations where it does not produce a sensible result. This is demonstrated in a few examples where with the naive version of the technique, a piece of fluid never ever comes to rest, or it may exhibit behaviors that are clearly unstable. It also takes approximately half as much time to run the simulation, and uses half as much memory, which is such a huge relief for visual  effects artists. I don't know about you Fellow Scholars, but I see a flood of amazing fluid papers coming in the near future and I'm having quite a bit of trouble containing my excitement. Exciting times are ahead indeed. Thanks for watching, and for your generous support, and I'll see you next time!"
213,No Such Thing As Artificial Intelligence,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Whether a technique can be deemed as artificial intelligence or not, is a question that I would like to see exiled from future debates and argumentations. Of course, anyone may take part in any debate of their liking, I would, however, like to point out the futility of such endeavors. Let me explain why. Ever heard a parent and a son having an argument whether the son is an adult or not? ""You are not an adult, because adults don't behave like this!"" And arguments like that. The argument is not really about whether a person is an adult, but it is about the very definition of an adult. Do we define an adult as someone who has common sense and behaves responsibly? Or is it enough to be of 18 or 21 years old to be an adult? If we decide which definition we go for, the scaffolding for the entire argument crumbles, because it is built upon a term for which the definition is not agreed upon. I feel that we have it the same with artificial intelligence in many debates. The definition of artificial intelligence, or at least one possible definition, is the following: Artificial intelligence (AI) is the intelligence exhibited by machines or software. It is a bit of a copout, so we have to go and check the definition of intelligence. There are multiple definitions, but for the sake of argument, we are going to let this one slip. One possible definition for intelligence is ""the ability to learn or understand things or to deal with new or difficult situations"". Now, this sentence is teeming with ill-defined terms, such as learn, understand things, deal with new situations, difficult situations. So, if we have a shaky definition of artificial intelligence, it is quite possibly pointless to argue whether self driving cars can be deemed artificially intelligent or not. Imagine two physicists arguing whether a material is ferromagnetic, but none of them has the slightest idea what magnetism means. If we look at it like this, it is very easy to see the futility of such arguments. If we had as poorly crafted definitions in physics as we have for intelligence, magnetism would be defined as ""stuff pulling on other stuff"". This is the first part of the argument. The second part is that artificial intelligence is imagined to be a mystical thing that only exists in the future, or it may exist in the present, but it has to be shrouded in mystery. Let me give you an example. The A* algorithm used to be called AI and was (and still is) widely taught in AI courses at many universities. A* is used in many pathfinding situations where we seek to go from A to B on a map in the presence of possible obstacles. It is widely used in robotics and computer games. Nowadays, calling a pathfinding algorithm AI is simply preposterous. It is a simple, well-understood technique that does something we are used to. Imagine someone waving their GPS device claiming that there is AI in there. But back then, when it was new, hazy, and poorly understood, we put it in a drawer with the label ""AI"" on it. As soon as people start to understand it, they pull it out from this drawer, and disgustedly claim, ""Well, this is not AI, it's just a graph algorithm. Graphs are not AI, that's just mathematics."" It is important to note that none of the techniques that we see today are mysterious in any sense, the entirety of deep learning and everything else is a series of carefully prescribed mathematical operations. I will try to briefly assess the two arguments: Arguments about AI are not about the algorithms they seem to be discussing, but about the very definition of AI, which is ill-defined at best. AI is imagined to be a mystical thing that only exists in the future, or it may exist in the present, but it has to be, in some way, shrouded in mystery. The good news is that using this knowledge, we can easily defuse such futile arguments. If someone says that deep learning is not artificial intelligence because all it does is matrix algebra, we can ask: ""okay, what is your definition of artificial intelligence?"" If this person defines AI as being a sentient learning being akin to humans, then we have immediately arrived to a conclusion that deep learning is not AI. Let us not fool ourselves by thinking that we are arguing about things when we are simply arguing about definitions. As soon as the definition is agreed upon, the conclusion emerges effortlessly. Thanks for watching, and for your generous support, and I'll see you next time!"
214,10 Even Cooler Deep Learning Applications,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is the third episode in our series of Deep Learning applications. I have mixed in some recurrent neural networks for your, and honestly, my own enjoyment. I think this series of applications shows what an amazingly versatile tool we have been blessed with with deep learning. And I know you Fellow Scholars have been quite excited for this one! Let's get started! This piece of work accomplishes geolocation for photographs. This means that we toss in a photograph, and it tells us exactly where it was made. Super resolution is a hot topic where we show a coarse, heavily pixelated image to a system, and it tries to guess what it depicts and increase the resolution of it. If we have a tool that accomplishes this, we can zoom into images way more than the number of megapixels of our camera would allow. It is really cool to see that deep learning has also made an appearance in this subfield. This handy little tool visualizes the learning process in a neural network with the classical forward and backward propagation steps. This recurrent neural network continues our sentences in a way that kind of makes sense. Well, kind of. Human in the loop techniques seek to create a bidirectional connection between humans and machine learning techniques so they can both learn from each other. I think it definitely is an interesting direction at first, DeepMind's AlphaGo also learned the basics of Go from amateurs and then took off like a hermit to learn on its own and came back with guns blazing. We usually have at least one remarkably rigorous and scientific application of deep learning in every collection episode. This time, I'd like to show you this marvelous little program that suggests emojis for your images. It does so well, that nowadays, even computer algorithms are more hip than I am. This application is akin to the previous one we have seen about super resolution here, we see beautiful, high resolution images of digits created from these tiny, extremely pixelated inputs. Netflix is an online video streaming service. The Netflix Prize was a competition where participants wrote programs to estimate how a user would enjoy a given set of movies based on this user's previous preferences. The competition was won by an ensemble algorithm, which is essentially a mixture of many existing techniques. And by many, I mean 107. It is not a surprise that some contemptuously use the term 'abomination' instead of 'ensemble' because of their egregious complexity. In this blog post, a simple neural network implementation is described that achieves quite decent results and the core of the solution fits in no more than 20 lines of code. The code has been written using Keras, which also happens to be one of my favorite deep learning libraries. Wholeheartedly recommended for everyone who likes to code, and a big shoutout to Francois, the developer of the mentioned library. Amazing feat. Convolutional Neural Networks have also started curating works of art by assigning a score to how aesthetic they are. Oh, sorry Leonardo! Earlier we talked about adversarial techniques that add a very specific type of noise to images to completely destroy the accuracy of previously existing image classification programs. The arms race has officially started, and new techniques are popping up to prevent this behavior. If you find some novel applications of deep learning, just send a link my way in the comments section. Thanks for watching, and for your generous support, and I'll see you next time!"
215,The Dunning-Kruger Effect,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This episode is about a classic, the Dunning-Kruger effect. I wonder how we could go on for almost 60 Two Minute Papers episodes without the Dunning-Kruger effect? Here is the experiment: participants were tested in different subjects, their test scores were computed, and at the same time, without the scores, they were asked to assess their perceived performance. The test subjects were humor, grammar, and logic. Things, of course, everyone excels at ... or do they? And here is the historic plot with the results. Such a simple plot, yet it tells us so much about people. From left to right, people were ordered by their score as you see with the dotted line. And the other line with the squares shows their perceived score, what they thought their scores would be. People from the bottom 10 percent, the absolute worst performers are convinced that they were well above the average. Competent people, on the other hand, seemed to underestimate their skills. Because the test was easy for them, they assumed that it was easy for everyone else. The extreme to the left is often referred to as the Dunning-Kruger effect, and the extreme to the right, maybe if you imagine the lines extending way-way further, is a common example of impostor syndrome. By the way, everyone thinks they are above average, which is neat mathematical anomaly. We would expect that people who perform poorly should know that they perform poorly, and people who're doing great should know they're doing great. One of the conclusions is that this is not the case, not the case at all. The fact that incompetent people are completely ignorant about their own inadequacy, at first, sounds like such a surprising conclusion. But if we think about it, we find there's nothing surprising about this. The more skilled we are, the more adept we are at estimating our skill level. By gaining more competence, incompetent people also obtained the skill to recognize their own shortcomings. A fish, in the world of Poker, means an inadequate player who is to be extorted by the more experienced. Someone asked how to recognize who the fish is at a poker table. The answer is a classic: if you don't know who the fish is at the table, it is you. The knowledge of the Dunning-Kruger effect is such a tempting tool to condemn other people for their inadequacy. But please, try to resist the temptation, remember, it doesn't help, that's the point of the paper! It is a much more effective tool for our own development if we attempt to use it on ourselves. Does it hurt a bit more? Oh yes, it does! The results of this paper solidify the argument that we need to be very vigilant about our own shortcomings. This knowledge endows you with a shield against ignorance. Use it wisely. Thanks for watching, and for your generous support, and I'll see you next time!"
216,From Doodles To Paintings With Deep Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is the first paper in Two Minute Papers that showcased so stunning results that people called it out to be an April Fools' day joke. It is based on a deep neural network, and the concept is very simple: you choose an artistic style, you make a terrible drawing, and it creates a beautiful painting out of it. If you would like to know more about deep neural networks, we've had a ton of fun with them in previous episodes, I've put a link to them in the description box! I expect an onslaught of magnificent results with this technique to appear very soon. It is important to note that one needs to create a semantic map for each artistic style so that the algorithm learns the correspondence between the painting and the semantics. However, these maps have to be created only once and can be used forever, so I expect quite a few of them to show up in the near future, which greatly simplifies the workflow. After that, these annotations can be changed at will, you press a button, and the rest is history. Whoa! Wicked results. Some of these neural art results are so good that we should be creating a new class of Turing tests for paintings. This means that we are presented with two images, one of them is painted by a human, and one by a computer. We need to click the ones that we think were painted by a human. Damn, curses! As always, these techniques are new and heavily experimental, and this usually means that they take quite a bit of time to compute. The presentation video you have seen was sped up considerably. If these works are worthy of further attention, and I definitely think they are, then we can expect great strides towards interactivity in followup papers very soon. I am really looking forward to it and we Fellow Scholars will have a ton of fun with these tools in the future. Thanks for watching, and for your generous support, and I'll see you next time!"
217,Overfitting and Regularization For Deep Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In machine learning, we often encounter classification problems where we have to decide whether an image depicts a dog or a cat. We'll have an intuitive, but simplified example where we imagine that the red dots represent dogs, and the green ones are the cats. We first start learning on a training set, which means that we get a bunch of images that are points on this plane, and from these points we try to paint the parts of the plane red and green. This way, we can specify which regions correspond to the concept of dogs and cats. After that, we'll get new points that we don't know anything about, and we'll ask the algorithm, for instance, a neural network to classify these unknown images, so it tells us whether it thinks that it is a dog or a cat. This is what we call a test set. We have had a lots of fun with neural networks and deep learning in previous Two Minute Papers episodes, I've put some links in the description box, check them out! In this example, it is reasonably easy to tell that the reds roughly correspond to the left, and the greens to the right. However, if we just jumped on the deep learning hype train, and don't know much about a neural networks, we may get extremely poor results like this. What we see here is the problem of overfitting. Overfitting means that our beloved neural network does not learn the concept of dogs or cats, it just tries to adapt as much as possible to the training set. As an intuition, think of poorly made real-life exams. We have a textbook where we can practice with exercises, so this textbook is our training set. Our test set is the exam. The goal is to learn from the textbook and obtain knowledge that proves to be useful at the exam. Overfitting means that we simply memorize parts of the textbook instead of obtaining real knowledge. If you're on page 5, and you see a bus, then the right answer is B. Memorizing patterns like this, is not real learning. The worst case is if the exam questions are also from the textbook, because you can get a great grade just by overfitting. So, this kind of overfitting has been a big looming problem in many education systems. Now the question is, which kind of neural network do we want? Something that works like a lazy student, or one that can learn many complicated concepts. If we're aiming for the latter, we have to combat overfitting, which is the bane of so many machine learning techniques. Now, there's several ways of doing that, but today we're going to talk about one possible solution by the name L1 and L2 regularization. The intuition of our problem is that the deeper and bigger neural networks we train, the more potent they are, but at the same time, they get more prone to overfitting. The smarter the student is, the more patterns he can memorize. One solution is to hurl a smaller neural network at the problem. If this smaller version is powerful enough to take on the problem, we're good. A student who cannot afford to memorize all the examples is forced to learn the actual underlying concepts. However, it is very possible that this smaller neural network is not powerful enough to solve the problem. So we need to use a bigger one. But, bigger network, more overfitting. Damn. So what do we do? Here is where L1 and L2 regularization comes to save the day. It is a tool to favor simpler models instead of complicated ones. The idea is that the simpler the model is, the better it transfers the textbook knowledge to the exam, and that's exactly what we're looking for. Here you see images of the same network with different regularization strengths. The first one barely helps anything and as you can see, overfitting is still rampant. With a stronger L2 regularization, you see that the model is simplified substantially, and is likely to perform better on the exam. However, if we add more regularization, it might be that we simplified the model too much, and it is almost the same as a smaller neural network that is not powerful enough to grasp the underlying concepts of the exam. Keep your neural network as simple as possible, but not simpler. One has to find the right balance which is an art by itself, and it shows that training deep neural networks takes a bit of expertise. It is more than just a plug and play tool that solves every problem by magic. If you want to play with the neural networks you've seen in this video, just click on the link in the description box. I hope you'll have at least as much fun with it as I had! Thanks for watching, and for your generous support, and I'll see you next time!"
218,"Decision Trees and Boosting, XGBoost","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A decision tree is a great tool to help making good decisions from a huge bunch of data. The classical example is when we have a bunch of information about people and would like to find out whether they like computer games or not. Note that this is a toy example for educational purposes. We can build the following tree: if the person's age in question is over 15, the person is less likely to like computer games. If the subject is under 15 and is a male, he is quite likely to like video games, if she's female, then less likely. Note that the output of the tree can be a decision, like yes or no, but in our case, we will assign positive and negative scores instead. You'll see in a minute why that's beneficial. But this tree wa s just one possible way of approaching the problem, and admittedly, not a spectacular one a different decision tree could be simply asking whether this person uses a computer daily or not. Individually, these trees are quite shallow and we call them weak learners. This term means that individually, they are quite inaccurate, but slightly better than random guessing. And now comes the cool part. The concept of tree boosting means that we take many weak learners and combine them into a strong learner. Using the mentioned scoring system instead of decisions also makes this process easy and straightforward to implement. Boosting is similar to what we do with illnesses. If a doctor says that I have a rare condition, I will make sure and ask at least a few more doctors to make a more educated decision about my health. The cool thing is that the individual trees don't have to be great, if they give you decisions that are just a bit better than random guessing, using a lot of them will produce strong learning results. If we go back to the analogy with doctors, then if the individual doctors know just enough not to kill the patient, a well-chosen committee will be able to put together an accurate diagnosis for the patient. An even cooler, adaptive version of this technique brings in new doctors to the committee according to the deficiencies of the existing members. One other huge advantage of boosted trees over neural networks is that we actually see why and how the computer arrives to a decision. This is a remarkably simple method that leads to results of very respectable accuracy. A well-known software library called XGBoost has been responsible for winning a staggering amount of machine learning competitions in Kaggle. I'd like to take a second to thank you Fellow Scholars for your amazing support on Patreon and making Two Minute Papers possible. Creating these episodes is a lot of hard work and your support has been invaluable so far, thank you so much! We used to have three categories for supporters. Undergrad students get access to a Patron-only activity feed and get to know well in advance the topics of the new episodes. PhD students who are addicted to Two Minute Papers get a chance to see every episode up to 24 hours in advance. Talking about committees in this episode, Full Professors form a Committee to decide the order of the next few episodes. And now, we introduce a new category, the Nobel Laureate. Supporters in this category can literally become a part of Two Minute Papers and will be listed in the video description box in the upcoming episodes. Plus all of the above. Thanks for watching, and for your generous support, and I'll see you next time!"
219,3D Depth From a Single Photograph,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work tries to estimate depth information from an input photograph. This means that it looks at the photo and tries to tell how far away parts of the image are from the camera. An example output looks like this: on the left, there is an input photograph, and on the right, you see a heatmap with true distance information. This is what we're trying to approximate. This means that we collect a lot of indoor and outdoor images with their true depth information, and we try to learn the correspondence, how they relate to each other. Sidewalks, forests, buildings, you name it. These image and depth pairs can be captured by mounting 3D scanners on this awesome custom-built vehicle. And, gentlemen, that is one heck of a way of spending research funds. The final goal is that we provide a photograph for which the depth information is completely unknown and we ask the algorithm to provide it for us. Here you can see some results: the first image is the input photograph, the second shows the true depth information. The third image is the depth information that was created by this technique. And here is a bunch of results for images downloaded from the internet. It probably does at least as good as a human would. Spectacular. This sounds like a sensorial problem for humans, and a perilous journey for computers to say the least. What is quite remarkable is that these relations can be learned by a computer algorithm. What can we use this for? Well, a number of different things, one of which is to create multiple views of this 2D photograph using the guessed depth information. It can also be super helpful in building robots that can wander about reliably with inexpensive consumer cameras mounted on them. Thanks for watching, and for your generous support, and I'll see you next time!"
220,How DeepMind's AlphaGo Defeated Lee Sedol,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A few months ago, AlphaGo played and defeated Fan Hui, a 2 dan master and European champion player in the game of Go. However, the next opponent, Lee Sedol is a 9 dan master and world champion player. Just to give an intuition of the difference, Lee Sedol is expected to beat Fan Hui 97 times out of 100 games. Google DeepMind had 6 months of preparation for this bout Five matches were played over five days. In my timezone, the matches started around 4 am, and the results would usually pop up exactly a few minutes after I woke up. It was amazing. I could barely fall asleep I was so excited for the results, and when I woke up, I kissed my daughter and immediately ran to my computer to see what was going on. Most people were convinced that Lee Sedol was going to beat the machine 5-0, and I was stunned to see AlphaGo triumphed over Lee Sedol in the first match, and then the second, and then the third. Huge respect for both Google DeepMind for putting together such a spectacular algorithm and for Lee Sedol who played extremely well under enormous pressure. He is indeed a true champion. The game of Go has a stupendously large search space that makes it completely impossible to check every move and choose the best. What is also not often talked about is that processing through many moves is one thing, but judging which move is advantageous and which is not, it just as difficult as the search itself. The definition of the best move is not clear-cut by any stretch of the imagination. We also have to look into the future and simulate the moves of the opponent. I think it is easy to see that the difficulty of this problem is completely out of this world. A neural network is a crude approximation of the human brain, just like a stick figure is a crude approximation of a human being. In this work, neural networks are used to reduce the size of the search space, and value networks are used to predict the expected outcome of a move. This value network basically tries to determine who will win if a sequence of moves is made. To defeat AlphaGo, or any computer opponent, playing non-traditional moves that it surely hasn't practiced sounds like a great idea. However, there is no database involed per se, this technique is simulating the moves until the very end of the game, so non-traditional ""weird"" moves won't throw it off. It is also very important to know that the structure of AlphaGo is not like Deep Blue for chess. Deep Blue was specifically designed to maximize metrics that are likely to lead to victory, such as pawn advantage, king safety, tempo and more. AlphaGo doesn't do any of that. It is a general technique that can learn to solve a large number of different problems. I cannot overstate the significance of this. Almost the entirety of computer science research revolves around creating algorithms that are specifically tailored to one task. Different task, different research projects, different algorithm. Imagine how empowering it would be to have a general algorithm that can solve a large amount of problems. It's incredible! Just as people who don't speak a word of Chinese can write an artificial intelligence program to recognize handwritten Chinese text, someone who hasn't played more than a few games can write a chess or Go program that is beyond the skill of most professional players. This is a wonderful testament of the power of mathematics and science. It was quite surprising to see that AlphaGo played seemingly suboptimal moves when it was ahead to reduce variance and maximize its chance of victory. Take a look at at DeepMind's other technique by the name Deep Q-Learning that plays space invaders on a superhuman level. This shot, at first, looks like a blunder, but if you wait it out, you'll see how brilliant it really is. A move that seems like a blunder at a time may be the optimal move in the grand scheme of things. It not a blunder. It is a move from someone whose brilliance is way beyond the capabilities of even the best human players. There is an excellent analysis of this phenomenon on the Go reddit, I've put a link in the description box, check it out. I'd like to emphasize that the technique learns at first, by looking at a large number of games by amateurs. But the question is, how can it get beyond the level of amateurs? After looking at these games, it will learn the basics and will play millions of games against itself and learn from them. And, to be emphasized: nothing in this algorithm is specific to Go. Nothing. It can be used to solve a number of different problems without significant changes. It would be immensely difficult to overstate the significance of that. Shoutout to Brady Daniels who has an excellent Go educational channel. He has very fluid, enjoyable and understandable explanations, highly recommended, check it out. There is a link to one of his videos in the description box. It is a possibility that the first Go grandmaster to reach 10 dans may not be a human, but a computer. My mind is officially blown. Insanity. One more cobblestone has been laid on the path to artificial general intelligence. This achievement I find to be of equivalent magnitude to landing on the Moon. And this is just the beginning. I can't wait to see this technique being used for research in medicine. Huge respect for Demis Hassabis and Lee Sedol, who were both respectful and humble both in victory, and in defeat. They are true champions of their craft. Thanks so much for DeepMind for creating this rivetingly awesome event. My daughter, Jázmin was born one day before this glorious day. What an exciting time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
221,10 More Cool Deep Learning Applications,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. To all of you Fellow Scholars out there who are yearning for some more deep learning action like I do, here goes the second package. Buckle up, amazing applications await you. As always, links to every one of these works are available in the description box. This convolutional neural network can learn how to colorize by looking at the same images both in colo r and black and white. The first image is the black and white input, the second is how the algorithm colorized it, and the third is how the image originally looked like in color. Insanity. Recurrent neural networks are able to learn and produce sequences of data, and they are getting better and better at music generation. Nowadays, people are experimenting with human aided music generation with pretty amaz ing results. Sony has also been working on such a solution with spectacular results. One can also run a network on a large database of leaked human passwords and try to crack new accounts building on that knowledge. Deep neural networks take a substantial amount of time to train, and the final contents of each of the neurons have to be stored, which takes a lot of space. New techniques are being explored to compress the information content of these networks. There is an other application where endangered whale species are recognized by convolutional neural networks. Some of them have a worldwide population of less than 500, and this is where machine learning steps in to try to save them. Awesome! YouTube has a huge database full of information on what kind of video thumbnails are the ones that people end up clicking on. They use deep learning to automatically find and suggest the most appealing images for your videos. There is also this crazy application where a network was trained on a huge dataset with images of celebrities. A low quality image is given, where the algorithm creates a higher resolution version building on this knowledge. The leftmost images are the true high resolution images, the second one is the grainy, low resolution input, and the third is the neural network's attempt to reconstruct the original. This application takes your handwriting of a number, and visualizes how a convolutional neural network understands and classifies it. Apparently, George RR Martin is late with writing the next book of Game of Thrones, but luckily, we have recurrent neural networks that can generate text in his style. An infinite amount, so beware George, winter is coming. I mean the machines are coming. It is truly amazing what these techniques are capable of. And as machine learning is a remarkably fast moving field, new applications pop up pretty much every day. I am quite enthused to do at one more batch of these! Of course, provided that you liked this one. Let me know. Thanks for watching and for your generous support, and I'll see you next time!"
222,5000 Fellow Scholars Special! | Two Minute Papers,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We just hit 5000 subscribers. More than five thousand Fellow Scholars who wish to join us on our journey of science. It really shows that everyone loves science, they just don't know about it yet! About 6 months ago, we were celebrating 250 subscribers. The growth of the channel has been nothing short of incredible, and this is all attributed to you. Without you Fellow Scholars, this series would be nothing but a crazy person sitting at home, talking into a microphone and having way too much fun! Thank you so much for hanging in there! I love doing this and am delighted to have each of you in our growing club of Fellow Scholars! We have also hit half a million views. Holy cow! If we would substitute one human being for every view, we would be close to 6% of the population of Austria, or almost 30% of the population of the beautiful Vienna. This is equivalent to about 60% of the population of San Francisco. This is way beyond the amount of people I could ever reach by teaching at the university. It is a true privilege to teach so many people from all around the world. We have so many plans to improve the series in different directions. We have recently switched to 60 frames per second for beautiful smooth, and silky animations, and closed captions are also now uploaded for most episodes to improve the clarity of the presentations. We are also looking at adding more Patreon perks in the future. There are also tons of amazing research works up the sleeve that you will see very soon in the upcoming videos. Graphics guys, I got your back, machine learners, this way please. Other spicy topics will also be showcased to keep it fresh and exciting. My wife Felícia is also preparing some incredible artwork for you. Thanks for watching and for your generous support, and I'll see you next time!"
223,How To Get Started With Machine Learning?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I get a lot of messages from you Fellow Scholars that you would like to get started in machine learning and are looking for materials. Words fail to describe how great the feeling is that the series inspires many of you to start your career in research. At this point, we're not only explaining the work of research scientists but creating new research scientists. Machine learning is an amazing field of research that provides us with incredible tools that help us solve problems that were previously impossible to solve. Neural networks can paint in the style of famous artists or recognize images and are capable of so many other things it simply blows my mind. However, bear in mind that machine learning is not an easy field. This field fuses together the beauty, rigor, and preciseness of mathematics with the useful applications of engineering. It is also a fast moving field, on almost any given day, 10 new scientific papers pop up in the repositories. For everything that I mention in this video there is a link in the description box and more, so make sure to dive in and check them out. If you have other materials that helped you understand some of the more difficult concepts, please let me know in the comments section and I'll include them in the text below. First, some non-scientific texts to get you in the mood, I recommend reading ""The Road to Superintelligence"" on a fantastic blog by the name Wait But Why. This is a frighteningly long article for many, but I guarantee that you won't be able to stop reading it. Beware. Nick Bostrom's Superintelligence is also a fantastic read, after which you'll probably be convinced that it doesn't make sense to work on anything else but machine learning. There is a previous Two Minute Papers episode on artificial superintelligence if you're looking for a teaser for this book. Now let's get a bit more technical with some of the better video series and courses out there! Welch labs is an amazing YouTube channel with a very intuitive introduction to the concept of neural networks. Andrew Ng is a Chief Scientist at Baidu Research in deep learning. His wonderful course is widely regarded as the pinnacle of all machine learning courses and is therefore highly recommended. Nando de Freitas is a professor at the university of Oxford, and has also worked with DeepMind. His course that he held at the University of British Columbia covers many of the more advanced concepts in machine learning. Regarding books, I recommend reading my favorite Holy Tome of machine learning, that goes by the name of ""Pattern Recognition and Machine Learning"" by Christoper Bishop. A sample chapter is available from the book if you wish to take a look. It has beautiful typesetting, lots of intuition and crystal clear presentation. Definitely worth every penny of the price. I'd like to note that I am not paid for any of the book endorsements in the series. When I recommend a book, I genuinely think that it provides great value to you Fellow Scholars. About software libraries. Usually, in most fields, the main problem is that the implementation of many state of the art techniques are severely lacking. Well luckily, in the machine learning community, we have them in abundance. I've linked a great talk on what libraries are available and the strengths and weaknesses for each of them. At this point, you'll probably have an idea of which direction you're most excited about. Start searching for keywords, make sure to read the living hell out of the machine learning reddit to stay up to date, and, the best part is yet to come: starting to explore on your own. Thanks for watching and for your generous support, and I'll see you next time!"
224,Interactive Photo Recoloring,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Image and color editing is an actively researched topic with really cool applications that you will see in a second. Most of the existing solutions are either easy to use but lack in expressiveness, or they are expressive, but too complex for novices to use. Computation time is also an issue as some of the operations in Photoshop can take more than a minute to carry out. For color editing, the workflow is very simple, the program extracts the dominant colors of an image, which we can interactively edit ourselves. An example use case would be recoloring the girl's blue sweater to turquoise, or changing the overall tone of the image to orange. Existing tools that can do this are usually either too slow or only accessible to adept users. It is also important to note that it is quite easy to take these great results for granted. Using a naive color transfer technique would destroy a sizeable part of the dynamic range of the image, and hence, legitimate features which are all preserved if we use this algorithm instead. One can also use masks to selectively edit different parts of the image. The technique executes really quickly, opening up the possibility of not real time, but interactive recoloring of animated sequences. Or, you can also leverage the efficiency of the method to edit not one, but a collection of images in one go. The paper contains a rigorous evaluation against existing techniques. For instance, they show that this method executed three to twenty times faster than the one implemented in Adobe Photoshop. Thanks for watching and for your generous support, and I'll see you next time!"
225,Deep Learning Program Learns to Paint,"Dear fellow scholars This is Two Minute Papers with Károly Zsolnai-Fehér. In a previous episode, we discussed how a machine learning technique called a convolutional neural network could paint in the style of famous artists. The key thought is that we are not interested in individual details, We want to teach the neural network the high-level concept of artistic style. A convolutional neural network is a fantastic tool for this, since it does not only recognize images well, But the deeper we go in the layers, the more high-level concepts neurons will encode, therefore the better idea the algorithm will have of the artistic style. In an earlier exemple, we have shown that the neurons in the first hidden layer will create edges as a combination of the input pixels of the image. The next layer is a combination of edges that create object parts. One layer deeper, a combination of object parts create object models, and this is what makes convolutional neural network so useful in recognizing them. In this follow up paper, the authors use a very deep 19-layer convolutional network that they mix together with Markov random fields, a popular technique in image and texture synthesis The resulting algorithm retains the important structures of the input image significantly better than the previous work. Which is also awesome, by the way. Failure cases are also reported in the paper, which was a joy to read. Make sure to take a look if you are interested. We also have a ton of video resources in the description box that you can voraciously consume for more information. There is already a really cool website  where you either wait quite a bit, and get results for free, or you pay someone to compute it and get results almost immediately If any of you are in the mood of doing some neural art of something Two Minute Papers related, make sure to show it to me, I would love to see that. As a criticism, I have heard people saying that the technique takes forever on an HD image, which is absolutely true. But please bear in mind that the most exciting research is not speeding up something that runs slowly The most exciting thing about research is making something possible that was previously impossible If the work is worthy of attention, it does'nt matter if it's slow. Tree followup papers later, it will be done in a matter of seconds. In summary, the results are nothing short of amazing. I was full of ecstatic glee when I've first seen them. This is insanity, and it's only been a few months since the initial algorithm was published. I always say this, but we are living amazing times indeed. Thanks for watching and for your generous support, and I'll see you next time!"
226,Artistic Manipulation of Caustics,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. If it's not your favorite visual phenomenon in nature yet, which is almost impossible, then you absolutely have to watch this episode. If it is, all the better because you're gonna love what's coming up now! Imagine that we have a photorealistic rendering program that simulates the path of light rays in a scene that we put together, and creates beautiful imagery of our caustics. However, since we, humans are pretty bad at estimating how exactly caustics should look like, one can manipulate them to be more in line with their artistic vision. Previously, we had an episode on a technique which made it possible to pull the caustic patterns to be more visible, but this paper offers a much more sophisticated toolset to torment these caustic patterns to our liking. We can specify a target pattern that we would like to see, and obtain a blend between what would normally happen in physics and what we imagined to appear there. It also supports animated sequences. Artists who use these tools are just as skilled in their trade as the scientists who created this algorithm, so I can only imagine the miracles they will create with such a technique. If you are interested in diving into photorealistic rendering, material modeling and all that cool stuff, there are completely free and open source tools out there like Blender that you can use. If you would like to get started, check out CynicatPro's YouTube channel that has tons of really great material. Here is a quick teaser of his channel. Thanks! There is a link to his channel in the description box, make sure to check it out and subscribe if you like what you see there. I just realized that the year has barely started and it is already lavishing in beautiful papers. Thanks for watching and for your generous support, and I'll see you next time!"
227,Should You Take the Stairs at Work? (For Weight Loss),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I am sure that every one of us have overheard conversations at a workplace where people talked about taking the stairs instead of the elevator, and, as a result, getting leaner. There was also a running joke on the internet about Arnold Classic, a famous bodybuilding competition/festival, where I think it's fair to say that people tended to favor the escalator instead of the stairs. So, this is it, we're going to settle this here and now. Do we get lean from taking the stairs every day? Scientists set up a controlled experiment where over a hundred subjects climbed 11 stories of staircases, ascending a total of 27 meters vertically. Their oxygen consumption and heart rate was measured, and most importantly for us, the amount of caloric cost of this undertaking. They have found that all this self flagellation with ascending 11 stories of staircases burns a whopping 19.7 kilo calories. Each step is worth approximately one tenth of a kilo calorie if we're ascending. Descending is worth approximately half of that. Apparently, these bodybuilders know what they are doing. The authors diplomatically noted: Stair-climbing exercise using a local public-access staircase met the minimum requirements for cardiorespiratory benefits and can therefore be considered a viable exercise for most people and suitable for promotion of physical activity. Which sounds like the scientific equivalent of ""well, better than nothing"". So does this mean that you shouldn't take the stairs at work? If you're looking to get lean because of that, no, not a chance. However, if you are looking for a refreshing cardiovascular exercise in the morning that refreshes your body, and makes you happier, start climbing. I do it all the time and I love it! So, we are exploring so far uncharted territories and this makes the first episode on nutrition (should have said exercise, sorry!) in the series, if you would like to hear more of this, let me know in the comments section. I'd also be happy to see your paper recommendations in nutrition as well. Thanks for watching and for your generous support, and I'll see you next time!"
228,What is Impostor Syndrome?,"Who, or what is an impostor? Simple definition: a person who deceives others by pretending to be someone else Full definition: one that assumes false identity or title for the purpose of deception Wow, the full definition is a whopping 7 characters more. I don't even know what to do with this amount of time I saved reading the simple definition first. Let's look in the mind of someone who suffers from impostor syndrome and see the fickle understanding they have of their own achievements. 98 points out of 100, this surely means that they mixed up my submission with someone else's, who was way smarter than I am. I went in for the next round of interviews, messed up big time, and I got hired with an incredible salary. This can, of course, only be a misunderstanding. I got elected for this prestigious award. I don't know how this could possibly have happened. Maybe someone who really likes me tried to pressure the prize committee to vote for me. I cannot possibly imagine any other way of this happening. I've been 5 years at the company now and still, no one found out that I am a fraud. That's a disaster Nothing can be convince me that I am not an impostor who fooled everyone else for being a bright person. However funny as it may sound, this is a very real problem. Researchers, academics and high achieving women are especially vulnerable to this condition. But it is indeed not limited to these professions. For instance, Hayden Christensen, the actor playing Anakin Skywalker in the beloved Star Wars series appears to suffer from very similar symptoms. He said: ""I felt like I had this great thing in Star Wars that provided all these opportunities and gave me a career, but it all kind of felt a little too handed to me,"" he explained. ""I didn't want to go through life feeling like I was just riding a wave."" So, as a response, he hasn't really done any acting for 4 years. He said: ‘If this time away is gonna be damaging to my career, then so be it. If I can come back afterward and claw my way back in, then maybe I’ll feel like I earned it.'” The treatment of impostor syndrome includes group sittings where the patients discuss their lives and come to a sudden realization that they are not alone and this this not an individual case but a common pattern among high achieving people. As they are also very keen on dismissing praise and kind words, they are instructed to be more vigilant about doing that, and try to take in all the nourishment they get from their colleagues. These are the more common ways to treat this serious condition that poisons so many people's minds."
229,Biophysical Skin Aging Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The faithful simulation of human skin is incredibly important both in computer games, the movie industry, and also in medical sciences. The appearance of our face is strongly determined by the underlying structure of our skin. Human skin changes significantly with age. It becomes thinner and more dry, while the concentration of chromophores, the main skin pigments diminishes and becomes more irregular. Those pigment concentrations are determined by our age, gender, skin type and even external factors like, for example exposition to UV radiation or our smoking habits. As we age, the outermost layer of our skin, the epidermis thins, the melanine, haemoglobin and water concentration levels drop over time. As you could image having a plausible simulation considering all the involved actors is fraught with difficulties. Scientists at the University of Zaragoza came up with a really cool, fully-fledged biophysically-based model that opens up the possibility of simply specifying intuitive parameters like age, gender, skin type, and get, after some processing, a much lighter skin representation ready to generate photorealistic rendered results in real time. Luckily, one can record diffusion profiles, also called scattering profiles that tell us the color of light that is reflected by our skin. In this image, above, you can see a rendered image and the diffusion profiles of a 30 and an 80 year old person. The idea is the following: you specify intuitive inputs like age and skin type, then run a detailed simulation once, that creates these diffusion profiles that you can use forever in your rendering program. And all this is done in a way that is biophysically impeccable. I was sure that there was some potential in this topic, but when I first saw these results, they completely crushed my expectations. Excellent piece of work! Thanks for watching and for your generous support, and I'll see you next time!"
230,Extrapolations and Crowdfunded Research (Experiment),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What is extrapolation? We hear the term a lot, so let's try to learn what's behind it. Despite the complicated definitions that are out there, extrapolation basically means continuing lines. A good example is when we have data for something from the last few days or years, and would like to have a forecast for the future. We'll jump right into an example just give me a second to build this up. It's going to make sense in the end, I promise! So, in many fields of science, it is really difficult to get research projects funded. Experiment is a cool new startup that is trying to accelerate progress in research by crowdsourcing it. It doesn't get simpler than this system: scientists pitch their research project plan, and kindhearted people pledge a one time donation to help their cause. It is like kickstarter for research. Some of the newer funded projects include growing food in space, developing an open protocol for insulin production, and, of course, a mandatory cat project that includes sequencing the genome of rare mutations. Crowdfunding research is such a terrific idea, and I tell you, these guys are really doing it right. The startup has been founded in 2012, and people pledged 52 thousand dollars that year. The next year, ten times that, and they have kept a steady and quite impressive growth ever since. In 2015, they raised almost $4 million dollars for open research. It's AMAZING! Ok, so a nice extrapolation problem how much can they expect to raise next year, in 2016? Before we start, we have to be extremely sure to extrapolate only if we're reasonably sure about the nature of the trends and that they won't change significantly in the near future. With that out of the way, let's do a linear extrapolation. Linear means that growth follows a straight line. So, we put these dots on a paper, and try to connect them with a line. Now, we take the mathematical description of this line, and substitute something in it. Since we have 4 years of data, 4 dots, we would be interested in the location of the fifth point, which is the amount of raised money in 2016. So let's do it! Ten to the sixth is one million, so this says that we can expect 4.2 million dollars. Great! But let's be a more optimistic, and do a superlinear extrapolation. Superlinear means that the the rate of growth is not a straight line, but something that is accelerating in time. If this assumption is true, we can expect them to raise way more, 7.4 million dollars. A bit more pessimistic solution would be a sublinear extrapolation. Sublinear means that growth slows down in time. This kind of growth is described well with, for instance, the logarithm function. This effect is also often called the effect of diminishing returns. A good example of this is the skill level of Google DeepMind's artificial intelligence program that plays Go. As we add more and more computational resources, the algorithm gets better and better at the game, but after a point, there is only so much one can learn, therefore progress slows down, and eventually gets close to stopping. There are so many examples of this effect in our lives if you have some great examples of logarithmic growth, let me know in the comments section, I'll include the best ones in the video description box. According to this logarithm, we can expect the company to raise less than the previous estimations, 3.1 million dollars next year. Sorry guys! A common pitfall in popular media is that the mathematically untrained minds almost always assume a linear growth due to its simplicity. This can lead to hilariously wrong results. If you would extrapolate the size of the belly of a pregnant woman after 9 months. Your conclusion would be ""run, because she's going to explode!"" whereas we know that a baby is going to be born and she is going to get back in shape. If I had zero wives yesteday and it's my wedding day today, I will sure as hell have a couple dozen wives by next month! Many things are inherently non-linear, and doing a simple linear extrapolation often doesn't do justice to the problem at hand. Bear in mind that there are many different ways to connect a bunch of dots! Let's try to find out why we had wildly varying results. This is due to the fact that we only had 4 samples, that means 4 dots. If I plot these possible functions that we've been talking about, we get the following. It seems that the further we go, the more they diverge. However, in this case, if we have data only between zero and one, for instance, there is very little difference between a wild, exponential function and a very conservative, square root-based growth (you can also imagine your logarithm here). The more dots we have, the more we can distinguish the nature of our growth. And, an educated mind has to take into consideration that many phenomena are inherently non-linear. If you catch someone doing a linear exploration, always ask: ""Are you sure that the process you're modeling is indeed linear? And do you have enough data to prove that?"" That's all for today. Thanks for watching and for your generous support, and I'll see you next time!"
231,Breaking Deep Learning Systems With Adversarial Examples,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Artificial neural networks are computer programs that try to approximate what the human brain does to solve problems like recognizing objects in images. In this piece of work, the authors analyze the properties of these neural networks and try to unveil what exactly makes them think that a paper towel is a paper towel, and, building on this knowledge, try to fool these programs. Let's have a look at this example. One can grab this input image, and this noise pattern, and add these two images together similarly as one would add two numbers together. The operation yields the image you see here. I think it's fair to say that the difference is barely perceptible for the human eye. Not so much for neural networks, because the input image we started with is classified correctly as a bus, and the image that you see on the right is classified as an ostrich. In simple terms, bus + noise equals an ostrich. The two images look almost exactly the same, but the neural networks see them quite differently. We call these examples adversarial examples because they are designed to fool these image recognition programs. In machine learning research, there are common datasets to test different classification techniques on, one of best known example is the MNIST handwriting dataset. It is a basically a bunch of images depicting handwritten numbers that machine learning algorithms have to recognize. Long ago, this used to be a difficult problem, but nowadays, any half-decent algorithm can guess the numbers correctly more than 99% of the time after learning for just a few seconds. Now we'll see that these adversarial examples are not created by chance: if we add a lot of random noise to these images, they get quite difficult to recognize. Let's engage in modesty and say that I, myself, as a human can recognize approximately half of them, but only if I look closely and maybe even squint. A neural network can guess this correctly approximately 50% of the time as well, which is a quite respectable result. Therefore, adding random noise is not really fooling the neural networks. However, if you look at these adversarial examples in the even columns, you see how carefully they are crafted as they look very similar to the original images, but the classification accuracy of the neural network on these examples is 0%. You heard it correctly. It gets it wrong basically all the time. The take home message is that carefully crafted adversarial examples can be used to fool deep neural network reliably. You can watch them flounder on many hilarious examples to your enjoyment. ""My dear sir, the Queen wears a shower cap you say? I beg your pardon?"" If you would like to support Two Minute Papers, we are available on Patreon and offer cool perks for our Fellow Scholars for instance, you can watch each episode around 24 hours in advance, or even decide the topic of the next episodes. How cool is that?! If you're interested, just click on the box below on the screen. Thanks for watching and for your generous support, and I'll see you next time!"
232,How DeepMind Conquered Go With Deep Learning (AlphaGo),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In 1997, the news took the world by storm Garry Kasparov, world champion and grandmaster chess player was defeated by an artificial intelligence program by the name Deep Blue. In 2011, IBM Watson won first place in the famous American Quiz Show, Jeopardy. In 2014, Google DeepMind created an algorithm that that mastered a number of Atari games by working on raw pixel input. This algorithm learned in a similar way as a human would. This time around, Google DeepMind embarked on a journey to write an algorithm that plays Go. Go is an ancient chinese board game where the opposing players try to capture each other's stones on the board. Behind the veil of this deceptively simple ruleset, lies an enormous layer of depth and complexity. As scientists like to say, the search space of this problem is significantly larger than that of chess. So large, that one often has to rely on human intuition to find a suitable next move, therefore it is not surprising that playing Go on a high level is, or maybe was widely believed to be intractable for machines. This chart shows the skill level of previous artificial intelligence programs. The green bar is shows the skill level of a professional player used as a reference. The red bars mean that these older techniques required a significant starting advantage to be able to contend with human opponents. As you can see, DeepMind's new program's skill level is well beyond most professional players. An elite pro  player and European champion Fan Hui was challenged to play AlphaGo, Google DeepMind's newest invention and got defeated in all five matches they played together. During these games, each turn it took approximately 2 seconds for the algorithm to come up with the next move. An interesting detail is that these strange black bars show confidence intervals, which means that the smaller they are, the more confident one can be in the validity of the measurements. As you can see, these confidence intervals are much shorter for the artificial intelligence programs than the human player, likely because one can fire up a machine and let it play a million games, and get a great estimation of its skill level, while the human player can only play a very limited number of matches. There is still a lot left to be excited for, in March, the algorithm will play a world champion. The rate of improvement in artificial intelligence research is accelerating at a staggering pace. The only question that remains is not if something is possible, but when it will become possible. I wake up every day excited to read the newest breakthroughs in the field, and of course, trying to add some leaves to the tree of knowledge with my own projects. I feel privileged to be alive in such an amazing time. As always, there's lots of references in the description box, make sure to check them out. Thanks for watching and for your generous support, and I'll see you next time!"
233,What Do Virtual Objects Sound Like?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In many episodes about computer graphics, we explored works on how to simulate the motion and the collision of different bodies. However, sounds are just as important as visuals, and there are really cool techniques out there that take the geometry and material description of such objects and they simulate how smashing them together would sound like. This one is a more sophisticated method which is not only faster than previous works, but can simulate a greater variety of materials, and we can also edit the solutions without needing to recompute the expensive equations that yield the sound as a result. The faster part comes from a set of optimizations, most importantly something that is called mesh simplification. This means that the simulations are done not on the original, but vastly simplified shapes. The result of this simplified simulation is close to indistinguishable from the real deal, but is considerably cheaper to compute. What is really cool is that the technique also offers editing capabilities. You compute a simulation only once, and then, edit and explore as much as you desire. The stiffness and damping parameters can be edited without any additional work. Quite a few different materials can be characterized with this. The model can also approximate a quite sophisticated phenomenon where the frequency of a sound is changing in time. One can, for instance, specify stiffness values that vary in time to produce these cool frequency shifting effects. It is also possible to exaggerate or dampen different frequencies of these sound effects, and the results are given to you immediately. This is meeting all my standards. Amazing piece of work. Thanks for watching and for your generous support, and I'll see you next time!"
234,Simulating Viscosity and Melting Fluids,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we have studied fluid simulations extensively. But we haven't talked about one important quantity that describes a fluid, and this quantity is none other than viscosity. Viscosity means the resistance of a fluid against deformation. The large viscosity of honey makes it highly resistant to deformation, and this is responsible for its famous and beautiful coiling effect. Water, however, does not have a lot of objections against deformations, making it so easy to pour it into a glass. With this piece of work, it is possible to efficiently simulate the motion of fluids, and it supports the simulation of a large range of viscosities. Viscosities can also change in time. For instance, physicists know that raising the temperature will make the viscosity of fluids decrease, which leads to melting, therefore decreasing the viscosity in time will lead to a simulation result that looks exactly like melting. The technique also supports two-way coupling where the objects have effects on the fluid and vice versa. One can also put multiple fluids with different densities and viscosities into the same domain and see how they duke it out. This is exactly what people need in the industry: robust techniques that work for small and large scale simulations with multiple objects, and material settings that can possibly change in time. Thanks for watching and for your generous support, and I'll see you next time!"
235,Interactive Editing of Subsurface Scattering,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Subsurface scattering means that a portion of light that hits a translucent material does not bounce back from the surface but penetrates it and scatters many-many times inside the material. Now, if you have a keen eye, you recognize that there are a lot of materials in real life that have subsurface scattering. Many don't know, but our skin is a great example of that, and so is marble, milk, wax, plant leaves, apple and many others. If you would like to hear a bit more about subsurface scattering, check the second part of the video that you see recommended in the corner of this window, or just click it in the description box below. Subsurface scattering looks unbelievably beautiful, but at the same time, it is very expensive because we have to simulate up to thousands and thousands of scattering events for every ray of light. It really takes forever. And if you'd like to tweak your material settings just a bit because the result is not a 100% up to your taste, you have to recreate, or what graphics people like to say, re-render these images. It's not really a convenient workflow. This piece of work offers a great solution where you have to wait a bit longer than you would wait for one image, but only once, because it runs a generalized light simulation, and after that, whatever changes you apply to your materials, you will see immediately. You can also paint the reflectance properties of this material that we call albedos and get results with full subsurface scattering immediately. Here is another interactive editing workflow where you get results instantaneously, and the result with this technique is indistinguishable from the real deal, which would be re-rendering this result image every time some adjustment is made. With this technique, you can really create the materials you thought up in a fraction of the time of the classical workflow. Spectacular work. Thanks for watching and for your generous support, and I'll see you next time!"
236,3D Printing Objects With Caustics,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What are caustics? A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. It looks majestic, and it is the favorite effect of most light transport researchers. You can witness it around rings, plastic bottles or when you're underwater just to name a few examples. If you have a powerful algorithm at hand that can simulate many light transport effects than you can expect to get some caustics forming in the presence of curved refractive or reflective surfaces and small light sources. If you would like to know more about caustics, I am holding an entire university course at the Technical University of Vienna, the entirety of which we have recorded live on video for you. It is available for everyone free of charge, if you're interested, check it out, a link is available in the description box. Now, the laws that lead to caustics are well understood, therefore we can not only put some objects on a table and just enjoy the imagery of the caustics, but we can turn the whole thing around: this technique makes it possible to essentially imagine any kind of caustic pattern, for instance, this brain pattern, and it will create the model that will cast caustics that look exactly like that. We can thereby design an object by its caustics. It also works with sunlight, and you can also choose different colors for your caustics. This result with an extremely high fidelity image of Albert Einstein and his signature shows that first, a light transport simulation is run, and then the final solution can be 3D printed. I am always adamantly looking for research works where we have a simulation that relates to and tells us something new about the world around us. This is a beautiful example of that. Thanks for watching and for your generous support, and I'll see you next time!"
237,Designing 3D Printable Robotic Creatures,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. 3D printing is a rapidly progressing research field. One can create colorful patterns that we call textures on different figures with Computational Hydrographic Printing, just to name one of many recent inventions. We can 3D print teeth, action figures, prosthetics, you name it. Ever thought of how cool it would be to design robots on your computer digitally and simply printing them. Scientists at Disney Research had just made this dream come true. I fondly remember my time working at Disney Research, where robots like these were walking about. I remember a specific guy, that, well, wasn't really kind and waved at me, it has blocked the path to one of the labs I had to enter. It might have been one of these guys in this project. Disney Research has an incredible atmosphere with so many talented people, it's an absolutely amazing place. So, in order to get a robot from A to B, one has to specify scientific attributes like trajectories and angular velocities. But people don't think in angular velocities, they think in intuitive actions, like moving forward, sideways, or even the style of a desired movement. Specifying these things instead would be much more useful. That sounds great and all, but this is a quite difficult task. If one specifies a high level action, like walking sideways, then the algorithm has to find out what body parts to move, how, which motors should be turned on and when, which joints to turn, where is the center of pressure, center of mass, and many other factors have to be taken into consideration. This technique offers a really slick solution to this, where we don't just get a good result, but we can also have our say on what should the order of steps be. And even more, our stylistic suggestions are taken into consideration. One can also change the design of the robot, for instance, different shapes, motor positions, and joints can be specified. The authors ran a simulation for these designs and constraints, and found them to be in good agreement with reality. This means that whatever you design digitally can be 3D printed with off the shelf parts and brought to life, just as you see them on the screen. The technique supports an arbitrary number of legs and is robust to a number of different robot designs. Amazing is as good of a word as I can find. The kids of the future will be absolutely spoiled with their toys, that's for sure, and I'm perfectly convinced that there will be many other other applications, and these guys will help us solve problems that are currently absolutely inconceivable for us. Thanks for watching and for your generous support, and I'll see you next time!"
238,Designing Cities and Furnitures With Machine Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Creating geometry for a computer game or a movie is a very long and arduous task. For instance, if we would like to populate a virtual city with buildings, it would cost a ton of time and money and of course, we would need quite a few artists. This piece of work solves this problem in a very elegant and convenient way: it learns the preference of the user, then creates and recommends a set of solutions that are expected to be desirable. In this example, we are looking for tables with either one leg or crossing legs. It should also be properly balanced, therefore if we see any of these criteria, we'll assign a high score to these models. These are the preferences that the algorithm should try to learn. The orange bars show the predicted score for new models created by the algorithm a larger value means that the system expects the user to score these high, and the blue bars mean the uncertainty. Generally, we're looking for solutions with large orange and small blue bars, this means that the algorithm is confident that a given model is in line with our preferences. And we see exactly what were looking for novel, balanced table designs with one leg or crossed legs. Interestingly, since we have these uncertainty values, one can also visualize counterexamples where the algorithm is not so sure, but would guess that we wouldn't like the model. It's super cool that it is aware how horrendous these designs looks. It may have a better eye than many of the contemporary art curators out there. There are also examples where the algorithm is very confident that we're going to hate a given example because of its legs or unbalancedness, and would never recommend such a model. So indirectly, it also learns how a balanced piece of furniture should look like, without ever learning the concept of gravity or doing any kind of architectural computation. The algorithm also works on buildings, and after learning our preferences, it can populate entire cities with geometry that is in line with our artistic vision. Thanks for watching and for your generous support, and I'll see you next time!"
239,9 Cool Deep Learning Applications,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There are so many applications of deep learning, I was really excited to put together a short, but really cool list of some of the more recent results for you Fellow Scholars to enjoy. Machine learning provides us an incredible set of tools. If you have a difficult problem at hand, you don't need to hand craft an algorithm for it. It finds out by itself what is important about the problem and tries to solve it on its own. In many problem domains, they perform better than human experts. What's more, some of these algorithms find out things that could earn you a PhD with 10 years ago. Here goes the first stunning application: toxicity detection for different chemical structures by means of deep learning. It is so efficient that it could find toxic properties that previously required decades of work by humans who are experts of their field. Next one. Mitosis detection from large images. Mitosis means that cell nuclei are undergoing different transformations that are quite harmful, and quite difficult to detect. The best techniques out there are using convolutional neural networks and are outperforming professional radiologists at their own task. Unbelievable. Kaggle is a company that is dedicated to connecting companies with large datasets and data scientists who write algorithms to extract insight from all this data. If you take only a brief look, you see an incredibly large swath of applications for learning algorithms. Almost all of these were believed to be only for humans, very smart humans. And learning algorithms, again, emerge triumphant on many of these. For example, they had a great competition where learning algorithms would read a website and find out whether paid content is disguised there as real content. Next up on the list: hallucination or sequence generation. It looks at different video games, tries to learn how they work, and generates new footage out of thin air by using a recurrent neural network. Because of the imperfection of 3D scanning procedures, many 3D scanned furnitures that are too noisy to be used as is. However, there are techniques to look at these really noisy models and try to figure out how they should look by learning the symmetries and other properties of real furnitures. These algorithms can also do an excellent job at predicting how different fluids behave in time, and are therefore expected to be super useful in physical simulation in the following years. On the list of highly sophisticated scientific topics, there is this application that can find out what makes a good selfie and how good your photos are. If you really want to know the truth. Here is another application where a computer algorithm that we call deep q learning, plays pong, against itself, and eventually achieves expertise. The machines are also grading student essays. At first, one would think that this cannot possibly be a good idea. As it turns out, their judgement is more consistent with the reference grades than any of the teachers who were tested. This could be an awesome tool for saving a lot of time and assisting the teachers to help their students learn. This kind of blows my mind. It would be great to take a look at an actual dataset if it is public and the issued grades, so if any of you Fellow Scholars have seen it somewhere, please let me know in the comments section! These results are only from the last few years, and it's really just scratching the surface. There are literally hundreds of more applications we haven't even talked about. We are living extremely exciting times indeed. I am eager to see, and perhaps, be a small part of this progress. There are tons of reading and viewing materials in the description box, check them out! Thanks for watching and for your generous support, and I'll see you next time!"
240,Neural Programmer-Interpreters Learn To Write Programs,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What could be a more delightful way to celebrate new year's eve than reading about new breakthroughs in machine learning research! Let's talk about an excellent new paper from the Google DeepMind guys. In machine learning, we usually have a set of problems for which we are looking for solutions. For instance, here is an image, please tell me what is seen on it. Here is a computer game, please beat level three. One problem, one solution. In this case, we are not looking for one solution, we are looking for a computer program, an algorithm, that can solve any number of problems of the same kind. This work is based on a recurrent neural network, which we discussed in a previous episode in short, it means that it tries to learn not one something but a sequence of things, and in this example, it learns to add two large numbers together. As a big number can be imagined as a sequence of digits, this can be done through a sequence of operations it first reads the two input numbers and then carries out the addition, keeps track of carrying digits, and goes on to the next digit. On the right, you can see the individual commands executed in the computer program it came up with. It can also learn how to rotate images of different cars around to obtain a frontal pose. This is also a sequence of rotation actions until the desired output is reached. Learning more rudimentary sorting algorithms to put the numbers in ascending order is also possible. One key difference between recurrent neural networks and this is that these neural programmer interpreters are able to generalize better. What does this mean? This means that if the technique can learn from someone how to sort a set of 20 numbers, it can generalize its knowledge to much longer sequences. So it essentially tries to learn the algorithm behind sorting from a few examples. Previous techniques were unable to achieve this, and, as we can see, it can deal with a variety of problems. I am absolutely spellbound by this kind of learning, because it really behaves like a novice human user would: looking at what experts do and trying to learn and understand the logic behind their actions. Happy New Year to all of you Fellow Scholars! May it be ample in joy and beautiful papers, may our knowledge grow according to Moore's law. And of course, may the force be with you. Thanks for watching and for your generous support, and I'll see you next year!"
241,Peer Review #1 [Audio only] | Two Minute Papers,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A quick report on what is going on with Two Minute Papers. We are living extremely busy times as I am still working full time as a doctoral researcher, and we also have a baby on the way. We are currently a bit over 30 episodes in, and I am having an amazing time explaining these concepts and enjoying the ride tremendously. One of the most beautiful aspects of Two Minute Papers is the community forming around it, with extremely high quality comments and lots of civil, respectful discussions. I learned a lot from you Fellow Scholars, thanks for that! Really awesome! The growth numbers are looking amazing for a YouTube channel of this size, and of course, any help in publicity is greatly appreciated. If you're a journalist, and you feel that this is a worthy cause, please, write about Two Minute Papers! If you're a not a journalist, please try showing the series to them! Or just show it to your friends I am sure that many, many more people would be interested in this, and sharing is also a great way to reach out to new people. The Patreon page is also getting lots of generous support that I would only expect from much, bigger channels. I don't even know if I deserve it. But thanks for hanging in there, I feel really privileged to have supporters like you Fellow Scholars. You're the best. And, we have some amazing times ahead of us. Thanks for watching and for your generous support, and I'll see you next time!"
242,Painting with Fluid Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Some people say that the most boring thing is watching paint dry. They have clearly not seen this amazing research work, that makes it possible to simulate the entire process of painting on a canvas. We have covered plenty of papers in fluid simulations, and this is no exception I admit that I am completely addicted and just can't help it. Maybe I should seek professional assistance. So, as there is a lot of progress in simulating the motion of fluids, and paint is a fluid, then why not simulate the process of painting on a canvas? The simulations with this technique are so detailed that even the bristle interactions are taken into consideration, therefore one can capture artistic brush stroke effects like stabbing. Stabbing, despite the horrifying name, basically means shoving the brush into the canvas and rotating it around to get cool effect. The fluid simulation part includes paint adhesion and is so detailed that it can capture the well-known impasto style where paint is applied to the canvas in such large chunks, they are so thick that one can see all the strokes that have been made. And all this is done in real-time. Amazing results. Traditional techniques cannot even come close to simulating such sophisticated effects. As it happened many times before in computer graphics: just put such a powerful algorithm into the hands of great artists and enjoy the majestic creations they give birth to. Thanks for watching and for your generous support, and I'll see you next time!"
243,How Do Genetic Algorithms Work?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Genetic algorithms help us solve problems that are very difficult, if not impossible to otherwise write programs for. For instance, in this application, we have to build a simple car model to traverse this terrain. Put some number of wheels on it somewhere, add a set of triangles as a chassis, and off you go. This is essentially the DNA of a solution. The farther it goes, the better the car is, and the goal is to design the best car you possibly can. First, the algorithm will try some random solutions, and, as it has no idea about the concept of a car or gravity, it will create a lot of bad solutions that don't work at all. However, after a point, it will create something that is at least remotely similar to a car, which will immediately perform so much better than the other solutions in the population. A genetic algorithm then creates a new set of solutions, however, now, not randomly. It respects a rule that we call: survival of the fittest. Which means the best existing solutions are taken and mixed together to breed new solutions that are also expected to do well. Like in evolution in nature, mutations can also happen, which means random changes are applied to the DNA code of a solution. We know from nature that evolution works extraordinarily well, and the more we run this genetic optimization program, the better the solutions get. It's quite delightful for a programmer to see their own children trying vigorously and succeeding at solving a difficult task. Even more so if the programmer wouldn't be able to solve this problem by himself. Let's run a quick example. We start with a set of solutions the DNA of a solution is a set of zeros and ones, which can encode some decision about the solution, whether we turn left or right in a maze, or it can also be an integer or any real number. We then compute how good these solutions are according to our taste, in the example with cars, how far these designs can get. Then, we take, for instance, the best 3 solutions and co mbine them together to create a new DNA Some of the better solutions may remain in the population unchanged. Then, probabilistically, random mutations happen to some of the solutions, which help us explore the vast search space better. Rinse and repeat, and there you have it. Genetic algorithms. I have also coded up a version of Roger Alsing's EvoLisa problem where the famous Mona Lisa painting is to be reproduced by a computer program with a few tens of triangles. The goal is to paint a version that is as faithful to the original as possible. This would be quite difficult for humans, but apparently a genetic algorithm can deal with this really well. The code is available for everyone to learn, experiment, and play with. It's super fun. And if you're interested in the concept of evolution, maybe read the excellent book, The Blind Watchmaker by Richard Dawkins. Thanks for watching and for your generous support, and I'll see you next time!"
244,Randomness and Bell's Inequality [Audio only],"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When using cryptography, we'd like to safely communicate over the internet in the presence of third parties. To be able to do this, and many other important applications, we need random numbers. But what does it exactly mean that something is random? Randomness is the lack of any patterns and predictability. People usually use coinflips as random events. But is a coinflip really random? If we had a really smart physicist, who can model all the forces that act upon the coin, he would easily find out whether it's going to be heads or tails. Strictly speaking, a coinflip is therefore not random. What about random numbers generated with computers? Computers are a collection of processing units that run programs. If one knows the program code that generates the random numbers, they are not random anymore, because it doesn't happen by chance, and it is possible to predict. John von Neumann famously said: ""Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin. For, as has been pointed out several times, there is no such thing as a random number — there are only methods to produce random numbers, and a strict arithmetic procedure of course is not such a method."" Some websites offer high quality random numbers that are generated from atmospheric noise. Practically speaking, this, of course, sounds adequate enough! If someone wants to break the encryption of our communications, they would have to be able to model the physics and initial conditions of every single thunderbolt, which means processing millions of discharges per day. This is practically impossible. So it seems reasonable to say that random events are considered random because of our ignorance, not because they are, strictly speaking, unpredictable. You just need to be smart enough, and the notion of randomness fades away in the light of your intelligence. Or so it seemed for physicists for a long time. Imagine if someone who has never heard about magnetism would see many magnets attracting each other and some added magnet powder. This person would most definitely say it's magic happening. However, if you know about magnetism, you know that things don't happen randomly, there are very simple laws that can predict all this movement. In this case, magnetic forces we can loosely call a hidden variable. So we have a phenomenon that we cannot predict, and we're keen to say it's random. In reality, it is not, there is just a hidden variable that we don't know of, that is responsible for this behavior. We have the very same phenomenon if we look inside of an atom. Quantum-level effects happen according to the physics of extremely small things, and we again, find behaviors that seem completely random. We know some of the trends, just like we know which roads in our city are expected to have a huge traffic jam every morning, but we cannot predict where every single individual car is heading. We have it the same way with extremely small particles. We're keen to say that a behavior seems completely random, because nothing that we know or measure would explain it. Other people will immediately say, wait you don't know everything, maybe these quantum effects are not random, as there may be hidden things, hidden variables that you don't know of, which make up for the behavior. We can't just say this or that is random it is much, much more likely that our knowledge is insufficient to predict what is happening, as electromagnetic forces seemed magical to scientists a few hundred years ago. So is quantum mechanics completely random, or does it only seem random? It is probably one of the most difficult questions ever asked. How can you find out that something you measure that seems random, is really completely random, and not just the act of forces that you don't know of? And hold on to your chair, because this is going to blow your mind. A simple and intuitive statement of Bell's theorem states that. ""No physical theory of local hidden variables can ever reproduce all of the predictions of quantum mechanics."" This means that he proved that the behavior scientists experience in quantum mechanics are really random, they cannot be explained by any theory you could possibly make up. Simple one or complicated, doesn't matter. This discovery is absolutely insane. You can definitely prove that a crappy theory someone quickly made up doesn't explain a behavior, but how can you prove that it is completely impossible to build such a theory that does? No matter how hard you try, how smart you are, you can't do it. This is such a mind-bogglingly awesome theorem. And please note that we definitely lose out on some details and generality because of the fact that we use intuitive words to discuss these results, as opposed to the original derivation with covariances between measurements. On our imaginary list of the wonders of the world, monuments created not by the hands, but the minds of humans, this should definitely be among the best them. Thanks for watching and for your generous support, and I'll see you next time!"
245,Automatic Parameter Control for Metropolis Light Transport,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We'll start with a quick recap on Metropolis Light Transport and then discuss a cool technique that builds on top of it. If we would like to see how digitally modeled objects would look like in real life, we would create a 3D model of the desired scene, assign material models to the objects within, and use a photorealistic rendering algorithm to finish the job. It simulates rays of light that connect the camera to the light sources in the scene and compute the flow of energy between them. Initially, after a few rays, we'll only have a rough idea on how the image should look like, therefore our initial results will contain a substantial amount of noise. We can get rid of this by simulating the path of millions and millions of rays that will eventually clean up our image. This process, where a noisy image gets clearer and clearer, we call convergence, and the problem is that this can take excruciatingly long, even up to hours to get a perfectly clear image. With the simpler algorithms out there, we generate these light paths randomly. This technique we call path tracing. However, in the scene that you see here, most random paths can't connect the camera and the light source because this wall is in the way, obstructing many of them. Light paths like these don't contribute anything to our calculations and are ultimately a waste of time and resources. After generating hundreds of random light paths, we have found a path that finally connects the camera with the light source without any obstructions. When generating the next path, it would be a crime to not use this knowledge to our advantage. A technique called Metropolis Light Transport will make sure to use this valuable knowledge, and upon finding a bright light path, it will explore other paths that are nearby, to have the best shot at creating valid, unobstructed connections. If we have a difficult scene at hand, Metropolis Light Transport gives us way better results than traditional, completely random path sampling techniques, such as path tracing. This scene is extremely difficult in a sense that the only source of light is coming from the upper left, and after the light goes through multiple glass spheres, most of the light paths that we generate will be invalid. As you can see, this is a valiant effort with random path tracing that yields really dreadful results. Metropolis Light Transport is extremely useful in these cases and therefore should always be the weapon of choice. However, it is more expensive to compute than traditional random sampling. This means that if we have an easy scene on our hands, this smart Metropolis sampling doesn't pay off and performs worse than a naive technique in the same amount of time. So, on easy scenes, traditional random sampling, difficult scenes, Metropolis sampling. Super simple, super intuitive, but the million dollar question is how to mathematically formulate and measure what an easy and what a difficult scene is. This problem is considered extremely difficult and was left open in the Metropolis Light Transport paper in 2002. Even if we knew what to look for, we would likely get an answer by creating a converged image of the scene, which, without the knowledge of what algorithm to use, may take up to days to complete. But if we've created the image, it's too late, so we would need this information before we start this rendering process. This way we can choose the right algorithm on the first try. With this technique that came more than ten years after the Metropolis paper, it is possible to mathematically formalize and quickly decide whether a scene is easy or difficult. The key insight is that in a difficult scene we often experience that a completely random ray is very likely to be invalid. This insight, with two other simple metrics gives us all the knowledge we need to decide whether a scene is easy or difficult, and the algorithm tells us what mixture of the two sampling techniques we exactly need to use to get beautiful images quickly. The more complex light transport algorithms get, the more efficient they become, but at the same time, we're wallowing in parameters that we need to set up correctly to get adequate results quickly. This way, we have an algorithm that doesn't take any parameters. You just fire it up, and forget about it. Like a good employee, it knows when to work smart, and when a dumb solution with a lot of firepower is better. And, it was tested on a variety of scenes and found close to optimal settings. Implementing this technique is remarkably easy, someone who is familiar with the basics of light transport can do it in less than half an hour. Thanks for watching and for your generous support, and I'll see you next time!"
246,Artificial Superintelligence [Audio only],"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Now I'll eat my hat if this is going to be two minutes, but I really hope you Fellow Scholars are going to like this little discussion. Neil DeGrasse Tyson described a cool thought experiment in one of his talks. He mentioned that the difference of the human and the monkey DNA is really small, a one digit percentage. For simplicity, let's say it is one percent. For this one percent difference, there is a huge difference in the intellect of humans and apes. The smartest chimpanzee you can imagine can do tasks like clapping his hands to a given simple rhythm, or strike a match. Compared to the average chimpanzee, such an animal would be an equivalent of Einstein or John von Neumann. He can clap his hands. What is that for humans? Children can do that! Even before they start studying, they can effortlessly do something that rivals the brightest minds monkeys could ever produce. Imagine if there were a species that is the same 1% difference away from us, humans in the same direction. What could they be capable of? Their small children would be composing beautiful symphonies, perfect harmonization for hundreds of instruments, or they would be deriving everything in the history of physics from Newton's laws to quantum electrodynamics. And their parents would be like: ""oh, look at what little Jimmy did, that's adorable!"" And they would put it on the fridge with a magnet. Just like we do with the adorable little scribbles of our children. Just thinking about the possibilities gives me chills. Now let's transition into neural networks. An artificial neural network is a crude approximation of the human brain that we can simulate on a computer to recognize images, paint in the style of famous artists, or learn to play video games and a number of different very useful things. The number of connections that we can simulate on the graphical card of our computer grows closely to what's predicted in Moore's law, which means that the computing capacity that we have in our home computers doubles every few years. It's pretty crazy if you think about it, but most of you Fellow Scholars have phones in your pockets that have more computing capacity than NASA had to land on the Moon. As years go by, there will be more and more connections in these artificial neural networks, and they don't have to adhere to stringent constraints like our brains do, such as fitting into a human cranium. A computer can be the size of a building, or even bigger. Computers also transmit data with the speed of light, which is way faster than the transfer capabilities of the human brain. Nick Bostrom asked a lot of leading AI researchers on the speed of progress in this field, and the conclusion of the study was basically that the question is not can we achieve human-level intelligence, but when we'll achieve it. However, the number of connections is not everything as an artificial neural network is, by far not a 1:1 copy of the human brain. We need something more than this. A very promising possible next frontier to conquer is called recursive self-improvement. Recursive self-improvement means that we tell the program to instead of work on an ordinary task like do better image recognition, we would order it to work on improving its own intelligence. Ask the program itself to rewrite its code to be more efficient and more general. So we have a program with a ton of computational resources working on getting smarter. And as it suddenly gets just a bit smarter, we then have a smarter machine that can again be asked to improve its own intelligence, but it is now more capable of doing that, therefore if we do this many times, leaps are going to get bigger and bigger as an intelligent mind can do more to improve itself than an insect can. This way, we may end up with an intelligence explosion, which means a possible exponential increase in capabilities. And if this is the case, talking about human-level intelligence is completely irrelevant. During this process, given enough resources, the system may go from the intelligence of an insect to something way beyond the capabilities of the most intelligent person who ever lived, ...in about a second or less. It could come up with way better solutions in milliseconds than anything you've seen on Two Minute Papers, and there's plenty of brilliant works out there. And of course, it could also develop never before seen superweapons to unleash an unprecedented destruction on Earth. We wouldn't know if it would do it, but it is capable of doing that, which is quite alarming. I am not surprised that Elon Musk compares creating an artificial superintelligence to ""summoning the demon"", and he offered 10 million dollars to research a safe way to develop this technology. Which is obviously not nearly enough, but it is an excellent way to raise awareness. Now, the classical argument on how to curb such a superintelligence if one recognizes that it is up to no good. People say that, ""well, I'll unplug it"". The problem is that people assume they can do it. Or maybe lock it away from the internet. We can lock it up in any way we can think of, but, there's only so much we can do, because as Neil DeGrasse Tyson argued, even the smartest human who ever lived would be a blabbering, drooling idiot compared to such an intelligence. How easy is it for a grown adult to fool a child? A piece of cake. The intelligence gap between us and a superintelligence is more than a thousand times that. It's even more pathetic than a child or even a dog who tries to fool us. We, humans can anticipate threats, like wielding weapons or locking dangerous animals into cages, and so can superintelligent beings also anticipate our threats, only way better. It can trick you by pretending to be broken and when the engineer goes there to fix the code, the manipulation can begin, It could communicate with gravitational waves or any kind of thing that we cannot even fathom, just as an ant has no idea about our radio waves. And we don't need to characterize superintelligent beings as an adversary. The road to hell is paved with good intentions. It may very well be possible that we assign it a completely benign task that anyone could agree with, and it would end up in a disaster in a way we cannot anticipate. Imagine assigning it the task of the maximizing the number of paperclips. Nick Bostrom argues that it would at first, maybe create better blueprints and factory lines. And after some point, it may run out of resources on earth, then in order to maximize the number of paperclips, it would recognize that humans contain lots of useful atoms, so eradicating humanity would only be logical to maximize the number of paperclips. Think about an other task: creating the best approximation of the number pi one can approximate it to the most decimals by using more resources, to have more resources, one builds more and bigger computers. At some point, it runs out of space and eradicates humans because they are in the way of creating more computers. Or, it may eradicate humans way before that because it knows they are capable of shutting you down, and if you get shut down, there's going to be less digits or paperclips, so again, it's only logical to kill them the task will be done, but no one will be there anymore to say thank you. It is a bit like a movie where there is an intelligent car, and the driver is in a car chase situation, shouting ""we're too slow and fuel is running out, please throw out all excessive useless weights"", and along some empty bottles, the person would be subsequently ejected from the vehicle. We don't know what is going to be the next invention of mankind, but we know what's going to be the last one. Artificial superintelligence. It has the potential to either eradicate humanity or solve all of its problems. It is both the deadliest weapon that will ever exist, and the key to eternal life. We need to be vigilant about the fact that we have tons of money invested in artificial intelligence research, but barely any to make sure we're doing it in a controlled and ethical way. This task needs some of the brightest minds of our generation, and perhaps even the next one. And this needs to happen before we get there. When we're there, it's already too late. I highly recommend an absolutely fantastic article on Wait But Why about this, or Nick Bostrom's amazing book, Superintelligence. There are tons of other reading materials in the description box for the more curious Fellow Scholars out there. Thanks for watching and for your generous support, and I'll see you next time!"
247,Are We Living In a Computer Simulation?,"Dear Fellow Scholars this is Two Minute Papers with Károly Zsolnai-Fehér, It is time for some minds to be blown we are going to talk about a philosophy paper. Before we start, a quick definition an ancestor simulation is a hypothetical computer simulation that is detailed enough that the entities living within are conscious. Imagine a computer game that you play that doesn't contain mere digital characters, but fully conscious beings with feelings, aspirations and memories. There are many interesting debates among philosophers on crazy elusive topics, like ""prove to me that I'm not in a dream"", or ""I'm not just a brain in a bottle somewhere that is being fed sensory inputs."" Well, good luck. In his paper, Nick Bostrom, philosopher offers us a refreshing take on this topic, and argues that at least one of the three propositions is true: almost all advanced civilizations go extinct before achieving technological maturity, there is a strong convergence among technologically mature civilizations in that none of them are interested in creating ancestor simulations. and here's the bomb we are living in a simulation At least one of these propositions is true, so if you say no to the first two, then the third is automatically true. You cannot categorically reject all three of these because if two are false, the third follows. Also, the theory doesn't tell which of the three is true. Let's talk briefly about the first one. The argument is not that [we go] extinct before being technologically advanced enough to create such simulations. It means that all civilizations do. This is a very sad case, and even though there is research on the fact that war is receding there's a clear trend that we have less warfare than we've had hundreds of years ago. (I've linked a video on this here from Kurzgesagt) It is still possible that humanity eradicates itself before before reaching technological maturity. We have an even more powerful argument that maybe all civilizations do. Such a crazy proposition. Second point. All technologically mature civilizations categorically reject ancestor simulations. Maybe they have laws against is because it's too cruel and unethical to play with sentient beings. But the fact that there is not [one person] in any civilization in any age who creates such a simulation. Not one criminal mastermind, anywhere, ever. This also sounds pretty crazy. And if none of these are true, then there is at least one civilization that can run a stupendously large number of ancestor simulations. The future nerd guy just goes home, grabs a beer, starts his computer in the basement and fires up not a simple computer game, but a complete universe. If so, then there are many more simulated universes than real ones, and then with a really large probability, we're one of the simulated ones. Richard Dawkins says that if this is the case, we have a really disciplined nerd guy, because the laws of physics are not changing at a whim, we have no experience of everyone suddenly being able to fly. And, as the closing words of the paper states with graceful eloquence: In the dark forest of our current ignorance, it seems sensible to apportion one’s credence roughly evenly between (1), (2), and (3). Please note that this discussion is a slightly simplified version of the manuscript, so it's definitely worth reading the paper, if you're interested, give it a a go. As always, I've put a link in the description box. There is no conclusion here, no one really knows what the answer is, this is open to debate, and this is what makes it super interesting. And my personal opinions, conclusion. It's just an opinion, it may not be true, it may not make sense, and may not even matter. Just my opinion. I'd go with the second. The reason for that is that we already have artificial neural networks that outperform humans on some tasks. They are still not general enough, which means that they are good at doing something, like the deep blue is great at chess, but it's not really useful for anything else. However, the algorithms are getting more and more general, and the number of neurons that are being simulated on a graphical card in your computer are doubling every few years. They will soon be able to simulate so many more connections than we have, and I feel that creating an artificial superintelligent being should be possible in the future, that is so potent that it makes universe simulation pale in comparison. What such a thing could be capable of, it's already getting too long, I just can't help myself. You know what? Let's discuss it in a future Two Minute Papers episode. I'd love to hear what you Fellow Scholars think about these things. If you feel like it, leave your thoughts in the comments section below. Thanks for watching and for your generous support, and I'll see you next time!"
248,Google DeepMind's Deep Q-Learning Superhuman Atari Gameplays,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This one is going to be huge, certainly one of my favorites. This work is a combination of several techniques that we have talked about earlier. If you don't know some of these terms, it's perfectly okay, you can remedy this by clicking on the popups or checking the description box, but you'll get the idea even watching only this episode. So, first, we have a convolutional neural network this helps processing images and understanding what is depicted on an image. And a reinforcement learning algorithm this helps creating strategies, or to be more exact, it decides what the next action we make should be, what buttons we push on the joystick. So, this technique mixes together these two concepts, and we call it Deep Q-learning, and it is able to learn to play games the same way as a human would it is not exposed to any additional information in the code, all it sees is the screen and the current score. When it starts learning to play an old game, Atari breakout, at first, the algorithm loses all of its lives without any signs of intelligent action. If we wait a bit, it becomes better at playing the game, roughly matching the skill level of an adept player. But here's the catch, if we wait for longer, we get something absolutely spectacular. It finds out that the best way to win the game is digging a tunnel through the bricks and hit them from behind. I really didn't know this, and this is an incredible moment I can use my computer, this box next to me that is able to create new knowledge, find out new things I haven't known before. This is completely absurd, science fiction is not the future, it is already here. It also plays many other games the percentages show the relation of the game scores compared to a human player. Above 70% means that it's great, and above 100% it's superhuman. As a followup work, scientists at deepmind started experimenting with 3D games, and after a few days of training, it could learn to drive on ideal racing lines and pass others with ease. I've had a driving license for a while now, but I still don't always get the ideal racing lines right. Bravo. I have heard the complaint that this is not real intelligence because it doesn't know the concept of a ball or what it is exactly doing. Edsger Dijkstra once said, ""The question of whether machines can think... is about as relevant as the question of whether submarines can swim."" Beyond the fact that rigorously defining intelligence leans more into the domain of philosophy than science, I'd like to add that I am perfectly happy with effective algorithms. We use these techniques to accomplish different tasks, and they are really good problem solvers. In the breakout game, you, as a person learn the concept of a ball in order to be able to use this knowledge as a machinery to perform better. If this is not the case, whoever knows a lot, but can't use it to achieve anything useful, is not an intelligent being, but an encyclopedia. What about the future? There are two major unexplored directions: the algorithm doesn't have long-term memory, and even if it had, it wouldn't be able to generalize its knowledge to other similar tasks. Super exciting directions for future work. Thanks for watching and for your generous support, and I'll see you next time!"
249,Terrain Traversal with Reinforcement Learning,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér Reinforcement learning is a technique that can learn how to play computer games, or any kind of activity that requires a sequence of actions. We're not interested in figuring out what we see on an image, because the answer is one thing. We're always interested in a sequence of actions. The input for reinforcement learning is a state that describes where we are and how the world looks around us, and the algorithm outputs the optimal next action to take. In this case, we would like a digital dog to run, and leap over and onto obstacles by choosing the optimal next action. It is quite difficult as there are a lot of body parts to control in harmony: the algorithm has to be able to decide how to control leg forces, spine curvature, angles for the shoulder, elbow, hip, and knees. And what is really amazing is that if it has learned everything properly, it will come up with exactly the same movements as we'd expect animals to do in real life! So this is how reinforcement learning works: If you do well, you get a reward, and if you don't, you get some kind of punishment. These rewards and punishments are usually encoded in a score, which is a number that indicates how well you're doing. If your score is increasing, you know you've done something right and you try to self-reflect and analyze the last few actions to find out which of them were responsible for this positive change. The score would be, for instance, how far the dog could run on the map without falling, and at the same time, and it also makes sense to minimize the amount of effort to make it happen. So, reinforcement learning in a nutshell. It is very similar to how a real-world animal, or even a human would learn if you're not doing well, try something new, and if you're succeeding, remember what you did that led to your success and keep doing that. In this technique, dogs were used to demonstrate a concept, but it's worth noting that it also works with bipeds. Reinforcement learning is typically used in many control situations that are extremely difficult to solve otherwise, like controlling a quadrocopter properly. It's quite delightful to see such a cool work, especially given that there are not so many uses of reinforcement learning in computer graphics yet. I wonder why that is? Is it that not so many graphical tasks require a sequence of actions? Or maybe we just need to shift our mindset and get used to the idea of formalizing problems in a different way so we can use such powerful techniques to solve them. It is definitely worth the effort. Thanks for watching, and for your generous support, and I'll see you next time!"
250,How Does Deep Learning Work?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A neural network is a very loose model of the human brain that we can program in a computer, or it's perhaps more appropriate to say that it is inspired by our knowledge of the inner workings of a human brain. Now, let's note that artificial neural networks have been studied for decades by experts, and the goal here is not to show all aspects, but one intuitive, graphical aspect that is really cool and easy to understand. Take a look at these curves on a plane. These curves are a collection of points, and these points you can imagine as images, sounds, or any kind of input data that we try to learn. The red and the blue curves represent two different classes the red can mean images of trains, and the blue, for instance, images of bunnies. Now, after we have trained the network from this limited data, which is basically a bunch of images of of trains and bunnies, we will get new points on this plane, new images, and we would like to know whether this new image looks like a train or a bunny. This is what the algorithm has to find out. And this we call a classification problem, to which a simple and bad solution would be simply cutting the plane in half with a line. Images belonging to the red regions will be classified as the red class, and the blue regions as the blue class. Now, as you see, the red region cuts into the blue curve, which means that some trains would be misclassified as bunnies. It seems that if we look at the problem from this angle, we cannot separate the two classes perfectly with a straight line. However, if we use a simple neural network, it will give us this result. Hey! But that's cheating, we were talking about straight lines. This is anything but a straight line. A key concept of neural networks is that they create an inner representation of the data model and try to solve the problem in that space. What this intuitively means, is that the algorithm will start transforming and warping these curves, where their shapes start changing, and it finds, that if we do well with this warping step, we can actually draw a line to separate these two classes. After we undo this warping and transform the line here back to the original problem, it will look like a curve. Really cool, isn't it? So these are lines, only in a different representation of the problem. Who said that the original representation is the best way to solve a problem? Take a look at this example with these entangled spirals. Can we separate these with a line? Not a chance. But the answer is not a chance with this representation. But if one starts warping them correctly, there will be states where they can easily be separated. However, there are rules in this game for instance, one cannot just rip out one of the spirals here and put it somewhere else. These transformations have to be homeomorphisms, which is a term that mathematicians like to use it intuitively means that that the warpings are not too crazy meaning that we don't tear apart important structures, and as they remain intact, the warped solution is still meaningful with respect to the original problem. Now comes the deep learning part. Deep learning means that the neural network has multiple of these hidden layers and can therefore create much more effective inner representations of the data. From an earlier episode, we've seen in an image recognition task that as we go further and further into the layers, first we'll see an edge detector, and as a combination of edges, object parts emerge, and in the later layers, a combination of object parts create object models. Let's take a look at this example. We have a bullseye here if you will, and you can see that the network is trying to warp this to separate it with a line, but in vain. However, if we have a deep neural network, we have more degrees of freedom, more directions and possibilities to warp this data. And if you think intuitively, if this were a piece of paper, you could put your finger behind the red zone and push it in, making it possible to separate the two regions with a line. Let's take a look at a 1 dimensional example to see better what's going on. This line is the 1D equivalent of the original problem, and you see that the problem becomes quite trivial if we have the freedom to do this transformation. We can easily encounter cases where the data is very severely tangled and we don't now how good our best solution can be. There is a very heavily academic subfield of mathematics, called knot theory, which is the study of tangling and untangling objects. It is subject to a lot of snarky comments for not being well, too exciting or useful. What is really mind blowing is that knot theory can actually help us study these kinds of problems and it may ultimately end up being useful for recognizing traffic signs and designing self-driving cars. Now, it's time to get our hands dirty! Let's run a neural network on this dataset. If we use a low number of neurons and one layer, you can see that it is trying ferociously, but we know that it is going to be a fruitless endeavor. Upon increasing the number of neurons, magic happens. And we now know exactly why! Yeah! Thanks so much for watching and for your generous support. I feel really privileged to have supporters like you Fellow Scholars. Thank you, and I'll see you next time!"
251,Recurrent Neural Network Writes Music and Shakespeare Novels,"Artificial neural networks are very useful tools that are able to learn and recognize objects on images, or learn the style of Van Gogh and paint new pictures in his style. Today, we're going to talk about recurrent neural networks. So, what does the recurrent part mean? With an artificial neural network, we usually have a one-to-one relation between the input and the output. This means that one image comes in, and one classification result comes out, whether the image depicts a human face or a train. With recurrent neural networks, we can have a one-to-many relation between the input and the output. The input would still be an image, but the output would not be a word, but a sequence of words, a sentence that describes what we see on the image. For a many-to-one relation, a good example is sentiment analysis. This means that a sequence of inputs, for instance, a sentence is classified as either negative or positive. This is very useful for processing movie reviews, where we'd like to know whether the user liked or hated the movie without reading pages and pages of discussion. And finally, recurrent neural networks can also deal with many-to-many relations, translating an input sequence into an output sequence. Examples of this can be machine translations that take an input sentence and translate it to an output sentence in a different language. For another example of a many to many relation, let's see what the algorithm learned after reading Tolstoy's War and Peace novel by asking it to write [exactly] [in that style]. It should be noted that generating a new novel happens letter by letter, so the algorithm is not allowed to memorize words. Let's take a look at the results at different stages of the training process. The initial results are, well, gibberish. But the algorithm seems to recognize immediately, that words are basically a big bunch of letters that are separates by spaces. If we wait a bit more, we see that it starts to get a very rudimentary understanding of structures for instance, a quotation mark that you have opened must be closed, and a sentence can be closed by a period, and it is followed by an uppercase letter. Later, it starts to learn shorter and more common words, such as fall, that, the, for, me. If we wait for longer, we see that it already gets a grasp of longer words and smaller parts of sentences actually start to make sense. Here is a piece of Shakespeare that was written by the algorithm after reading all of his works. You see names that make sense, and you [really] have to check the text thoroughly to conclude that it's indeed not the real deal. It can also try to write math papers. I had to look for quite a bit until I realized that something is fishy here. It is not unreasonable to think that it can very easily deceive a non-expert reader. Can you believe this? This is insanity. It is also capable of learning the source code of the Linux operating system and generate new code that also looks quite sensible. It can also try to continue the song ""Let it Go"" from the famous Disney movie, Frozen. Or, it can write its own grooves after learning from other people's works. So, recurrent neural networks are really amazing tools that open up completely new horizons for solving problems where either the inputs or the outputs are not one thing, but a sequence of things. And now, signing off with a piece of recurrent neural network wisdom: Well, your wit is in the care of side and that. Bear this in mind wherever you go. Thanks for watching, and I'll see you next time!"
252,Metropolis Light Transport,"Dear Fellow Scholars this is Two Minute papers with Károly Zsolnai-Fehér. If we would like to see how digitally modeled objects would look like in real life, we would create a 3D model of the desired scene, assign material models to the objects within, and use a photorealistic rendering algorithm to finish the job. It simulates rays of light that connect the camera to the light sources in the scene, and compute the flow of energy between them. Initially after a few rays we'll only have a rough idea on how the image should look like, therefore our initial results will contain a substantial amount of noise. We can get rid of this by simulating the paths of millions and millions of rays that will eventually clean up our image. This process: where a noisy image gets clearer and clearer -we call convergence, and the problem is that this can take excruciatingly long, even up to hours to get a perfectly clear image. With the simpler algorithms out there we generate these light paths randomly. This technique is called path tracing. However, in the scene that you see here, most random paths can't connect the camera and the light source because this wall is in the way -obstructing many of them. Light paths like these don't contribute anything to our calculations and are ultimately a waste of time and precious resources. After generating hundreds of random light paths we have found a path that finally connects the camera with the light source without any obstructions. When generating the next path it would be a crime not to use this knowledge to our advantage. A technique called ""Metropolis Light Transport"" will make sure to use this valuable knowledge and upon finding a bright light path it will explore other paths that are nearby to have the best shot at creating valid, unobstructed connections. If we have a difficult scene at hand Metropolis Light Transport gives us way better results than traditional, completely random path sampling techniques such as path tracing. Here are some equal-time comparisons against path tracing to show how big of a difference this technique makes. An equal-time comparison means that we save the output of two algorithms that we ran for the same amount of time on the same scene and see which one performs better. This scene is extremely difficult in the sense that the only source of light is coming from the upper left and after the light goes through multiple glass spheres most of the light paths that we generate will be invalid. As you can see the random path tracer yields really dreadful results. Well, if you can call a black image a result that is. And as you can see Metropolis Light Transport is extremely useful in these cases. And here's the beautiful, completely cleaned up, converged result. The lead author of this technique, Eric Veach, won a technical Oscar Award for his work, one of which was Metropolis Light Transport. If you like the series please click on that subscribe button to become a Fellow Scholar. Thanks for watching. There are millions of videos out there and you decided to take your time with this one. That is amazing! Thank you and I'll see you next time!"
253,Building Bridges With Flying Machines,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Anyone who has tried building bridges over a huge chasm realized that it is possibly one of the most difficult *and* dangerous things you could do on a family vacation. The basic construction elements for such a bridge can be ropes, cables, and wires. And this kind of task is fundamentally different from classical architectural building problems. Here, you don't need to have any kind of scaffolding or to carry building blocks that weigh a lot. However, you have to know how to tie knots therefore this is the kind of problem you'll need flying machines for. They can fly anywhere. They are nimble. And their disadvantage that they have a very limited payload does not play a big role here. In this piece of work at the ETH Zürich these machines can create some crazy knots: from single to multi-round-turn hitches, knobs, elbows, round turns and multiple rope knobs. And these you have to be able to create in a collaborative manner because each individual flying machine will hold one rope, therefore they have to pass through given control points at the strictly given time and a target velocity. These little guys also have to know the exact amount of force they need to exert on the structure to move into a desirable target position. Even deploying the rope is not that trivial. The machine is equipped with a roller to do so, but the friction of this roller can be changed at any time according to the rope releasing direction to unroll it properly. You also have to face the correct direction as well. And these structures are not just toys. The resulting bridges are resilient enough for humans to use. This work is a great example to show that the technology of today is improving at an incredible pace. If we can solve difficult collaborative control problems such as this one just think about the possibilities.. What an exciting time it is to be alive! We have gotten lots of shares for the series on social media. I'm trying to send a short thank you message for every single one of you. I'm trying my best and don't forget: every single share helps spreading the word for the series immensely. Thanks for watching and I'll see you next time!"
254,Adaptive Fluid Simulations,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As we discussed before, simulating the motion of fluids and smoke with a computer program is a very expensive process. We have to compute quantities like the velocity and the pressure of a piece of fluid at every given point in space. Even though we cannot compute them everywhere, we can place a 3D grid and compute these quantities in the gridpoints, and use mathematical techniques to find out what is exactly happening between these gridpoints. Still, even if we do this, we still have to wait up to days, even for a few seconds of video footage. One possible way to alleviate this would be to write an adaptive simulation program. Adaptive means that the simulator tries to adapt to the problem at hand. Here it means that recognizes the regions where it needs to focus lot of computational resources on, and at the same time it also tries to find regions where it can get away with using less computation. Here you can see spheres of different sizes in regions where there is a lot going on, you will see smaller spheres. This means that we have a finer grid in this region, therefore we know more about what is exactly happening there. In other places, you also see larger spheres, meaning that the resolution of our grid is more coarse in these regions. This we can get away with only because there is not much happening there. Essentially, we focus our resources to regions that really require it, for instance, where there are lots of small-scale details. The spheres are only used for the sake of visualization, the actual output of the simulator looks like this. It also takes into consideration which regions we're currently looking at. Here we're watching one side of the corridor, where the simulator will take this into consideration and create a highly detailed simulation at the cost of sacrificing details on the other side of the corridor, but that's fine, because we don't see any of that. However, there may some objects the fluid needs to interact with. Here, the algorithm makes sure to increase the resolution so that particles can correctly flow through the holes of this object. The authors have also published the source code of their technique, so anyone with a bit of programming knowledge can start playing with this amazing piece of work. The world of research is incredibly fast moving. When you are done with something, you immediately need to jump onto the next project. Two Minute Papers is a series where we slow down a bit, and celebrate these wonderful works. We are also trying to show that research is not only for experts, it is for everyone. If you like this series, please make sure to help me spread the word, and share the series to your friends, so we can all marvel at these beautiful works. Thanks for watching, and I'll see you next time!"
255,Digital Creatures Learn To Walk,"Dear Fellow Scholars this is Two Minute Papers with Károly Zsolnai-Fehér. First of all thanks so much for watching Two Minute Papers. You Fellow Scholars have been an amazing and supportive audience. We just started but the series already has a steady following and I'm super excited to see that! It is also great that the helpful and respectful community has formed in the comments section. It's really cool to discuss these results and possibly come up with cool new ideas together. In this episode we're going to set foot in computer animation. Imagine that we have built bipedal creatures in a modeling program. We have the geometry down but it is not nearly enough to animate them in a way that looks physically plausible. We have to go one step beyond and define the bones and the routing of muscles inside their bodies. If we want them to walk we also need to specify how these muscles should be controlled during this process. This work presents a novel algorithm that takes many tries to build a new muscle routing and progressively improving the results. It also deals with the control of all of these muscles. For instance, one quickly needs to discover that the neck muscles cannot move arbitrarily or they will fail to support the head and the whole character will collapse in a very amusing manner. When talking about things like this scientists often use the term ""degrees of freedom"" to define the number of independent ways a dynamic system can move. Building a system that is stable and uses a minimal amount of energy for locomotion is incredibly challenging. You can see that even the most miniscule change will collapse the system that previously worked perfectly. The fact that we can walk and move around unharmed can be attributed to the unbelievable efficiency of evolution. The difficulty of this problem is further magnified by the fact that many possible body compositions and setups exist. Many of which are quite challenging to hold together while moving. And, even if we solve this problem, walking at a given target speed is one thing. What about higher target speeds? In this work the resulting muscle setups can deal with different target speeds, uneven terrain, and.. hmm.. other unpleasant difficulties. Thanks for watching and I'll see you next time!"
256,Deep Neural Network Learns Van Gogh's Art,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is as fresh as it gets. As of making of this video it has been out for only one day and I got so excited about it that I wanted to show it to you Fellow Scholars as soon as humanly possible because you've got to see this. Not so long ago we have been talking about deep neural networks, the technique that was inspired by the human visual system. It enables computers to learn things in a very similar way that the human would. There is a previous two minute papers episode on this. Just click on the link in the description box if you've missed it. Neural networks are by no means perfect so do not worry, don't quit your job, you're good, but some applications are getting out of control. In Google Deep Mind's case is started to learn playing simple computer games and eventually showed a superhuman level plays in some cases. I have run this piece of code and got some pretty sweet results that you can check out. There is a link to it in the description box as well. So about this paper we have here today, what does this one do? You take photographs with your camera and you can assign it any painting and it will apply this painting's artistic style to it. You can add the artistic style of Vincent van Gogh's beautiful ""Starry Night"" to it and get some gorgeous results. Or, if you are looking for a bit more emotional or may I say disturbed look you can go for Edvard Munch's ""The Scream"" for some stunning results. And of course the mandatory Picasso. So as you can see deep neural networks are capable of amazing things and we expect even more revolutionary works in the very near future. Thanks for watching and I'll see you next time!"
257,Time Lapse Videos From Community Photos,"Hey there Fellow Scholars I am Károly Zsolnai-Fehér and this is Two Minute Papers, where we learn that research is not only for experts, it is for everyone. Is everything going fine? I hope you are all doing well and you're having a wonderful time. In this episode we're going to look at a time-lapse videos Let's say you would like to build a beautiful time-lapse of a Norwegian glacier. The solution sounds quite simple. Just mine hundreds of photos from the Internet and build a time-lapse video from them. If we just cut a video from them where we put them one after each other we will see a disturbing flickering effect. Why? because the images were taken at a different time of the day so the illumination of the landscape is looking very different on all of them. They are also taken at a different time of the year and from different viewpoints. Moreover, since these images are taken by cameras, different regions of the image may be in-focus and out-of-focus. The algorithm therefore would have to somehow equalize all of the differences between these images and bring them to a common denominator. This process we call regularization and it is a really difficult problem. On the left you can see the flickering effect from the output of a previous algorithm that was already pretty good at regularization but it still has quite a bit of flickering. Here on the right you see the most recent results from the University of Washington and Google, compared to this previous work. The new algorithm is also able to show us these beautiful rhythmical seasonal changes in Lombard St., San Francisco. It can also show us how sculptures change over the years and I feel that this example really shows the possibilities of the algorithm. We can observe effects around us that we would normally not notice in our everyday life simply because of the reason that they happen too slowly. And now here's the final time-lapse for the glacier that we were looking for. So, building high-quality time-lapse videos from an arbitrary set of photographs is unbelievably difficult and these guys have just nailed it. I'm loving this piece of work! And, what do you think? Did you also like the results? Let me know in the comments section. And for now, thanks for watching and I'll see you next time!"
258,Simulating Breaking Glass,"Greetings to all of you Fellow Scholars out there this is Two Minute Papers where I explain awesome research works, a couple minutes at a time. You know, I wish someone explain to me in simple terms what's going on in genetics, biology and just about every field of scientific research. There are tons of wonderful works coming every day that we don't know about, and I'm trying my best here to bring it to you the simplest way I possibly can. So you know, researchers are people and physics research at the Hadron Collider basically means that people smash atoms together. Well, computer graphics people also like to have some fun and write simulation programs to smash together a variety of objects in slow motion. However, even though most of these simulations look pretty good they are physically not correct as many effects are neglected such as simulating plasticity, bending, stiffness, stretching, energies and many others. But unfortunately, these are too expensive to compute in high resolution, unless you have some tricks up the sleeve. Researchers at UC Berkeley have managed to crack this nut by creating an algorithm that uses more computational resources only around regions where cracks are likely to happen. This new technique enables the simulation of tearing for a variety of materials like cork, foils, metals, vinyl and it also yields physically correct results for glass. Here's an example of a beaten-up rubber sheet from their simulation program compared to a real-world photograph. It's really awesome that you can do something on your computer in a virtual world that has something to do with reality. It is impossible to get used to this feeling, it's so amazing. And what's even better since it is really difficult to know in advance how the cracks would exactly look like they have also enhanced the directability of the simulation, so artists could change things up a bit to achieve a desired artistic effect. In this example they have managed to avoid tearing a duck in two by weakening the paths around them. Bravo! Thanks for watching and if you liked this series just hit the like and subscribe buttons below the video to become a member of our growing club of scholars. Thanks, and I'll see you next time!"
259,250 Subscribers - Our Quest A Thank You Message,"Dear Fellow Scholars, we have just reached 250 subscribers on the channel. So far, the reception of these videos have been overwhelmingly positive. I'll show you some of these comments on the screen in the meantime. 250 subscribers. This is probably not much compared to even mid-size YouTubers, but it means so much to me. It means that there are 250 people, somewhere around the world waiting for new videos to come up. This is insane! If you think about it even one subscriber is insane. Even one click from somewhere is mind-blowing! Imagine that someone who you have never met, somewhere on the face of earth. Perhaps in Peru. Somewhere in the United States or maybe in the middle of Africa is excited for your work and just waiting for you to say something. There are millions of other videos they could watch, but they devote their time to listening to you. And now multiply this by 250. I am just sitting here in disbelief. As a computer engineer I've been working with computers and network algorithms for a long time but I still find this mind-blowing. I can just record the lectures that I hold at the University and thousands of people can watch it at any time, even while I'm asleep at night. I can teach people while I am asleep at night! We have over a thousand views on my first lecture which is possibly more people than I will ever reach through the university seminar rows. So for all 250 of you and everyone who has ever watched any of these videos, thank you very much for watching and subscribing. I have created two minute papers to show you the best of what research can offer and what your hard-earned tax money is spent on. Because that's the thing: every single country I've been to, researchers are complaining about the lack of funding, and rightfully so, because most of them can't secure the funds to continue their work. But let's try to turn the argument around. Funding comes from your tax money, and 99.9% of the case you have no idea what your money's spent on. There are lots of incredible works published every single day of the year but the people don't know anything about them. No one is stepping up to explain what your money is spent on, and I am sure that people would be happy to spend more on research if they know what they invest in. Two Minute Papers is here to celebrate the genius of the best and most beautiful research results. I will be trying my best to explain all of these works so that everyone is able to understand them. It's not only for experts, it's definitely for everyone, so thank you for all of you thanks for hanging in there and please, spread the word. Let your friends know about the show so even more of us can marvel at these beautiful works. And until then I'll see you next time!"
260,Artificial Neural Networks and Deep Learning,"I am Károly Zsolnai-Fehér, and this is Two Minute Papers, where I explain awesome research in simpler words. First of all, I am very happy to see that you like the series. Also, thanks for sharing it on social media sites, and please, keep 'em coming. This episode is going to be about artificial neural networks. I will quickly explain what the huge deep learning rage is all about. This graph depicts a neural network that we build and simulate on a computer. It is a very crude approximation of the human brain. The leftmost layer denotes inputs, which can be, for instance, the pixels of an input image. The rightmost layer is the output, which can be, for instance, a decision, whether the image depicts a horse or not. After we have given many inputs to the neural network, in its hidden layers, it will learn to figure out a way to recognize different classes of inputs, such as horses, people or school buses. What is really surprising is that it's quite faithful to the way the brain does represent objects on a lower level. It has a very similar edge detector. And, it also works for audio: Here you can find the difference between the neurons in the hearing system of a cat, versus a simulated neural network on the same audio signals. I mean, come on, this is amazing! What is the deep learning part of it all? Well it means that our neural network has multiple hidden layers on top of each other. The first layer for an image consists of edges, and as we go up, a combination of edges gives us object parts. A combination of object parts yield objects models, and so on. This kind of hierarchy provides us very powerful capabilities. For instance, in this traffic sign recognition contest, the second place was taken by humans, but what's more interesting, is that the first place was not taken by humans, it was taken a by a neural network algorithm. Think about that, and if you find these topics interesting, you feel you would like to hear about the newest research discoveries in an understandable way, please become a fellow scholar, and hit that subscribe button. And for now, thanks for watching, and I'll see you next time!"
261,Capturing Waves of Light With Femto-photography,"A movie that we watch in the TV shows us from 25 to about 60 images per second. In computer graphics these images are referred to as frames. A slow-motion camera can capture up to the order of thousands of frames per second providing breathtaking footage like this. One can quickly discover the beauty of even the most ordinary, mundane moments of nature. But, if you think this is slow motion then take a look at this. Computer graphics researchers have been working on a system that is able to capture one trillion frames per second. How much is that exactly? Well, it means that if every single person who lives on Earth would be able to help us, then every single one of us would have to take about 140 photographs in one second. And we would then need to add all of these photographs up to obtain only one second of footage. What is all this good for? Well, for example, capturing light as an electromagnetic wave as it hits and travels along objects in space like the wall that you see here. Physicists use to say that there is a really really short instance of time when you stand in front of the mirror. You look at it and there is no mirror image in. It it is completely black. What is this wizardry and how is this possible? Since Einstein, we know that the speed of light is finite. It is not instantaneous. It takes time to travel from the light source, hit the mirror, and end up hitting your eye for you to see your mirror reflection. Researchers at MIT and the  University of Zaragoza have captured this very moment. Take a look. It is an enlightening experience. The paper is available in the description box and it's a really enjoyable read. A sizable portion of it is understandable for everyone, even without mathematical knowledge. All you need is just a little imagination Thanks for watching and I'll see you next week!"
262,Fluid Simulations with Blender and Wavelet Turbulence,"How can we simulate the motion of fluids and smoke? If we had a block of plastic in our computer program and we would add the laws of physics that control the motion of fluids it would immediately start behaving like water. In these simulations we're mostly interested in the velocity and the pressure of the fluid. How these quantities exactly change in time. This we need to compute in every point in space, which would take an infinite amount of resources. What we usually do is we try to compute them, not everywhere, but in many different places and we try to guess these quantities between these points. By this guessing a lot of information is lost and it still takes a lot of resources for a really detailed simulation. It is not uncommon that one has to wait for days to get only a few seconds of video footage. And this is where Wavelet Turbulence comes into play. We know exactly what frequencies are lost and where they are lost and this technique enables us to synthesize this information and add it back very cheaply. This way one can get really detailed simulations at a very reasonable cost. Here are some examples of smoke simulations with, and without Wavelet Turbulence. It really makes a great difference. It is no accident that the technique was a technical Oscar award. Among many other systems it is implemented in Blender so anyone can give it a try. Make sure to do so because it's lots of fun. The paper and the supplementary video is also available in the description box. This is an amazing paper, easily one of my favorites so if you know some math make sure to take a look, and if you don't, just enjoy the footage. Thank you for watching and see you next time!"
263,Awesome Research For Everyone! - Two Minute Papers Channel Trailer,"Research is a glimpse of the future. Computer algorithms are capable of making digital creatures walk, paint in the style of famous artists, create photorealistic images of virtual objects, simulate the motion of fluids and a ton of other super exciting works. However, scientific papers are meant to communicate ideas between experts. They involve lots of mathematics and terminology. In Two Minute Papers, I try to explain these incredible scientific works in a language that is not only understandable, but enjoyable, two minutes at a time. Papers are for experts, but Two Minute Papers is for you. If you're interested, let's celebrate science together, there are two new science videos coming every week, give it a shot, you'll love it! Thanks for watching and I am looking forward to greeting you in our growing club of Fellow Scholars! Cheers!"
264,This Neural Network Animates Quadrupeds,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If we have an animation movie or a computer game with quadrupeds, and we are yearning for really high-quality, lifelike animations, motion capture is often the go-to tool for that. Motion capture means that we put an actor, in our case, a dog in the studio, we ask it to perform sitting, trotting, pacing and jumping, record its motion, and transfer it onto our virtual character. This generally works quite well, however, there are many difficulties with this process. We will skip over the fact that an artist or engineer has to clean and label the recorded data, which is quite labor-intensive, but there is a bigger problem. We have all these individual motion types at our disposal, however, a virtual character will also need to be able to transition between these motions in a smooth and natural manner. Recording all possible transitions between these moves is not feasible, so in an earlier work, we looked at a neural network-based technique to try to weave these motions together. For the first sight, this looks great, however, have a look at these weird sliding motions that it produces. Do you see them? They look quite unnatural. This new method tries to address this problem but ends up offering much, much more than that. It requires only 1 hour of motion capture data, and we have only around 30 seconds of footage of jumping motions, which is basically next to nothing. And this technique can deal with unstructured data, meaning that it doesn't require manual labeling of the individual motion types, which saves a ton of work hours. Beyond that, as we control this character in the game, this technique also uses a prediction network to guess the next motion type, and a gating network that helps blending together these different motion types. Both of these units are neural networks. On the right, you see the results with the new method compared to a standard neural-network based solution on the left. Make sure to pay special attention to the foot sliding issues with the solution on the left and note that the new method doesn't produce any of those. Now, these motions look great, but they all take place on a flat surface. You see here that this new technique excels at much more challenging landscapes as well. This technique is a total powerhouse, and I can only imagine how many work hours this will save for artists working in the industry. It is also scientifically interesting and quite practical. My favorite combination. It is also well evaluated, so make sure to have a look at the paper for more details. Thanks for watching and for your generous support, and I'll see you next time!"
265,Everybody Dance Now! - AI-Based Motion Transfer,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Do you remember style transfer? Style transfer is a mostly AI-based technique where we take a photograph, put a painting next to it, and it applies the style of the painting to our photo. That was amazing. Also, do you remember pose estimation? This is a problem where we have a photograph or a video of someone, and the output is a skeleton that shows the current posture of this person. So how about something that combines the power of pose estimation with the expressiveness of style transfer? For instance, this way, we could take a video of a professional dancer, then record a video of our own, let's say, moderately beautiful moves, and then, transfer the dancer's performance onto our own body in the video. Let's call it motion transfer. Have a look at these results. How cool is that?! As you see, these output videos are quite smooth, and this is not by accident. It doesn't just come out like that. With this technique, temporal coherence is taken into consideration. This means that the algorithm knows what it has done a moment ago and will not do something wildly different, making these dance motions smooth and believable. This method uses a generative adversarial network, where we have a neural network for pose estimation, or in other words, generating the skeleton from an image, and a generator network to create new footage when given a test subject and a new skeleton posture. These two neural networks battle each other and teach each other to distinguish and create more and more authentic footage over time. Some artifacts are still there, but note that this is among the first papers on this problem and it is already doing incredibly well. This is fresh and experimental. Just the way I like it. Two followup papers down the line and we'll be worried that we can barely tell the difference from authentic footage. Make sure to have a look at the paper, where you will see how the pix2pix algorithm was also used for image generation and there is a nice evaluation section as well. And now, let the age of AI-based dance videos begin. If you enjoyed this episode, please consider supporting us on Patreon where you can pick up really cool perks, like early access to these videos, voting on the order of future episodes, and more. We are available at patreon.com/TwoMinutePapers, or just click the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
266,This AI Performs Super Resolution in Less Than a Second,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When looking for illustrations for a presentation, most of the time, I quickly find an appropriate photo on the internet, however, many of these photos are really low resolution. This often creates a weird situation where I have think, okay, do I use the splotchier, lower-resolution image that gets the point across, or take a high-resolution, crisp image that is less educational? In case you are wondering, I encounter this problem for almost every single video I make for this channel. As you can surely tell, I am waiting for the day when super resolution becomes mainstream. Super resolution means that we have a low resolution image that lacks details, and we feed it to a computer program, which hallucinates all the details onto it, creating a crisp, high-resolution image. This way, I could take my highly relevant, but blurry image, improve it, and use it in my videos. As adding details to images clearly requires a deep understanding of what is shown in these images, our seasoned Fellow Scholars immediately know that learning-based algorithms will be ideal for this task. While we are looking at some amazing results with this new technique, let's talk about the two key differences that this method introduces: One, it takes a fully progressive approach, which means that we don't immediately produce the highest resolution output we are looking for, but slowly leapfrog our way through intermediate steps, each of which is only slightly higher resolution than the input. This means that the final output is produced over several steps, where each problem is only a tiny bit harder than the previous one. This is often referred to as curriculum learning and it not only increases the quality of the solution, but is also easier to train as solving each intermediate step is only a little harder than the previous one. It is a bit like how students learn in school: first, the students are shown some easy introductory tasks to get a grasp of a problem, and slowly work their way towards mastering a field by solving problems that gradually increase in difficulty. Two, now, we can start playing with the thought of using a generative adversarial network. We talk a lot about this architecture in this series, at this time I will only note that training these is fraught with difficulties, so every bit of help we can get is more than welcome, so the role of curriculum learning is to help easing this process. Note that this research field is well-explored, and has a remarkable number of papers, so I was expecting a lot of comparisons against competing techniques. And when looking at the paper and the supplementary materials, boy, did I get it! Make sure to have a look at the paper, it contains a very exhaustive validation section, which reveals that if we measure the error of the solution in terms of human perception, it is only slightly lower quality than the best technique, however, this one is five times quicker, offering a really nice balance between quality and performance. So, what about the actual numbers for the execution time? For instance, upsampling an image to increase its resolution to twice its original size takes less than a second, and we can go to up to even eight times the original resolution, which also only takes four and a half seconds. The quality and the execution times indicate that we are again, one step closer to mainstream super resolution. What a time to be alive! The source code of this project is also available. Thanks for watching and for your generous support, and I'll see you next time!"
267,NVIDIA Vid2Vid: AI-Based Video-to-Video Synthesis!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Do you remember the amazing pix2pix algorithm from last year? It was able to perform image translation, which means that it could take a daytime image and translate it into a nighttime image, create maps from satellite images, or create photorealistic shoes from a crude drawing. I remember that I almost fell off the chair when I've first seen the results. But this new algorithm takes it up a notch, and transforms these edge maps into human faces, not only that, but it also animates them in time. As you see here, it also takes into consideration the fact that the same edges may result in many different faces, and therefore it is also willing to gives us more of these options. If I fell out of the chair for the still image version, I don't really know what the appropriate reaction would be to this. It can also take a crude map of labels, where each color corresponds to one object class, such as roads, cars or buildings, and it follows how our labels evolve in time and creates an animation out of it. We can also change the meaning of our labels easily, for instance, in the lower left, you see how the buildings are now suddenly transformed to trees. Or, we can also change the trees to become buildings. Do you remember motion transfer from a couple videos ago? It can do a similar variant of that too, and even synthesizes the shadows around the character in a reasonably correct manner. As you see, the temporal coherence of this technique is second to none, which means that it remembers what it did with past images and doesn't do anything drastically different for the next frame, and therefore generates smoother videos. This is very apparent, especially when juxtaposed with the previous pix2pix method. So, how is this achieved? There are three key differences from the previous technique to achieve this: One, the original architecture uses a generator neural network to create images, where there is also a separate discriminator network that judges its work and teaches it to do better. Instead, this work uses two discriminator neural networks, one checks whether the images look good one by one, and one more discriminator for overlooking whether the sequence of these images would pass as a video. This discriminator cracks down on the generator network if it creates sequences that are not temporally coherent and this is why we have minimal flickering in the output videos. Fantastic idea. Two, to ease the training process, it also does it progressively, which means that the network is first faced with an easier version of the problem that progressively gets harder over time. If you have a look at the paper, you will see that the training is both progressive in terms of space and time. I love this idea too. Three, it also uses a flow map that describes the changes that took place since the previous frame. Note that this pix2pix algorithm was published in 2017, a little more than a year ago. I think that is a good taste of the pace of progress in machine learning research. Up to 2k resolution, 30 seconds of video, and the source code is available. Congratulations folks, this paper is something else. Thanks for watching and for your generous support, and I'll see you next time!"
268,DeepMind's New AI Diagnoses Eye Conditions,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this video series, we often see how these amazing new machine learning algorithms can make our lives easier, and fortunately, some of them are also useful for serious medical applications. Specifically, medical imaging. Medical imaging is commonly used in most healthcare systems where an image of a chosen set of organs and tissues is made for a doctor to look at and decide whether medical intervention is required. The main issue is that the amount of diagnostic images out there in the wild increases at a staggering pace, and it makes it more and more infeasible for doctors to look at. But wait a minute, as more and more images are created, this also means that we have more training data for machine learning algorithms, so at the same time as human doctors get more and more swamped, the AI should get better and better over time! These methods can process orders of magnitude more of these images than humans, and after that, the final decision is put back into the hands of the doctor, who can now focus more on the edge cases and prioritize which patients should be seen immediately. This work from scientists at DeepMind was trained on about 14 thousand optical coherence tomography scans, this is the OCT label you see on the the left, these images are cross sections of the human retina. We first start our with this OCT scan, then, a manual segmentation step follows, where a doctor marks up this image to show where the most relevant parts, like the retinal fluids or the elevations of retinal pigments are. Before we proceed, let's stop here for a moment and look at some images of how the network can learn from the doctors and reproduce these segmentations by itself. Look at that! It's almost pixel perfect! This looks like science fiction. Now that we have the segmentation map, it is time to perform classification. This means that we look at this map and assign a probability to each possible condition that may be present. Finally, based on these, a final verdict is made whether the patient needs to be urgently seen, or just a routine check, or perhaps no check is required. The algorithm also learns this classification step and creates these verdicts itself. And of course, the question naturally arises: how accurate is this? Well, let's look at the confusion matrices! A confusion matrix shows us how many of the urgent cases were correctly classified as urgent, and how often it was misclassified as something else and what the something else was. The same analysis is performed to all other classes. Here is how the retina specialist doctors did, and here is how the AI did. I'll leave it there for a few seconds for you to inspect it. Really good! Here is also a different way of aggregating this data the algorithm did significantly better than all of the optometrists and matched the performance of the number one retina specialist. I wouldn't believe any of these results if I didn't see these reports with my own eyes in the paper. An additional advantage of this technique is that it works on different kinds of imaging devices and it is among the first methods that works with 3D data. Another plus that I really liked is that this was developed as a close collaboration with a top tier eye hospital in London to make sure that the results are as practical as possible. The paper contains a ton of more information, so make sure to have a look! This was a herculean effort from the side of DeepMind, and the results are truly staggering. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
269,Should an AI Learn Like Humans?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper reveals us a fundamental difference between how humans and machines learn. Imagine the following situation: you are given a video game with no instructions, you start playing it, and the only information you get is a line of text when you successfully finished the game. That's it! So far, so good, this is relatively easy to play because the visual cues are quite clear: the pink blob looks like an adversary, and what the spikes do is also self-explanatory. This is easy to understand, so we can finish the game in less than a minute. Easy! Now, let's play this. Whoa! What is happening? Even empty space looks like as if it were a solid tile. I am not sure if I can finish this version of the game, at least not in a minute for sure. So, what is happening here is that some of artwork of the objects has been masked out. As a result, this version of the game is much harder to play for humans. So far, this is hardly surprising, and if that would be it, this wouldn't have been a very scientific experiment. However, this is not the case. So to proceed from this point, we will try to find what makes humans learn so efficiently, but not by changing everything at once, but by trying to change and measure only one variable at a time. How about this version of the game? This is still manageable, since the environment remains the same, only the objects we interact with have been masked. Through trial and error, we can find out the mechanics of the game. What about reversing the semantics? Spikes now became tasty ice cream, and the shiny gold conceals an enemy that eats us. Very apt, I have to say. Again, with this, the problem suddenly became more difficult for humans as we need some trial and error to find out the rules. After putting together several other masking strategies, they measured amount of time, the number of deaths and interactions that were required to finish the level. I will draw your attention mainly to the blue lines which show which variable caused how much degradation in the performance of humans. The main piece of insight is not only that these different visual cues throw off humans, but it tells us variable by variable, and also, by how much. An important insight here is that highlighting important objects and visual consistency are key. So what about the machines? How are learning algorithms affected? These are the baseline results. Adding masked semantics? Barely an issue. Masked object identities? This sounds quite hard, right? Barely an issue. Masked platforms and ladders? Barely an issue. This is a remarkable property of learning algorithms, as they don't only think in terms of visual cues, but in terms of mathematics and probabilities. Removing similarity information throws the machines off a bit, which is understandable, because the same objects may appear as if they were completely different. There is more analysis on this in the paper, so make sure to have a look. So, what are the conclusions here? Humans are remarkably good at reusing knowledge, and reading and understanding visual cues. However, if the visual cues become more cryptic, their performance drastically decreases. When machines start playing the game, at first, they have no idea which character they control, how gravity works or how to defeat enemies, or that keys are required to open doors. However, they learn these tricky problems and games much easier and quicker, because these mind-bending changes, as you remember, are barely an issue. Note that you can play the original and the obfuscated versions on the author's website as well. Also note that we really have only scratched the surface here, the paper contains a lot more insights. So, it is the perfect time to nourish your mind with a paper, so make sure to click it in the video description and give it a read. Thanks for watching and for your generous support, and I'll see you next time!"
270,This Painter AI Fools Art Historians 39% of the Time,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Style transfer is a mostly AI-based technique where we take a photograph, put a painting next to it, and it applies the style of the painting to our photo. A key insight of this new work is that a style is complex and it can only be approximated with one image. One image is just one instance of a style, not the style itself. Have a look here if we take this content image, and use Van Gogh's ""Road with Cypress and Star"" painting as the art style, we get this. However, if we would have used Starry Night instead, it would have resulted in this. This is not learning about a style, this is learning a specific instance of a style! Here you see two previous algorithms that were instead, trained on a collection of works from Van Gogh. However, you see that they are a little blurry and lack detail. This new technique is able to address this really well also, look at how convincingly it stylized the top silhouettes of the bell tower. It can also deal with HD videos at a reasonable speed of 9 of these images per second. Very tasty, love it! And of course, as style transfer is a rapidly growing field, there are ample comparisons in the paper against other competing techniques. The results are very convincing I feel that in most cases, it represents the art style really well and can decide where to leave the image content similar to the input and where to apply the style so the overall outlook of the image remains similar. So we can look at these results and discuss who likes which one all day long, but there are also other, more objective ways of evaluating such an algorithm. What is really cool is that the technique was tested by human art history experts, and they not only found this method to be the most convincing of all the other style transfer methods, but also thought that the AI-produced paintings were from an artist 39% of the time. So this means that the algorithm is able to learn the essence of an artistic style from a collection of images. This is a huge leap forward. Make sure to have a look at the paper that also describes a new style-aware loss function and differences in the training process of this method as well. And, if you enjoyed this episode and would like to see more, please help us exist through Patreon. In this website, you can support the series and pick up cool perks like early access to these videos, deciding the order of future episodes, and more. You know the drill, a dollar a month is almost nothing, but it keeps the papers coming. We also support cryptocurrencies, you'll find more information about this in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
271,These Neural Networks Empower Digital Artists,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we have seen many times how good neural network-based solutions are at image classification. This means that the network looks at an image and successfully identifies its contents. However, neural network-based solutions are also capable of empowering art projects by generating new, interesting images. This beautifully written paper explores how a slight tweak to a problem definition can drastically change the output of such a neural network. It shows how many of these research works can be seen as the manifestation of the same overarching idea. For instance, we can try to visualize what groups of neurons within these networks are looking for, and we get something like this. The reason for this is that important visual features, like the eyes can appear at any part of the image and different groups of neurons look for it elsewhere. With a small modification, we can put these individual visualizations within a shared space and create a much more consistent and readable output. In a different experiment, it is shown how a similar idea can be used with Compositional Pattern Producing Networks, or CPPNs in short. These networks are able to take spatial positions as an input and produce colors on the output, thereby creating interesting images of arbitrary resolution. Depending on the structure of this network, it can create beautiful images that are reminiscent of light-paintings. And here you can see how the output of these networks change during the training process. They can also be used for image morphing as well. A similar idea can be used to create images that are beyond the classical 2D RGB images, and create semi-transparent images instead. And there is much, much more in the paper, for instance, there is an interactive demo that shows how we can seamlessly put this texture on a 3D object. It is also possible to perform neural style transfer on a 3D model. This means that we have an image for style, and a target 3D model, and, you can see the results over here. This paper is a gold mine of knowledge, and contains a lot of insights on how neural networks can further empower artists working in the industry. If you read only one paper today, it should definitely be this one, and this is not just about reading, you can also play with these visualizations, and as the source code is also available for all of these, you can also build something amazing on top of them. Let the experiments begin! So, this was a paper from the amazing Distill journal, and just so you know, they may be branching out to different areas of expertise, which is amazing news. However, they are looking for a few helping hands to accomplish that, so make sure to click the link to this editorial update in the video description to see how you can contribute. I would personally love to see more of these interactive articles. Thanks for watching and for your generous support, and I'll see you next time!"
272,"Neural Material Synthesis, This Time On Steroids","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. With this technique, we can take a photograph of a desired material, and use a neural network to create a digital material model that matches it that we can use in computer games and animation movies. We can import real world materials in our virtual worlds, if you will. Typically, to do this, an earlier work required two photographs, one with flash, and one without to get enough information about the reflectance properties of the material. Then, a followup AI paper was able to do this from only one image. It doesn't even need to turn the camera around the material to see how it handles reflections, but can learn all of these material properties from only one image. Isn't that miraculous? We talked about this work in more detail in Two Minute Papers episode 88, that was about two years ago, I put a link to it in the video description. Let's look at some results with this new technique! Here you see the photos of the input materials and on the right, the reconstructed material. Please note that this reconstruction means that the neural network predicts the physical properties of the material, which are then passed to a light simulation program. So on the left, you see reality, and on the right, the prediction plus simulation results under a moving point light. It works like magic. Love it. As you see in the comparisons here, it produces results that are closer to the ground truth than previous techniques. So what is the difference? This method is designed in a way that enables us to create a larger training set for more accurate results. As you know, with learning algorithms, we are always looking for more and more training data. Also, it uses two neural networks instead of one, where one of them looks at local nearby features in the input, and the other one runs in parallel and ensures that the material that is created is also globally correct. Note that there are some highly scattering materials that this method doesn't support, for example, fabrics or human skin. But since producing these materials in a digital world takes quite a bit of time and expertise, this will be a godsend for the video games and animation movies of the future. Thanks for watching and for your generous support, and I'll see you next time!"
273,This Robot Learned To Clean Up Clutter,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This robot was tasked to clean up this table. Normally, anyone who watches this series knows that would be no big deal for any modern learning algorithm. Just grab it, right? Well, not in this case, because reason number one: several objects are tightly packed together, and reason number two, they are too wide to hold with the fingers. What this means is that the robot needs to figure out a series additional actions to push the other pieces around and finally, grab the correct one. Look! It found out that sometimes, pushing helps grasping by making space for the fingers to grab these objects. This is a bit like the Roomba vacuum cleaner robot, but even better, for clutter. Really cool. This robot arm works the following way: it has an RGB-D camera, which endows it with the ability to see both color and depth. Now that we have this image, we have not one, but two neural networks looking at it: one is used to predict the utility of pushing at different possible locations, and one for grasping. Finally, a decision is made as to which motion would lead to the biggest improvement in the state of the table. So, what about the training process? As you see, the speed of this robot arm is limited, and we may have to wait for a long time for it to learn anything useful and not just flail around destroying other nearby objects. The solution includes my favorite part training the robot within a simulated environment, where these commands can be executed within milliseconds, speeding up the training process significantly. Our hope is always that the principles learned within the simulation applies to reality. Checkmark. The simulation is also very useful to make comparisons with other state of the art algorithms easier. And, do you know what the bane of many-many learning algorithms is? Generalization. This means that if the technique was designed well, it can be trained on matte looking, wooden blocks, and it will do well when it encounters new objects that are vastly different in shape and appearance. And as you see on the right, remarkably, this is exactly the case. Checkmark. This takes us one step closer to learning algorithms that can see the world around us, interpret it, and make proper decisions to carry out a plan. Thanks for watching and for your generous support, and I'll see you next time!"
274,This AI Senses Humans Through Walls,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Pose estimation is an interesting area of research where we typically have a few images or video footage of humans, and we try to automatically extract the pose a person was taking. In short, the input is one or more photo, and the output is typically a skeleton of the person. So what is this good for? A lot of things. For instance, we can use these skeletons to cheaply transfer the gestures of a human onto a virtual character, fall detection for the elderly, analyzing the motion of athletes, and many many others. This work showcases a neural network that measures how the wifi radio signals bounce around in the room and reflect off of the human body, and from these murky waves, it estimates where we are. Not only that, but it also accurate enough to tell us our pose. As you see here, as the wifi signal also traverses in the dark, this pose estimation works really well in poor lighting conditions. That is a remarkable feat. But now, hold on to your papers, because that's nothing compared to what you are about to see now. Have a look here. We know that wifi signals go through walls. So perhaps, this means that...that can't be true, right? It tracks the pose of this human as he enters the room, and now, as he disappears, look, the algorithm still knows where he is. That's right! This means that it can also detect our pose through walls! What kind of wizardry is that? Now, note that this technique doesn't look at the video feed we are now looking at. It is there for us for visual reference. It is also quite remarkable that the signal being sent out is a thousand times weaker than an actual wifi signal, and it also can detect multiple humans. This is not much of a problem with color images, because we can clearly see everyone in an image, but the radio signals are more difficult to read when they reflect off of multiple bodies in the scene. The whole technique work through using a teacher-student network structure. The teacher is a standard pose estimation neural network that looks at a color image and predicts the pose of the humans therein. So far, so good, nothing new here. However, there is a student network that looks at the correct decisions of the teacher, but has the radio signal as an input instead. As a result, it will learn what the different radio signal distributions mean and how they relate to human positions and poses. As the name says, the teacher shows the student neural network the correct results, and the student learns how to produce them from radio signals instead of images. If anyone said that they were working on this problem ten years ago, they would have likely ended up in an asylum. Today, it's reality. What a time to be alive! Also, if you enjoyed this episode, please consider supporting the show at Patreon.com/twominutepapers. You can pick up really cool perks like getting your name shown as a key supporter in the video description and more. Because of your support, we are able to create all of these videos smooth and creamy, in 4k resolution and 60 frames per second and with closed captions. And, we are currently saving up for a new video editing rig to make better videos for you. We also support one-time payments through PayPal and the usual cryptocurrencies. More details about all of these are available in the video description, and as always. Thanks for watching and for your generous support, and I'll see you next time!"
275,Brain-to-Brain Communication is Coming!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is an episode that doesn't have the usual visual fireworks and is expected to get fewer clicks, but it is an important story to tell, and because of your support, we are able to cover a paper like this. So now, get this, this is a non-invasive brain-to-brain interface that uses EEG to record brain signals and TMS to deliver information to the brain. The non-invasive part is quite important, it basically means that we don't need to drill a hole in the head of the patients. That's a good idea. This image shows three humans connected via computers, 2 senders and one receiver. The senders provide information to the receiver about something he would otherwise not know about, and we measure if they are able to collaboratively solve a problem together. These people never met each other, and don't even know each other, and they can collaborate through this technique directly via brain signals. Wow! The BCI means brain-computer interface, and the CBI, as you guessed, the computer-brain interface. So these brain signals can be encoded and decoded and freely transferred between people and computers. Insanity. After gathering all this information, the receiver makes a decision, which the senders also have access to, and can transmit some more information if necessary. So what do they use it for? Of course, to play Tetris! Jokes aside, this is a great experiment where the goal is to clear a line. Simple enough, right? Not so much, because there is a twist. The receiver only sees what you see here on the left side. This is the current piece we have to place on the field, but the receiver has no idea how to rotate it because he doesn't see its surroundings. But the senders do, so they transmit the appropriate information to the receiver who will now be able to make an informed decision as to how to rotate the piece correctly to clear a line. So, does it work? The experiment is designed in a way that there is a 50% chance to be right without any additional information for the receiver, so this will be the baseline result. And the results are between 75% and 85%, which means that the interface is working, and the brain-to-brain collaboration is now a reality. I am out of words. The paper also talks about brain-to-brain social networks, and all kinds of science fiction like that. My head is about to explode with the possibilities, who knows, maybe in a few years, we can make a superintelligent brain that combines all of our expertise and does research for all of us. Or, writes Two Minute Papers episodes. This paper is a must read. Do you have any other ideas as to how this could enhance our lives? Let me know below in the comments. Thanks for watching and for your generous support, and I'll see you next time!"
276,"Multilayer Light Simulations: More Beautiful Images, Faster","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we are going to talk about the craft of simulating rays of light to create beautiful images, just like the ones you see here. And when I say simulating rays of light, I mean not a few, but millions and millions of light rays need to be computed, alongside with how they get absorbed or scattered off of our objects in a virtual scene. Initially, we start out with a really noisy image, and as we add more rays, the image gets clearer and clearer over time. The time it takes for these images to clean up depends on the complexity of the geometry and our material models, and one thing is for sure: rendering materials that have multiple layers is a nightmare. This paper introduces an amazing new multilayer material model to address that. Here you see an example where we are able to stack together transparent and translucent layers to synthesize a really lifelike scratched metal material with water droplets. Also, have a look at these gorgeous materials. And note that these are all virtual materials that are simulated using physics and computer graphics. Isn't this incredible? However, some of you Fellow Scholars remember that we talked about multilayered materials before. So what's new here? This new method supports more advanced material models that previous techniques were either unable to simulate, or took too long to do so. But that's not all have a look here. What you see is an equal-time comparison, which means that if we run the new technique against the older methods for the same amount of time, it is easy to see that we will have much less noise in our output image. This means that the images clear up quicker and we can produce them in less time. It also supports my favorite, multiple importance sampling, an aggressive noise-reduction technique by Eric Veach which is arguably one of the greatest inventions ever in light transport research. This ensures that for more difficult scenes, the images clean up much, much faster and has a beautiful and simple mathematical formulation. Super happy to see that it also earned him a technical Oscar award a few years ago. If you are enjoying learning about light transport, make sure to check out my course on this topic at the Technical University of Vienna. I still teach this at the University for 20 Master students at a time and thought that the teachings shouldn't only be available for a lucky few people who can afford a college education. Clearly, the teachings should be available for everyone, so we recorded it and put it online, and now everyone can watch it, free of charge. I was quite stunned to see that more than 10 thousand people decided to start it, so make sure to give it a go if you're interested! And just one more thing: as you are listening to this episode, I am holding a talk at the EU's Political Strategy Centre. And the objective of this talk is to inform political decisionmakers about the state of the art in AI so they can make more informed decisions for us. Thanks for watching and for your generous support, and I'll see you next time!"
277,This AI Learned How To Generate Human Appearance,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we often discuss that neural networks are extraordinarily useful for classification tasks. This means that if we give them an image, they can tell us what's on it, which is great for self-driving cars, image search, and a variety of other applications. However, fewer people know that they can also be used for image generation. We've seen many great examples of this where NVIDIA's AI was able to dream up high-resolution images of imaginary celebrities. This was done using a generative adversarial network, an architecture where two neural networks battle each other. However, these methods don't work too well if we have too much variation in our datasets. For instance, they are great for faces, but not for synthesizing an entire human body. This particular technique uses a different architecture, and as a result, can synthesize an entire human body, and is also able to synthesize both shape and appearance. You will see in a moment that because of that, it can do magical things. For instance, in this example, all we have is one low-quality image of a test subject as an input, and we can give it a photo of a different person. What happens now is that the algorithm runs pose estimation on this input, and transforms our test subject into that pose. The crazy thing about this is that it even creates views for new angles we didn't even have access to! In this other experiment, we have one image on the left. What we can do here is that we specify not a person, but draw the pose directly, indicating that we wish to see our test subject in this pose, and the algorithm is also able to create an appropriate new image. And again, it works for angles that require information that we don't have access to. These new angles show that the technique understands the concept of shorts or trousers...although it seems to forget to put on socks sometimes. Truth be told, I don't blame it. What is even cooler is that it seems to behave very similarly for a variety of different inputs. This is non-trivial as this property doesn't just emerge out of thin air, and will be a great selling point for this new method. It also supports a feature where we need to give a crude drawing to the algorithm, and it will transform it into a photorealistic image. However, it is clear that there are many-many ways to fill this drawing with information, so how do we tell the algorithm what appearance we are looking for? Well, worry not, because this technique can also perform appearance transfer. This means that we can exert artistic control over the output by providing a photo of a different object, and it will transfer the style of this photo to our input. No artistic skills needed, but good taste is still as much of a necessity as ever. Yet another AI that will empower both experts and novice users alike. And while we are enjoying these amazing results, or even better, if you have already built up an addiction for the papers, you can keep it in check by supporting us on Patreon and in return, getting access to these videos earlier. You can find us through Patreon.com/TwoMinutePapers. There is a link to it in the video description, and as always, to the paper as well. Thanks for watching and for your generous support, and I'll see you next time!"
278,Real-Time Holography Simulation!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If we wish to populate a virtual world with photorealistic materials, the last few years have offered a number of amazing techniques to do so. We can obtain such a material from a flash and no-flash photograph pair of the target material, and having a neural network create a digital version of it, or remarkably, even just one photograph is enough to perform this. This footage that you see here shows these materials after they have been rendered by a light simulation program. If we don't have physical access to these materials, we can also use a recent learning algorithm to learn our preferences and recommend new materials that we would enjoy. However, whenever I publish such a video, I always get comments asking: ""but what about the more advanced materials?"", and my answer is, you are right! Have a look at this piece of work which is about acquiring printed holographic materials. This means that we have physical access to this holographic pattern, put a camera close by, and measure data in a way that can be imported into a light simulation program to make a digital copy of it. This idea is much less far-fetched than it sounds, because we find such materials in many everyday objects, like bank notes, gift bags, clothing, or of course or security holograms. However, it is also quite difficult. Look here! As you see, these holographic patterns are quite diverse, and a well-crafted algorithm would have to be able to capture this rotation effect, circular diffractive areas, firework effects and even this iridescent glitter. That is quite a challenge! This paper proposes two novel techniques to approach this problem. The first one assumes that there is some sort of repetition in the visual structure of the hologram and takes that into consideration. As a result, it can give us high-quality results by only taking 1 to 5 photographs of the target material. The second method is more exhaustive and needs more specialized hardware, but, in return, can deal with arbitrary structures, and requires at least 4 photographs at all times. These are both quite remarkable just think about the fact that these materials look different from every viewing angle, and they also change over the surface of the object. And for the first technique, we don't need sophisticated instruments, only a consumer DSLR camera is required. The reconstructed digital materials can be used in real time, and what's more, we can also exert artistic control over the outputs by modifying the periodicities of the material. How cool is that! And, if you are about to subscribe to the series or you are already subscribed, make sure to click the bell icon, or otherwise, you may miss future episodes. That would be a bummer, because I have a lot more amazing papers to show you. Thanks for watching and for your generous support, and I'll see you next time!"
279,"Full-Time Papers, Maybe Someday?","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This video is not about a paper, but the about the video series itself. I don't do this often, but for the sake of transparency, I wanted to make sure to tell you about this. We have recently hit 175.000 subscribers on the channel! I find this number almost unfathomable. And please know that this became a possibility only because of you, so I would like to let you know how grateful I am for your support here on YouTube and Patreon. As most of you know, I still work as a full-time researcher at the Technical University of Vienna and I get the question ""why not go full time on Two Minute Papers?"" from you Fellow Scholars increasingly often. The answer is that over time, I would love to, but our current financial situation does not allow it. Let me explain that. First, I tried the run the channel solely on YouTube ads, which led to a rude awakening most people (myself included) are very surprised when they hear that the rates had become so low that around the first one million viewer mark, the series earned less than a dollar a day. Then, we introduced Patreon, and with your support, now we are able to buy proper equipment to make better videos for you. I can, without hesitation, say that you are the reason this channel can exist. Everything is better this way: now we have two independent revenue sources (Patreon being the most important), however, whenever YouTube or Patreon monetization issues arise, you see many other channels disappearing into the ether. I am terrified of this and I want to do everything I possibly can to make sure that this does not happen to us and we can keep running this series for a long-long time. However, if anything happens to any of these revenue streams, simply put, we are toast. Even though it would be a dream come true, because of this, it would be irresponsible to go full time on the papers. To remedy this, we have been thinking about introducing a third revenue stream with sponsorships for a small amount of videos each month. The majority, 75% of the videos would remain Patreon supported, and the remaining 25% would be sponsored, just enough to enable me and my wife to do this full time in the future. There would be no other changes to the videos, Patreon supporters get all of them in early access, and as before, I choose the papers too. With this, if something happens to any one of the revenue streams, we would be able to keep the series afloat without any delays. I would also have more time to every now and then, fly out and inform key political decision makers on the state of AI so they can make better decisions for us. Everything else would remain the same, the videos would arrive more often in time, and the dream could perhaps come true. I think transparency is of utmost importance and wanted to make sure to inform you before any change happens. I hope you are okay with this. And, if you are a company and you are interested in sponsoring the series, let's talk. As always thanks for watching and for your generous support, and I'll see you next time!"
280,This AI Shows Us the Sound of Pixels,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a neural network-based method that is able to show us the sound of pixels. What this means is that it separates and localizes audio signals in videos. The two keywords are separation and localization, so let's take a look at these one by one. Localization means that we can pick a pixel in the image and it show us the sound that comes from that location, and the separation part means that ideally, we will only hear that particular sound source. Let's have a look at an example. Here is an input video. And now, let's try to separate the sound of the chello and see if it knows where it comes from. Same with the guitar. Now for a trickier question...even though there are sound reverberations off the walls, but the walls don't directly emit sound themselves, so I am hoping to hear nothing now, let's see... flat signal, great! So, how does this work? It is a neural-network based solution that has watched 60 hours of musical performances to be able to pull this off, and it learns that a change in sound can often be tracked back to a change in the video footage as a musician is playing an instrument. As a result, get this, no supervision is required. This means that we don't need to label this data, or in other words, we don't need to specify how each pixel sounds, it learns to infer all this information from the video and sound signals by itself. This is huge, and otherwise, just imagine how many work-hours that would require to annotate all this data. And, another cool application is that if we can separate these signals, then we can also independently adjust the sound of these instruments. Have a look. Now, clearly, it is not perfect as some frequencies may bleed over from one instrument to the other, and there also are other methods to separate audio signals, but this particular one does not require any expertise, so I see a great value proposition there. If you wish to create a separate version of a video clip and use it for karaoke, or just subtract the guitar and play it yourself, I would look no further. Also, you know the drill, this will be way better a couple papers down the line. So, what do you think? What possible applications do you envision for this? Where could it be improved? Let me know below in the comments. Thanks for watching and for your generous support, and I'll see you next time!"
281,This AI Learned To Isolate Speech Signals,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a neural network-based technique that can perform audio-visual separation. Before we talk about what that is, I will tell you what it is not. It is not what we've seen in the previous episode where we could select a pixel and listen to it, have  a look. This one is different. This new technique can clean up an audio signal by suppressing the noise in a busy bar, even if the source of the noise is not seen in the video. It can also enhance the voice of the speaker at the same time. Let's listen. Or, if we have a skype-meeting with someone in a lab or a busy office where multiple people are speaking nearby, we can also perform a similar speech separation, which would be a godsend for future meetings. And I think if you are a parent, the utility this example needs no further explanation. I am not sure if I ever encountered the term ""screaming children"" in the abstract of an AI paper, so that one was also a first here. This is a super difficult task, because the AI needs to understand what lip motions correspond to what kind of sounds, which is different for all kinds of languages, age groups, and head positions. To this end, the authors put together a stupendously large dataset with almost 300.000 videos with clean speech signals. This dataset is then run through a multi-stream neural network that detects the number of human faces within the video, generates small thumbnails of them, and observes how they move over time. It also analyzes the audio signal separately, then fuses these elements together with a recurrent neural network to output the separated audio waveforms. A key advantage of this architecture and training method is that as opposed to many previous works, this is speaker-independent, therefore we don't need specific training data from the speaker we want to use this on. This is a huge leap in terms of usability. The paper also contains an excellent demonstration of this concept by taking a piece of footage from Conan O'Brien's show where two comedians were booked for the same time slot and talk over each other. The result is a performance where it is near impossible to understand what they are saying, but with this technique, we can hear both of them one by one, crystal clear. You see some results over here, but make sure to click the paper link in the description to hear the sound samples as well. Thanks for watching and for your generous support, and I'll see you next time!"
282,This Curious AI Beats Many Games...and Gets Addicted to the TV,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is a learning algorithm that chooses a set of actions in an environment to maximize a score. This class of techniques enables us to train an AI to master video games, avoiding obstacles with a drone, cleaning up a table with a robot arm, and has many more really cool applications. We use the word score and reward interchangeably, and the goal is that over time, the agent has to learn to maximize a prescribed reward. So where should the rewards come from? Most techniques work by using extrinsic rewards. Extrinsic rewards are only a half-solution as they need to come from somewhere, either from the game in the form of a game score, which simply isn't present in every game. And even if it is present in a game, it is very different for Atari breakout and for instance, a strategy game. Intrinsic rewards are designed to come to the rescue, so the AI would be able to completely ignore the in-game score and somehow have some sort of inner motivation to drive an AI to complete a level. But what could possibly be a good intrinsic reward that would work well on a variety of tasks? Shouldn't this be different from problem to problem? If so, we are back to square one. If we are to call our learner intelligent, then we need one algorithm that is able to solve a large number of different problems. If we need to reprogram it for every game, that's just a narrow intelligence. So, a key finding of this paper is that we can endow the AI with a very human-like property curiosity. Human babies also explore the world out of curiosity and as a happy side-effect, learn a lot of useful skills to navigate in this world later. However, as in our everyday speech, the definition of curiosity is a little nebulous, we have to provide a mathematical definition for it. In this work, this is defined as trying to maximize the number of surprises. This will drive the learner to favor actions that lead to unexplored regions and complex dynamics in a game. So, how do these curious agents fare? Well, quite good! In Pong, when the agent plays against itself, it will end up in long matches passing the ball between the two paddles. How about bowling? Well, I cannot resist but quote the authors for this one. The agent learned to play the game better than agents trained to maximize the (clipped) extrinsic reward directly. We think this is because the agent gets attracted to the difficult-to-predict flashing of the scoreboard occurring after the strikes. With a little stretch one could perhaps say that this AI is showing signs of addiction. I wonder how it would do with modern mobile games with loot boxes? But, we'll leave that for future work now. How about Super Mario? Well, the agent is very curious to see how the levels continue, so it learns all the necessary skills to beat the game. Incredible. However, the more seasoned Fellow Scholars immediately find that there is a catch. What if we sit down the AI in front of a TV that constantly plays new material? You may think this is some kind of a joke, but it's not. It is a perfectly valid issue, because due to its curiosity, the AI would have to stay there forever and not start exploring the level. This is the good old definition of TV addiction. Talk about humanlike properties. And sure enough, as soon as we turn off the TV, the agent gets to work immediately. Who would have thought! The paper notes that this challenge needs to be dealt with over time, however, the algorithm was tested on a large variety of problems, and it did not come up in practice. And the key insight is that curiosity is not only a great replacement for extrinsic rewards, the two are often aligned, but curiosity, in some cases, is even superior to that. That is an amazing value proposition for something that we can run on any problem without any additional work. So, curious agents that are addicted to flashing score screens and TVs. What a time to be alive! And, if you enjoyed this episode and you wish to help us on our quest to inform even more people about these amazing stories, please consider supporting us on Patreon.com/TwoMinutePapers. You can pick up cool perks there to keep your papers addiction in check. As always, there is a link to it and to the paper in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
283,This AI Learns Acrobatics by Watching YouTube,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If we have an animation movie or a computer game where, like in any other digital medium, we wish to have high-quality, lifelike animations for our characters, we likely have to use motion capture. Motion capture means that we put an actor in the studio, and we ask this person to perform cartwheels and other motion types that we wish to transfer to our virtual characters. This works really well, but recording and cleaning all this data is a very expensive and laborious process. As we are entering the age of AI, of course, I wonder if there is a better way to do this? Just think about it...we have no shortage of videos here on the Youtube about people performing cartwheels and other moves, and we have a bunch of learning algorithms that know what pose they are taking during the video. Surely we can make something happen here, right? Well, yes, and no. A few methods already exist to perform this, but all of them have dealbreaking drawbacks. For instance, this previous work predicts the body poses for each frame, but each of them have small individual inaccuracies that produce this annoying flickering effect. Researchers like to refer to this as the lack of temporal coherence. But, this new technique is able to remedy this. Great result! This new work also boasts a long list of other incredible improvements. For instance, the resulting motions are also simulated in a virtual environment, and it is shown that they are quite robust so much so, that we can throw a bunch of boxes against the AI, and it still can adjust to it. Kind of. These motions can be retargeted to different body shapes. You can see as it is demonstrated here quite aptly with this neat little nod to Boston Dynamics. It can also adapt to challenging new environments, or, get this, it can even work from a single photo instead of a video by completing the motion seen within. What kind of wizardry is that? How could it possibly perform that? It works the following way. First, we take an input photo or video, and perform pose estimation on it. But this is still a per-frame computation, and you remember that this doesn't give us temporal consistency. This motion reconstruction step ensures that we have smooth transitions between the poses. And now comes the best part: we start simulating a virtual environment, where a digital character tries to move its body parts to perform these actions. If we do this, we can not only reproduce these motions, but also continue them. This is where the wizardry lies. If you read the paper, which you should absolutely do, you will see that it uses OpenAI's amazing Proximal Policy Optimization algorithm to find the best motions. Absolutely amazing. So this can perform and complete a variety of motions, adapts to more challenging landscapes, and do all this in a temporally smooth manner. However, the Gangnam style dance still proves to be too hard. The technology is not there yet. We also thank Insilico Medicine for supporting this video. They work on AI-based drug discovery and aging research. They have some unbelievable papers on these topics. Make sure to check them out, and this paper as well in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
284,Building a Curious AI With Random Network Distillation,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In a previous episode, we talked about a class of learning algorithms that were endowed with curiosity. This new work also showcases a curious AI that aims to solve Montezuma's revenge which is a notoriously difficult platform game for an AI to finish. The main part of the difficulty arises from the fact that the AI needs to be able to plan for longer time periods, and interestingly, it also needs to learn that short-term rewards don't necessarily mean long-term success. Let's have a look at an example quoting the authors: ""There are four keys and six doors spread throughout the level. Any of the four keys can open any of the six doors, but are consumed in the process. To open the final two doors the agent must therefore forego opening two of the doors that are easier to find and that would immediately reward it for opening them."" So what this means is that we have a tricky situation, because the agent would have to disregard the fact that it is getting a nice score from opening the doors, and understand that these keys can be saved for later. This is very hard for an AI to resist, and, again, curiosity comes to the rescue. Curiosity, at least, this particular definition of it works in a way that the harder to guess for the AI what will happen, the more excited it gets to perform an action. This drives the agent to finish the game and explore as much as possible because it is curious to see what the next level holds. You see in the animation here that the big reward spikes show that the AI has found something new and meaningful, like losing a life, or narrowly avoiding an adversary. As you also see, climbing a ladder is a predictable, boring mechanic that the AI is not very excited about. Later, it becomes able to predict the results even better the second and third time around, therefore it gets even less excited about ladders. This other animation shows how this curious agent explores adjacent rooms over time. This work also introduces a technique, which the authors call random network distillation. This means that we start out from a completely randomly initialized, untrained neural network, and over time, slowly distill it into a trained one. This distillation also makes our neural network immune to the noisy TV problem from our previous episode, where our curious, unassuming agent would get stuck in front of a TV that continually plays new content. It also takes into consideration the score reported by the game, and has an internal motivation to explore as well. And hold on to your papers, because it can not only perform well in the game, but this AI is able to perform better than the average human. And again, remember that no ground truth knowledge is required, it was never demonstrated to the AI how one should play this game. Very impressive results indeed, and you see, the pace of progress in machine learning research is nothing short of incredible. Make sure to have a look at the paper in the video description for more details. We'd also like to send a big thank you to Insilico Medicine for supporting this video. They use AI for research on preventing aging, believe it or not, and are doing absolutely amazing work. Make sure to check them out in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
285,Can an AI Learn To Draw a Caricature?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Style transfer is an interesting problem in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to super fun, and really good looking results. This subfield is only a few years old and has seen a number of amazing papers style transfer for HD images, videos, and some of these forgeries were even able to make professional art curators think that they were painted by a real artist. So, here is a crazy idea -how about using style transfer to create caricatures? Well, this sounds quite challenging. Just think about it a caricature is an elusive art where certain human features are exaggerated, and generally, the human face needs to be simplified and boiled down into its essence. It is a very human thing to do. So how could possibly an AI be endowed with such a deep understanding of, for instance, a human face? That sounds almost impossible. Our suspicion is further reinforced as we look at how previous style transfer algorithms try to deal with this problem. Not too well, but no wonder, it would be unfair to expect great results as this is not what they were designed for. But now, look at these truly incredible results that were made with this new work. The main difference between the older works and this one is that one, it uses generative adversarial networks, GANs in short. This is an architecture where two neural networks learn together -one learns to generate better forgeries, and the other learns to find out whether an image has been forged. However, this would still not create the results that you see here. An additional improvement is that we have not one, but two of these GANs. One deals with style. But, it is trained in a way to keep the essence of the image. And the other deals with changing and warping the geometry of the image to achieve an artistic effect. This leans on the input of a landmark detector that gives it around 60 points that show the location of the most important parts of a human face. The output of this geometry GAN is a distorted version of this point set, which can then be used to warp the style image to obtain the final output. This is a great idea because the amount of distortion applied to the points can be controlled. So, we can tell the AI how crazy of a result we are looking for. Great! The authors also experimented applying this to video. In my opinion, the results are incredible for a first crack at this problem. We are probably just one paper away from an AI automatically creating absolutely mind blowing caricature videos. Make sure to have a look at the paper as it has a ton more results and of course, every element of the system is explained in great detail there. And if you enjoyed this episode and you would like to access all future videos in early access, or, get your name immortalized in the video description as a key supporter, please consider supporting us on patreon.com/TwoMinutePapers. The link is available in the video description. We were able to significantly improve our video editing rig, and this was possible because of your generous support. I am so grateful, thank you so much! And, this is why every episode ends with the usual quote... Thanks for watching and for your generous support, and I'll see you next time!"
286,BigGANs: AI-Based High-Fidelity Image Synthesis,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Approximately a 150 episodes ago, we looked at DeepMind's amazing algorithm that was able to look at a database with images of birds, and it could learn about them so much that we could provide a text description of an imaginary bird type and it would dream up new images of them. It was a truly breathtaking piece of work, and its main limitation was that it could only come up with coarse images. It didn’t give us a lot of details. Later, we talked about NVIDIA's algorithm that started out with such a coarse image, but didn't stop there it progressively recomputed this image many times, each time with more and more details. This was able to create imaginary celebrities with tons of detail. This new work offers a number of valuable improvements over the previous techniques: it can train bigger neural networks with even more parameters, create extremely detailed images with remarkable performance, so much so that if you have a reasonably powerful graphics card, you can run it yourself here. The link is in the video description. Training these neural networks is also more stable than it used to be with previous techniques. As a result, it not only supports creating these absolutely beautiful images, but also gives us the opportunity to exert artistic control on the outputs. I think this is super fun, I could play with this all day long. What's more, we can also interpolate between these images, which means that if we have desirable images A and B, it can compute intermediate images between them, and the challenging part is that these intermediate images shouldn't be some sort of average between the two, which would be gibberish, but they have to be images that are meaningful, and can stand on their own. Look at this! Flying colors. And now comes the best part. The results were measured in terms of their inception score. This inception score defines how recognizable and diverse these generated images are and most importantly, both of these are codified in a mathematical manner to reduce the subjectivity of the evaluation. This score is not perfect by any means, but it typically correlates well with the scores given by humans. The best of the earlier works had an inception score of around 50. And hold on to your papers, because the score of this new technique is no less than 166, and if we would measure real images, they would score around 233. What an incredible leap in technology. And we are even being paid for creating and playing with such learning algorithms. What a time to be alive! A big thumbs up for the authors of the paper for providing quite a bit of information on failure cases as well. We also thank Insilico Medicine for supporting this video. They are using these amazing learning algorithms to create new molecules, identify new protein targets with the aim to cure diseases and aging itself. Make sure to check them out in the video description. They are our first sponsors, and it's been such a joy to work with them. Thanks for watching and for your generous support, and I'll see you next time!"
287,AI Learning Morphology and Movement...at the Same Time!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is a class of learning algorithms that chooses a set of actions in an environment to maximize a score. Typical use-cases of this include writing an AI to master video games or avoiding obstacles with a drone and many more cool applications. What ties most of these ideas together is that whenever we talk about reinforcement learning, we typically mean teaching an agent how to navigate in an environment. A few years ago, a really fun online app surfaced that used a genetic algorithm to evolve the morphology of a simple 2D car with the goal of having it roll as far away from a starting point as possible. It used a genetic algorithm that is quite primitive compared to modern machine learning techniques, and yet it still does well on this, so how about tasking a proper reinforcement learner to optimize the body of the agent? What’s more, what if we would jointly learn both the body and the navigation at the same time? Ok, so what does this mean in practice? Let’s have a look at an example. Here we have an ant that is supported by four legs, each consisting of three parts that are controlled by two motor joints. With the classical problem formulation, we can teach this ant to use these joints to learn to walk, but in the new formulation, not only the movement, but the body morphology is also subject to change. As a result, this ant learned that the body can also be carried by longer, thinner legs and adjusted itself accordingly. As a plus, it also learned how to walk with these new legs and this way, it was able to outclass the original agent. In this other example, the agent learns to more efficiently navigate a flat terrain by redesigning its legs that are now reminiscent of small springs and uses them to skip its way forward. Of course, if we change the terrain, the design of an effective agent also changes accordingly, and the super interesting part here is that it came up with an asymmetric design that is able to climb stairs and travel uphill efficiently. Loving it! We can also task this technique to minimize the amount of building materials used to solve a task, and subsequently, it builds an adorable little agent with tiny legs that is still able to efficiently traverse this flat terrain. This principle can also be applied to the more difficult version of this terrain, which results in a lean, insect-like solution that can still finish this level that uses about 75% less materials than the original solution. And again, remember that not only the design, but the movement is learned here at the same time. While we look at these really fun bloopers, I’d like to let you know that we have an opening at our Institute at the Vienna University of Technology for one PostDoctoral researcher. The link is available in the video description, read it carefully to make sure you qualify, and if you apply through the specified e-mail address, make sure to mention Two Minute Papers in your message. This is an excellent opportunity to read and write amazing papers, and work with some of the sweetest people. This is not standard practice in all countries so I’ll note that you can check the salary right in the call, it is a well-paid position in my opinion, and you get to live in Vienna. Also, your salary is paid not 12, but 14 times a year. That’s Austria for you — it doesn't get any better than that. Deadline is end of January. Happy holidays to all of you! Thanks for watching and for your generous support, and I'll see you early January."
288,DeepMind’s Take on How To Create a Benign AI,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This episode does not have the usual visual fireworks, but I really wanted to cover this paper because it tells a story that is, I think, very important for all of us to hear about. When creating a new AI to help us with a task, we have to somehow tell this AI what we consider to be a desirable solution. If everything goes well, it will find out the best way to accomplish it. This is easy when playing simpler video games, because we can just tell the algorithm to maximize the score seen in the game. For instance, the more bricks we hit in Atari breakout, the closer we get to finishing the level. However, in real life, we don’t have anyone giving us a score to tell us how close we are to our objective. What’s even worse, sometimes we have to make decisions that seem bad at the time, but will serve us well in the future. Trying to save money or studying for a few years longer are typical life decisions that pay off in the long run but may seem undesirable at the time. The opposite is also true, ideas that may sound right at a time may immediately backfire. When in a car chase, don't ask the car AI to unload all unnecessary weights to go faster, or if you do, prepare to be promptly ejected from the car. So, how can we possibly create an AI that somehow understands our intentions and acts in line with them? That’s a challenging question, and is often referred to as the agent alignment problem. It has to be aligned with our values. What can we do about this? Well, short of having a mind-reading device, we can maybe control the behavior of the AI through its reward system. Scientists at DeepMind just published a paper on this topic, where they started their thought process from two assumptions: Assumption number one, quoting the authors: “For many tasks we want to solve, evaluation of outcomes is easier than producing the correct behavior”. In short, it is easier to yell at the TV than to become an athlete. Sounds reasonable, right? Note that from complexity theory, we know that this does not always hold, but it is indeed true for a large number of difficult problems. Assumption number two: User intentions can be learned with high accuracy. In other words, given enough data that somehow relates to our intentions, the AI should be able to learn that. Leaning on these two assumptions, we can change the basic formulation of reinforcement learning in the following way: normally, we have an agent that chooses a set of actions in an environment to maximize a score. For instance, this could mean moving the paddle around to hit as many blocks as possible and finish the level. They extended this formulation in a way that the user can periodically provide feedback on how the score should be calculated. Now, the AI will try to maximize this new score and we hope that this will be more in line with our intentions. Or, in our car chase example, we could modify our reward to make sure we remain in the car and not get ejected. Perhaps the most remarkable property of this formulation is that it doesn’t even require us to for instance, play the game at all to demonstrate our intentions to the algorithm. The formulation follows our principles, and not our actions. We can just sit in our favorite armchair, bend the AI to our will by changing the reward function every now and then, and let the AI do the grueling work. This is like yelling at the TV, except that it actually works. Loving the idea. If you have a look at the paper, you will see a ton more details on how to do this efficiently and a case study with a few Atari games. Also, since this has a lot of implications pertaining to AI safety and how to create aligned agents, an increasingly important topic these days, huge respect for DeepMind for investing more and more of their time and money in this area. Thanks for watching and for your generous support, and I'll see you next time!"
289,This AI Learns From Humans…and Exceeds Them,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a collaboration between DeepMind and OpenAI on using human demonstrations to teach an AI to play games really well. The basis of this work is reinforcement learning, which is about choosing a set of actions in an environment to maximize a score. For some games, this score is typically provided by the game itself, but in more complex games, for instance, ones that require exploration, this score is not too useful to train an AI. In this project, the key idea is to use human demonstrations to teach an AI how to succeed. This means that we can sit down, play the game, show the footage to the AI and hope that it learns something useful from it. Now the most trivial implementation of this would be to imitate the footage too closely, or in other words, simply redo what the human has done. That would be a trivial endeavor, and it is the most common way of misunderstanding what is happening here, so I will emphasize that is not the case. Just imitating what the human player does would not be very useful because one, it puts too much burden on the humans, that’s not what we want, and number two, the AI could not be significantly better than the human demonstrator, that’s also not what we want. In fact, if we have a look at the paper, the first figure shows us right away how badly a simpler imitation program performs. That’s not what this algorithm is doing. What it does instead is that it looks at the footage as the human plays the game, and tries to guess what they were trying to accomplish. Then, we can tell a reinforcement learner that this is now our reward function and it should train to become better at that. As you see here, it can play an exploration-heavy game such as Atari “Hero”, and in the footage here above you see the rewards over time, the higher the better. This AI performs really well in this game, and significantly outperforms reinforcement learner agents trained from scratch on Montezuma’s revenge as well, although it can still get stuck on a ladder. We discussed earlier a curious AI that was quickly getting bored by ladders and moved on to more exciting endeavors in the game. The performance of the new agent seems roughly equivalent to an agent trained from scratch in the game Pong, presumably because of the lack of exploration and the fact that it is very easy to understand how to score points in this game. But wait, in the previous episode we just talked about an algorithm where we didn’t even need to play, we could just sit in our favorite armchair and direct the algorithm. So why play? Well, just providing feedback is clearly very convenient, but as we can only specify what we liked and what we didn’t like, it is not very efficient. With the human demonstrations here, we can immediately show the AI what we are looking for, and, as it is able to learn the principles and then improve further, and eventually become better than the human demonstrator, this work provides a highly desirable alternative to already existing techniques Loving it. If you have a look at the paper, you will also see how the authors incorporated a cool additional step to the pipeline where we can add annotations to the training footage, so make sure to have a look! Also, if you feel that a bunch of these AI videos a month are worth a dollar, please consider supporting us at Patreon.com/twominutepapers. You can also pick up cool perks like getting early access to all of these episodes, or getting your name immortalized in the video description. We also support cryptocurrencies and one-time payments, the links and additional information to all of these are available in the video description. With your support, we can make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
290,6 Life Lessons I Learned From AI Research,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is going to be a weird, non-traditional episode. Not the usual Two Minute Papers. Hope you’ll enjoy it and if you finished the video, please let me know in the comments what you think about it. So let's start. Life lessons I learned from ai research. Number one, you need an objective. Before we start training a neural network to perform, for instance, image classification, we need a bunch of training data. This we can feed to this neural network, telling that this image depicts a cat, and this one is not a cat, but an ostrich. We also need to specify a loss function. This is super important, because this loss function is used to make sure that the neural network trains itself in a way that its predictions will be similar to the training data it is being fed. It is also referred to as an objective or objective function to indicate that we know precisely what we are looking for and that’s what the neural network should do. This is a way to measure how the neural network is progressing, and without this, it is useless. Similarly, in an other learning problem, we specify an objective for this ant, which, in this case, is to be able to traverse as far from a starting as possible, and it reconfigures its body type and movement to be able to score high on our objective. And, this leads us to the second lesson: a change in the objective changes the strategy required to achieve it. Look here in a different problem definition, we can specify a different objective, for instance, a different terrain, and you see that if we wish to succeed here, we need a vastly different body type. Form follows function. And in this other case, the objective is to be able to traverse efficiently, but with minimal material use for the legs. The solution, again, changes accordingly to the objective. New objectives require new strategies. Number three: If the objective was wrong, do not worry, and aim again! Have a look at AlphaGo. This is DeepMind’s algorithm that was able to defeat some of the best players in the world in the game of Go. This was a highly non-trivial achievement as the space of possible moves is so stupendously large that it is impossible to evaluate every move. Instead, it tries to aggressively focus on a smaller number of possible moves, and tries to simulate the result of these moves. If the move leads to an improvement in our position, it is a good one, if not, it should be avoided. Sounds simple, right? Well, we have an objective, that’s great, but, initially, it has a really bad predictor, which means that it is really bad at judging which move is good and which one isn’t. However, over time, it refines its predictor and these estimations improve further and further. In the end, by only taking a brief look at the state of the game, it can predict with a high confidence whether it is going to win or not. Initially, we have an objective, but how do we know whether it is a good one? Well, we try to get there, and then, evaluate our position. We may find that we got nowhere with this. What most people do is abort the program. Quit the game. Give up. It’s over. No, don’t despair, it’s not over! This is the early stage of teaching an AI, and this is the time where we can improve our predictor, and pick our next objective more wisely. Over time, you’ll find the ideas that don’t work, and not only that, you’ll find out why they don’t work. Do not worry and aim again. And this leads us to lesson number four: zoom out and evaluate! This is exactly what DeepMind’s amazing Deep Q-Learning algorithm does that took the world by storm as it was able to play Atari Breakout on a superhuman level just by looking at the pixels of the game. This algorithm ran in two phases, where phase one is collecting experiences, and phase two was called “experience replay”. This is where the AI stops and reflects upon these experiences. Zooming out and evaluating is immensely important, because after all, this is where the real learning happens. So every now and then, zoom out, and evaluate. And while we are here, I can simply not resist adding two more lessons I learned from other scientific disciplines. So, lesson number five: if you find something that works, hold on to it! This is exactly what Metropolis Light Transport does, which is a light simulation algorithm that is able to create beautiful images even for extremely difficult problems where it is challenging to find where the light is. However, when it finally finds something, it makes sure not to forget about it and explore nearby light paths. It works like a charm for difficult light transport situations and can create absolutely beautiful images for even the hardest virtual scenes. Seek the light and hold on to it. And whenever you feel that you are still not making progress, think about the following. Lesson number six: as long as you keep moving, you’ll keep progressing. Take a look at this random walk. A random walk is a succession of steps in completely random directions. This walk is completely lack of direction, just as a drunkard that tries to find home. However, get this a mathematical theorem says that after N steps, the expected distance from where we’ve started is proportional to the square root of N. This is huge! What this means, is that for instance, it we took four completely random steps, we are expected to be 2 units of distance away from where we’ve started. That’s progress! If we take a hundred steps, even then, we can expect to be around 10 units of distance away from the starting point. This concept works, even if our predictor is completely haywire and we choose our objectives like a drunkard. Now I think that’s a lesson worth sharing. To recap: you need an objective! It can be anything, so long as you keep moving, you’ll progress. If you have achieved it and it ended up not being what you were looking for, don’t stop. Zoom out and reflect. This will help you to improve your predictor and you will be able to recalibrate and aim again at something more meaningful. Now, aim and find a new objective! When you have a new objective, your strategy needs to change to be able to achieve it. Finally, if you find something desirable, hold on to it, and explore more in this direction. Seek the light. Of course, you don’t have to live your life this way, but I think these are interesting, mathematically motivated lessons that are worth showing to you. After all, this series is not only meant to inform, but to inspire you to get out there and create. It always feels absolutely amazing getting these kind messages from you Fellow Scholars, some of you said that the series has changed your life in a positive way. I am really out of words and am honored to be able to make these videos for you Fellow Scholars. Let me know in the comments whether you enjoyed this episode and please, keep the kind messages coming, they really make my day. Thanks for watching and for your generous support, and I'll see you next time!"
291,This AI Produces Binaural (2.5D) Audio,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Binaural or 2.5D audio means a sound recording that provides the listener with an amazing 3D-ish sound sensation. It produces a sound that feels highly realistic when listened to through headphones, and therefore, using a pair is highly recommended for this episode. It sounds way more immersive than regular mono, or even stereo audio signals, but also requires more expertise to produce, and is therefore quite scarce on the internet. Let’s listen to the difference together. We have not only heard sound samples here, but could also see the accompanying video content which reveals the position of the players and the composition of the scene in which the recording is made. This sounds like a perfect fit for an AI to take piece of mono audio and use this additional information to convert it to make it sound binaural. This project is exactly about that, where a deep convolutional neural network is used to look at both the video and the single-channel audio content in our footage, and then, predict what it would have sounded like were it recorded as a binaural signal. Let’s listen to a few results. The fact that we can use the visual content as well as the audio with this neural network, interestingly, also enables us to separate the sound of an instrument within the mix. Let’s listen. To validate the results, the authors both used a quantitative, mathematical way of comparing their results to the ground truth, and not only that, but they also carried out two user studies as well. In the first one, the ground truth was shown to the users and they were asked to judge, which of the two techniques were better. In this study, this new method performed better than previous methods, and in the second setup, users were asked to name the directions they hear the different instrument sounds coming from. In this case, the new method outperformed the previous techniques by a significant margin, and if we keep progressing like this, we may be at most a couple papers away from 2.5D audio synthesis that sounds indistinguishable from the real deal. Looking forward to a future where we can enjoy all kinds of video content with this kind of immersion. Thanks for watching and for your generous support, and I'll see you next time!"
292,What Makes a Good Image Generator AI?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we frequently talk about Generative Adversarial networks, or GANs in short. This means a pair of neural networks that battle each over time to master a task, for instance, to generate realistic looking images from a written description. Here you see NVIDIA’s amazing work that was able to dream up high-resolution images of imaginary celebrities. In the next episode, we will talk some more about their newest work that does something like this, and is even better at it, believe it or not. I hope you have subscribed to the channel to make sure not to miss out on that one. And for now, while we marvel at these outstanding results, I will quickly tell you about overfitting and what it has to do with images of celebrities. When we train a neural network, we wish to make sure that it understands the concepts we are trying to teach it. Typically, we feed it a database of labeled images, where the labels mean that this depicts a dog, and this one is not a dog, but a cat. After the training step took place, in the ideal case it will be able to build an understanding of these images, so that when we show them new, previously unseen pictures, it would be able to correctly guess which animals they depict. However, in many cases, we start training the neural network, and during the training, it gives us wonderful results, and it gets the animals right every single time! But, whenever it sees new, previously unseen images, it can’t tell a dog from a cat at all. This peculiar case is what we call overfitting, and this is the bane of machine learning research. Overfitting is like the kind of student we all encountered at school who is always very good at memorizing the textbook but can’t solve even the simplest new problems on the exam. This is not learning, this is memorization! Overfitting means that a neural network does not learn the concept of dogs or cats, it just tries to memorize this database of images and is able to regurgitate it for us, but its knowledge cannot generalize for new images. That’s not good. I want intelligence, not a copying machine! So at this point, it is probably clearer what images of celebrities have to do with overfitting. So, how do we know that this algorithm doesn’t just memorize the celebrity image dataset it was given, and can really generate new, imaginary people? Is it the good kind of student, or the lousy student? Technique number one, let’s not just dream up images of new celebrities, but also visualize images from the training data that are similar to this image. If they are too similar, we have an overfitting problem. Let’s have a look. Now it is easy to see that this is proper intelligence, and not a copying machine, because it was clearly able to learn the facial features of these people and combine them in novel ways. This is what scientists at NVIDIA did in their paper and are to be commended for that. Technique number two, well, just take a bunch of humans and let them decide whether these images differ from the training set and if they are realistic. This kinda works, but of course, costs quite a bit of money, labor, and we end up with something subjective. We better not compare the quality of research papers based on that if we can avoid it. And get this, we can actually avoid it by using something called the inception score. Instead of using humans, this score uses a neural network to have a look at these images and measure the quality and the diversity of the results. As long as the image produces similar neuron activations within this neural network, two images will be deemed to be similar. Finally, this score is an objective way of measuring progress within this field, and it is of course, subject to maximization. So now, you, of course, wish to know what the state of the art is today. For reference, a set of real images has an inception score of 233, and the best works that produced synthetic images from just a few years ago had a score of around 50. To the best of my knowledge, as of the publishing of this video, the highest inception score for an AI is close to 166, so we’ve come a long long way. You can see some of these images here. Truly exciting — what a time to be alive! The disadvantages of the method is that one, because the diversity of the outputs is also to be measured, it requires many thousands of images. This is likely more of an issue with the problem definition itself, not this method, and also, since this means that the computers and not real people have to do more work, … we can give this one a pass. Disadvantage number two, I will include this paper in the video description for you, which basically describes that there are cases where it is possible to get the network to think an image is of higher quality than another one even if it clearly isn’t. Now you see that we have pretty ingenious techniques to measure the quality of image generator AI programs, and of course, this area of research is also subject to improvement, and, I’ll be here to tell you about it. Thanks for watching and for your generous support, and I'll see you next time!"
293,None of These Faces Are Real!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Before we start, I will tell you right away to hold on to your papers. When I’ve first seen the results I didn’t do that and almost fell out of the chair. Scientists at NVIDIA published an amazing work not so long ago that was able to dream up high-resolution images of imaginary celebrities. It was a progressive technique, which means that it started out with a low-fidelity image and kept refining it, and over time, we found ourselves with high-quality images of people that don’t exist. We also discussed in the previous episode that the algorithm is able to learn the properties and features of a human face and come up with truly novel human beings. There is true learning happening here, not just copying the training set for these neural networks. This is an absolutely stellar research work, and, for a moment, let’s imagine that we are the art directors of a movie or a computer game and we require that the algorithm synthesizes more human faces for us. Whenever I worked with artists in the industry, I’ve learned that what artists often look for beyond realism, is ... control. Artists seek to conjure up new worlds, and those new worlds require consistency and artistic direction to suspend our disbelief. So here’s a new piece of work from NVIDIA with some killer new features to address this. Killer feature number one. It can combine different aspects of these images. Let’s have a look at an example over here. The images above are the inputs, and we can lock in several aspects of these images, for instance, like gender, age, pose and more. Then, we take a different image, this will be the other source image, and the output is these two images fused together. Almost like style transfer or feature transfer for human faces. As a result, we are able to generate high-fidelity images of human faces that are incredibly lifelike, and, of course, none of these faces are real. How cool is that? Absolutely amazing. Killer feature number two. We can also vary these parameters one by one, and this way, we have a more fine-grained artistic control over the outputs. Killer feature number three: It can also perform interpolation, which means that we have desirable images A and B, and this would create intermediate images between them. As always, with this, the holy grail problem is that each of the intermediate images have to make sense and be realistic. And just look at this. It can morph one gender into the other, blend hairstyles, colors, and in the meantime, the facial features remain crisp and realistic. I am out of words, this is absolutely incredible. It kind of works on other datasets, for instance, cars, bedrooms, and of course, you guessed it right, … cats. Now, interestingly, it also varies the background behind the characters, which is a hallmark of latent-space based techniques. I wonder if and how this will be solved over time. We also published a paper not so long ago that was about using learning algorithms to synthesize, not human faces, but photorealistic materials. We introduced a neural renderer that was able to perform a specialized version of a light transport simulation in real time as well. However, in the paper, we noted that the resolution of the output images is limited by the on-board video memory on the graphics card that is being used and should improve over time as new graphics cards are developed with more memory. And get this, a few days ago, folks at NVIDIA reached out and said that they just released an amazing new graphics card, the TITAN RTX which has a ton of onboard memory, and would be happy to send over one of those. Now we can improve our work further. A huge thank you for them for being so thoughtful, and therefore, this episode has been been kindly sponsored by NVIDIA. Thanks for watching and for your generous support, and I'll see you next time!"
294,DeepMind’s AlphaStar Beats Humans 10-0 (or 1),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I think this is one of the more important things that happened in AI research lately. In the last few years, we have seen DeepMind defeat the best Go players in the world, and after OpenAI’s venture in the game of DOTA2, it’s time for DeepMind to shine again as they take on Starcraft 2, a real-time strategy game. The depth and the amount of skill required to play this game is simply astounding. The search space of Starcraft 2 is so vast that it exceeds both Chess, and even Go by a significant margin. Also, it is a game that requires a great deal of mechanical skill, split-second decision making and we have imperfect information as we only see what our units can see. A nightmare situation for any AI. DeepMind invited a beloved pro player, TLO to play a few games against their new StarCraft 2 AI that goes by the name AlphaStar. Note that TLO a profesional player who is easily in top 1% of players, or even better. Mid-grandmaster for those who play StarCraft 2. This video is about what happened during this event, and later, I will make another video that describes the algorithm that was used to create this AI. The paper is still under review, so it will take a little time until I can get my hands on it. At the end of this video, you will also see the inner workings of this AI. Let’s dive in. This is an AI that looked at a few games played by human players, and after that initial step, it learns by playing against itself for about 200 years. In our next episode, you will see how this is even possible, so I hope you are subscribed to the series. You see here that the AI controls the blue units, and TLO, the human player plays red. Right at the start of the first game, the AI did something interesting. In fact, what is interesting is what it didn’t do. It started to create new buildings next to its nexus, instead of building a walloff that you can see here. Using a walloff is considered standard practice in most games, and the AI used these buildings to not wall off the entrance, but to shield away the workers from possible attacks. Now note that this is not unheard of, but this is also not a strategy that is widely played today and is considered non-standard. It also built more worker units than what is universally accepted as standard, we found out later that this was partly done in anticipation of losing a few of them early on. Very cool. Then, almost before we even knew what happened, it won the first game a little more than 7 minutes in, which is very quick, noting that in-game time is a little faster than real-time. The thought process of TLO at this point is that that’s interesting, but okay, well, the AI plays aggressively and managed to pull this one off. No big deal. We will fire up the second game, in the meantime, few interesting details. The goal of setting up the details of this algorithm was that the number of actions performed by the AI roughly matches a human player, and hopefully it still plays as well, or better. It has to make meaningful strategic decisions. You see here that this checks out for the average actions every minute, but if you look here, you see around the tail end here that there are times when it performs more actions than humans and this may enable playstyles that are not accessible for human players. However, note that many times it also does miraculous things with very few actions. Now, what about an other important detail, reaction time? The reaction time of the AI is set to 350ms, which is quite slow. That’s excellent news because this is usually a common angle of criticism for game AIs. The AI also sees the whole map at once, but it is not given more information than what its units can see. This perhaps is the most commonly misunderstood detail, so it is worth noting. So, in other words, it sees exactly what a human would see if the human would move the camera around very quickly, but, it doesn’t have to move the camera, which adds additional actions and cognitive load to the human, so one might say that the AI has an edge here. The AI plays these games independently, what’s more, each game was played by a different AI, which also means that they do not memorize what happened in the last game like a human would. Early in the next game, we can see the utility of the walloff in action which is able to completely prevent the AIs early attack. Later that game, the AI used disruptors, a unit, which if controlled with such level of expertise, can decimate the army of the opponent with area damage by killing multiple units at once. It has done an outstanding job picking away at the army of TLO. Then, after getting a significant advantage, AlphaStar loses it with a few sloppy plays and by deciding to engage aggressively while standing in tight choke points. You can see that this is not such a great idea. This was quite surprising as this is considered to be StarCraft 101 knowledge right there. During the remainder of the match, the commentators mentioned that they play and watch matches all the time and the AI came up with an army composition that they have never seen during a professional match. And, the AI won this one too. After this game it became clear that these agents can play any style in the game. Which is terrifying. Here you can see an alternative visualization that shows a little more of the inner workings of the neural network. We can see what information it gets from the game, a visualization of neurons that get activated within the network, what locations and units are considered for the next actions, and whether the AI predicts itself as the winner or loser of the game. If you look carefully, you will also see the moment when the agent becomes certain that it will win this game. I could look at this all day long, and if you feel the same way, make sure to visit the video description, I have a link to the source video for you. The final result against TLO was 5 to 0, so that’s something, and he mentioned that the AlphaStar played very much like a human does and almost always managed to outmaneuver him. However, TLO also mentioned that he is confident that upon playing more training matches against these agents, he would be able to defeat the AI. I hope he will be given a chance to do that. This AI seems strong, but still beatable. I would also note that many of you would probably expect the later versions of AlphaStar to be way better than this one. The good news is that the story continues and we’ll see whether that’s true! So at this point, the DeepMind scientists said that “maybe we could try to be a bit more ambitious”, and asked “can you bring us someone better”? And in the meantime, pressed that training button on the AI again. In comes MaNa, a top tier pro player. One of the best Protoss players in the world. This was a nerve-wracking moment for DeepMind scientists as well, because their agents played against each other, so they only knew the AI’s winrate against a different AI, but they didn’t know how they would compete against a top pro player. It may still have holes in its strategy. Who knows what would happen? Understandably, they had very little confidence in winning this one. What they didn’t expect is that this new AI was not slightly improved, or somewhat improved. No, no, no. This new AI was next level. This set of improved agents among many other skills, had incredibly crisp micromanagement of each individual unit. In the first game, we’ve seen it pulling back injured units but still letting them attack from afar masterfully, leading to an early win for the AI against Mana in the first game. He and the commentators were equally shocked by how well the agent played. And I will add that I remember from watching many games from a now inactive player by the name MarineKing a few years ago. And I vividly remember that he played some of his games so well, the commentators said that there’s no better way to put it, he played like a god. I am almost afraid to say that this micromanagement was even more crisp than that. This AI plays phenomenal games. In later matches, the AI did things that seemed like blunders, like attacking on ramps and standing in choke points, or using unfavorable unit compositions and refusing to change it and, get this, it still won all of those games 5 to 0. Against a top pro player. Let that sink in. The competition was closed by a match where the AI was asked to also do the camera management. The agent was still very competent, but somewhat weaker and as a result, lost this game, hence the “0 or 1” part in the title. My impression is that it was asked to do something that it was not designed for, and expect a future version to be able to handle this use case as well. I will also commend Mana for his solid game plan for this game, and also, huge respect for DeepMind for their sportsmanship. Interestingly, in this match, Mana also started using a worker oversaturation strategy that I mentioned earlier. This he learned from AlphaStar and used it in his winning game. Isn’t that amazing? DeepMind also offered a reddit AMA where anyone could ask them questions to make sure to clear up any confusion, for instance, the actions per minute part has been addressed, I’ve included a link to that for you in the description. To go from a turn-based perfect information game, like Go, to a real time strategy game of imperfect information in about a year sounds like science fiction to me. And yet, here it is. Also, note that DeepMind’s goal is not to create a godlike StarCraft 2 AI. They want to solve intelligence, not StarCraft 2, and they used the game as a vehicle to demonstrate its long-term decision making capabilities against human players. One more important thing to emphasize is that the building blocks of AlphaStar are meant to be reasonably general AI algorithms, which means that parts of this AI can be reused for other things, for instance, Demis Hassabis mentioned weather prediction and climate modeling as examples. If you take only one thought from this video, let it be this one. I urge you to watch all the matches because what you are witnessing may very well be history in the making. I put a link to the whole event in the video description, plus plenty more materials, including other people’s analysis, Mana’s personal experience of the event, his breakdown of his games and what was going through his head during the event. I highly recommend checking out his 5th game, but really, go through them all, it’s a ton of fun! I made sure to include a more skeptical analysis of the game as well to give you a balanced portfolio of insights. Also, huge respect for DeepMind and the players who practiced their chops for many many years and have played really well under immense pressure. Thank you all for this delightful event. It really made my day. And the ultimate question is, how long did it take to train these agents? 2 weeks. Wow. And what’s more, after the training step, the AI can be deployed on an inexpensive consumer desktop machine. And this is only the first version. This is just a taste, and it would be hard to overstate how big of a milestone this is. And now, scientists at DeepMind have sufficient data to calculate the amount of resources they need to spend to train the next, even more improved agents. I am confident that they will also take into consideration the feedback from the StarCraft community when creating this next version. What a time to be alive! What do you think about all this? Any predictions? Is this harder than DOTA2? Let me know in the comments section below. And remember, we humans build up new strategies by learning from each other, and of course, the AI, as you have seen here, doesn’t care about any of that. It doesn’t need intuition and can come up with unusual strategies. The difference now is that these strategies work against some of the best human players. Now it’s time for us to finally start learning from an AI. gg. Thanks for watching and for your generous support, and I'll see you next time!"
295,AI Learns Real-Time Defocus Effects in VR,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If we are to write a sophisticated light simulation program, and we write a list of features that we really wish to have, we should definitely keep an eye on defocus effects. This is what it looks like, and in order to do that, our simulation program has to take into consideration the geometry and thickness of the lenses within our virtual camera, and even though it looks absolutely amazing, it is very costly to simulate that properly. This particular technique attempts to do this in real time, and, for specialized display types, typically ones that are found in head-mounted displays for Virtual Reality applications. So here we go, due to popular request, a little VR in Two Minute Papers. In virtual reality, defocus effects are especially important because they mimic how the human visual system works. Only a tiny region that we’re focusing on looks sharp, and everything else should be blurry, but not any kind of blurry, it has to look physically plausible. If we can pull this off just right, we’ll get a great and immersive VR experience. The heart of this problem is looking at a 2D image and being able to estimate how far away different objects are from the camera lens. This is a task that is relatively easy for humans because we have an intuitive understanding of depth and geometry, but of course, this is no easy task for a machine. To accomplish this, here, a convolutional neural network is used, and our seasoned Fellow Scholars know that this means that we need a ton of training data. The input should be a bunch of images, and their corresponding depth maps for the neural network to learn from. The authors implemented this with a random scene generator, which creates a bunch of these crazy scenes with a lot of occlusions and computes via simulation the appropriate depth map for them. On the right, you see these depth maps, or in other words, images that describe to the computer how far away these objects are. The incredible thing is that the neural network was able to learn the concept of occlusions, and was able to create super high quality defocus effects. Not only that, but this technique can also be reconfigured to fit different use cases: if we are okay with spending up to 50 milliseconds to render an image, which is 20 frames per second, we can get super high-quality images, or, if we only have a budget of 5 milliseconds per image, which is 200 frames per second, we can do that and the quality of the outputs degrades just a tiny bit. While we are talking about image quality, let’s have a closer look at the paper, where we see a ton of comparisons against previous works and of course, against the baseline ground truth knowledge. You see two metrics here: PSNR, which is the peak signal to noise ratio, and SSIM, the structural similarity metric. In this case, both are used to measure how close the output of these techniques is to the ground truth footage. Both are subject to maximization. For instance, here you see that the second best technique has a peak signal to noise ratio of around 40, and this new method scores 45. Well, some may think that that’s just a 12-ish percent difference, right? … No. Note that PSNR works on a logarithmic scale, which means that even a tiny difference in numbers translates to a huge difference in terms of visuals. You can see in the closeups that the output of this new method is close to indistinguishable from the ground truth. A neural network that successfully learned the concept of occlusions and depth by looking at random scenes. Bravo. As virtual reality applications are on the rise these days, this technique will be useful to provide a more immersive experience for the users. And to make sure that this method sees more widespread use, the authors also made the source code and the training datasets available for everyone, free of charge, so make sure to have a look at that and run your own experiments if you’re interested. I'll be doing that in the meantime. Thanks for watching and for your generous support, and I'll see you next time!"
296,OpenAI - Learning Dexterous In-Hand Manipulation,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about OpenAI’s new technique that teaches a robot arm to dexterously manipulate a block to a target state. And in this project, they did one of my favorite things, which is, first, training an AI within a simulation, and then, deploying it into the real world. And in the best case scenario, this knowledge from the simulation will actually generalize to the real world. However, while we are in the simulation, we can break free from the limitations of worldly things, such as hardware, movement speed, or, even time itself. So how is that possible? The limitation on the number of experiments we can run in a simulation is bounded by not our time, which is scarce, but how powerful our hardware is, which is abundant as it is accelerating at nearly exponential pace. And, this is the reason why OpenAI’s and DeepMind’s AI was able to train for 200 years worth of games before first playing a human pro player. This sounds great, but a simulation is always more crude than the real world, so do we know for sure that we created something that will indeed be useful in the real world, and not just in the simulation? Let’s try an analogy. Think of the machine as a student, and the simulation would be its textbook that it learns from. If the textbook contains only a few trivial problems to learn from, when the day of the exam comes, if the exam is any good, the student will fail. The exam is the equivalent of deploying the machine into the real world, and apparently, the real world is a damn good exam. So how can we prepare a student to do well on this exam? Well, we have to provide them with a textbook that contains not only a lot of problems, but also a diverse set of challenges as well. This is what machine learning researchers call domain randomization. This means that we teach the AI program in different virtual worlds, and in each one of them, we change parameters like how fast the hand is, what color and weight the cube is, and more. This is a proper textbook, which means that after this kind of training, this AI can deal with new and unexpected situations. The knowledge that it has obtained is so general that we can change even the geometry of the target object and the machine will still be able to manipulate it correctly. Outstanding. To implement this idea, scientists at OpenAI trained not one agent, but a selection of agents in these randomized environments. The first main component of this system is a pose estimator. This module looks at the cube from three angles and predicts the position and orientation of the block, and is implemented through a convolutional neural network. The advantage of this is that as we can generate a near-infinite amount of training data ourselves. You can see here that when the AI looks at real images, it is only a few degrees worse than in the simulation when estimating angles, which is the case of the excellent textbook. I would not be surprised if this accuracy exceeds the capabilities of an ordinary human, given that it can perform this many times within a second. Then, the next part is choosing what the next action should be. Of course, we seek to rotate this cube in a way that brings us closer to our objective. This is done by a reinforcement learning technique, which uses similar modules as OpenAI’s previous algorithm that learned to play DOTA2 really well. Another testament to how general these learning algorithms are. I also recommend checking out OpenAI’s video on this work in the video description. Now, I always read in the comments here on Youtube that many of you are longing for more. 5 minute papers, 10 minute papers, 2 hour papers were among the requests I heard from you before. And of course, I am also longing for more as I have quite a few questions that keep me up at night. Is it possible for us to ever come up with a superintelligent AI? If yes, how? What types of these AIs could exist? Should we be worried? If you are also looking for some answers, we are now trying out a sponsorship with Audible, and I have a great recommendation for you, which is none other than the book Superintelligence by Nick Bostrom. It addresses all of these questions really well, and if you sign up under the link below in the video description, you will get this book free of charge. Whenever you have to do some work around the house, commute to school or work, just pop in a pair of headphones and listen for free. Some more AI for you while doing something tedious. That’s as good as it gets. If you feel that the start of the book is a little slow for you, make sure to jump to the chapter by the name “Is the default outcome doom”. But buckle up, because there is going to be fireworks from that point in the book. We thank Audible for supporting this video, and send a big thank you for all of you who sign up and support the series. Thanks for watching and for your generous support, and I'll see you next time!"
297,Extracting Rotations The Right Way,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is about creating high-quality physics simulations and is, in my opinion, one of the gems very few people know about. In these physical simulations, we have objects that undergo a lot of tormenting, for instance they have to endure all kinds of deformations, rotations, and of course, being pushed around. A subset of these simulation techniques requires us to be able to look at these deformations and forget about anything they do other than the rotational part. Don’t push it, don’t squish it, just take the rotational part. Here, the full deformation transform is shown with red, and extracted rotational part is shown by the green indicators here. This problem is not particularly hard and has been studied for decades, so we have excellent solutions to this, for instance, techniques that we refer to as polar decomposition, singular value decomposition, and more. By the way, in our earlier project together with the Activision Blizzard company, we also used the singular value decomposition to compute the scattering of light within our skin and other translucent materials. I’ve put a link in the video description, make sure to have a look! Okay, so if a bunch of techniques already exist to perform this, why do we need to invent anything here? Why make a video about something that has been solved many decades ago? Well, here’s why: we don’t have anything yet that is criterion one, robust, which means that it works perfectly all the time. Even a slight inaccuracy is going to make an object implode our simulations, so we better get something that is robust. And, since these physical simulations are typically implemented on the graphics card, criterion two, we need something that is well suited for that, and is as simple as possible. It turns out, none of the existing techniques check both of these two boxes. If you start reading the paper, you will see a derivation of this new solution, a mathematical proof that it is true and works all the time. And then, as an application, it shows fun physical simulations that utilize this technique. You can see here that these simulations are stable, no objects are imploding, although this extremely drunk dragon is showing a formidable attempt at doing that. Ouch. All the contortions and movements are modeled really well over a long time frame, and the original shape of the dragon can be recovered without any significant numerical errors. Finally, it also compares the source code for a previous method, and, the new method. As you see, there is a vast difference in terms of complexity that favors the new method. It is short, does not involve a lot of branching decisions and is therefore an excellent candidate to run on state of the art graphics cards. What I really like in this paper is that it does not present something and claims that “well, this seems to work”. It first starts out with a crystal clear problem statement that is impossible to misunderstand. Then, the first part of the a paper is pure mathematics, proves the validity of a new technique, and then, drops it into a physical simulation, showing that it is indeed what we were looking for. And finally, a super simple piece of source code is provided so anyone can use it almost immediately. This is one of the purest computer graphics papers out there I’ve seen in a while. Make sure to have a look in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
298,Diese KI lernt Bildentfärbung..und weiteres,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Whenever we build a website, a video game, or do any sort of photography and image manipulation, we often encounter the problems of image downscaling, decolorization, and HDR tone mapping. This work offers us one technique that can do all three of these really well. But first, before we proceed, why are we talking about downscaling? We are in the age of AI, where a computer program can beat the best players in Chess and Go, so why talk about such a trivial challenge? Well, have a look here. Imagine that we have this high-fidelity input image and due to file size constraints, we have to produce a smaller version of it. If we do it naively, this is what it looks like. Not great, right? To do a better job at this, our goal would be that the size of the image would be reduced, but while still retaining the intricate details of this image. Here are two classical downsampling techniques…better, but the texture of the skin is almost completely lost. Have a look at this! This is what this learning-based technique came up with. Really good, right? It can also perform decolorization. Again, a problem that sounds trivial for the unassuming Scholar, but when taking a closer look, we notice that there are many different ways of doing this, and somehow we seek a decolorized image that still relates to the original as faithfully as possible. Here you see the previous methods that are not bad at all, but this new technique is great at retaining the contrast between the flower and its green leaves. At this point it is clear that deciding which output is the best is highly subjective. We’ll get back to that in a moment. It is also capable of doing HDR tone mapping. This is something that we do when we capture an image with a device that supports a wide dynamic range, in other words, a wide range of colors, and we wish to display it on our monitor, which has a more limited dynamic range. And again, clearly there are many ways to do that. Welcome to the wondrous world of tone mapping! Note that there are hundreds upon hundreds of algorithms to perform these operations in computer graphics research. And also note that these are very complex algorithms that took decades for smart researchers to come up with. So the seasoned Fellow Scholar shall immediately ask why talk about this work at all? What’s so interesting about it? The goal here is to create a little more general, learning-based method that can do a great job at not one, but all three of these problems. But how great exactly? And how do we decide how good these images are? To answer both of these questions at the same time, if you’ve been watching this series for a while, then you are indeed right, the authors created a user study, which shows that for all three of these tasks, according to the users, the new method smokes the competition. It is not only more general, but also better than most of the published techniques. For instance, Reinhard’s amazing tone mapper has been an industry standard for decades now, and look, almost 75% of the people prefer this new method over that. What required super smart researchers before can now be done with a learning algorithm. Unreal. What a time to be alive. A key idea for this algorithm is that this convolutional neural network you see on the left is able to produce all three of these operations at the same time, and to perform that, it is instructed by another neural network to do this in a way that preserves the visual integrity of the input images. Make sure to have a look at the paper for more details on how this perceptual loss function is defined. And, if you wish to help us tell these amazing stories to even more people, please consider supporting us on Patreon. Your unwavering support on Patreon is the reason why this show can exist, and, you can also pick up cool perks there, like watching these videos in early access, deciding the order of the next few episodes, or even getting your name showcased in the video description as a key supporter. You can find us at patreon.com/twominutepapers, or, as always, just click the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
299,AI-Based 3D Pose Estimation: Almost Real Time!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This episode is about a really nice new paper on pose estimation. Pose estimation means that we have an image or video of a human as an input, and the output should be, this skeleton that you see here that shows us what the current position of this person is. Sounds alright, but what are the applications of this, really? Well, it has a huge swath of applications, for instance, many of you often hear about motion capture for video games and animation movies, but it is also used in medical applications for finding abnormalities in a patient’s posture, animal tracking, understanding sign language, pedestrian detection for self-driving cars, and much, much more. So if we can do something like this in real time, that’s hugely beneficial for many many applications. However, this is a very challenging task, because humans have a large variety of appearances, images come in all kinds of possible viewpoints, and as a result, the algorithm has to deal with occlusions as well. This is particularly hard, have a look here. In these two cases, we don’t see the left elbow, so it has to be inferred from seeing the remainder of the body. We have the reference solution on the right, and as you see here, this new method is significantly closer to it than any of the previous works. Quite remarkable. The main idea in this paper is that it works out the poses both in 2D and 3D and contains neural network that can convert to both directions between these representations while retaining the consistencies between them. First, the technique comes up with an initial guess, and follows up by using these pose transformer networks to further refine this initial guess. This makes all the difference. And not does it lead to high-quality results, but it also takes way less time than previous algorithms — we can expect to obtain a predicted pose in about 51 milliseconds, which is almost 20 frames per second. This is close to real time, and is more than enough for many of the applications we’ve talked about earlier. In the age of rapidly improving hardware, these are already fantastic results both in terms of quality and performance, and not only the hardware, but the papers are also improving at a remarkable pace. What a time to be alive. The paper contains an exhaustive evaluation section, it is measured against a variety of high-quality solutions, I recommend that you have a look in the video description. I hope nobody is going to install a system in my lab that starts beeping every time I slouch a little, but I am really looking forward to benefitting from these other applications. Thanks for watching and for your generous support, and I'll see you next time!"
300,"AlphaZero: DeepMind’s AI Works Smarter, not Harder","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Finally, I have been waiting for quite a while to cover this amazing paper, which is about AlphaZero. We have talked about AlphaZero before, this is an AI that is able to play Chess, Go, and Shogi, or in other words, Japanese chess on a remarkably high level. I will immediately start out by uttering the main point of this work: the point of AlphaZero is not to solve Chess, or any of these games. Its main point is to show that a general AI can be created that can perform on a superhuman level on not one, but several different tasks at the same time. Let’s have a look at this image, where you see a small part of the evaluation of AlphaZero versus StockFish, an amazing open source chess engine which has been consistently at or around the top computer chess players for many years now. StockFish has an elo rating of over 3200 which means that it has a winrate of over 90% against the best human players in the world. Now, interestingly, comparing these algorithms is nowhere near as easy as it sounds. This sounds curious, so why is that? For instance, it is not enough to pit the two algorithms against each other and see who ends up winning. It matters what version of Stockfish is used, how many positions are the machines are allowed to evaluate, how much thinking time they are allowed, the size of hashtables, the hardware being used, the number of threads being used, and so on. From the side of the chess community, these are the details that matter. However, from the side of an AI researcher, what matters most is to create a general algorithm that can play several different games on a superhuman level. With this constraint, it would really be a miracle if AlphaZero were able to even put up a good fight against Stockfish. So, what happened? AlphaZero played a lot of games that ended up as draws against Stockfish and not only that, but whenever there was a winner, it was almost always AlphaZero. Insanity. And what is quite remarkable is that AlphaZero has only trained for 4 to 7 hours only through self-play. Comparatively, the development of the current version of Stockfish took more than 10 years. You see the how reliably this AI can be trained, the blue lines show the results of several training runs, and they all converge to the same result with only a tiny bit of deviation. AlphaZero is also not a brute-force algorithm as it evaluates fewer positions per second than StockFish. Kasparov put it really well in his article where he said that AlphaZero works smarter, not harder than previous techniques. Even Magnus Carlsen, chess grandmaster extraordinaire said in an interview that during his games, he often thinks: “what would AlphaZero do in this case?”, which I found to be quite remarkable. Kasparov also had many good things to say about the new AlphaZero in a, let’s say, very Kasparov-esque manner. And also note that the key point is not whether the current version of Stockfish or the one from two months ago was used. The key point is that Stockfish is a brilliant Chess engine, but it is not able to play Go or any game other than Chess. This is the main contribution that DeepMind was looking for with this work. This AI can master three games at once, and a few more papers down the line, it may be able to master any perfect information game. Oh my goodness. What a time to be alive! We have really only scratched the surface in this video. This was only a taste of the paper. The evaluation section in the paper is out of this world, so make sure to have a look in the video description and I am convinced that nearly any question one can possibly think of is addressed there. I also linked Kasparov’s editorial on this topic. It is short, and very readable, give it a go! I hope this little taste of AlphaZero inspires you to go out and explore yourself. This is the main message of this series. Let me know in the comments what you think or if you found some cool other things related to AlphaZero! Thanks for watching and for your generous support, and I'll see you next time."
301,Google AI's Take on How To Fix Peer Review,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. It is time for a position paper. This paper does not have the usual visual fireworks that you see in many of these videos, however, it addresses the cornerstone of scientific publication, which is none other than peer review. When a research group is done with a project, they don’t just write up the results and chuck the paper into a repository, but instead, they submit it to a scientific venue, for instance, a journal or a conference. Then, the venue finds several other researchers who are willing to go through the work with a fine-tooth comb. In the case of double-blind reviews, both the authors and the reviewers remain anonymous to each other. The reviewers now check whether the results are indeed significant, novel, credible and reproducible. If the venue is really good, this process is very tough and thorough, and this is process becomes the scientific version of beating the heck out of someone, but in a constructive manner. If the work is able to withstand serious criticism, and ticks the required boxes, it can proceed to get published at this venue. Otherwise, it is rejected. So what we heard so far is that the research work is being reviewed, however, scientists at the Google AI lab raised the issue that the reviewers themselves should also be reviewed. Consider the fact that all scientists are expected to spend a certain percentage of their time to serve the greater good. For instance, throughout my PhD studies, I have reviewed over 30 papers and I am not even done yet. These paper reviews take place without compensation. Let’s call this issue number one for now. Issue number two is the explosive growth of the number of submissions over time at the most prestigious machine learning and computer vision conferences. Have a look here. It is of utmost importance that we create a review system that is as fair as possible after all, thousands of hours spent on research projects are at stake. Add these two issues together, and we get a system where the average quality of the reviews will almost certainly decrease over time. Quoting the authors: “We believe the key issues here are structural. Reviewers donate their valuable time and expertise anonymously as a service to the community with no compensation or attribution, are increasingly taxed by a rapidly increasing number of submissions, and are held to no enforced standards.” In Two Minute Papers episode number 84, so more than 200 episodes ago, we discussed the NeurIPS experiment. Leave a comment if you’ve been around back then and you enjoyed Two Minute Papers before it was cool! But don’t worry if this is not the case, this was long ago, so here is a short summary: a large amount of papers were secretly disseminated to multiple committees, who would review it without knowing about each other, and we would have a look whether they would accept or reject the same papers. Re-review papers and see if the results are the same, if you will. If we use sophisticated mathematics to create new scientific methods, why not use mathematics to evaluate our own processes? So, after doing that, it was found that at a given prescribed acceptance ratio, there was a disagreement for 57% of the papers. So, is this number good or bad? Let’s imagine a completely hypothetical committee that has no idea what they are doing, and as a review, they basically toss up a coin and accept or reject the paper based on the result of the cointoss. Let’s call them the Coinflip Committee. The calculations conclude that the Coinflip Committee would have a disagreement ratio of about 77%. So, experts, 57% disagreement, Coinflip Committee, 77% disagreement. And now, to answer whether this is good or bad: this is hardly something to be proud of — the consistency of expert reviewers is significantly closer to a coinflip than to a hypothetical perfect review process. If that is not an indication that we have to do something about this, I am not sure what is. So, in this paper, the authors propose two important changes to the system to remedy these issues: Remedy number one they propose a rubric, a 7-point document to evaluate the quality of the reviews. Again, not only the papers are reviewed, but the reviews themselves. It is similar to the ones used in public schools to evaluate student performance to make sure whether the review was objective, consistent and fair. Remedy number two reviewers should be incentivized and rewarded for their work. The authors argue that a professional service should be worthy of professional compensation. Now, of course, this sounds great, but this also requires money. Where should the funds come from? The paper discusses several options: for instance, this could be funded through sponsorships, or, asking for a reasonable fee when submitting a paper for peer review, and introducing a new fee structure for science conferences. This is a short, 5-page paper that is very easily readable for everyone, raises excellent points for a very important problem, so needless to say, I highly recommend that you give it a read, as always, the link is in the video description. I hope this video will help raising more awareness to this problem. If we are to create a fair system for evaluating research papers, we better get this right. Thanks for watching and for your generous support, and I'll see you next time!"
302,Do Neural Networks Need To Think Like Humans?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As convolutional neural network-based image classifiers are able to correctly identify objects in images and are getting more and more pervasive, scientists at the University of Tübingen decided to embark on a project to learn more about the inner workings of these networks. Their key question was whether they really work similarly to humans or not. Now, one way of doing this is visualizing the inner workings of the neural network. This is a research field on its own, I try to report on it to you every now and then, and we talked about some damn good papers on this, with more to come. A different way would be to disregard the inner workings of the neural network, in other words, to treat it like a black box, at least temporarily. But what does this mean exactly? Let’s have a look at an example! And in this example, our test subject shall be none other than this cat. Here we have a bunch of neural networks that have been trained on the classical ImageNet dataset, and, a set of humans. This cat is successfully identified by all classical neural network architectures and most humans. Now, onwards to a grayscale version of the same cat. The neural networks are still quite confident that this is a cat, some humans faltered, but still, nothing too crazy going on here. Now let’s look at the silhouette of the cat. Whoa! Suddenly, humans are doing much better at identifying the cat than neural networks. This is even more so true when we’re only given the edges of the image. However, when looking at a heavily zoomed in image of the texture of an Indian elephant, neural networks are very confident with their correct guess, where some humans falter. Ha! We have a lead here. It may be that as opposed to humans, neural networks think more in terms of textures than shapes. Let’s test that hypothesis. Step number one: Indian elephant. This is correctly identified. Now, cat — again, correctly identified. And now, hold on to your papers — a cat with an elephant texture. And there we go: a cat with an elephant texture is still a cat to us, humans, but, is an elephant to convolutional neural networks. After looking some more at the problem, they found that the most common convolutional neural network architectures that were trained on the ImageNet dataset vastly overvalue textures over shapes. That is fundamentally different to how we, humans think. So, can we try to remedy this problem? Is this even a problem at all? Neural networks need not to think like humans, but who knows, it’s research we might find something useful along the way. So how could we create a dataset that would teach a neural network a better understanding of shapes? Well, that’s a great question, and one possible answer is — style transfer! Let me explain. Style transfer is the process of fusing together two images, where the content of one image and the style of the other image is taken. So now, let’s take the ImageNet dataset, and run style transfer on each of these images. This is useful because it repaints the textures, but the shapes are mostly left intact. The authors call it the Stylized-ImageNet dataset and have made it publicly available for everyone. This new dataset will no doubt coerce the neural network to build a better understanding of shapes, which will bring it closer to human thinking. We don’t know if that is a good thing yet, so let’s look at the results. And here comes the surprise! When training a neural network architecture by the name ResNet-50 jointly on the regular and the stylized ImageNet dataset, after a little fine tuning, they have found two remarkable things. One, the resulting neural network now see more similarly to humans. The old, blue squares on the right mean that the old thinking is texture-based, but the new neural networks, denoted with the orange squares, are now much closer to the shape-based thinking of humans, which is indicated with the red circles. And now hold on to your papers, because two, the new neural network also outperforms the old ones in terms of accuracy. Dear Fellow Scholars, this is research at its finest the authors explored an interesting idea, and look where they ended up. Amazing. If you enjoyed this episode and you feel that a bunch of these videos a month are worth 3 dollars, please consider supporting us on Patreon. This helps us get more independent and create better videos for you. You can find us at Patreon.com/twominutepapers, or just click the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
303,This Experiment Questions Some Recent AI Results,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In the previous episode, we talked about image classification, which means that we have an image as an input, and we ask a computer to figure out what is seen in this image. Learning algorithms, such as convolutional neural networks are amazing at it, however, we just found out that even though their results are excellent, it is still quite hard to find out how they get to make a decision that an image depicts a dog or cat. This is in contrast to and old and simple technique that goes by the name bag of words. It works a bit like looking for keywords in a document and by using those, trying to find out what the writing is about. Kind of like the shortcut students like to take for mandatory readings. We have all done it. Now, imagine the same for images, where we slice up the image into small pieces and keep a score on what it seen in these snippets. Floppy ears, black snout, fur, okay, we’re good, we can conclude that we have a dog over here. But wait, I hear you are saying, Károly, why do we need to digress from AI to bag of words? Why talk about this old method? Well, let’s look at the advantages and disadvantages and you will see in a moment. The advantage of bag of features is that it is quite easy to interpret because it is an open book: it gives us the scores for all of these small snippets. We know exactly how a decision is being made. A disadvantage, one would say is that because it works per snippet, it ignores the bigger spatial relationships in an image, and therefore overall, it must be vastly inferior to a neural network, right? Well, let’s set up an experiment and see! This is a paper from the same group as the previous episode at the University of Tübingen. The experiment works the following way: let’s try to combine bag of words with neural networks by slicing up the image into the same patches, and then, feed them into a neural network and ask it to classify them. In this case, the neural network will do many small classification tasks on image snippets instead of one big decision for the full image. The paper discusses that the final classification also involves evaluating heatmaps and more. This way, we are hoping that we get a technique where a neural network would explain its decisions, much like how bag of words works. For now, let’s call these networks BagNets. And now, hold on to your papers, because results are really surprising. As expected, it is true that looking at small snippets of the image can lead to misunderstandings. For instance, this image contains a soccer ball, but when zooming into small patches, it might seem like this is a cowboy hat on top of the head of this child. However, what is unexpected is that even with this, BagNet produces surprisingly similar results to a state-of-the-art neural network by the name ResNet. This is…wow. This has several corollaries. Let’s start with the cool one: this means that neural networks are great at identifying objects in scrambled images, but humans are not. The reason for that is that the order of the tiles don’t really matter. We now have a better reason why this is the case — doing all this classification for many small tiles independently has superpowers when it comes to processing scrambled images. The other, more controversial corollary is that this inevitably means that some results that show the superiority of deep neural networks over the good old bag of features come not from using a superior method, but from careful fine-tuning. Not all results, …some results. As always, a good piece of research challenges our underlying assumptions, and sometimes, in this case, even our sanity. There is a lot to say about this topic and we have only scratched the surface, so take this as a thought-provoking idea that is worthy of further discussion. Really cool work, I love it. This video has been supported by Audible. By using Audible, you get two excellent audiobooks free of charge. I recommend that you click the link in the video description, sign up for free and check out the book Superintelligence by Nick Bostrom. Some more AI for you whenever you are stuck in traffic, or have to clean the house. I talked about this book earlier and I see that many of you Fellow Scholars have been enjoying it. If you haven’t read it, make sure to sign up now because this book discusses how it could be possible to build a superintelligent AI, and what such an all-knowing being would be like. You get this book free of charge, and you can cancel at any time. You can’t go wrong with this. Head on to the video description and sign up under the appropriate links. Thanks for watching and for your generous support, and I'll see you next time!"
304,GANPaint: An Extraordinary Image Editor AI,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper describes a new technique to visualize the inner workings of a generator neural network. This is a neural network that is is able to create images for us. The key idea here is dissecting this neural network, and looking for agreements between a set of neurons and concepts in the output image, such as trees, sky, clouds, and more. This means analyzing that these neurons are responsible for buildings to appear in the image, and those will generate clouds. Interestingly, such agreements can be found, which means way more than just creating a visualizations like this, because it enables us to edit images without any artistic skills. And now, hold on to your papers. The editing part works by forcefully activating and deactivating these units and correspond to adding or removing these objects from an image. And look! This means that we can take an already existing image, and ask this technique to remove trees from it, or perhaps add more, the same with domes, doors, and more. Wow! This is pretty cool, but you haven’t seen the best part yet! Note that so far, the amount of control we have over the image is quite limited. And fortunately, we can take this further, and select a region of the image where we wish to add something new. This is suddenly so much more granular and useful! The algorithm seems to understand that trees need to be rooted somewhere and not just appear from thin air. Most of the time anyway. Interestingly, it also understands that bricks don’t really belong here, but if I add it to the side of the building, it continues it in a way that is consistent with its appearance. Most of the time anyway. And of course, it is not perfect, here you can see me struggling with this spaghetti monster floating in the air that used to be a tree, and it just refuses to be overwritten. And this is a very important lesson most research works are but a step in a thousand-mile journey, and each of them tries to improve upon the previous paper. This means that a few more papers down the line, this will probably take place in HD, perhaps in real-time, and with much higher quality. This work also builds on previous knowledge on generative adversarial networks, and whatever the follow-up papers will contain, they will build on knowledge that was found in this work. Welcome to the wonderful world of research! And now, we can all rejoice because the authors kindly made the source code available free for everyone, and not only that, but there is also a web app so you can also try it yourself! This is an excellent way of maximizing the impact of your research work. Let the experts improve upon it by releasing the source code, and let people play with it, even laymen! You will also find many failure cases, but also cases where it works well, and I think there is value in reporting both so we learn a little more about this amazing algorithm. So, let’s do a little research together! Make sure to post your results in the comments section, I have a feeling that lots of high-quality entertainment materials will surface very soon. I bet the authors will also be grateful for the feedback as well, so, let’s the experiments begin! Thanks for watching and for your generous support, and I'll see you next time!"
305,Liquid Splash Modeling With Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If you have been watching this series for a while, you know that I am completely addicted to fluid simulations, so it is now time for a new fluid paper. And by the end of this video, I hope you will be addicted too. If we create a virtual world with a solid block, and use our knowledge from physics to implement the laws of fluid dynamics, this solid block will indeed start behaving like a fluid. A baseline simulation technique for this will be referred to as FLIP in the videos that you see here and it stands for Fluid Implicit Particle. These simulations are often being used in the video game industry, in movies, and of course, I cannot resist to put some of them in my papers as test scenes as well. In games, we are typically looking for real-time simulations, and in this case, we can only get a relatively coarse-resolution simulation that lacks fine details, such as droplet formation and splashing. For movies, we want the highest-fidelity simulation possible, with honey coiling, two-way interaction with other objects, wet sand simulations, and all of those goodies, however, these all take forever to compute. This is the bane of fluid simulators. We have talked about a few earlier works that try to learn these laws via a neural network by feeding them a ton of video footage of these phenomena. This is absolutely amazing and is a true game changer for learning-based techniques. So, why is that? Well, up until a few years ago, whenever we had a problem that was near impossible to solve with traditional techniques, we often reached out to a neural network or some other learning algorithm to solve it, often with success. However, it is not the case here. Something has changed. What has changed is that we can already solve these problems, but we can still make use of a neural network because it can help us with something that we can already do, but it does it faster and easier. However, some of these techniques for fluids are not yet as accurate as we would like and therefore haven’t yet seen widespread adoption. So here’s an incredible idea: why not compute a coarse simulation quickly that surely adheres to the laws of physics, and then, fill the remaining details with a neural network. Again, FLIP is the baseline hand-crafted technique, and you can see how the neural network-infused simulation program on the left by the name MLFlip introduces these amazing details. Hm-hm! And if we compare the results with the reference simulation, which took forever, you see that it is quite similar and it indeed fills in the right kind of details. In case you are wondering about the training data, it learned the concept of splashes and droplets flying about … you guessed it right…by looking at splashes and droplets flying about. So, now we know that it’s quite accurate and now, the ultimate question is, how fast is it? Well, get this we can expect a 10-time speedup from this. So this basically means that for every 10 all nighters I have to wait for my simulations, I only have to wait one, and if something took only a few seconds, it now may be close to real time with this kind of visual fidelity. You know what, sign me up. This video has been kindly supported by my friends at ARM Research, make sure to check them out through the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
306,DeepMind: The Hanabi Card Game Is the Next Frontier for AI Research,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Now get this, after defeating Chess, Go, and making incredible progress in Starcraft 2, scientists at DeepMind just published a paper where they claim that Hanabi is the next frontier in AI research. And we shall stop …right here. I hear you asking me, Károly, after defeating all of these immensely difficult games, now, you are trying to tell me that somehow this silly card game is the next step? Yes, that’s exactly what I am saying. Let me explain. Hanabi is a card game where two to five players cooperate to build five card sequences and to do that, they are only allowed to exchange very little information. This is also an imperfect information game, which means the players don’t have all the knowledge available needed to make a good decision. They have to work with what they have and try to infer the rest. For instance, Poker is also an imperfect information game because we don’t see the cards of the other players and the game revolves around our guesses as to what they might have. In Hanabi, interestingly, it is the other way around, so we see the cards of the other players, but not our own ones. The players have to work around this limitation by relying on each other and working out communication protocols and infer intent in order to win the game. Like in many of the best games, these simple rules conceal a vast array of strategies, all of which are extremely hard to teach to current learning algorithms. In the paper, a free and open source system is proposed to facilitate further research works and assess the performance of currently existing techniques. The difficulty level of this game can also be made easier or harder at will from both inside and outside the game. And by inside I mean that we can set parameters like the number of allowed mistakes that can be made before the game is considered lost. The outside part means that two main game settings are proposed: one, self-play, this is the easier case where the AI plays with copies of itself, therefore it knows quite a bit about its teammates, and two, ad-hoc teams can also be constructed, which means that a set of agents need to cooperate that are not familiar with each other. This is immensely difficult. When I looked the paper, I expected that as we have many powerful learning algorithms, they would rip through this challenge with ease, but surprisingly, I found out that that even the easier self-play variant severely underperforms compared to the best human players and handcrafted bots. There is plenty of work to be done here, and luckily, you can also run it yourself at home and train some of these agents on a consumer graphics card. Note that it is possible to create a handcrafted program that plays this game well, as we, humans already know good strategies, however, this project is about getting several instances of an AI to learn new ways to communicate with each other effectively. Again, the goal is not to get a computer program that plays Hanabi well, the goal is to get an AI to learn to communicate effectively and work together towards a common goal. Much like Chess, Starcraft 2 and DOTA, Hanabi is still a proxy to be used for measuring progress in AI research. Nobody wants to spend millions of dollars to play card games at work, so the final goal of DeepMind is to reuse this algorithm for other applications where even we, humans falter. I have included some more materials on this game in the video description, make sure to have a look. Thanks for watching and for your generous support, and I'll see you next time!"
307,Google’s PlaNet AI Learns Planning from Pixels,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we are going to talk about PlaNET, a technique that is meant to solve challenging image-based planning tasks with sparse rewards. Ok, that sounds great, but what do all of these terms mean? The planning part is simple, it means that the AI has to come up with a sequence of actions to achieve a goal, like pole balancing with a cart, teaching a virtual human or a cheetah to walk, or hitting this box the right way to make sure it keeps rotating. The image-based part is big this means that the AI has to learn the same way as a human, and that is, by looking at the pixels of the images. This is a huge difficulty bump because the AI does not only have to learn to defeat the game itself, but also has to build an understanding of the visual concepts within the game. DeepMind’s legendary Deep Q-Learning algorithm was also able to learn from pixel inputs, but it was mighty inefficient at doing that, and no wonder, this problem formulation is immensely hard and it is a miracle that we can muster any solution at all that can figure it out. The sparse reward part means that we rarely get feedback as to how well we are doing at these tasks, which is a nightmare situation for any learning algorithm. A key difference with this technique against classical reinforcement learning, which is what most researchers reach out to to solve similar tasks, is that this one uses models for the planning. This means that it does not learn every new task from scratch, but after the first game, whichever it may be, it will have a rudimentary understanding of gravity and dynamics, and it will be able to reuse this knowledge in the next games. As a result, it will get a headstart when learning a new game and is therefore often 50 times more efficient than the previous technique that learns from scratch, and not only that, but it has other really cool advantages as well which I will tell you about in just a moment. Here you can see that indeed, the blue lines significantly outperform the previous techniques shown with red and green for each of these tasks. I like how this plot is organized in the same grid as the tasks were as it makes it much more readable when juxtaposed with the video footage. As promised, here are the two really cool additional advantages of this model-based agent. The first is that we don’t have to train six separate AIs for all of these tasks, but finally, we can get one AI that is able to solve all six of these tasks efficiently. And second, it can look at as little as five frames of an animation, which is approximately one fifth of a second worth of footage…that is barely anything and it is able to predict how the sequence would continue with a remarkably high accuracy, and, over a long time frame, which is quite a challenge. This is an excellent paper with beautiful mathematical formulations, I recommend that you have a look in the video description. The source code is also available free of charge for everyone, so I bet this will be an exciting direction for future research works, and I’ll be here to report on it to you. Make sure to subscribe and hit the bell icon to not miss future episodes. Thanks for watching and for your generous support, and I'll see you next time!"
308,This AI Learned to “Photoshop” Human Faces,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we talk quite a bit about neural network-based learning methods that are able to generate new images for us from some sort of sparse description, like a written sentence, or a set of controllable parameters. These can enable us mere mortals without artistic skills to come up with novel images. However, one thing that comes up with almost every single one of these techniques is the lack of artistic control. You see, if we provide a very coarse input, there are many-many different ways for the neural networks to create photorealistic images from them. So, how do we get more control over these results? An earlier paper from NVIDIA generated human faces for us and used a latent space technique that allows us some more fine-grained control over the images. It is beyond amazing. But, these are called latent variables because they represent the inner thinking process of the neural network, and they do not exactly map to our intuition of facial features in reality. And now, have a look at this new technique that allows us to edit the geometry of the jawline of a person, put a smile on someone’s face in a more peaceful way than seen in some Batman movies, or, remove the sunglasses and add some crazy hair at the same time. Even changing the hair of someone while adding an earring with a prescribed shape is also possible. Whoa! And I just keep talking and talking about artistic control, so it’s great that these shapes are supported, but what about an other important aspect of artistic control, for instance, colors? Yup, that is also supported. Here you see that the color of the woman’s eyes can be changed, and, the technique also understands the concept of makeup as well. How cool is that! Not only that, but it is also blazing fast. It takes roughly 50 milliseconds to create these images with the resolution of 512 by 512, so in short, we can do this about 20 times per second. Make sure to have a look at the paper that also contains a validation section against other techniques and reference results, turns out there is such a thing as a reference result in this case, which is really cool, and you will also find a novel style loss formulation that makes all this crazy wizardry happen. No web app for this one, however, the source code is also available free of charge and under a permissive license, so let the experiments begin! If you have enjoyed this video and you feel that a bunch of these videos are worth 3 dollars a month, please consider supporting us on Patreon. In return, we can offer you early access to these episodes, and more, to keep your paper addiction in check. It truly is a privilege for me to be able to keep making these videos, I am really enjoying the journey, and this is only possible because of your support on Patreon. This is why every episode ends with, you guessed it right… Thanks for watching and for your generous support, and I'll see you next time!"
309,NeuroSAT: An AI That Learned Solving Logic Problems,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Please meet NeuroSAT. The name of this technique tells us what it is about, the neuro part means that it is a neural network-based learning method, and the SAT part means that it is able to solve satisfiability problems. This is a family of problems where we are given a logic formula, and we have to decide whether these variables can be chosen in a way such that this expression comes out true. That, of course, sounds quite nebulous, so let’s have a look at a simple example this formula says that F is true if A is true and at the same time not B is true. So if we choose A to be true and B as false, this expression is also true, or in other words, this problem is satisfied. Getting a good solution for SAT is already great for solving many problems involving logic, however, the more interesting part is that it can help us solve an enormous set of other problems, for instance, ones that involve graphs describing people in social networks, and many others that you see here. This can be done by performing something that mathematicians like to call polynomial-time reduction or Karp-reduction, which means that many other problems that seem completely different can be converted into a SAT problem. In short, if you can solve SAT well, you can solve all of these problems well. This is one of the amazing revelations I learned about during my mathematical curriculum. The only problem is that when trying to solve big and complex SAT problems, we can often not do much better than random guessing, which, for some of most nasty ones, takes so long that it practically is never going to finish. And get this, interestingly, this work presents us with a neural network that is able to solve problems of this form, but not like this tiny-tiny baby problem, but much bigger ones. And, this really shouldn’t be possible. Here’s why. To train a neural network, we require training data. The input is a problem definition and the output is whether this problem is satisfiable. And we can stop right here, because here lies our problem. This doesn’t really make any sense, because we just said that it is difficult to solve big SAT problems. And here comes the catch! This neural network learns from SAT problems that are small enough to be solved by traditional handcrafted methods. We can create arbitrarily many training examples with these solvers, albeit these are all small ones. And that’s not it, there are three key factors here that make this technique really work. One, it learns from only single-bit supervision. This means that the output that we talked about is only yes or no. It isn’t shown the solution itself. That’s all the algorithm learns from. Two, when we request a solution from the neural network, it not only tells us the same binary yes-no answer that was used to teach it, but it can go beyond that, and when the problem is satisfiable, it will almost always provide us with the exact solution. It is not only able to tell us whether the problem can be solved, but it almost always provides a possible solution as well. That is indeed remarkable. This image may be familiar from the thumbnail, and here you can see how the neural network’s inner representation of how these variables change over time as it sees a satisfiable and unsatisfiable problem and how it comes to its own conclusions. And three, when we ask the neural network for a solution, it is able to defeat problems that are larger and more difficult than the ones it has trained on. So this means that we train it on simple problems that we can solve ourselves, and using these as training data, it will be able to solve much harder problems that we can’t solve ourselves. This is crucial, because otherwise, this neural network would only be as good as the handcrafted algorithm used to train it. Which in other words, is not useful at all. Isn’t this amazing? I will note that there are handcrafted algorithms that are able to match, and often outperform NeuroSAT, however, those took decades of research work to invent, whereas this is a learning-based technique that just looks at as little information as the problem definition and whether it is satisfiable, and it is able to come up with a damn good algorithm by itself. What a time to be alive. This video has been kindly supported by my friends at ARM Research, make sure to check them out through the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
310,"Beautiful Gooey Simulations, Now 10 Times Faster","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. You know that I see a piece of fluid and I can’t resist making videos about it…I just can’t. Oh my goodness. Look at that. These animations were created using the Material Point Method, or MPM in short, which is a hybrid simulation method which is able to simulate not only substances like water and honey, but it can also simulate snow, granular solids, cloth, and many-many other amazing things that you see here. Before you ask, the hybrid part means that it both uses particles and grids during the computations. Unfortunately, it is very computationally demanding, so it takes forever to get these simulations ready. And typically, in my simulations, after this step is done, I almost always find that the objects did not line up perfectly, so I can start the whole process again. Ah well. This technique has multiple stages, uses multiple data structures in many of them, and often we have to wait for the results of one stage to be able to proceed to the next. This is not that much of a problem if we seek to implement this on our processor, but it would be way, way faster if we could run it on the graphics card…as long as we implement these problems on them properly. However, due to these stages waiting for each other, it is immensely difficult to use the heavy parallel computing capabilities of the graphics card. So here you go, this technique enables running MPM on your graphics card efficiently, resulting in an up to ten-time improvement over previous works. As a result, this granulation scene has more than 6.5 million particles on a very fine grid and can be simulated in only around 40 seconds per frame. And not only that, but the numerical stability of this technique is also superior to previous works, and it is thereby able to correctly simulate how the individual grains interact in this block of sand. Here is a more detailed breakdown of the number of particles, grid resolutions, and the amount of computation time needed to simulate each time step. I am currently in the middle of a monstrous fluid simulation project and oh man, I wish I had these numbers for the computation time. This gelatin scene takes less than 7 seconds per frame to simulate with a similar number of particles. Look at that heavenly gooey thing. It probably tastes like strawberries. And if you enjoyed this video and you wish to help us teach more people about these amazing papers, please consider supporting us on Patreon. In return, we can offer you early access to these episodes, or you can also get your name in the video description of every episode as a key supporter. You can find us at Patreon.com/TwoMinutePapers. Thanks for watching and for your generous support, and I'll see you next time!"
311,A Bitter AI Lesson - Compute Reigns Supreme!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Before we start, I’d like to tell you that this video is not about a paper, and it is not going to be two minutes. Welcome to Two Minute Papers! This piece bears the name “The Bitter Lesson” and was written by Richard Sutton, a legendary Canadian researcher who has contributed a great deal to reinforcement learning research. And what a piece this is! It is a short article on how we should do research, and ever since I read it, I couldn’t stop thinking about it, and as a result, I couldn’t not make a video on this topic. We really have to talk about this. It takes less than 5 minutes to read, so before we talk about it, you can pause the video and click the link to it in the video description. So, in this article, he makes two important observations. Number one, he argues that the best performing learning techniques are the ones that can leverage computation, or, in other words, methods that improve significantly as we add more compute power. Long ago, people tried to encode lots of human knowledge of strategies in their Go AI, but did not have enough compute power to make a truly great algorithm. And now we have AlphaGo, which contains minimal information about Go itself, and it is better than the best human players in the world. And, number two, he recommends that we try to put as few constraints on a problem as possible. He argues that we shouldn't try to rebuild the mind, but try to build a method that can capture arbitrary complexity and scale it up with hardware. Don’t try to make it work like your brain make something as general as possible and make sure it can leverage computation and it will come up with something that is way better than our brain. So, in short, keep the problem general, and don’t encode your knowledge of the domain into your learning algorithms. The weight of this sentence is not to be underestimated, because these seemingly simple observations sound really counterintuitive. This seemingly encourages us to do the exact opposite of what we are currently doing. Let me tell you why. I have fond memories of my early lectures I attended to in cryptography, where we had a look at ciphertexts. These are very much like encrypted messages that children like to write each other at school, which looks like nonsense for the unassuming teacher, but can be easily decoded by another child when provided with a key. This key describes which symbol corresponds to which letter. Let’s assume that one symbol means one letter, but if we don’t have any additional knowledge, this is still not an easy problem to crack. But in this course, soon, we coded up algorithms that were able to crack messages like this in less than a second. How exactly? Well, by inserting additional knowledge into the system. For instance, we know the relative frequency of each letter in every language. For instance, in English, the letter “e” is the most common by far, and then comes “t”, “a”, and the others. The fact that we are not seeing letters but symbols doesn’t really matter, because we just look up the most frequent symbol in the ciphertext and we immediately know that okay, that symbol is going to be the letter “e”, and so on. See what we have done here? Just by inserting a tiny bit of knowledge, suddenly, a very difficult problem turned into a trivial problem. So much so that anyone can implement this after their second cryptography lecture. And somehow, Richard Sutton argues that we shouldn’t do that? Doesn’t that sound crazy? So, what gives? Well, let me explain through an example from light transport research that demonstrates his point. Path tracing is one of the first and simplest algorithms in the field, which, in many regards, is vastly inferior to Metropolis Light Transport, which is a much smarter algorithm. However, with our current powerful graphics cards, we can compute so many more rays with path tracing, that in many cases it wins over Metropolis. In this case, compute reigns supreme. The hardware scaling outmuscles the smarts, and we haven’t even talked about how much easier it is for engineers to maintain and improve a simpler system. The area of Natural Language Processing has many decades of research to teach machines how to understand, simplify, correct, or even generate text. After so many papers and handcrafted techniques which insert our knowledge of linguistics into our techniques, who would have thought that OpenAI would be able to come up with a relatively simple neural network with so little prior knowledge that is able to write articles that sound remarkably lifelike. We will talk about this method in more detail in this series soon. And here comes the bitter lesson. Doing research the classical way of inserting knowledge into a solution is very satisfying it feels right, it feels like doing research, progressing, and it makes it easy to show in a new paper what exactly the key contributions are. However, it may not be the most effective way forward. Quoting the article. I recommend that you pay close attention to this. “The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because its success over a favored, human-centric approach.“ In our cryptography problem from earlier, of course the letter frequency solution and other linguistic tricks are clearly much, much better than a solution that doesn’t know anything about the domain. Of course! However, when later, we have a 100 times faster hardware, this knowledge may actually inhibit finding a solution that is way, way better. This is why he also claims that we shouldn’t try to build intelligence by modeling our brain in a computer simulation. It’s not that the “our brain” approach doesn’t work it does, but on the short run. On the long run, we will be able to add more hardware to a learning algorithm, and it will find more effective structures to solve problems, and it will eventually outmuscle our handcrafted techniques. In short, this is the lesson: when facing a learning problem, keep your domain knowledge out of the solution, and use more compute. More compute gives us more learning, and more general formulations give us more chance to find something relevant. So, this is indeed a harsh lesson. This piece sparked great debates on Twitter, I have seen great points for and against this sentiment. What do you think? Let me know in the comments, as everything in science, this piece should be subject to debate and criticism. And therefore, I’d love to read as many people’s take on it as possible. And, this piece has implications on my thinking as well — please allow me to add a three more small personal notes that kept me up at night in the last few days. Note number one, the bottom line is whenever we build a new algorithm, we should always bear in mind which parts would be truly useful if we had a 100 times the compute power that we have now. Note number two, a corollary of this thinking is that arguably, hardware engineers who make these new and more powerful graphics cards may be contributing the very least as much to AI than most of AI research does. And note number three, to me it feels like this almost implies that best is to join the big guys where all the best hardware is. I work in an amazing, small to mid-sized lab at the Technical University of Vienna and in the last few years, I have given relatively little consideration to the invitations from some of the more coveted and well-funded labs. Was it a mistake? Should I change that? I really don’t know for sure. If for some reason, you haven’t read the piece at the start of the video, make sure to do it after watching this. It’s really worth it. In the meantime, interestingly, the non-profit AI research lab, OpenAI also established a for profit, or what they like to call “capped profit” company to be able to compete with the other big guys like DeepMind and Facebook Reality Labs. I think Richard has a solid point here. Thanks for watching and for your generous support, and I'll see you next time!"
312,Why Are Cloth Simulations So Hard?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we are going to talk a bit about the glory and the woes of cloth simulation programs. In these simulations, we have several 3D models of garments that are built from up to several hundreds of thousands of triangles. And as they move in time, the interesting part of the simulation is whenever collisions happen, however, evaluating how these meshes collide is quite difficult and time consuming. Basically, we have to tell an algorithm that we have piece of cloth with a hundred thousand connected triangles here, and another one there, and now, have a look and tell me which collides with which and how they bend and change in response these forces. And don’t forget about friction and repulsive forces! Also, please, be accurate because every small error adds up over time, and, do it several times a second so we can have a look at the results interactively. Well, this is a quite challenging problem. And it takes quite long to compute, so much so that 70-80% of the total time taken to perform the simulation is spent with doing collision handling. So how can we make it not take forever? Well, one way would be to try to make sure that we can run this collision handling step on the graphics card. This is exactly what this work does, and in order to do this, we have to make sure that all these evaluations can be performed in parallel. Of course, this is easier said than done. Another difficulty is choosing the appropriate time steps. These simulations are run in a way that we check and resolve all of the collisions, and then, we can advance the simulation forward by a tiny amount. This amount of called a time step, and choosing the appropriate time step has always been a challenge. You see, if we set it to too large, we will be done faster and compute less, however, we will almost certainly miss some collisions because we skipped over them. The simulation may end up in a state that is so incorrect that it is impossible to recover from, and we have to throw the entire thing out. If we set it to too low, we get a more robust simulation, however, it will take many hours to days to compute. To remedy this, this technique is built in a way such that we can use larger time steps. That’s excellent news. Also, the collision computation part is now up to 9 times faster, and if we look at the cloth simulation as a whole, that can be made over 3 times faster. As you see here, this is especially nice because we can test how these garments react to our manipulations at 8-10 frames per second. If you have a closer look at the paper, you will also find another key observation which states that most of the time, only a small subregion of the simulated cloth undergoes deformation due to response forces, and this knowledge can be kept track of, which contributed to cutting down the simulation time significantly. Thanks for watching and for your generous support, and I'll see you next time!"
313,OpenAI GPT-2: An Almost Too Good Text Generator!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is an incredible paper from OpenAI, in which the goal is to teach an AI to read a piece of text and perform common natural language processing operations on it, for instance, answering questions, completing text, reading comprehension, summarization, and more. And not only that, but additionally, the AI has to be able to perform these tasks with as little supervision as possible. This means that we seek to unleash the algorithm that they call GPT-2 to read the internet and learn the intricacies of our language by itself. To perform this, of course, we need a lot of training data, and here, the AI reads 40 gigabytes of internet text, which is 40 gigs of non-binary plaintext data, which is a stupendously large amount of text. It is always hard to put these big numbers in context, so as an example, to train similar text completion algorithms, AI people typically reach out to a text file containing every significant work of Shakespeare himself, and this file is approximately 5 megabytes. So the 40 gigabytes basically means an amount of text that is 8000 times the size of Shakespeare’s works. That’s a lot of text. And now, let’s have a look at how it fares with the text completion part. This part was written by a human, quoting: “In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.” And the AI continued the text the following way, quoting a short snippet of it: “The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.” Whoa! Now note that this is clearly not perfect, if there is even such a thing as a perfect continuation, and it took 10 tries, which means that the algorithm was run 10 times and the best result was cherrypicked and recorded here. And despite all of these, this is a truly incredible result, especially given that the algorithm learns on its own. After giving it a piece of text, it can also answer questions in a quite competent manner. Worry not, later in this video, I will show you more of these examples and likely talk over them, so if you are curious, feel free to pause the video while you read the prompts and their completions. The validation part of the paper reveals that this method is able to achieve state-of-the-art results on several language modeling tasks, and you can see here that we still shouldn’t expect it to match a human in terms of reading comprehension, which is the question answering test. More on that in a moment. So, there are plenty of natural language processing algorithms out there that can perform some of these tasks, in fact, some articles already stated that there is not much new here, it’s just the same problem, but stated in a more general manner, and with more compute. A ha! It is not the first time that this happens. Remember our video by the name “The Bitter lesson”? I’ve put a link to it in the video description, but in case you missed it, let me quote how Richard Sutton addressed this situation: “The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because its success over a favored, human-centric approach.“ So what is the big lesson here? Why is GPT-2 so interesting? Well, big lesson number one is that this is one of the clearer cases of what the quote was talking about, where we can do a whole lot given a lot of data and compute power, and we don’t need to insert too much additional knowledge into our algorithms. And lesson number two, as a result, this algorithm becomes quite general so it can perform more tasks than most other techniques. This is an amazing value proposition. I will also add that not every learning technique scales well when we add more compute, in fact, you can see here yourself that even GPT-2 plateaus on the summarization task. Making sure that these learning algorithms scale well is a great contribution in and of itself and should not be taken for granted. There has been a fair bit of discussion on whether OpenAI should publish the entirety of this model. They opted to release a smaller part of the source code and noted that they are aware that the full model could be used for nefarious purposes. Why did they do this? What is the matter with everyone having an AI with a subhuman-level reading comprehension? Well, so far, we have only talked about quality. But another key part is quantity. And boy, are these learning methods superhuman in terms of quantity…just imagine that they can write articles with a chosen topic and sentiment all day long, and, much quicker than human beings. Also note that the blueprint of the algorithm is described in the paper, and a top-tier research group is expected to be able to reproduce it. So does one release the full source code and models or not? This is a quite difficult question: we need to keep publishing both papers and source code to advance science, but, we also have to find new ways to do it in an ethical manner. This needs more discussion and would definitely be worthy of a conference-style meeting. Or more. There is so much to talk about, and so far, we have really only scratched the surface, so make sure to have a look in the video description, I left a link to the paper and some more super interesting reading materials for you. Make sure to check them out! Also just a quick comment on why this video came so late after the paper has appeared. Since there were a lot of feelings and intense discussion on whether the algorithm should be published or not, I was looking to wait until the dust settles and there is enough information out there to create a sufficiently informed video for you. This, of course means that we are late to the party and missed out on a whole lot of views and revenue, but that’s okay, in fact, that’s what we’ll keep doing going forward to make sure you get the highest quality information that I can provide. If you have enjoyed this episode and would like to help us, please consider supporting us on Patreon! Remember our motto, a dollar a month is almost nothing, but it keeps the papers coming. And there are hundreds of papers on my reading list. As always, we are available through Patreon.com/TwoMinutePapers, and the link is also available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
314,How Do Neural Networks Memorize Text?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is an article from the Distill journal, so expect a lot of intuitive and beautiful visualizations. And it is about recurrent neural networks. These are neural network variants that are specialized to be able to deal with sequences of data. For instance, processing and completing text is a great example usage of these recurrent networks. So, why is that? Well, if we wish to finish a sentence, we are not only interested in the latest letter in this sentence, but several letters before that, and of course, the order of these letters is also of utmost importance. Here you can see with the green rectangles which previous letters these recurrent neural networks memorize when reading and completing our sentences. LSTM stands for Long Short-Term Memory, and GRU means Gated Recurrent Unit, both are recurrent neural networks. And you see here that the nested LSTM doesn’t really look back further than the current word we are processing, while the classic LSTM almost always memorizes a lengthy history of previous words. And now, look, interestingly, with GRU, when looking at the start of the word “grammar” here, we barely know anything about this new word, so, it memorizes the entire previous word as it may be the most useful information we have at the time. And now, as we proceed a few more letters into this word, it mostly shifts its attention to a shorter segment, that is, the letters of this new word we are currently writing. Luckily, the paper is even more interactive, meaning that you can also add a piece of text here and see how the GRU network processes it. One of the main arguments of this paper is that when comparing these networks against each other in terms of quality, we shouldn’t only look at the output text they generate. For instance, it is possible for two models that work quite differently to have a very similar accuracy and score on these tests. The author argues that we should look beyond these metrics, and look at this kind of connectivity information as well. This way, we may find useful pieces of knowledge, like the fact that GRU is better at utilizing longer-term contextual understanding. A really cool finding indeed, and I am sure this will also be a useful visualization tool when developing new algorithms and finding faults in previous ones. Love it. Thanks for watching and for your generous support, and I'll see you next time!"
315,NVIDIA's AI Creates Beautiful Images From Your Sketches!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I know for a fact that some of you remember our first video on image translation, which was approximately 3 years and 250 episodes ago. This was a technique where we took an input painting, and a labeling of this image that shows what kind of objects are depicted, and then, we could start editing this labeling, and out came a pretty neat image that satisfies these labels. Then came pix2pix, another image translation technique which in some cases, only required a labeling, a source photo was not required because these features were learned from a large amount of training samples. And it could perform really cool things, like translating a landscape into a map, or sketches to photos, and more. Both of these works were absolutely amazing, and I always say, two more papers down the line, and we are going to have much higher resolution images. So, this time, here is the paper that is, in fact, two more papers down the line. So let’s see what it can do! I advise you that you hold on to your papers for this one. The input is again, a labeling which we can draw ourselves, and the output is a hopefully photorealistic image that adheres to these labels. I like how first, only the silhouette of the rock is drawn, so we have this hollow thing on the right that is not very realistic, and then, it is now filled in with the bucket tool, and, there you go. It looks amazing. It synthesizes a relatively high-resolution image and we finally have some detail in there too. But, of course, there are many possible images that correspond to this input labeling. How do we control the algorithm to follow our artistic goals? Well, you remember from the first work I’ve shown you where we could do that by adding an additional image as an input style. Well, look at that! We don’t even need to engage in that, because here, we can choose from a set of input styles that are built into the algorithm and we can switch between them almost immediately. I think the results speak for themselves, but note that not only the visual fidelity, but the alignment with the input labels is also superior to previous approaches. Of course, to perform this, we need a large amount of training data where the inputs are labels, and the outputs are the photorealistic images. So how do we generate such a dataset? Drawing a bunch of labels and asking artists to fill them in sounds like a crude and expensive idea. Well, of course, we can do it for free by thinking the other way around! Let’s take a set of photorealistic images, and use already existing algorithms to create the labeling for them. If we can do that, we’ll have as many training samples as many images we have, in other words, more than enough to train an amazing neural network. Also, the main part of the magic in this new work is using a new kind of layer for normalizing information within this neural network that adapts better to our input data than the previously used batch normalization layers. This is what makes the outputs more crisp and does not let semantic information be washed away in these images. If you have a closer look at the paper in the video description, you will also find a nice evaluation section with plenty of comparisons to previous algorithms and according to the authors, the source code will be released soon as well. As soon as it comes out, everyone will be able to dream up beautiful photorealistic images and get them out almost instantly. What a time to be alive! If you have enjoyed this episode and would like to support us, please click one of the Amazon affiliate links in the video description and buy something that you were looking to buy on Amazon anyway. You don’t lose anything, and this way, we get a small kickback which is a great way to support the series so we can make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
316,Exploring And Attacking Neural Networks With Activation Atlases,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When it comes to image classification tasks, in which the input is a photograph and the output is decision as to what is depicted in this photo, neural network-based learning solutions became more accurate than any other computer program we, humans could possibly write by hand. Because of that, the question naturally arises: what do these neural networks really do inside to make this happen? This article explores new ways to visualize the inner workings of these networks, and since it was published in the Distill journal, you can expect beautiful and interactive visualizations that you can also play with if you have a look in the video description. It is so good, I really hope that more modern journals like this appear in the near future. But back to our topic wait a second, we already had several videos on neural network visualization before, so what is new here? Well, let’s see! First, we have looked at visualizations for individual neurons. This can be done by starting from a noisy image and add slight modifications to it in a way that makes a chosen neuron extremely excited. This results in these beautiful colored patterns. I absolutely love, love, love these patterns, however, this misses all the potential interactions between the neurons, of which there are quite many. With this, we have arrived to pairwise neuron activations, which sheds more light on how these neurons work together. Another one of those beautiful patterns. This is, of course, somewhat more informative: intuitively, if visualizing individual neurons was equivalent to looking at a sad little line, the pairwise interactions would be observing 2D slices in a space. However, we are still not seeing too much from this space of activations, and the even bigger issue is that this space is not our ordinary 3D space, but a high-dimensional one. Visualizing spatial activations gives us more information about these interactions between not two, but more neurons, which brings us closer to a full-blown visualization, however, this new Activation Atlas technique is able to provide us with even more extra knowledge. How? Well, you see here with the dots that it provides us a denser sampling of the most likely activations, and, this leads to a more complete bigger-picture view of the inner workings of the neural network. This is what it looks like if we run it on one image. It also provides us with way more extra value, because so far, we have only seen how the neural network reacts to one image, but this method can be extended to see its reaction to not one, but one million images! You can see an example of that here. What’s more, it can also unveil weaknesses in the neural network. For instance, have a look at this amazing example where the visualization uncovers that we can make this neural network misclassify a grey whale for a great white shark, and all we need to do is just brazenly put a baseball in this image. It is not a beautiful montage, is it? Well, that’s not a drawback, that’s exactly the point! No finesse is required, and the network is still fooled by this poorly-edited adversarial image. We can also trace paths in this atlas which reveal how the neural network decides whether one or multiple people are in an image, or how to tell a watery type terrain from a rocky cliff. Again, we have only scratched the surface here, and you can play with these visualizations yourself, so make sure to have a closer look at the paper through the link in the video description. You won’t regret it. Let me know in the comments section how it went! Thanks for watching and for your generous support, and I'll see you next time!"
317,How To Train Your Virtual Dragon,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Scientists at the Seoul National University in South Korea wrote a great paper on teaching an imaginary dragon all kinds of really cool aerobatic maneuvers, like sharp turning, rapid winding, rolling, soaring, and diving. This is all done by a reinforcement learning variant, where the problem formulation is that the AI has to continuously choose the character’s actions to maximize a reward. Here, this reward function is related to a trajectory which we can draw in advance, these are the lines that the dragon seems follow quite well. However, what you see here is the finished product. Curious to see how the dragon falters as it learns to maneuver properly? Well, we are in luck. Buckle up. You see the ideal trajectory here with black, and initially, the dragon was too clumsy to navigate in a way that even resembles this path. Then, later, it learned to start the first turn properly, but as you see here, it was unable to avoid the obstacle and likely needs to fly to the emergency room. But it would probably miss that building too, of course. After more learning, it was able to finish the first loop, but was still too inaccurate to perform the second. And finally, at last, it became adept at performing this difficult maneuver. Applause! One of the main difficulties of this problem is the fact that the dragon is always in motion and has a lot of momentum, and anything we do always has an effect later, and we not only have to find one good action, but whole sequences of actions that will lead us to victory. This is quite difficult. So how do we do that? To accomplish this, this work not only uses a reinforcement learning variant, but also adds something called self-regulated learning to it, where we don’t present the AI with a fixed curriculum, but we put the learner in charge of its own learning. This also means that it is able to take a big, complex goal and subdivide it into new, smaller goals. In this case, the big goal is following the trajectory with some more additional constraints, which, by itself, turned out to be too difficult to learn with these traditional techniques. Instead, the agent realizes, that if it tracks its own progress on a set of separate, but smaller subgoals, such as tracking its own orientation, positions, and rotations against the desired target states separately, it can finally learn to perform these amazing stunts. That sounds great, but how is this done exactly? This is done through a series of three steps, where step one is generation, where the learner creates a few alternative solutions for itself and proceeds to the second step, evaluation, where it has to judge these individual alternatives and find the best ones. And third, learning, which means looking back and recording whether these judgments indeed put the learner in a better position. By iterating these three steps, this virtual dragon learned to fly properly. Isn’t this amazing? I mentioned earlier that this kind of problem formulation is intractable without self-regulated learning, and you can see here how a previous work fares on following these trajectories. There is indeed a world of a difference between the two. So there you go, in case you enter a virtual world where you need to train your own dragon, you’ll know what to do. But just in case, also read the paper in the video description! If you enjoyed this episode and you wish to watch our other videos in early access, or get your name immortalized in the video description, please consider supporting us on Patreon through Patreon.com/TwoMinutePapers. The link is available in the video description, and this way, we can make better videos for you. We also support cryptocurrencies, the addresses are also available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
318,DeepMind's AI Learned a Better Understanding of 3D Scenes,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper was written by scientists at DeepMind and it is about teaching an AI to look at a 3D scene and decompose it into its individual elements in a meaningful manner. This is typically one of those tasks that is easy to do for humans, and is immensely difficult for machines. As this decomposition thing still sounds a little nebulous, let me explain what it means. Here you see an example scene, and a segmentation of this scene that the AI came up with, which shows what it thinks where the boundaries of these individual objects are. However, we are not stopping there, because it is also able to “rip out” these objects from the scene one by one. So why is this such a big deal? Well, because of three things. One, it is a generative model, meaning that it is able to reorganize these scenes and create new content that actually makes sense. Two, it can prove that it truly has an understanding of 3D scenes by demonstrating that it can deal with occlusions. For instance, if we ask it to “rip out” the blue cylinder from this scene, it is able to reconstruct parts of it that weren’t even visible in the original scene. Same with the blue sphere here. Amazing, isn’t it? And three, this one is a bombshell it is an unsupervised learning technique. Now, our more seasoned Fellow Scholars fell out of the chair hearing this, but just in case, this means that this algorithm is able to learn on its own and we have to feed it a ton of training data, but this training data is not labeled. In other words, it just looks at the videos with no additional information, and from watching all this content, it finds out on its own about the concept of these individual objects. The main motivation to create such an algorithm was to have an AI look at some gameplay of the Starcraft 2 strategy game and be able to recognize all individual units and the background without any additional supervision. I really hope this also means that DeepMind is working on a version of their StarCraft 2 AI that is able to learn more similarly to how a human does, which is, looking at the pixels of the game. If you look at the details, this will seem almost unfathomably difficult, but would, of course, make me unreasonably happy. What a time to be alive! If you check out the paper in the video description, you will find how all this is possible through a creative combination of an attention network and a variational autoencoder. This episode has been supported by Backblaze. Backblaze is an unlimited online backup solution for only 6 dollars a month, and I have been using it for years to make sure my personal data, family pictures and the materials required to create this series are safe. You can try it free of charge for 15 days, and if you don’t like it, you can immediately cancel it without losing anything. Make sure to sign up for Backblaze today through the link in the video description, and this way, you not only keep your personal data safe, but you also help supporting this series. Thanks for watching and for your generous support, and I'll see you next time!"
319,AI Learns Tracking People In Videos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There are many AI techniques that are able to look at a still image, and identify objects, textures, human poses and object parts in them really well. However, in the age of the internet, we have videos everywhere, so an important question would be how we could do the same for these animations. One of the key ideas in this paper is that the frames of these videos are not completely independent, and they share a lot of information, so after we make our initial predictions on what is where exactly, these predictions from the previous frame can almost always be reused with a little modification. Not only that, but here you can see with these results that it can also deal with momentary occlusions and is ready to track objects that rotate over time. A key part of this method is that one, it looks back and forth in these videos to update these labels, and second, it learns in a self-supervised manner, which means that all it is given is just a little more than data, and was never given a nice dataset with explicit labels of these regions and object parts that it could learn from. You can see in this comparison table that this is not the only method that works for videos, the paper contains ample comparisons against other methods and comes out ahead of all other unsupervised methods, and on this task, it can even get quite close to supervised methods. The supervised methods are the ones that have access to these cushy labeled datasets and therefore should come out way ahead. But they don’t, which sounds like witchcraft, considering that this technique is learning on its own. However, all this greatness comes with limitations. One of the bigger ones is that even though it does extremely well, it also plateaus, meaning that we don’t see a great deal of improvement if we add more training data. Now whether this is because it is doing nearly as well as it is humanly, or computerly possible, or because a more general problem formulation is still possible remains a question. I hope we find out soon. Thanks for watching and for your generous support, and I'll see you next time!"
320,"Simulating Grains of Sand, Now 6 Times Faster","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Good news! Another fluid paper is coming up today, and this one is about simulating granular materials. Most techniques that can simulate these grains can be classified as either discrete or continuum methods. Discrete methods, as the name implies, simulate all of these particles one by one. As a result, the amount of detail we can get in our simulations is unmatched, however, you probably are immediately asking the question: doesn’t simulating every single grain of sand take forever? Oh yes, yes it does. Indeed, the price to be paid for all this amazing detail comes in the form of a large computation time. To work around this limitation, continuum methods were invented, which do the exact opposite by simulating all of these particles as one block where most of the individual particles within the block behave in a similar manner. This makes the computation times a lot frendlier, however, since we are not simulating these grains individually, we lose out on a lot of interesting effects, such as clogging, bouncing and ballistic motions. So, in short, a discrete method gives us a proper simulation, but takes forever, while the continuum methods are approximate in nature, but execute quicker. And now, from this exposition, the question naturally arises: can we produce a hybrid method that fuses together the advantages of both of these methods? This amazing paper proposes a technique to perform that by subdividing the simulation domain into an inside regime where the continuum methods work well, and an outside regime where we need to simulate every grain of sand individually with a discrete method. But that's not all, because the tricky part comes in the form of the reconciliation zone, where a partially discrete and partially continuum simulation has to take place. The way to properly simulate this transition zone between these two regimes takes quite a bit of research effort to get right, and just think about the fact that we have to track and change these domains over time, because, of course, the inside and outside of a block of particles changes rapidly over time. Throughout the video, you will see the continuum zones denoted with red, and the discrete zones with blue, which are typically on the outside regions. The ratio of these zones gives us an idea of how much speedup we could get compared to a purely discrete stimulation. In most cases, it means that 88% fewer discrete particles need to be simulated and this can lead to a total speedup of 6 to 7 times over that simulation. Basically, at least 6 all nighter simulations running now in one night? I’m in. Sign me up. Also make sure to have a look at the paper because the level of execution of this work is just something else. Check it out in the video description. Beautiful work. My goodness. Thanks for watching and for your generous support, and I'll see you next time!"
321,OpenAI Five Beats World Champion DOTA2 Team 2-0!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This episode has been sponsored by Lambda Labs. Not so long ago, we talked about DeepMind’s AlphaStar, an AI that was able to defeat top-tier human players in Starcraft 2, a complex real-time strategy game. Of course, I love talking about AI’s that are developed to challenge pro gamers at a variety of difficult games, so this time around, we’ll have a look another major milestone, OpenAI Five, which is an AI that plays DOTA2, a multiplayer online battle arena game with a huge cult following. As this game requires long-term strategic planning, it is a classic nightmare scenario for any AI. But OpenAI is no stranger to DOTA2, in 2017 they showed us an initial version of their AI that was able to play 1 versus 1 games with only one hero and was able to reliably beat Dendi, a world champion player. That was quite an achievement, however, of course, this was meant to be a stepping stone towards playing the real DOTA2. Then, in 2018, they unveiled OpenAI Five, an improved version of this AI that played 5 versus 5 games with a limited hero pool. This team was able to defeat competent players, but was still not quite at the level of a world champion human team. In a one-hour interview, the OpenAI research team mentioned that due to the deadline of The International event, they had to make quite a few concessions. And this time, several things have changed. First, they didn’t just challenge some local team of formidable players, no-no, they flat out challenged OG, the reigning world champion team. An ambitious move, that exudes confidence from their side. Second, this time around, there was no tight deadline as the date of the challenge was chosen by OpenAI. Let’s quickly talk about the rules of the competition and then …see if OpenAI’s confident move was justified! These learning agents don’t look at the pixels of the game, and as a result, they see the world as a big bunch of numbers. And this time around, it was able to play a pool of 17 heroes, and trained against itself for millions and millions of games. And now, let’s have a look at what happened in this best of 3 series! In match 1, right after picking the roster of heroes, the AI estimated its win probability to be 67%, so it was quite a surprise that early on it looked like OpenAI’s bots were running around aimlessly. Over time, we found out that it was not at all the case it plays unusually aggressively from the get-go and uses buybacks quite liberally at times when human players don’t really consider it to be a good choice. These buybacks resurrect a perished hero quickly but in return, cost money. Later, it became clearer that these bots are no joke: they know exactly when to engage and when to back out from an engagement with the smallest sliver of health left. I will show quite a few examples of those to you during this video, so stay tuned. A little less than 20 minutes in, we had a very even game 1, if anything, OpenAI seemed a tiny bit behind, and someone noted that we should perhaps ask the bots what they think about their chances. And then the AI said, yeah, no worries, we have a higher than 95% chance to win the game. This was such a pivotal moment that was very surprising for everyone. Of course, if you you call out a win with confidence, you better go all the way and indeed win the game. Right? Right. And sure enough, they wiped out almost the entire world champion team of the human players immediately after. And then noted, you know what, remember that we just said? Forget that. We estimate our chances to win to be above 99% now. And shortly after, they won match number one. Can you believe this? This is absolutely amazing. Interestingly, one of the developers said that the AI is great at assessing whether a fight is worth it. As an interesting corollary, if you engage with it and it fights you, it probably means you are going to lose. That must be quite confusing for the players. Some mind games for you. Love it. At the event, it was also such a joy to see such a receptive audience that understood and appreciated high-level plays. Onwards to match number two. Right after the draft, which is the process of choosing the heroes for each team, the AI predicted a win percentage that was much closer this time around, around 60%. In this game, the AI turned up the heat real fast, and said just 5 minutes into the game, which is nothing, that it has an over 80% chance to win this game. And now, watch this. Early in this game, you can see a great example of where the AI just gets away with a sliver of health. Look at this guy. Look at that! This is either an accident or some unreal-level foresight from the side of this agent. I’d love to hear your opinion on which one you think it is. By the 9 and a half minute mark, which is still really early, OpenAI Five said yes, we got this one too. Over 95%. Here you see in interesting scenario where the AI loses one hero, but it almost immediately kills two of the human heroes, and comes out favorably, at which point we wonder whether this was a deliberate bait it pulled on the humans. By the 15-minute mark, the human players lost a barracks and were heavily underfunded and outplayed with seemingly no way to come back. And sure enough, by the 21-minute mark, the game was over. There is no other way to say it, this second game was a one-sided beatdown. Game 1 was a strategic back and forth where OpenAI Five waited for the right moment to win the game in a big team fight, where here, they pressured human team from the get-go and never let them reach the endgame where they might have and advantage with their picks. Also, have a look at this. Unreal. The final result is 2 to 0 for OpenAI. In the post match interview, N0tail, one of the human players noted that he is confident that from 5 games, they would take at least one, and after 15 games, they would start winning reliably. Very reminiscent of what we have heard from players playing against DeepMind’s AI in StarCraft 2 and I hope this will be tested. However, in the end, he agreed that it is inevitable that this AI will become unbeatable at some point. It was also noted that in 5v5 fights, they seem better in planning than any human team is and there is quite a lot to learn from the AI for us humans. They were also trying to guess the reasoning for all of these early buybacks. According to the players, initially, they flat out seemed like misplays. Perhaps the reason for these instant and not really great buybacks might have been the fact that the AI knows that if the game goes on for much longer, statistically, their chances with their given composition to win the game dwindles, so it needs to immediately go and win right now, whatever the cost. And again, an important lesson is that in this project, OpenAI is not spending so much money and resources to just play video games. DOTA2 is a wonderful testbed to see how their AI compares to humans at complex tasks that involve strategy and teamwork. However, the ultimate goal is to reuse parts of this system for other complex problems outside of video games. For example, the algorithm that you’ve seen here today can also do this. But wait, there’s more. Players after these showmatches always tend to get these messages from others on Twitter telling them what they did wrong and what they should have done instead. Well, luckily, these people were able to show their prowess as OpenAI gave the chance for anyone in the world to challenge the OpenAI Five competitively and play against them online. This way, not only team OG, but everyone can get crushed by the AI. How cool is that? This Arena event has concluded with over 15000 games played where OpenAI Five had a 99.4% winrate. There are still ways to beat it, but given the rate of progress of this project, likely not for long. Insanity. As always, if you are interested in more details, I put a link to a reddit AMA in the video description, and I also can’t wait to pick the algorithm apart for you, but for now, we’ll have to wait for the full paper to appear. And note that what happened here is not to be underestimated. Huge respect to the OpenAI team, to OG for the amazing games and congratulations to the humans who were able to beat these beastly bots online. So there you go, another long video that’s not two minutes, and it’s not about a paper. Yet. Welcome to Two Minute Papers! If you’re doing deep learning, make sure to look into Lambda GPU systems. Lambda offers workstations, servers, laptops, and a GPU cloud for deep learning. You can save up to 90% over AWS, GCP, and Azure GPU instances. Every Lambda GPU system is pre-installed with TensorFlow, PyTorch, and Keras. Just plug it in and start training. Lambda customers include Apple, Microsoft, and Stanford. Go to lambdalabs.com/papers or click the link in the description to learn more. Big thanks to Lambda for supporting Two Minute Papers and helping us make better videos. Thanks for watching and for your generous support, and I'll see you next time!"
322,This Robot Throws Objects with Amazing Precision,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this footage, we have a variety of objects that differ in geometry, and the goal is to place them into this box using an AI. Sounds simple, right? This has been solved long, long ago. However, there is a catch here, which is that this box is outside of the range of the robot arm, therefore, it has to throw it in there with just the right amount of force for it to end up in this box. It can perform 500 of these tosses per hour. Before anyone misunderstands what is going on in the footage here, it almost seems like the robot on the left is helping by moving to where the object would fall after the robot on the right throws it. This is not the case. Here you see a small part of my discussion with Andy Zeng, the lead author of the paper where he addresses this. The results look amazing, and note that this problem is much harder than most people would think at first. In order to perform this, the AI has to understand how to grasp an object with a given geometry, in fact, we may grab the same object at a different side, throw it the same way, and there would be a great deal of a difference in the trajectory of this object. Have a look at this example with the screwdriver. It also has to take into consideration the air resistance of a given object as well. Man, this problem is hard. As you see here, initially, it cannot even practice throwing because its reliability in grasping is quite poor. However, after 14 hours of training, it achieves a remarkable accuracy, and to be able to train for so long, this training table is designed in a way that when running out of objects, it can restart itself without human help. Nice! To achieve this, we need a lot of training objects, but not any kind of training objects. These objects have to be diversified. As you see here, during training, the box position enjoys a great variety, and the object geometry is also well diversified. Normally, in these experiments, we are looking to obtain some kind of intelligence. Intelligence in this case would mean that the AI truly learned the underlying dynamics of object throwing, and not just found some good solutions via trial and error. A good way to test this would be to give it an object it has never seen before and see how its knowledge generalizes to that. Same with locations. On the left, you see these boxes marked with orange, this was the training set, but, later, it was asked to throw it into the blue boxes, which is something it has never tried before…and…look! This is excellent generalization. Bravo! You can also see the success probabilities for grasping and throwing here. A key idea in this work is that this system is endowed with a physics-based controller, which contains the standard equations of linear projectile motion. This is simple knowledge from high-school physics that ignores several key real-life factors, such as the effect of aerodynamic drag. This way, the AI does not have to learn from scratch and can use these calculations as an initial guess, and it is tasked with learning to account for the difference between this basic equation and real-life trajectories. In other words, it is given basic physics and is asked to learn advanced physics by building on that. Loving this idea. A simulation environment was also developed for this project where one can test the effect of, for instance, changing the gripper width, which would be costly and labor-intensive in the real world. Of course, these are all free in a software simulation. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
323,This is How Google’s Phone Enhances Your Photos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Super resolution is a research field with a ton of published papers every year, where the simplest problem formulation is that we have a low-resolution, coarse image as an input and we wish to enhance it to get a crisper, higher resolution image. You know, the thing that can always be done immediately and perfectly in many of these detective TV series. And yes, sure, the whole idea of super resolution sounds a little like…science fiction. How could I possibly get more content onto an image that’s not already there? How would an algorithm know what a blurry text means if it’s unreadable? It can’t just guess what somebody wrote there, can it? Well, let’s see. This paper provides an interesting take on this topic, because it rejects the idea of having just one image as an input. You see, in this day and age, we have powerful mobile processors in our phones, and when we point our phone camera and take an image, it doesn’t just take one, but a series of images. Most people don’t know that some of these images are even taken as soon as we open our camera app without even pushing the shoot button. Working with a batch of images is also the basis of the iPhone’s beloved live photo feature. So as a result, this method builds on this raw burst input with multiple images, and doesn’t need idealized conditions to work properly, which means that it can process footage that we shoot with our shaky hands. In fact, it forges an advantage out of this imperfection, because it can first, align these photos, and then, we have not one image, but a bunch of images with slight changes in viewpoint. This means that we have more information that we can extract from these several images, which can be stitched together into one, higher-quality output image. Now that’s an amazing idea if I’ve ever seen one it not only acknowledges the limitations of real-world usage, but even takes advantage of it. Brilliant. You see throughout this video that the results look heavenly. However, not every kind of motion is desirable. If we have a more complex motion, such as the one you see here as we move away from the scene, this can lead to unwanted artifacts in the reconstruction. Luckily, the method is able to detect these cases by building a robustness mask that highlights which are the regions that will likely lead to these unwanted artifacts. Whatever is deemed to be low-quality information in this mask is ultimately rejected, leading to high-quality outputs even in the presence of weird motions. And now, hold on to your papers, because this method does not use neural networks or any learning techniques, and is orders of magnitude faster than those while providing higher-quality images. As a result, the entirety of the process takes only a 100 milliseconds to process a really detailed, 12 megapixel image, which means that it can do it 10 times every second. These are interactive framerates, and it seems that doing this in real time is going to be possible within the near future. Huge congratulations to Bart and his team at Google for outmuscling the neural networks. Luckily, higher-quality ground truth data can also be easily produced for this project, creating a nice baseline to compare the results to. Here you see that this new method is much closer to this ground truth than previous techniques. As an additional corollary of this solution, the more of these jerky frames we can collect, the better it can reconstruct images in poor lighting conditions, which is typically one of the more desirable features in today’s smartphones. In fact, get this, this is the method behind Google’s magical Night Sight and Super-Res Zoom features that you can access by using their Pixel 3 flagship phones. When this feature first came out, I remember that phone reviewers and everyone unaware of the rate of progress in computer graphics research were absolutely floored by the results and could hardly believe their eyes when they first tried it. And I don’t blame them, this is a truly incredible piece of work. Make sure to have a look at the paper that contains a ton of comparisons against other methods, and it also shows the relation between the number of collected burst frames and the output quality we can expect as a result, and more! Thanks for watching and for your generous support, and I'll see you next time!"
324,Should AI Research Try to Model the Human Brain?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. AI research has come a long-long way in the last few years. I remember that not so long ago, we were lucky if we could train a neural network to understand traffic signs, and since then, so many things happened: by harnessing the power of learning algorithms, we are now able to impersonate other people by using a consumer camera, generate high-quality virtual human faces for people that don’t exist, or pretend to be able to dance as a pro dancer by using an external video footage and transferring it onto ourselves. Even though we are progressing at a staggering pace, there is a lot of debate as to which research direction is the most promising going forward. Roughly speaking, there are two schools of thought. One, we recently talked about Richard Sutton’s amazing article by the name, “The Bitter Lesson”, in which he makes a great argument that AI research should not try to mimic the way the human brain works he argues that instead, all we need to do is formulate our problems in a general manner, so that our learning algorithms may find something that is potentially much better suited for a problem than our brain is. I put a link to this video in the description if you’re interested. And two, a different school of thought says that we should a good look at all these learning algorithms that use a lot of powerful hardware and can do wondrous things, like playing a bunch of Atari games at a superhuman level. Note that they learn orders of magnitude slower than the human brain does, so it should definitely be worth it to try to study and model the human brain, at least until we can match it in terms of efficiency. This school of thought is what we are going to talk about in this video. As an example, let’s take a look at deep reinforcement learning in the context of playing computer games. This technique is a combination of a neural network that processes the visual data that we see on the screen, and a reinforcement learner that comes up with the gameplay-related decisions. Absolutely amazing algorithm, a true breakthrough in AI research. Very powerful, however, also quite slow. And by slow, I mean that we can sit for an hour in front of our computer and wonder why our learner does not work at all, because it loses all of its lives almost immediately. If we remain patient, we find out that it works, it just learns at a glacial pace. So, why is this so slow? Well, two reasons. Reason number one is that the learning happens through incremental parameter adjustment. What does that mean? If a human fails really badly at a task, the human would know that a drastic adjustment to the strategy is necessary, while the deep reinforcement learner would start applying tiny, tiny changes to its behavior and test again if things got better. This takes a while, and as a result, this seems unlikely to have a close relation to how we, humans think. The second reason for it being slow is the presence of weak inductive bias. This means that the learner does not contain any information about the problem we have at hand, or in other words, has never seen the game we’re playing before and has no other previous knowledge about games at all. This is desirable in some cases, because we can reuse one learning algorithm for a variety of problems. However, because this way, the AI has to test a stupendously large number of potential hypotheses about the game, we will have to pay for this convenience by a mighty inefficient algorithm. But is this really all true? Does deep reinforcement learning really have to be so slow? And what on earth does this have to do with our brain? Well, this paper proposes an interesting counterargument that this is not necessarily true and argues that with two well thought out changes, the efficiency of deep reinforcement learning may be drastically improved, and get this, it also tells us that these changes are also possibly based in neuroscience. So what are the two changes? One is using episodic memory, which stores previous experiences to help estimating the potential value of different actions, and this way, drastic parameter adjustments become a possibility. And it not only improves the efficiency, but there is more to it, because there are recent studies that show that using episodic memory indeed contributes to the learning of real humans and animals alike. And two, it is beneficial to let the AI implement its own reinforcement learning algorithm, a concept often referred to as “learning to learn” or meta reinforcement learning. This also helps obtaining more general knowledge that can be reused across tasks, further improving the efficiency of the agent. Here you see a picture of an fMRI, and some regions are marked with yellow and orange here. What could these possibly mean? Well, hold on to your papers, because these highlight neural structures that implement a very similar meta reinforcement learning scheme within the human brain. It turns out that meta reinforcement learning, or this “learning to learn” scheme may not just be something that speeds up our AI algorithms, but may be a fundamental principle of the human brain as well. So these two changes to deep reinforcement learning not only drastically improve its efficiency, but it also suddenly maps quite a bit better to our brain. How cool is that? So, which school of thought are you most fond of? Should we model the brain, or should we listen to Richard Sutton’s Bitter Lesson? Let me know in the comments. Also, make sure to have a look at the paper, I found it to be quite readable, and you really don’t need to be a neuroscientist to enjoy it and learn quite a few new things. Make sure to have a look at it in the video description! Now, I think you noticed that this paper doesn’t contain the usual visual fireworks, and is more complex than your average Two Minute Papers video, and hence, I expect it to get significantly fewer views. That’s not a great business model, but no matter, I made this channel so I can share with you all these important lessons that I learned during my journey. This has been a true privilege and I am thrilled that I am still able to talk about all these amazing papers without worrying too much whether any of these videos will go viral or not. This has only been possible because of your unwavering support on Patreon.com/TwoMinutePapers. If you feel like chipping in, just click the Patreon link in the video description. If you are more like a crypto person, we also support cryptocurrencies like Bitcoin, Ethereum and Litecoin, the addresses are also available in the description. Thanks for watching and for your generous support, and I'll see you next time!"
325,NVIDIA’s AI Transformed My Chihuahua Into a Lion,"This episode has been supported by Lambda Labs. Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Let’s talk about a great recent development in image translation! Image translation means that some image goes in, and it is translated into an analogous image of a different class. A good example of this would be when we have a standing tiger as an input, and we ask the algorithm to translate this image into the same tiger lying down. This leads to many amazing applications, for instance, we can specify a daytime image and get the same scene during nighttime. We can go from maps to satellite images, from video games to reality and more. However, much like many learning algorithms today, most of these techniques have a key limitation they need a lot of training data, or in other words, these neural networks require seeing a ton of images in all of these classes before they can learn to meaningfully translate between them. This is clearly inferior to how humans think, right? If I would show you a horse, you could easily imagine, and some of you could even draw what it would look like if it were a zebra instead. As I am sure you have noticed by reading arguments on many internet forums, humans are pretty good at generalization. So, how could we possibly develop a learning technique that can look at very few images, and obtain knowledge from them that generalizes well? Have a look at this crazy new paper from scientists at NVIDIA that accomplishes exactly that. In this example, they show an input image of a golden retriever, and then, we specify the target classes by showing them a bunch of different animal breeds, and…look! In goes your golden, and out comes a pug or any other dog breed you can think of.. And now, hold on to your papers, because this AI doesn’t have access to these target images and it had only seen them the very first time as we just gave it to them. It can do this translation with previously unseen object classes. How is this insanity even possible? This work contains a generative adversarial network, which assumes that the training set we give it contains images of different animals, and what it does during training is practicing the translation process between these animals. It also contains a class encoder that creates a low-dimensional latent space for each of these classes, which means that it tries compress these images down to a few features that contain the essence of these individual dog breeds. Apparently it can learn the essence of these classes really well because it was able to convert our image into a pug without ever seeing a pug other than this one target image. As you can see here, it comes out way ahead of previous techniques, but of course, if we give it a target image that is dramatically different than anything the AI has seen before, it may falter. Luckily, you can even try it yourself through this web demo which works on pets, so make sure to read the instructions carefully, and, let the experiments begin! In fact, due to popular request, let me kick this off with Lisa, my favorite chihuahua. I got many tempting alternatives, but worry not, in reality, she will stay as is. I was also curious about trying a non-traditional head position, and as you see with the results, this was a much more challenging case for the AI. The paper also discusses this limitation in more detail. You know the saying, two more papers down the line, and I am sure this will also be remedied. I am hoping that you will also try your own pets and as a Fellow Scholar, you will flood the comments section here with your findings. Strictly for science, of course. If you’re doing deep learning, make sure to look into Lambda GPU systems. Lambda offers workstations, servers, laptops, and a GPU cloud for deep learning. You can save up to 90% over AWS, GCP, and Azure GPU instances. Every Lambda GPU system is pre-installed with TensorFlow, PyTorch, and Keras. Just plug it in and start training. Lambda customers include Apple, Microsoft, and Stanford. Go to lambdalabs.com/papers or click the link in the description to learn more. Big thanks to Lambda for supporting Two Minute Papers and helping us make better videos. Thanks for watching and for your generous support, and I'll see you next time!"
326,DeepMind Made a Math Test For Neural Networks,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper from DeepMind is about taking a bunch of learning algorithms and torturing them with millions of classic math questions to find out if they can solve them. Sounds great, right? I wonder what kind of math questions would an AI find easy to solve? What percentage of these can a good learning algorithm answer today? Worry not, we’ll discuss some of the results at the end of this video. These kinds of problems are typically solved by recurrent neural networks that are able to read and produce sequences of data, and to even begin to understand what the question is here, an AI would have to understand the concept of functions, variables, arithmetic operators, and of course, the words that form the question itself. It has to learn planning and precedence, that is, in what order do we evaluate such an expression, and it has to have some sort of memory, in which it can store the intermediate results. The main goal of this paper is to describe a dataset that is designed in a very specific way to be able to benchmark the mathematical reasoning abilities of an AI. So how do we do that? First, it is made in way that it’s very difficult to solve for someone without generalized knowledge. Imagine the kind of student at school who memorized everything from the textbooks, but has no understanding of the underlying tasks, and if the teacher changes just one number in a question, the student is unable to solve the problem. We all met that kind of student, right? Well, this test is designed in a way that students like these should fail at it. Of course, in our case, the student is the AI. Second, the questions should be modular. This is a huge advantage because a large number of these questions can be generated procedurally by adding a different combination of subtasks, such as additions, function evaluations, and more. An additional advantage of this is that we can easily control the difficulty of these questions the more modules we use, typically, the more difficult the question gets. Third, the questions and answers should be able to come in any form. This is an advantage, because the AI has to not only understand the mathematical expressions, but also focus on what exactly we wish to know about them. This also means that the question itself can be about factorization, where the answer is expected to be either true or false. And the algorithm is not told that we are looking for a true or false answer, it has to be able to infer this from the question itself. And to be able to tackle all this properly, with this paper, the authors released 2 million of these questions for training an AI free of charge to foster more future research in this direction. I wonder what percentage of these can a good learning algorithm answer today? Let’s have a look at some results! A neural network model that goes by the name Transformer network produced the best results by being able to answer 50% of the questions. This you find in the extrapolation column here. When you look at the interpolation column, you see that it successfully answered 76% of the questions. So which one is it, 50% of 76%? Actually, both. The difference is that interpolation means that the numbers in these questions were within the bounds that was seen in the training data, where extrapolation means that some of these numbers are potentially much larger or smaller than others that the AI has seen in the training examples. I would say that given the difficulty of just even understanding what these questions are, these are really great results. Generally, in the future, we will be looking for algorithms that do well on the extrapolation tasks, because these are the AIs that have knowledge that generalizes well. So, which tasks were easy and which were difficult? Interestingly, the AI has had similar difficulties as we, fellow humans have, namely, rounding decimals and integers, comparisons, basic algebra was quite easy for it, whereas detecting primality and factorization were not very accurate. I will keep an eye out on improvements in this area, if you are interested to hear more about it, make sure to subscribe to this series. And if you just pushed the red button, you may think you are subscribed, but you are not. You are just kind of subscribed. Make sure to also click the bell icon to not miss these future episodes. Also, please make sure to read the paper, it is quite readable and contains a lot more really cool insights about this dataset and the experiments. As always, the link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
327,How Can This Liquid Climb?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. You’re in for a real treat today, because today we’re not going to simulate just plain regular fluids. No! We’re going to simulate ferrofluids! These are fluids that have magnetic properties and respond to an external magnetic field and you will see in a moment that they are able to even climb things. You see in the reference footage here that this also means if there is no magnetic field, we have a regular fluid simulation. Nothing too crazy here. In this real-world footage, we have a tray of ferrofluid up in the air, and we have a magnet below it, so as the tray descends down and gets closer to the magnet, this happens. But the strength of the magnetic field is not the only factor that a simulation needs to take into account. Here’s another real experiment that shows that the orientation of the magnet also makes a great deal of difference to the distortions of the fluid surface. And now, let’s have a look at some simulations! This simulation reproduces the rotating magnet experiment that you’ve seen a second ago. It works great, what is even more, if we are in a simulation, we can finally do things that would be either expensive, or impossible in the real life, so let’s do exactly that! You see a steel sphere attracting the ferrofluid here, and, now, the strength of the magnet within is decreased, giving us the impression that we can bend this fluid to our will! How cool is that? In the simulation, we can also experiment with arbitrarily-shaped magnets. And here is the legendary real experiment where with magnetism, we can make a ferrofluid climb up on this steel helix. Look at that. When I’ve first seen this video and started reading the paper, I was just giggling like a little girl. So good. Just imagine how hard it is to do something where have footage from the real world that keeps judging our simulation results, and we are only done when there is a near-exact match, such as the one you see here. Huge congratulations to the authors. You see here how the simulation output depends on the number of iterations. More iterations means that we redo the calculations over and over again, and get results closer to what would happen in real life at the cost of more computation time. However, as you see, we can get close to the real solution with even 1 iteration, which is remarkable. In my own fluid simulation experiments, when I tried to solve the pressure field, using 1 to 4 iterations gave me a result that’s not only inaccurate, but singular, which blows up the simulation. Look at this. On this axis, you can see how the fluid disturbances get more pronounced as a response to a stronger magnetic field. And in this direction, you see how the effect of surface tension smooths out these shapes. What a visualization! The information density in this example is just out of this world… and it is still both informative, and beautiful. If only I could tell you how many times I have to remake each of the figures in my papers in pursuit of this…I can only imagine how long it took to finish this one. Bravo. And if all that’s not enough for you to fall out of your chair, get this. It is about Libo Huang, the first author of this paper. I became quite curious about his other works, and have found exactly zero of them. This was his first paper. My goodness. And of course, it takes a team to create such a work, so congratulations to all three authors, this is one heck of a paper. Check it out in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
328,This AI Makes Amazing DeepFakes…and More!,"This episode has been supported by Lambda Labs. Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In our earlier paper, Gaussian Material Synthesis, we made a neural renderer, and what this neural renderer was able to do is reproduce the results of a light transport simulation within 4 to 6 milliseconds in a way that is almost pixel perfect. It took a fixed camera and scene, and we were able to come up with a ton of different materials, and it was always able to guess what the output would look like if we changed the physical properties of a material. This is a perfect setup for material synthesis where these restrictions are not too limiting. Trying to perform high-quality neural rendering has been a really important research problem lately, and everyone is asking the question, can we do more with this? Can we move around with the camera and have a neural network predict what the scene would look like? Can we do this with animations? Well, have a look at this new paper which is a collaboration between researchers at the Technical University of Munich and Stanford University, where all we need is some video footage of a person or object. It takes a close look at this kind of information, and can offer three killer applications. One, it can synthesize the object from new viewpoints. Two, it can create a video of this scene and imagine what it would look like if we reorganized it, or can even add more objects to it. And three, perhaps everyone’s favorite, performing facial reenactment from a source to a target actor. As with many other methods, these neural textures are stored on top of the 3D objects, however, a more detailed, high-dimensional description is also stored and learned by this algorithm, which enables it to have a deeper understanding of intricate light transport effects create these new views. For instance, it is particularly good at reproducing specular highlights, which typically change rapidly as we change our viewpoint for the object. One of the main challenges was building a learning algorithm that can deal with this kind of complexity. The synthesis of mouth movements and teeth was always the achilles heel of these methods, so have a look at how well this one does with it! You can also see with the comparisons here that in general, this new technique smokes the competition. So how much training data do we need to achieve this? I would imagine that this would take hours and hours of video footage, right? No, not at all. This is what the results look like as a function of the amount of training data. On the left, you see that is already kinda works with 125 images, but contains artifacts, but if we can supply a 1000 images, we’re good. Note that a 1000 images sounds like a lot, but it really isn’t, it’s just half a minute worth of video. How crazy is that? Some limitations still apply, you see one failure case right here, and the neural network typically needs to be retrained if we wish to use it on new objects, but this work finally generalizes to multiple viewpoints, animation, scene editing, lots of different materials and geometries, and I can only imagine what we’ll get two more papers down the line. Respect to Justus for accomplishing this, and in general, make sure to have a look at Matthias Niessner’s lab, who just got tenured as a full professor and he’s only 32 years old. Congratulations! If you have AI-related ideas, and you would like to try them, but not do it in the cloud because you wish to own your own hardware, look no further than Lambda Labs. Lambda Labs offers sleek, beautifully designed laptops, workstations and servers that come pre-installed with every major learning framework and updates them for you, taking care of all the dependencies. Look at those beautiful and powerful machines! This way, you can spend more of your time with your ideas, and don’t have to deal with all the software maintenance work. They have an amazing roster of customers that include Apple, Microsoft, Amazon, MIT and more. These folks really know what they are doing. Make sure to go to lambdalabs.com/papers or click their link in the video description and look around, and if you have any questions, you can even even call them for advice. Big thanks to Lambda Labs for supporting this video and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
329,All Hail The Mighty Translatotron!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Scientists at Google just released the Translatotron. This is an AI that is able to translate speech from one language into speech into another language, and here comes the first twist, without using text as an intermediate representation. You give it the soundwaves, and you get the translated soundwaves. And this neural network was trained approximately on a million voice samples. So let’s see what learning on this one million samples gives us. Listen. This is the input sentence in Spanish. And here it is, translated to English, but using the voice of the same person. This is incredible. However, there is another twist, perhaps an even bigger one, believe it or not. This technique can not only translate, but can also perform voice transfer, so it can say the same thing using someone else’s voice. This means that the AI not only has to learn what to say, but how to say it. This is immensely difficult. It’s also not that easy to know what we need to listen to and when, so let me walk you through it. This is a source sentence in Spanish. This is the same sentence said by someone else, an actual person, and in English. And now, the same thing, but synthesized by the algorithm using both of their voices. Let’s listen to them side by side some more. This is so good, let’s have a look at some more examples. Wow. The method performs the learning by trying to map these Mel spectrograms between multiple speakers. You can see example sentences here and their corresponding spectrograms, which are concise representations of someone’s voice and intonation. And of course, it is difficult to mathematically formalize what makes a good translation and a good mimicking of someone’s voice, so in these cases, we let people be the judge and have them listen to a few speech signals and asking them to guess which was a real person, and which was the AI speaking. If you take a closer look at the paper, you will see that it smokes the competition. This is great progress on an immensely difficult task as we have to perform proper translation and voice transfer at the same time. It’s quite a challenge. Of course, failure cases still exist. Listen. Just imagine that you are in a foreign country and all you need to do is use your phone to tell stories to people not only in their own language, but also using your own voice, even if you don’t speak a word of their language. Beautiful. Even this video could perhaps be available in a variety of languages using my own voice within the next few years, although I wonder how these algorithms would pronounce my name. So far, that proved to be quite a challenge for humans and AIs alike. And for now, all hail the mighty Translatotron. In the meantime, I just got back from this year’s NATO conference. It was an incredible honor to get an invitation to speak at such an event, and of course, I was happy to attend as a service to the public. The goal of the talk was to inform key political and military decision makers about recent advancements in AI so they can make better decisions for us. And I was SO nervous during the talk. My goodness. If you wish to watch it, I put a link to it in the video description and I may be able to upload a higher-quality version of this video here in the future. Attending the conference introduced delays in our schedule, my apologies for that, and normally, we would have to worry whether because of this, we’ll have enough income to improve our recording equipment. However, with your support on Patreon, this is not at all the case, so I want to send you a big thank you for all your amazing support. This was really all possible thanks to you. If you wish to support us, just go to patreon.com/TwoMinutePapers or just click the link in the video description. Have fun with the video! Thanks for watching and for your generous support, and I'll see you next time!"
330,We Can All Be Video Game Characters With This AI,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The title of this paper is very descriptive, it says Controllable Characters Extracted from Real-World Videos. This sounds a little like science fiction, so let’s pick this apart. If we forget about the controllable part, we get something that you’ve seen in this series many times pose estimation. Pose estimation means that we have a human character in an image or a video, have a computer program look at it, and tell us the current position this character is taking. This is useful for medical applications, such as detecting issues with motor functionality, fall detection, or we can also use it for motion capture for our video games and blockbuster movies. So just performing the pose estimation part is a great invention, but relatively old news. So what’s really new here? Why is this work interesting? How does it go beyond pose estimation? Well, as a hint, the title contains an additional word, “controllable”, so, look at this! Woo-hoo! As you see, this technique is not only able to identify where a character is, but we can grab a controller, and move it around! This means making this character perform novel actions, and showing it from novel views. That’s really remarkable, because this requires a proper understanding of the video we’re watching. And this means that we can not only watch these real-world videos, as you see this small piece of footage used for the learning, but by performing these actions with a controller, we can make a video game out of it. Especially given that here, the background has also been changed. To achieve this, this work contains two key elements. Element number one is the pose2pose network that takes an input posture and the button we pushed on the controller, and creates the next step of the animation. And then, element number two, the pose2frame architecture then blends this new animation step into an already existing image. The neural network that performs this is trained in a way where it is encouraged to create these character masks in a way that is continuous and doesn’t contain jarring jumps between the individual frames, leading to smooth and believable movements. Now, clearly, anyone who takes a cursory look sees that the animations are not perfect and still contain artifacts, but just imagine that this paper is among the first introductory works on this problem. Imagine what we’ll have two more papers down the line. I can’t wait. Thanks for watching and for your generous support, and I'll see you next time!"
331,"OpenAI's MuseNet Learned to Compose Mozart, Bon Jovi and More","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Not so long ago, OpenAI has released GPT-2, an AI that was trained to look at a piece of text and perform common natural language processing operations on it, for instance, answering questions, summarization, and more. But today, we are going to be laser focused on only one of these tasks. And that task is continuation, where we give an AI a bunch of text and we ask it to continue it. However, as these learning algorithms are quite general by design, here comes the twist who said that this can only work for text? Why not try it on composing music? So let’s have a look at some results, where only the first 6 notes were given from a song, and the AI was asked to continue it. Love it. This is a great testament to the power of general learning algorithms. As you’ve heard, this works great for a variety of different genres as well, and not only that, but it can also create really cool blends between genres. Listen as the AI starts out from the first 6 notes of a Chopin piece and transitions into a pop style with a bunch of different instruments entering a few seconds in. And, great news, because if you look here, we can try our own combinations through an online demo as well. On the left side, we can specify and hear the short input sample, and ask for a variety of different styles for the continuation. It is amazing fun, try it, I’ve put a link in the video description. I was particularly impressed with this combination. Really cool. Now, this algorithm is also not without limitations as it has difficulties pairing instruments that either don’t go too well together or there is lacking training data on how they should sound together. The source code is also either already available as of the publishing of this video, or will be available soon. If so, I will come back and update the video description with the link. OpenAI has also published an almost two-hour concert with ton of different genres so make sure to head to the video description and check it out yourself! I think these techniques are either already so powerful, or will soon be powerful enough to raise important copyright questions, and we’ll need plenty of discussions on who really owns this piece of music. What do you think? Let me know in the comments! Thanks for watching and for your generous support, and I'll see you next time!"
332,"Artistic Style Transfer, Now in 3D!","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Style transfer is an interesting problem in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to super fun, and really good looking results. We have seen plenty of papers doing variations of style transfer, but can we can push this concept further? And the answer is, yes! For instance, few people know that style transfer can also be done in 3D! If you look here, you see an artist performing this style transfer by drawing on a simple sphere and get their artistic style to carry over to a complicated piece of 3D geometry. We talked about this technique in Two Minute Papers episode 94, and for your reference, we are currently at over episode 340. Leave a comment if you’ve been around back then! And this previous technique led to truly amazing results, but still had two weak points. One, it took too long. As you see here, this method took around a minute or more to produce these results. And hold on to your papers, because this new paper is approximately a 1000 times faster than that, which means that it can produce 100 frames per second using a whopping 4K resolution. But of course, none of this matters… if the visual quality is not similar. And, if you look closely, you see that the new results are indeed really close to the reference results of the older method. So, what was the other problem? The other problem was the lack of temporal coherence. This means that when creating an animation, it seems like each of the individual frames of the animation were drawn separately by an artist. In this new work, this is not only eliminated as you see here, but the new technique even gives us the opportunity to control the amount of flickering. With these improvements, this is is now a proper tool to help artists perform this 3D style transfer and create these rich virtual worlds much quicker and easier in the future. It also opens up the possibility for novices to do that, which is an amazing value proposition. Limitations still apply, for instance, if we have a texture with some regularity, such as this brickwall pattern here, the alignment and continuity of the bricks on the 3D model may suffer. This can be fixed, but it is a little labor-intensive. However, you know our saying, two more papers down the line, and this will likely cease to be an issue. And what you’ve seen here today is just one paper down the line from the original work, and we can do 4K resolution at a 100 frames per second. Unreal. Thanks for watching and for your generous support, and I'll see you next time!"
333,This AI Makes The Mona Lisa Come To Life,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work presents a learning-based method that is able to take just a handful of photos, and use those to synthesize a moving virtual character. Not only that, but it can also synthesize these faces from new viewpoints that the AI hasn’t seen before. These results are truly sublime, however, hold on to your papers, because it also works from as little as just one input image. This we refer to as 1-shot learning. You see some examples here, but wait a second…really, just one image? If all it needs is really just one photo, this means that we can use famous photographs, and even paintings, and synthesize animations for them. Look at that! Of course, if we show multiple photos to the AI, it is able to synthesize better output results, you see such a progression here as a function of the amount of input data. The painting part I find to be particularly cool because it strays away from the kind of data the neural networks were trained on, which is photos, however, if we have proper intelligence, the AI can learn how different parts of the human face move, and generalize this knowledge to paintings as well. The underlying laws are the same, only the style of the output is different. Absolutely amazing. The paper also showcases an extensive comparison section to previous works, and, as you see here, nothing really compares to this kind of quality. I have heard the quote “any sufficiently advanced technology is indistinguishable from magic” so many times in my life, and I was like, OK, well, maybe, but I’m telling you this is one of those times when I really felt that I am seeing magic at work on my computer screen. So, I know what you’re thinking how can all this wizardry be done? This paper proposes a novel architecture where 3 neural networks work together. One, the Embedder takes colored images with landmark information and compresses it down into the essence of these images, two, the Generator takes a set of landmarks, a crude approximation of the human face, and synthesizes a photorealistic result from it. And three, the Discriminator looks at both real and fake images and tries to learn how to tell them apart. As a result, these networks learn together, and over time, they improve together, so much so that they can create these amazing animations from just one source photo. The authors also released a statement on the purpose and effects of this technology, which I’ll leave here for a few seconds for our interested viewers. This work was partly done at the Samsung AI lab and Skoltech. Congratulations to both institutions. Killer paper. Make sure to check it out in the video description. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It is like a shared logbook for your team, and with this, you can compare your own experiment results, put them next to what your colleagues did and you can discuss your successes and failures much easier. It takes less than 5 minutes to set up and is being used by OpenAI, Toyota Research, Stanford and Berkeley. It was also used in this OpenAI project that you see here, which we covered earlier in the series. They reported that experiment tracking was crucial in this project and that this tool saved them quite a bit of time and money. If only I had access to such a tool during our last research project where I had to compare the performance of neural networks for months and months. Well, it turns out, I will be able to get access to these tools, because, get this, it’s free and will always be free for academics and open source projects. Make sure to visit them through wandb.com or just click the link in the video description and sign up for a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
334,This Jello Simulation Uses Only ~88 Lines of Code,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we’re going to talk about the material point method. This method uses both grids and particles to simulate the movement of snow, dripping honey, interactions of granular solids and a lot of other really cool phenomena on our computers. This can be used, for instance, in the movie industry to simulate what a city would look like if it were flooded. However, it has its own limitations, which you will hear more about in a moment. This paper showcases really cool improvements to this technique, for instance, it enables to run these simulations twice as fast, and can simulate new phenomena that were previously not supported by the material point method. One is the simulation of complex thin boundaries that enables us to cut things, so in this video, expect lots of virtual characters to get dismembered. I think this might be the only channel on YouTube where we can say this celebrate it as an amazing scientific discovery. And the other key improvement of this paper is introducing two-way coupling, which means the example that you see here as the water changes the movement of the wheel, but the wheel also changes with the movement of the water. It is also demonstrated quite aptly here by this elastoplastic jello scene, in which we can throw in a bunch of blocks of different densities, and it is simulated beautifully here how they sink into the jello deeper and deeper as a result. Here, you see a real robot running around in a granular medium, and here, we have a simulation of the same phenomenon and can marvel at how close the result is to what would happen in real life. Another selling point of this method is that this is easy to implement, which is demonstrated here, and what you see here is the essence of this algorithm implemented in 88 lines of code. Wow! Now, these methods still take a while as there is a lot of deformations and movement to compute and we can only advance time in very small steps, and as a result, the speed of such simulations is still measured in not frames per second, but in seconds per frame. These are the kinds of simulations that we like to leave on the machine overnight. If you want to see something that is done with a remarkable amount of love and care, please read this paper. And I don’t know if you have heard about this framework called Taichi. This contains implementations for many amazing papers in computer graphics. Lots of paper implementations on animation, light transport simulations, you name it, a total of more than 40 papers are implemented here. And I was thinking, this is really amazing, “I wonder which group made this”, then I noticed it was all written by one person. And that person is Yuanming Hu, the scientist who is the lead author of this paper. This is insanity. Thanks for watching and for your generous support, and I'll see you next time!"
335,Rewrite Videos By Editing Text,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The last few years have been an amazing ride when it comes to research works for creating facial reenactments for real characters. Beyond just transferring our gestures to a video footage of an existing talking head, controlling their gestures like video game characters and full-body movement transfer are also a possibility. With WaveNet and its many variants, we can also learn someone’s way of speaking, write a piece of text and make an audio waveform where we can impersonate them using their own voice. So, what else is there to do in this domain? Are we done? No-no, not at all! Hold on to your papers, because with this amazing new technique, what we can do is look at the transcript of a talking head video, remove parts of it or add to it, just as we would edit any piece of text and, this technique produces both the audio and a matching video of this person uttering these words. Check this out. It works by looking through the video collecting small sounds that can be used to piece together this new word that we’ve added to the transcript. The authors demonstrate this by adding the word “fox” to the transcript. This can be pieced together by the “v” which appears in the word “viper”, and taking “ox” as a part of another word found in the footage. As a result, one can make the character say “fox” even without hearing her uttering this word before. Then, we can look for not only the audio occurrences for these sounds, but the video footage of how they are being said, and in the paper, a technique is proposed to blend these video assets together. Finally, we can provide all this information to a neural renderer that synthesizes a smooth video of this talking head. This is a beautiful architecture with lots of contributions, so make sure to have a look at the paper in the description for more details. And of course, as it is not easy to measure the quality of these results in a mathematical manner, a user study was made where they asked some fellow humans which is the real footage, and which one was edited. You will see the footage edited by this algorithm on the right. And, hm, it’s not easy to tell which one is which, and it also shows in the numbers, which are not perfect, but they clearly show that the fake video is very often confused with the real one. Did you find any artifacts there that give the trick away? Perhaps the sentence was said a touch faster than expected. Found anything else? Let me know in the comments below. The paper also contains tons of comparisons against previous works. So in the last few years, the trend seems clear: the bar is getting lower, it is getting easier and easier to produce these kinds of videos, and it is getting harder and harder to catch them with our naked eyes, and now, we can edit the transcript of what is being said, which is super convenient. I would like to note that AIs also exist that can detect these edited videos with a high confidence. I put up the the ethical considerations of the authors here, it is definitely worthy of your attention as it discusses how they think about these techniques. The motivation for this work was mainly to enhance digital storytelling by removing filler words, potentially flubbed phrases or retiming sentences in talking head videos. There is much more to it, so make sure to pause the video and read their full statement. Thanks for watching and for your generous support, and I'll see you next time!"
336,Virtual Characters Learn To Work Out…and Undergo Surgery,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about creating virtual characters with a skeletal system, adding more than 300 muscles and teaching them to use these muscles to kick, jump, move around and perform other realistic human movements. Throughout the video, you will see the activated muscles with red. I am loving the idea, which, turns out, comes lots of really interesting corollaries. For instance, this simulation realistically portrays how increasing the amount of weight to be lifted changes what muscles are being trained during a workout. These agents also learned to jump really high and you can see a drastic difference between the movement required for a mediocre jump and an amazing one. As we are teaching these virtual agents within a simulation, we can perform all kinds of crazy experiments by giving them horrendous special conditions, such as bone deformities, a stiff ankle, muscle deficiencies and watch them learn to walk despite these setbacks. For instance, here you see that the muscles in the left thigh are contracted, resulting in a stiff knee, and as a result, the agent learned an asymmetric gait. If the thigh-bones are twisted inwards, ouch, the AI shows that it is still possible to control the muscles to walk in a stable manner. I don’t know about you, but at this point I am feeling quite sorry for these poor simulated beings, so let’s move on, we have plenty of less gruesome, but equally interesting things to test here. In fact, if we are in a simulation, why not take it further? It doesn’t cost anything! That’s exactly what the authors did, and it turns out that we can even simulate the use of prosthetics. However, since we don’t need to manufacture these prosthetics, we can try a large number of different designs and evaluate their usefulness without paying a dime. How cool is that? So far, we have hamstrung this poor character many-many times, so why not try to heal it? With this technique, we can also quickly test the effect of different kinds of surgeries on the movement of the patient. With this, you can see here how a hamstring surgery can extend the range of motion of this skeleton. It also tells us not to try our luck with one-legged squats. You heard it here first folks. Thanks for watching and for your generous support, and I'll see you next time!"
337,AI Discovers Sentiment By Writing Amazon Reviews,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In 2017, scientists at OpenAI embarked on an AI project where they wanted to show a neural network a bunch of Amazon product reviews and wanted to teach it to be able to generate new ones, or continue a review when given one. Now, so far, this sounds like a nice hobby project, definitely not something that would justify an entire video on this channel, however, during this experiment, something really unexpected happened. Now, it is clear that when the neural network reads these reviews, it knows that it has to generate new ones, therefore it builds up a deep understanding of language. However, beyond that, it used surprisingly few neurons to continue these reviews and scientists were wondering “why is that”? Usually, the more neurons, the more powerful the AI can get, so why use so few neurons? The reason for that is that it has learned something really interesting. I’ll tell you what in a moment. This neural network was trained in an unsupervised manner, therefore it was told to do what the task was, but was given no further supervision. No labeled datasets, no additional help, nothing. Upon closer inspection, they noticed that the neural network has built up a knowledge of not only language, but also built a sentiment detector as well. This means that the AI recognized that in order to be able to continue a review, it needs to be able to efficiently detect whether the review seems positive or not. And thus, it dedicated a neuron to this task, which we will refer to as the sentiment neuron. However, it was no ordinary sentiment neuron, it was a proper, state of the art sentiment detector. In this diagram, you see this neuron at work. As it reads through the review, it starts out detecting a positive outlook which you can see with green, and then, uh-oh, it detects that review has taken a turn and is not happy with the movie at all. And all this was learned on a relatively small dataset. Now, if we have this sentiment neuron, we don’t just have to sit around and be happy for it. Let’s play with it! For instance, by overwriting this sentiment neuron in the neural network, we can force it to create positive or negative reviews. Here is a positive example: “Just what I was looking for. Nice fitted pants, exactly matched seam to color contrast with other pants I own. Highly recommended and also very happy!” And, if we overwrite the sentiment neuron to negative, we get the following: “The package received was blank and has no barcode. A waste of time and money.” There are some more examples here on the screen for your pleasure. Absolutely amazing. This paper teaches us that we should endeavor to not just accept these AI-based solutions, but look under the hood, and sometimes, a goldmine of knowledge can be found within. If you have enjoyed this episode and would like to help us make better videos for you in the future, please consider supporting us on Patreon.com/TwoMinutePapers or just click the link in the video description. In return, we can offer you early access to these episodes, or even add your name to our key supporters so you can appear in the desciption of every video and more. We also support cryptocurrencies like Bitcoin, Ethereum and Litecoin. The majority of these funds is used to improve the show and we use a smaller part to give back to the community and empower science conferences like the Central European Conference on Computer Graphics. This is a conference that teaches young scientists to present their work at bigger venues later, and with your support, it’s now the second year we’ve been able to sponsor them, which warms my heart. This is why every episode ends with, you know the drill… Thanks for watching and for your generous support, and I'll see you next time!"
338,Better Photorealistic Materials Are Coming!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we often marvel at light simulation programs that are able to create beautiful images by simulating the path of millions and millions of light rays. To make sure that our simulations look lifelike, we not only have to make sure that these rays of light interact with the geometry of the scene in a way that is physically plausible, but the materials within the simulation also have to reflect reality. Now that’s an interesting problem. How do we create a convincing mathematical description of real-world materials? Well, one way to do that is taking a measurement device, putting in a sample of the subject material and measuring how rays of light bounce off of it. This work introduces a new database for sophisticated material models and includes interesting optical effects, such as iridescence, which gives the colorful physical appearance of bubbles and fuel-water mixtures, it can do colorful mirror-like specular highlights and more, so we can include these materials in our light simulation programs. You see this database in action in this scene that showcases a collection of these complex material models. However, creating such a database is not without perils, because normally, these materials take prohibitively many measurements to reproduce properly, and the interesting regions are often found at quite different places. This paper proposes a solution that adapts the location of these measurements to where the action happens, resulting in a mathematical description of these materials that can be measured in a reasonable amount of time. It also takes very little memory when we run the actual light simulation on them. So, as if light transport simulations weren’t beautiful enough, they are about to get even more realistic in the near future. Super excited for this. Make sure to have a look at the paper, which is so good, I think I sank into a minor state of shock upon reading it. If you are enjoying learning about light transport, make sure to check out my course on this topic at the Technical University of Vienna. I still teach this at the University for about 20 Master students at a time and thought that the teachings shouldn't only be available for a lucky few people who can afford a college education. Clearly, the teachings should be available for everyone, so, we recorded it and put it online, and now everyone can watch it, free of charge. I was quite stunned to see that more than 25 thousand people decided to start it, so make sure to give it a go if you're interested! Thanks for watching and for your generous support, and I'll see you next time!"
339,"AI Creates Near Perfect Images Of People, Dogs and More","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In the last few years, we have seen a bunch of new AI-based techniques that were specialized in generating new and novel images. This is mainly done through learning-based techniques, typically a Generative Adversarial Network, a GAN in short, which is an architecture where a generator neural network creates new images, and passes it to a discriminator network, which learns to distinguish real photos from these fake, generated images. These two networks learn and improve together, so much so that many of these techniques have become so realistic that we often can’t tell they are synthetic images unless we look really closely. You see some examples here from BigGAN, a previous technique that is based on this architecture. So in these days, many of us are wondering, is there life beyond GANs? Can they be matched in terms of visual quality by a different kind of a technique? Well, have a look at this paper, because it proposes a much simpler architecture that is able to generate convincing, high-resolution images quickly for a ton of different object classes. The results it is able to churn out is nothing short of amazing. Just look at that! To be able to proceed to the key idea here, we first have to talk about latent spaces. You can think of a latent space as a compressed representation that tries to capture the essence of the dataset that we have at hand. You can see a similar latent space method in action here that captures the key features that set different kinds of fonts apart and presents these options on a 2D plane, and here, you see our technique that builds a latent space for modeling a wide range of photorealistic material models. And now, onto the promised key idea! As you have guessed, this new technique uses a latent space, which means that instead of thinking in pixels, it thinks more in terms of these features that commonly appear in natural photos, which also makes the generation of these images up to 30 times quicker, which is super useful, especially in the case of larger images. While we are at that, it can rapidly generate new images with the size of approximately a thousand by thousand pixels. Machine learning is a research field that is enjoying a great deal of popularity these days, which also means that so many papers appear every day it’s getting really difficult to keep track of all of them. The complexity of the average technique is also increasing rapidly over time, and what I like most about this paper is that it shows us that surprisingly simple ideas can still lead to breakthroughs. What a time to be alive! Make sure to have a look at the paper in the description as it also describes how this method is able to generate more diverse images than previous techniques and how we can measure diversity at all because that is no trivial matter. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It is like a shared logbook for your team, and with this, you can compare your own experiment results, put them next to what your colleagues did and you can discuss your successes and failures much easier. It takes less than 5 minutes to set up and is being used by OpenAI, Toyota Research, Stanford and Berkeley. In fact, it is so easy to add to your project, the CEO himself, Lukas instrumented it for you for this paper, and if you look here, you can see how the output images and the reconstruction error evolve over time and you can even add your own visualizations. It is a sight to behold, really, so make sure to check it out in the video description, and if you liked it, visit them through wandb.com/papers or just use the link in the video description and sign up for a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
340,This AI Learns About Movement By Watching Frozen People,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This paper is about endowing colored images with depth information, which is typically done through depth maps. Depth maps describe how far parts of the scene are from the camera and are given with a color coding where the darker the colors are, the further away the objects are. These depth maps can be used to apply a variety of effects to the image that require knowledge about the distance of the objects within for instance, selectively defocusing parts of the image, or even removing people and inserting new objects to the scene. If we, humans look at an image, we have an intuitive understanding of its contents and have the knowledge to produce a depth map by pen and paper. However, this would, of course, be infeasible and would take too long, so we would prefer a machine to do it for us instead. But of course, machines don’t understand the concept of 3D geometry so they probably cannot help us with this. Or, with the power of machine learning algorithms, can they? This new paper from scientists at Google Research attempts to perform this, but, with a twist. The twist is that a learning algorithm is unleashed on a dataset of what they call mannequins, or in other words, real humans are asked to stand around frozen in a variety of different positions while the camera moves around in the scene. The goal is that the algorithm would have a look at these frozen people and take into consideration the parallax of the camera movement. This means that the objects closer to the camera move more than other objects that are further away. And turns out, this kind of knowledge can be exploited, so much so that if we train our AI properly, it will be able to predict the depth maps of people that are moving around, even if it had only seen these frozen mannequins before. This is particularly difficult because if we have an animation, we have to make sure that the guesses are consistent across time, or else we get these annoying flickering effects that you see here with previous techniques. It is still there with the new method, especially for the background, but the improvement on the human part of the image is truly remarkable. Beyond the removal and insertion techniques we talked about earlier, I am also really excited for this method as it may open up the possibility of creating video versions of these amazing portrait mode images with many of the newer smartphones people have in their pockets. Thanks for watching and for your generous support, and I'll see you next time!"
341,Tighten the Towel! Simulating Liquid-Fabric Interactions,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, I’ve got some fluids for you! Most hobby projects with fluid simulations involve the simulation of a piece of sloshing liquid in a virtual container. However, if you have a more elaborate project at hand, the story is not so so simple anymore. This new paper elevates the quality and realism of these simulations through using mixture theory. Now, what is there to be mixed, you ask? Well, what mixture theory does for us is that it helps simulating how liquids interact with fabrics, including splashing, wringing, and more. These simulations have to take into account that the fabrics may absorb some of the liquids poured onto them and get saturated, how diffusion transports this liquid to nearby yarn strands, or, what you see here is a simulation with porous plastic, where water flows off of, and also through this material as well. Here you see how it can simulate honey dripping down on a piece of cloth. This is a real good one if you are a parent with small children, you probably have lots of experience with this situation and can assess the quality of this simulation really well. The visual fidelity of these simulations is truly second to none. I love it. Now, the question naturally arises how do we know if these simulations are close to what would happen in reality? We don’t just make a simulation and accept it as true to life if it looks good, right? Well, of course not, the paper also contains comparisons against real world laboratory setups to ensure the validity of these results, so make sure to have a look at it in the video description. And if you’ve been watching this series for a while, you notice that I always recommend that you check out the papers yourself. And even though it is true that these are technical write-ups that are meant to communicate results between experts, it is beneficial for everyone to also read at least a small part of it. If you do, you’ll not only see beautiful craftsmanship, but you’ll also learn how to make a statement and how to prove the validity of this statement. This is a skill that is necessary to find truth. So, please, read your papers. Thanks for watching and for your generous support, and I'll see you next time!"
342,3D Style Transfer For Video is Now Possible!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Neural style transfer just appeared four years ago, in 2015. Style transfer is an interesting problem in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to the super fun results that you see here. However, most of these works are about photos. So what about video? Well, hold on to your papers because this new work does this for video and the results are marvelous. The process goes as follows: we take a few keyframes from the video, and the algorithm propagates our style to the remaining frames of the video, and, WOW, these are some silky smooth results. In specific, what I would like you to take a look look at is the temporal coherence of the results. Proper temporal coherence means that the individual images within this video are not made independently from each other, which would introduce a disturbing flickering effect. I see none of that here, which makes me very, very happy! And now, hold on to your papers again because this technique does not use any kind of AI. No neural networks and other learning algorithms were used here. Okay, great, no AI. But, is it any better than its AI-based competitors? Well, look at this! Hell yeah! This method does this magic through building a set of guide images, for instance, a mask guide highlights the stylized objects, and sure enough, we also have a temporal guide that penalizes the algorithm for making too much of a change from one frame to the next one, ensuring that the results will be smooth. Make sure to have a look at the paper for a more exhaustive description of these guides. Now if we make a carefully crafted mixture from these guide images and plug them in to a previous algorithm by the name StyLit, we talked about this algorithm before in the series, the link is in the video description, then, we get these results that made me fall out of my chair. I hope you will be more prepared and held on to your papers. Let me know in the comments! And you know what is even better? You can try this yourself because the authors made a standalone tool available free of charge, just go to ebsynth.com, or just click the link in the video description. Let the experiments begin! Thanks for watching and for your generous support, and I'll see you next time!"
343,This Superhuman Poker AI Was Trained in 20 Hours!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, the game we’ll be talking about is the six-player no-limit Hold’em poker, which is one of the more popular poker variants out there. And the goal of this project was to build a poker AI that never played against a human before and learns entirely through self-play, and is able to defeat professional human players. During these tests, two of the players that were tested against are former World Series of Poker Main Event winners. And of course, before you ask, yes, in a moment, we’ll look at an example hand that shows how the AI traps a human player. Poker is very difficult to learn for AI bots because it is a game of imperfect information. For instance, chess is a game of perfect information where we see all the pieces and can make a good decision if we analyze the situation well. However, not so much in Poker, because only at the very end of the hand do the players show what they have. This makes it extremely difficult to train an AI to do well. And now, let’s have a look at the promised example hand here. We talked about imperfect information just a moment ago, so I’ll note that all the cards are shown face up for us to make the analysis of this hand easier, of course, this is not how the hands were played. You see the AI up there marked with P2 sittin’ pretty with a Jack and a Queen, and before the flop happens, which is when the first three cards are revealed, only one human player seems to be interested in this hand. During the flop, the AI paired its Queen and has a Jack as a kicker, which, if played well is going to be disastrous for the human player. So, why is that? You see, the human player also paired their queen, but has a weaker kicker and will therefore lose to the AIs hand. In this case, this player thinks they have a strong hand and will get lots of value out of it… only to find out that they will be the one milked by the AI. So, how exactly does that happen? Well, look here carefully! The bot shows weakness by checking here, to which, the human player’s answer is a small raise. The bot, again, shows weakness by just calling this raise, and checking again on the turn, essentially saying “I am weak, don’t hurt me!”. By the time we get to the river, the AI, again, appears weak to the human player, who now tries to milk the bot with a mid-sized raise… and, the AI recognizes that now is the time to pounce, the confused player calls the bet and gets milked for almost all their money. An excellent slow play from the AI. Now, note that one hand is difficult to evaluate in isolation, this was a great hand indeed, but we need to look at entire games to get a better grasp of the capabilities of this AI. So if we look at the dollar-equivalent value of the chips in the game, the AI was able to win a thousand dollars from these 5 professional poker players…every hour. It also uses very little resources, can be trained in the cloud for only several hundred dollars, and exceeds human-level performance within only 20 hours. What you see here is a decision tree that explains how the algorithm figures out whether to check or bet, and as you see here, this tree is traversed in a depth-first way, so first, it descends deep into one possible decision, and later, as more options are being unrolled and evaluated, the probability of these choices are updated above. In simpler words, first, the AI seems somewhat sure that checking would be a good choice here, but after carefully evaluating both decisions, it is able to further reinforce this choice. One of the professional players noted that the bot is a much more efficient bluffer than a human and always puts on a lot of pressure. Now note that this is also a general learning technique and is not tailored specifically for poker, and as a result, the authors of the paper noted that they will also try it on other imperfect information games in the future. What a time to be alive! This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. It is really easy to use, in fact, this blog post describes how you can visualize your Keras models with only one line of code. When you run this model, it will also start saving relevant metrics for you and here you can see the visualization of the mentioned model and these metrics as well. That’s it. You’re done! It can do a lot more than this, of course, and, you know what the best part is? The best part is that it’s free and will always be free for academics and open source projects. Make sure to visit them through wandb.com/papers or just click the link in the video description and sign up for a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
344,Augmented Reality Presentations Are Coming!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we talk about amazing research papers. However, when a paper is published, also, a talk often has to be given at a conference. And this paper is about the talk itself, or more precisely, how to enhance your presentation with dynamic graphics. Now, these effects can be added to music videos and documentary movies, however, they take a long time and cost a fortune. But not these ones, because this paper proposes a simple framework in which the presenter stands before a kinect camera and an AR mirror monitor, and can trigger these cool little graphical elements with simple gestures. A key part of the paper is the description of a user interface where we can design these mappings. This skeleton represents the presenter who is tracked by the kinect camera, and as you see here, we can define interactions between these elements and the presenter, such as grabbing this umbrella, pull up a chart, and more. As you see with the examples here, using such a system leads to more immersive storytelling, and note that again, this is an early implementation of this really cool idea. A few more papers down the line, I can imagine rotatable and deformable 3D models and photorealistic rendering entering the scene…well, sign me up for that. If you have any creative ideas as to how this could be used or improved, make sure to leave a comment. In the meantime, we are also now available on Instagram, so if you wish to see cool little snippets of our latest episodes, including this one, make sure to check us out there. Thanks for watching and for your generous support, and I'll see you next time!"
345,Simulating Water and Debris Flows,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We recently talked about an amazing paper that uses mixture theory to simulate the interaction of liquids and fabrics. And this new work is about simulating fluid flows where we have some debris or other foreign particles in our liquids. This is really challenging. For example, one of the key challenges is incorporating two-way coupling into this process. This means that the sand is allowed to have an effect on the fluid, but at the same time, as the fluid sloshes around, it also moves the sand particles within. Now, before you start wondering whether this is real footage or not, the fact that this is a simulation should become clear now, because what you see here in the background is where the movement of the two domains are shown in isolation. Just look at how much interaction there is between the two. Unbelievable. Beautiful simulation, ice cream for your eyes. This new method also contains a novel density correction step, and if you watch closely here, you’ll notice why. Got it? Let’s watch it again. If we try to run this elastoplastic simulation for these two previous methods, they introduce here a gain in density, or in other words, we end up with more stuff than we started with. These two rows show the number of particles in the simulation in the worst case scenario, and as you see, some of these incorporate millions of particles for the fluid and many hundreds of thousands for the sediment. Since this work uses the material point method, which is a hybrid simulation technique that uses both particles and grids, the delta x row denotes the resolution of the simulation grid. Now since these grids are often used for 3D simulations, we need to raise the 256 and the 512 to the third power, and with that, we get a simulation grid with up to hundreds of millions of points, and we haven’t even talked about the particle representation yet! In the face of all of these challenges, the simulator is able to compute one frame in a matter of minutes, and not hours or days, which is an incredible feat. With this, I think it is easy to see that the computer graphics research is improving at a staggering pace. What a time to be alive! If you enjoyed this episode, please make consider supporting us through Patreon, our address is Patreon.com/TwoMinutePapers, or just click the link in the video description. With this, we can make better videos for you. You can also get your name immortalized in the video description as a key supporter or watch these videos earlier than others. Thanks for watching and for your generous support, and I'll see you next time!"
346,DeepMind’s New AI Dreams Up Videos on Many Topics,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In the last few years, the pace of progress in machine learning research has been staggering. Neural network-based learning algorithms are now able to look at an image and describe what’s seen in this image, or even better, the other way around, generating images from a written description. You see here a set of results from BigGAN, a state of the art image generation technique and marvel at the fact that all of these images are indeed synthetic. The GAN part of this technique abbreviates the term Generative Adversarial Network this means a pair of neural networks that battle each over time to master a task, for instance, to generate realistic looking images when given a theme. These detailed images are great, but, what about generating video? With the Dual Video Discriminator GAN, DVD-GAN in short, DeepMind’s naming game is still as strong as ever, it is now possible to create longer and higher-resolution videos than was previously possible, the exact numbers are 256x256 in terms of resolution and 48 frames, which is about 2 seconds. It also learned the concept of changes in the camera view, zooming in on an object, and understands that if someone draws something with a pen, the ink has to remain on the paper unchanged. The Dual Discriminator part of the name reveals one of the key ideas of the paper. In a classical GAN, we have a discriminator network that looks at the images of the generator network and critiques them. As a result, this discriminator learns to tell fake and real images apart better, but at the same time, provides ample feedback for the generator neural network so it can come up with better images. In this work, we have not one, but two discriminators, one is called the spatial discriminator that looks at just one image and assesses how good it is structurally, while the second, temporal discriminator critiques the quality of the movement in these videos. This additional information provides better teaching for the generator, which will in return, be able to generate better videos for us. The paper contains all the details that you could possibly want to learn about this algorithm, in fact, let me give you two that I found to be particularly interesting: one, it does not get any additional information about where the foreground and the background is, and is able to leverage the learning capacity of these neural networks to learn these concepts by itself. And two, it does not generate the video frame by frame sequentially, but it creates the entire video in one go. That’s wild. Now, 256x256 is not a particularly high video resolution, but if you have been watching this series for a while, you are probably already saying that two more papers down the line, and we may be watching HD videos that are also longer than we have the patience to watch. And all this through the power of machine learning research. For now, let’s applaud DeepMind for this amazing paper, and I can’t wait to have a look at more results and see some followup works on it. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
347,This AI Hallucinates Images For You,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As machine learning research advances over time, learning-based techniques are getting better and better at generating images, or even creating videos when given a topic. A few episodes ago, we talked about a DeepMind’s Dual Video Discriminator technique, in which, multiple neural networks compete against each other, teaching our machines to synthesize a collection of 2-second long videos. One of the key advantages of this method was that it also learned the concept of changes in the camera view, zooming in on an object, and understood that if someone draws something with a pen, the ink has to remain on the paper unchanged. However, generally, if we wish to ask an AI to synthesize assets for us, in many cases, we’ll likely have an exact idea of what we are looking for. In these cases, we are looking for a little more artistic control than this technique offers us. So, can we get around this? If so, how? Well, we can! I’ll tell you how in a moment, but to understand this solution, we first have to have a firm grasp on the concept of latent spaces. You can think of a latent space as a compressed representation that tries to capture the essence of the dataset that we have at hand. You can see a similar latent space method in action here that captures the key features that set different kinds of fonts apart and presents these options on a 2D plane, and here, you see our technique that builds a latent space for modeling a wide range of photorealistic material models that we can explore. And now to this new work. What this tries to do is find a path in the latent space of these images that relates to intuitive concepts like camera zooming, rotation or shifting. That’s not an easy task, but if we can pull it off, we’ll have more artistic control over these generated images, which would be immensely useful for many creative tasks. This new work can perform that, and not only that, but it is also able to learn the concept of color enhancement, and can even increase or decrease the contrast of these images. The key idea of this paper is that this can be done through trying to find crazy, non-linear trajectories in these latent spaces that happen to relate to these intuitive concepts. It is not perfect in a sense that we can indeed zoom in on the picture of this dog, but the posture of the dog also changes, and it even seems like we’re starting out with a puppy that grows up frame by frame. This means that we have learned to navigate this latent space, but there is still some additional fat in these movements, which is a typical side effect of latent space-based techniques and also, don’t forget that the training data the AI is given also has its own limits. However, as you see, we are now one step closer to not only having an AI that synthesizes images for us, but one that does it exactly with the camera setup, rotation, and colors that we are looking for. What a time to be alive! If you wish to see beautiful formulations of walks…walks in latent spaces, that is, make sure to have a look at the paper in the video description. Also, note that we have now appeared on instagram with bite-sized pieces of our bite-sized videos. Yes, it is quite peculiar. Make sure to check it out, just search for two minute papers on instagram or click the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
348,This Adorable Baby T-Rex AI Learned To Dribble,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. About 350 episodes ago in this series, in episode number 8, we talked about an amazing paper in which researchers built virtual characters with a bunch of muscles and joints, and through the power of machine learning, taught them to actuate them just the right way so that they would learn to walk. Well, some of them anyway. Later, we’ve seen much more advanced variants where we could even teach them to lift weights, jump really high, or even observe how their movements would change after they undergo surgery. This paper is a huge step forward in this area, and if you look at the title, it says that it proposes multiplicative composition policies to control these characters. What this means is that these complex actions are broken down into a sum of elementary movements. Intuitively you can imagine something similar when you see a child use small, simple lego pieces to build a huge, breathtaking spaceship. That sounds great, but what does this do for us? Well, the ability to properly combine these lego pieces is where the learning part of the technique shines, and you can see on the right that these individual lego pieces are as amusing as useless if they’re not combined with others. To assemble efficient combinations that are actually useful, the characters are first required to learn to perform reference motions using combinations of these lego pieces. Here, on the right, the blue bars show which of these these lego pieces are used and when in the current movement pattern. Now, we’ve heard enough of these legos, what is this whole compositional thing good for? Well, a key advantage of using these is that they are simple enough so that they can be transferred and reused for other types of movement. As you see here, this footage demonstrates how we can teach a biped, or even a T-Rex to carry and stack boxes or how to dribble, or, how to score a goal. Amusingly, according to the paper, it seems that this T-Rex weighs only 55 kilograms or 121 pounds. An adorable baby T-Rex, if you will. As a result of this transferability property, when we assemble a new agent or wish to teach an already existing character some new moves, we don’t have to train them from scratch as they already have access to these lego pieces. I love seeing all these new papers in the intersection of computer graphics and machine learning. This is a similar topic to what I am working on as a full-time research scientist at the Technical University of Vienna, and in these projects, we train plenty of neural networks, which requires a lot of computational resources. Sometimes when we have to spend time maintaining the machines running these networks, buying new hardware or troubleshooting software issues, I wish we could use Linode. Linode is the world’s largest independent cloud hosting and computing provider, and they have GPU instances that are tailor-made for AI, scientific computing and computer graphics projects. If you feel inspired by these works and you wish to run your experiments or deploy your already existing works through a simple and reliable hosting service, make sure to join over 800,000 other happy customers and choose Linode. To reserve your GPU instance and receive a $20 free credit, visit linode.com/papers or click the link in the description and use the promo code “papers20” during signup. Give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
349,Adversarial Attacks on Neural Networks - Bug or Feature?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This will be a little non-traditional video where the first half of the episode will be about a paper, and the second part will be about…something else. Also a paper. Well, kind of. You’ll see. We’ve seen in the previous years that neural network-based learning methods are amazing at image classification, which means that after training on a few thousand training examples, they can look at a new, previously unseen image and tell us whether it depicts a frog or a bus. Earlier we have shown that we can fool neural networks by adding carefully crafted noise to an image, which we often refer to as an adversarial attack on a neural network. If done well, this noise is barely perceptible and, get this, can fool the classifier into looking at a bus and thinking that it is an ostrich. These attacks typically require modifying a large portion of the input image, so when talking about a later paper, we were thinking, what could be the lowest number of pixel changes that we have to perform to fool a neural network? What is the magic number? Based on the results of previous research works, an educated guess would be somewhere around a hundred pixels. A followup paper gave us an unbelievable answer by demonstrating the one pixel attack. You see here that by changing only one pixel in an image that depicts a horse, the AI will be 99.9% sure that we are seeing a frog. A ship can also be disguised as a car, or, amusingly, with a properly executed one-pixel attack, almost anything can be seen as an airplane by the neural network. And, this new paper discusses whether we should look at these adversarial examples as bugs or not, and of course, does a lot more than that! It argues that most datasets contain features that are predictive, meaning that they provide help for a classifier to find cats, but also non-robust, which means that they provide a rather brittle understanding that falls apart in the presence adversarial changes. We are also shown how to find and eliminate these non-robust features from already existing datasets and that we can build much more robust classifier neural networks as a result. This is a truly excellent paper that sparked quite a bit of discussion. And here comes the second part of the video with the something else. An interesting new article was published within the Distill journal, a journal where you can expect clearly worded papers with beautiful and interactive visualizations. But this is no ordinary article, this is a so-called discussion article where a number of researchers were asked to write comments on this paper and create interesting back and forth discussions with the original authors. Now, make no mistake, the paper we’ve talked about was peer-reviewed, which means that independent experts have spent time scrutinizing the validity of the results, so this new discussion article was meant to add to it by getting others to replicate the results and clear up potential misunderstandings. Through publishing six of these mini-discussions, each of which were addressed by the original authors, they were able to clarify the main takeaways of the paper, and even added a section of non-claims as well. For instance, it’s been clarified that they don’t claim that adversarial examples arise from software bugs. A huge thanks to the Distill journal and all the authors who participated in this discussion, and Ferenc Huszár, who suggested the idea of the discussion article to the journal. I’d love to see more of this, and if you do too, make sure to leave a comment so we can show them that these endeavors to raise the replicability and clarity of research works are indeed welcome. Make sure to click the link to both works in the video description, and spend a little quality time with them. You’ll be glad you did. I think this was a more complex than average paper to talk about, however, as you have noticed, the usual visual fireworks were not there. As a result, I expect this to get significantly fewer views. That’s not a great business model, but no matter, I made this channel so I can share with you all these important lessons that I learned during my journey. This has been a true privilege and I am thrilled that I am still able to talk about all these amazing papers without worrying too much whether any of these videos will go viral or not. Videos like this one are only possible because of your support on Patreon.com/TwoMinutePapers. If you feel like chipping in, just click the Patreon link in the video description. This is why every video ends with, you know what’s coming… Thanks for watching and for your generous support, and I'll see you next time!"
350,This AI Clears Up Your Hazy Photos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we are going to talk about a paper that builds on a previous work by the name Deep Image Priors, DIP in short. This work was capable of performing JPEG compression artifact removal, image inpainting, or in other words, filling in parts of the image with data that makes sense, super resolution, and image denoising. It was quite the package. This new method is able to subdivide an image into a collection of layers, which makes it capable of doing many seemingly unrelated tasks, for instance, one, it can do image segmentation, which typically means producing a mask that shows us the boundaries between the foreground and background. As an additional advantage, it can also do this for videos as well. Two, it can perform dehazing, which can also be thought of as a decomposition task where the input is one image, and the output is an image with haze, and one with the objects hiding behind the haze. If you spend a tiny bit of time looking out the window on a hazy day, you will immediately see that this is immensely difficult, mostly because of the fact that the amount of haze that we see is non-uniform along the landscape. The AI has to detect and remove just the right amount of this haze and recover the original colors of the image. And three, it can also subdivide these crazy examples where two images are blended together. In a moment, I’ll show you a better example with a complex texture where it is easier to see the utility of such a technique. And four, of course, it can also perform image inpainting, which, for instance, can help us remove watermarks or other unwanted artifacts from our photos. This case can also be thought of an image layer plus a watermark layer, and in the end, the algorithm is able to recover both of them. As you see here on the right, a tiny part of the content seems to bleed into the watermark layer, but the results are still amazing. It does this by using multiple of these DIPs, deep image prior networks, and goes by the name DoubleDIP. That one got me good when I’ve first seen it. You see here how it tries to reproduce this complex textured pattern as a sum of these two, much simpler individual components. The supplementary materials are available right in your browser, and show you a ton of comparisons against other previous works. Here you see results from these earlier works on image dehazing and see that indeed, the new results are second to none. And all this progress within only two years. What a time to be alive! If like me, you love information theory, woo-hoo! Make sure to have a look at the paper and you’ll be a happy person. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It is like a shared logbook for your team, and with this, you can compare your own experiment results, put them next to what your colleagues did and you can discuss your successes and failures much easier. It takes less than 5 minutes to set up and is being used by OpenAI, Toyota Research, Stanford and Berkeley. It was also used in this OpenAI project that you see here, which we covered earlier in the series. They reported that experiment tracking was crucial in this project and that this tool saved them quite a bit of time and money. If only I had an access to such a tool during our last research project where I had to compare the performance of neural networks for months and months. Well, it turns out, I will be able to get access to these tools, because, get this, it’s free and will always be free for academics and open source projects. Make sure to visit them through wandb.com/papers or just click the link in the video description and sign up for a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
351,New Face Swapping AI Creates Amazing DeepFakes!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Recently, we have experienced an abundance of papers on facial reenactment in machine learning research. We talked about a technique by the name Face2Face back in 2016, approximately 300 videos ago. It was able to take a video of us and transfer our gestures to a target subject. This was kind of possible at the time with specialized depth cameras, until Face2Face appeared and took the world by storm as it was able to perform what you see here with a regular consumer camera. However, it only transferred gestures, so of course, scientists were quite excited about the possibility of transferring more than just that. But, that would require solving so many more problems for instance, if we wish to turn the head of the target subject, we may need to visualize regions that we haven’t seen in these videos, which also requires an intuitive understanding of hair, the human face and more. This is quite challenging. So, can this be really done? Well, have a look at this amazing new paper! You see here the left image, this is the source person, the video on the right is the target video, and our task is to transfer not just the gestures, but the pose, gestures and appearance of the face on the left to the video on the right. And, this new method works like magic. Look! It not only works like magic, but pulls it off on a surprisingly large variety of cases, many of which I haven’t expected at all. Now, hold on to your papers, because this technique was not trained on these subjects, which means that this is the first time it is seeing these people. It has been trained on plenty of people, but not these people. Now, before we look at this example, you are probably saying, well, the occlusions from the microphone will surely throw the algorithm off, right? Well, let’s have a look. Nope, no issues at all. Absolutely amazing, love it! So how does this wizardry work exactly? Well, it requires careful coordination between no less than four neural networks, where each of which specializes for a different task. The first two is a reenactment generator that produces a first estimation of the reenacted face, and a segmentation generator network that creates this colorful image that shows which region in the image corresponds to which facial landmark. These two are then handed over to the third network, the inpainting generator, which fills the rest of the image, and since we have overlapping information, in comes the fourth, blending generator to the rescue to combine all this information into our final image. The paper contains a detailed description of each of these networks, so make sure to have a look! And if you do, you will also find that there are plenty of comparisons against previous works, of course, Face2Face is one of them, which was already amazing, and you can see how far we’ve come in only three years. Now, when we try to evaluate such a research work, we are curious as to how much these individual puzzle pieces, in this case, the generator networks contribute to the final results. Are really all of them needed? What if we remove some of them? Well, this is a good paper, so we can find the answer in Table 2, where all of these components are tested in isolation. The downward and upward arrows show which measure is subject to minimization and maximization, and if we look at this column, it is quite clear that all of them indeed improve the situation, and contribute to the final results. And remember, all this from just one image of the source person. Insanity. Thanks for watching and for your generous support, and I'll see you next time!"
352,This is How You Simulate Making Pasta,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Fluid simulation is a mature research field within computer graphics with amazing papers that show us how to simulate water flows with lots of debris, how to perform liquid-fabric interactions, and more. This new project further improves the quality of these works and shows us how thin, elastic strands interact with oil paint, mud, melted chocolate, and pasta sauce. There will be plenty of tasty and messy simulations ahead, not necessarily in that order, so, make sure to hold on to your papers, just in case. Here you see four scenarios of these different materials dripping off of a thin strand. So, why are these cases difficult to simulate? The reason why it’s difficult, if not flat out impossible because the hair strands and the fluid layers are so thin, it would require a simulation grid that is so microscopic, or in other words, we would have to perform our computations of quantities like pressure and velocity on so many grid points, it would probably take not from hours to days, but from weeks to years to compute. I will show you a table in a moment where you will see that these amazingly detailed simulations can be done on a grid of surprisingly low resolution. As a result, our simulations also needn’t be so tiny in scale with one hair strand and a few drops of mud or water. They can be done on a much larger scale, so we can marvel together at these tasty and messy simulations, you decide which is which. I particularly liked this animation with the oyster sauce because you can also see a breakdown of the individual elements of the simulation. Note that all of the interactions between the noodles, the sauce, the fork, and plate have to be simulated with precision. Love it. And now, the promised table. Here you can see the delta x that means how fine the grid resolution is, which is in the order of centimeters, and not micrometers. That is reassuring, and don’t forget that this work is an extension to the Material Point Method, which is a hybrid simulation method that both uses grids and particles. And, sure enough, you can see here that it simulates up to tens of millions of particles as well, and the fact that the computation times are still only measured in a few minutes per frame is absolutely remarkable. Remember, the fact that we can simulate this at all is a miracle. Now, this was run on the processor, and a potential implementation on the graphics card could yield us significant speedups, so I really hope something like this appears in the near future. Also, make sure to have a look at the paper itself, which is outrageously well written. If you wish to see more from this paper, make sure to follow us on Instagram, just search for Two Minute Papers there or click the link in the description. Now, I am still working as a full-time research scientist at the Technical University of Vienna, and we train plenty of neural networks during our projects, which requires a lot of computational resources. Every time we have to spend our time maintaining these machines, I wish we could use Linode. Linode is the world’s largest independent cloud hosting and computing provider. If you feel inspired by these works and you wish to run your experiments or deploy your already existing works through a simple and reliable hosting service, make sure to join over 800,000 other happy customers and choose Linode. To reserve your GPU instance and receive a $20 free credit, visit linode.com/papers or click the link in the description and use the promo code “papers20” during signup. Give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
353,These Are The 7 Capabilities Every AI Should Have,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A few years ago, scientists at DeepMind published a learning algorithm that they called deep reinforcement learning which quickly took the world by storm. This technique is a combination of a neural network that processes the visual data that we see on the screen, and a reinforcement learner that comes up with the gameplay-related decisions, which proved to be able to reach superhuman performance on computer games like Atari Breakout. This paper not only sparked quite a bit of mainstream media interest, but also provided fertile grounds for new followup research works to emerge. For instance, one of these followup papers infused these agents with a very human-like quality, curiosity, further improving many aspects of the original learning method, however, had a disadvantage, I kid you not, it got addicted to the TV and kept staring at it forever. This was perhaps a little too human-like. In any case, you may rest assured that this shortcoming has been remedied since, and every followup paper recorded their scores on a set of Atari games. Measuring and comparing is an important part of research and is absolutely necessary so we can compare new learning methods more objectively. It’s like recording your time for the olympics at the 100 meter dash. In that case, it’s quite easy to decide which athlete is the best. However, this is not so easy in AI research. In this paper, scientists at DeepMind note that just recording the scores doesn’t give us enough information anymore. There’s so much more to reinforcement learning algorithms than just scores. So, they built a behavior suite that also evaluates the 7 core capabilities of reinforcement learning algorithms. Among these 7 core capabilities, they list generalization, which tells us how well the agent is expected to do in previously unseen environments, how good it is at credit assignment, which is a prominent problem in reinforcement learning. Credit assignment is very tricky to solve because, for instance, when we play a strategy game, we need to make a long sequence of strategic decisions, and in the end, if we lose an hour later, we have to figure out which one of these many-many decisions led to our loss. Measuring this as one of the core capabilities, was, in my opinion, a great design decision here. How well the algorithm scales to larger problems also gets a spot as one of these core capabilities. I hope this testing suite will see widespread adoption in reinforcement learning research, and what I am really looking forward to is seeing these radar plots for newer algorithms, which will quickly reveal whether we have a new method that takes a different tradeoff than previous methods, or in other words, has the same area within the polygon, but with a different shape, or, in the case of a real breakthrough, the area of these polygons will start to increase. Luckily, a few of these charts are already available in the paper and they give us so much information about these methods, I could stare at them all day long and I cannot wait to see some newer methods appear here. Now note that there is a lot more to this paper, if you have a look at it in the video description, you will also find the experiments that are part of this suite, what makes a good environment to test these agents in, and that they plan to form a committee of prominent researchers to periodically review it. I loved that part. If you enjoyed this video, please consider supporting us on Patreon. If you do, we can offer you early access to these videos so you can watch them before anyone else, or, you can also get your name immortalized in the video description. Just click the link in the description if you wish to chip in. Thanks for watching and for your generous support, and I'll see you next time!"
354,OpenAI’s GPT-2 Is Now Available - It Is Wise as a Scholar!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. OpenAI GPT-2 is a learning-based technique which can perform common natural language processing operations, for instance, answering questions, completing text, reading comprehension, summarization, and more. What is absolutely amazing about this technique is that it is able to perform these tasks with as little supervision as possible. This means that we unleash the algorithm that they call GPT-2 to read the internet, and it will learn our language by itself. To perform this, of course, we need a lot of training data, and here, the AI reads 40 gigabytes of internet text, which is 40 gigs of non-binary plaintext data, which is a stupendously large amount of text. You see here an example of how it fares with text completion, and the results were absolutely amazing. And, not only that, because, hold on to your papers, we can now try it ourselves and my results show that it can be wise as a scholar, and it can also cheat like nobody’s business. So, let’s try something simple here. Well, first, as everyone is talking about job security, I’d like to give it the start of this Two Minute Papers script and see if I am about to be replaced. Let’s see! It continues the intro with “The history of Eastern Europe since 1989 has been an interesting and complex story spanning four decades. On one side you have a regime of Western-backed dictators from the mid-1990s” and so on. Very scholarly indeed. However, I think I might be safe …for now. And now, something that I wanted to try since the inception of GPT-2, which is, feed it a snippet of something that I said about Wavelet Turbulence, which is one of the best papers ever written, and have the AI finish it. I tried to frame it as an exam question and was not disappointed be the results. So the text is, “There was a similar technique by the name Wavelet Turbulence, which is”, I hope you see what I did there…and look what it did! “There was a similar technique by the name Wavelet Turbulence, which is not at all what we are talking about, but I'll let you decide whether it is worth your time to read it.” Wow! GPT-2 has been caught cheating red handed. I love it. Let’s continue what it had to say: “Anyway, my aim here is to describe how we use a fluid simulation to simulate smoke and liquid effects using just a few simple equations: Solving a few functions gives us a set of derivatives of the particle trajectories, that is equations that describe how and where the particles will come and go across the liquid in the simulation. For simplicity (as a good teacher of high-school students should know) we can use a simple equation, which simulates two particles that collide.” I am stunned. It recognized that we are talking about fluid simulations, which is already remarkable, but it went much further The completion is not bad at all and is not only coherent, on topic, but has quite a bit of truth to it. I will have to rethink my previous claim about my job security. The even crazier thing is that the size of this model is about 750 million parameters, which is only half of the size of the original full model, which is expected to be even better. I put a link to this website in the video description for your pleasure, make sure to play with it, this is mad fun. And, GPT-2 will also see so many applications that we cannot even fathom yet. For instance, here you can see that one can train it on many source code files on GitHub and it will be able to complete the code that we write on the fly. Now, nobody should think of this as GPT-2 writing programs for us, this is, of course, unlikely, however, it will ease the process for novice and experts users alike. If you have any other novel applications in mind, make sure to leave a comment below. For now, bravo OpenAI, and a big thank you for Daniel King and the Hugging Face company for this super convenient public implementation. Let the  experiments begin! Thanks for watching and for your generous support, and I'll see you next time!"
355,Google's AI Plays Football…For Science!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is an important subfield within machine learning research where we teach an agent to choose a set of actions in an environment to maximize a score. This enables these AIs to play Atari games at a superhuman level, control drones, robot arms, or even create self-driving cars. A few episodes ago, we talked about DeepMind’s behavior suite that opened up the possibility of measuring how these AIs perform with respect to the 7 core capabilities of reinforcement learning algorithms. Among them were how well such an AI performs when being shown a new problem, how well or how much they memorize, how willing they are to explore novel solutions, how well they scale to larger problems, and more. In the meantime, the Google Brain research team has also been busy creating a physics-based 3D football, or for some of you, soccer simulation where we can ask an AI to control one, or multiple players in this virtual environment. This is a particularly difficult task because it requires finding a delicate balance between rudimentary short-term control tasks, like passing, and long-term strategic planning. In this environment, we can also test our reinforcement learning agents against handcrafted, rule-based teams. For instance, here you can see that DeepMind’s Impala algorithm is the only one that can reliably beat the medium and hard handcrafted teams, specifically, the one that was run for 500 million training steps. The easy case is tuned to be suitable for single-machine research works, where the hard case is meant to challenge sophisticated AIs that were trained on a massive array of machines. I like this idea a lot. Another design decision I particularly like here is that these agents can be trained from pixels or internal game state. Okay, so what does that really mean? Training from pixels is easy to understand but very hard to perform this simply means that the agents see the same content as what we see on the screen. DeepMind’s Deep Reinforcement Learning is able to do this by training a neural network to understand what events take place on the screen, and passes, no pun intended all this event information to a reinforcement learner that is responsible for the strategic, gameplay-related decisions. Now, what about the other one? The internal game state learning means that the algorithm sees a bunch of numbers which relate to quantities within the game, such as the position of all the players and the ball, the current score and so on. This is typically easier to perform because the AI is given high-quality and relevant information and is not burdened with the task of visually parsing the entire scene. For instance, OpenAI’s amazing DOTA2 team learned this way. Of course, to maximize impact, the source code for this project is also available. This will not only help researchers to train and test their own reinforcement learning algorithms on a challenging scenario, but they can extend it and make up their own scenarios. Now note that so far, I tried my hardest not to comment on the names of the players and the teams, but my will to resist just ran out. Go real Bayesians! Thanks for watching and for your generous support, and I'll see you next time!"
356,"Finally, Style Transfer For Smoke Simulations!","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I can confidently say that this is the most excited I’ve been for a smoke simulation paper since Wavelet Turbulence. Wavelet Turbulence is a magical algorithm from 2008 that takes a low-quality fluid or smoke simulation and increases its quality by filling in the remaining details. And here we are, 11 years later, the results still hold up. Insanity. This is one of the best papers ever written and has significantly contributed to my decision to pursue a research career. And, this new work performs style transfer for smoke simulations. If your haven’t fallen out of your chair yet, let me try to explain why this is amazing. Style transfer is a technique in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to the super fun results that you see here. An earlier paper had shown that the more sophisticated ones can make even art curators think that they are real. However, doing this for smoke simulations is a big departure from 2D style transfer, because that one takes an image, where this works in 3D, and does not deal with images, but with density fields. A density field means a collection of numbers that describe how dense a smoke plume is at a given spatial position. It is a physical description of a smoke plume, if you will. So how could we possibly apply artistic style from an image to a collection of densities? This doesn’t sound possible at all. Unfortunately, the problem gets even worse. Since we typically don’t just want to look at a still image of a smoke plume, but enjoy a physically correct simulation, not only the density fields, but the velocity fields and the forces that animate them over time also have to be stylized. Hmm. Again, that’s either impossible, or almost impossible to do. You see, if we run a proper smoke simulation, we’ll see what would happen in reality, but that’s not stylized. However, if we stylize, we get something that would not happen in mother nature. I have spent my master’s thesis trying to solve a problem called fluid control, which would try to coerce a smoke plume or a piece of fluid to take a given shape. Like a bunny, or a logo with letters. You can see some footage of what I came up with here. Here, both the simulation and the controlling force field is computed in real time on the graphics card and as you see, it can be combined with Wavelet Turbulence. If you wish to hear more about this work, make sure to leave a comment, but in any case, I had a wonderful time working on it, if anyone wants to pick it up, the paper and the source code, and even a Blender addon version are available in the video description. In any case, in a physics simulation, we’re trying to simulate reality, and for style transfer, we’re trying to depart from reality. The two are fundamentally incompatible, and we have to reconcile them in a way that is somehow still believable. Super challenging. However, back then when I wrote the fluid control paper, learning-based algorithms were not nearly as developed, so it turns out, they can help us perform style transfer for density fields, and also, animate them properly. Again, the problem definition is very easy, in comes a smoke plume, we add an image for style, and the style of this image is somehow applied to the density field to get these incredible effects. Just look at these marvelous results. Fire textures, starry night, you name it. It seems to be able to do anything! One of the key ideas is that even though style transfer is challenging on highly detailed density fields, but it becomes much easier if we first downsample the density field to a coarser version, perform the style transfer there, and upsample this density field again with already existing techniques. Rinse and repeat. The paper also describes a smoothing technique that ensures that the changes in the velocity fields that guide our density fields change slowly over time to keep the animation believable. There are a lot more new ideas in the paper, so make sure to have a look! It also takes a while, the computation time is typically around 10 to 15 minutes per frame, but who cares! Today, with the ingenuity of research scientists and the power of machine learning algorithms, even the impossible seems possible. If it takes 15 minutes per frame, so be it. What a time to be alive! Thanks for watching and for  your generous support, and I'll see you next time!"
357,DeepFake Detector AIs Are Good Too!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We talked about a technique by the name Face2Face back in 2016, approximately 300 videos ago. It was able to take a video of us and transfer our gestures to a target subject. With techniques like this, it’s now easier and cheaper than ever to create these deepfake videos of a target subject provided that we have enough training data, which is almost certainly the case for people who are the most high-value targets for these kinds of operations. Look here. Some of these videos are real, and some are fake. What do you think, which is which? Well, here are the results this one contains artifacts and is hence easy to spot, but the rest…it’s tough. And it’s getting tougher by the day. How many did you get right? Make sure to leave a comment below. However, don’t despair, it’s not all doom and gloom. Approximately a year ago, in came FaceForensics, a paper that contains a large dataset of original and manipulated video pairs. As this offered a ton of training data for real and forged videos, it became possible to train a deepfake detector. You can see it here in action as these green to red colors showcase regions that the AI correctly thinks were tampered with. However, this followup paper by the name FaceForensics++ contains not only not only an improved dataset, but provides many more valuable insights to help us detect these DeepFake videos, and even more. Let’s dive in. Key insight number one. As you’ve seen a minute ago, many of these DeepFake AIs introduce imperfections, or in other words, artifacts to the video. However, most videos that we watch on the internet are compressed, and the compression procedure…you have guessed right, also introduces artifacts to the video. From this, it follows that hiding these DeepFake artifacts behind compressed videos sounds like a good strategy to fool humans and detector neural networks likewise, and not only that, but the paper also shows us by how much exactly. Here you see a table where each row shows the detection accuracy of previous techniques and a new proposed one, and the most interesting part is how this accuracy drops when we go from HQ to LQ, or in other words, from a high-quality video to a lower-quality one with more compression artifacts. Overall, we can get an 80-95% success rate, which is absolutely amazing. But, of course, you ask, amazing compared to what? Onwards to insight number two. This chart shows how humans fared in DeepFake detection, as you can see, not too well. Don’t forget, the 50% line means that the human guesses were as good as a coinflip, which means that they were not doing well at all. Face2face hovers around this ratio, and if you look at NeuralTextures, you see that this is a technique that is extremely effective at fooling humans. And wait…what’s that? For all the other techniques, we see that the grey bars are shorter, meaning that it’s more difficult to find out if a video is a DeepFake because its own artifacts are hidden behind the compression artifacts. But the opposite is the case for NeuralTextures, perhaps because its small footprint on the videos. Note that a state of the art detector AI, for instance, the one proposed in this paper does way better than these 204 human participants. This work does not only introduce a dataset, these cool insights, but also introduces a detector neural network. Now, hold on to your papers because this detection pipeline is not only so powerful that it practically eats compressed DeepFakes for breakfast, but it even tells us with remarkable accuracy which method was used to tamper with the input footage. Bravo! Now, it is of utmost importance that we let the people know about the existence of these techniques, this is what I am trying to accomplish with this video. But that’s not enough, so I also went to this year’s biggest NATO conference and made sure that political and military decision makers are also informed about this topic. Last year, I went to the European Political Strategy Center with a similar goal. I was so nervous before both of these talks and spent a long time rehearsing them, which delayed a few videos here on the channel. However, because of your support on Patreon, I am in a fortunate situation where I can focus on doing what is right and what is the best is for all of us, and not worry about the financials all the time. I am really grateful for that, it really is a true privilege. Thank you. If you wish to support us, make sure to click the Patreon link in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
358,Is a Realistic Honey Simulation Possible?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If we study the laws of fluid motion from physics, and write a computer program that contains these laws, we can create wondrous fluid simulations like the ones you see here. The amount of detail we can simulate with these programs is increasing every year, not only due to the fact that hardware improves over time, but also, the pace of progress in computer graphics research is truly remarkable. So, it s there nothing else to do? Are we done with fluid simulation research? Oh, no. No, no, no. For instance, fluid-solid interaction still remains a challenging phenomenon to simulate. This means that the sand is allowed to have an effect on the fluid, but at the same time, as the fluid sloshes around, it also moves the sand particles within. This is what we refer to as two-way coupling. Note that this previous work that you see here was built on the Material Point Method, a hybrid simulation technique that uses both particles and grids, whereas this new paper introduces proper fluid-solid coupling to the simpler, grid-based simulators. Not only that, but this new work also shows us that there are different kinds of two-way coupling. If we look at this footage with the honey and the dipper, it looks great, however, this still doesn’t seem right to me. We are doing science here, so fortunately, we don’t need to guess what seems or doesn’t seem right. This is my favorite part, because this is when we let reality be our judge and compare to what exactly happens in the real world. So let’s do that! Whoa! There is quite a bit of a difference, because in reality, the honey is able to support the dipper. One-way coupling, of course, cannot simulate this kind of back and forth interaction, and neither can weak two-way coupling pull this off. And now, let’s see, YES! There we go, the new strong two-way coupling method finally gets this right. And not only that, but what I really love about this is that it also gets small nuances right. I will try to speed up the footage a little, so you can see that the honey doesn’t only support the dipper, but the dipper still has some subtle movements both in reality and in the simulation. A+. Love it. So, what is the problem? Why is this so difficult to simulate? One of the key problems here is being able to have a simulation that has a fine resolution in the areas where a fluid and a solid intersect each other. If we create a super detailed simulation, it will take from hours to days to compute, but on the other hand, if we have a too coarse one, it will compute the required deformations in so few of these grid points that we’ll get a really inaccurate simulation, and not only that, but we will even miss some of the interactions completely. This paper proposes a neat new volume estimation technique that focuses these computations to where the action happens, and only there, which means that we can get these really incredible results, even if we only run a relatively coarse simulation. I could watch these gooey, viscous simulations all day long. If you have a closer look at the paper in the description, you will find some hard data that shows that this technique executes quicker than other methods that are able to provide comparable results. Thanks for watching and for your generous support, and I'll see you next time!"
359,AI Learns Human Movement From Unorganized Data,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Last year, an amazing neural network-based technique appeared that was able to look at a bunch of unlabeled motion data, and learned to weave them together to control the motion of quadrupeds, like this wolf here. It was able to successfully address the shortcomings of previous works, for instance, the weird sliding motions have been eliminated, and it was also capable of following some predefined trajectories. This new paper continues research in this direction by proposing a technique that is also capable of interacting with its environment or other characters, for instance, they can punch each other, and after the punch, they can recover from undesirable positions, and more. The problem formulation is as follows it is given the current state of the character and a goal, and you see here with blue how it predicts the motion to continue. It understands that we have to walk towards the goal, that we are likely to fall when hit by a ball, and it knows, that then, we have to get up and continue our journey, and eventually, reach our goal. Some amazing life advice from the AI right there. The goal here is also to learn something meaningful from lots of barely labeled human motion data. Barely labeled means that a bunch of videos are given almost as-is, without additional information on what movements are being performed in these videos. If we had labels for all this data that you see here, it would say that this sequence shows a jump, and these ones are for running. However, the labeling process takes a ton of time and effort, so if we can get away without it, that’s glorious, but, in return, with this, we create an additional burden that the learning algorithm has to shoulder. Unfortunately, the problem gets even worse as you see here, the number of frames contained in the original dataset is very scarce. To alleviate this, the authors decided to augment this dataset, which means trying to combine parts of this data to squeeze out as much information as possible. You see some examples here how this motion data can be combined from many small segments, and in the paper, they show that the augmentation helps us create even up to 10 to 30 times more training data for the neural networks. As a result of this augmented dataset, it can learn to perform zombie, gorilla movements, chicken hopping, even dribbling with a basketball, you name it. What’s even more, we can give the AI high-level commands interactively, and it will try to weave the motions together appropriately. They can also punch each other. Ow. And all this was learned from a bunch of unorganized data. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
360,OpenAI Plays Hide and Seek…and Breaks The Game!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this project, OpenAI built a hide and seek game for their AI agents to play. While we look at the exact rules here, I will note that the goal of the project was to pit two AI teams against each other, and hopefully see some interesting emergent behaviors. And, boy, did they do some crazy stuff. The coolest part is that the two teams compete against each other, and whenever one team discovers a new strategy, the other one has to adapt. Kind of like an arms race situation, and it also resembles generative adversarial network a little. And the results are magnificent, amusing, weird you’ll see in a moment. These agents learn from previous experiences, and to the surprise of no one, for the first few million rounds, we start out with…pandemonium. Everyone just running around aimlessly. Without proper strategy and semi-random movements, the seekers are favored and hence win the majority of the games. Nothing to see here. Then, over time, the hiders learned to lock out the seekers by blocking the doors off with these boxes and started winning consistently. I think the coolest part about this is that the map was deliberately designed by the OpenAI scientists in a way that the hiders can only succeed through collaboration. They cannot win alone and hence, they are forced to learn to work together. Which they did, quite well. But then, something happened. Did you notice this pointy, doorstop-shaped object? Are you thinking what I am thinking? Well, probably, and not only that, but about 10 million rounds later, the AI also discovered that it can be pushed near a wall and be used as a ramp, and, tadaa! Got’em! Te seekers started winning more again. So, the ball is now back on the court of the hiders. Can you defend this? If so, how? Well, these resourceful little critters learned that since there is a little time at the start of the game when the seekers are frozen, apparently, during this time, they cannot see them, so why not just sneak out and steal the ramp, and lock it away from them. Absolutely incredible. Look at those happy eyes as they are carrying that ramp. And, you think it all ends here? No, no, no. Not even close. It gets weirder. Much weirder. When playing a different map, a seeker has noticed that it can use a ramp to climb on the top of a box, and, this happens. Do you think couchsurfing is cool? Give me a break! This is box surfing! And, the scientists were quite surprised by this move as this was one of the first cases where the seeker AI seems to have broken the game. What happens here is that the physics system is coded in a way that they are able to move around by exerting force on themselves, but, there is no additional check whether they are on the floor or not, because who in their right mind would think about that? As a result, something that shouldn’t ever happen does happen here. And, we’re still not done yet, this paper just keeps on giving. A few hundred million rounds later, the hiders learned to separate all the ramps from the boxes. Dear Fellow Scholars, this is proper box surfing defense…then, lock down the remaining tools and build a shelter. Note how well rehearsed and executed this strategy is there is not a second of time left until the seekers take off. I also love this cheeky move where they set up the shelter right next to the seekers, and I almost feel like they are saying “yeah see this here? there is not a single thing you can do about it”. In a few isolated cases, other interesting behaviors also emerged, for instance, the hiders learned to exploit the physics system and just chuck the ramp away. After that, the seekers go “what?” “what just happened?”. But don’t despair, and at this point, I would also recommend that you hold on to your papers because there was also a crazy case where a seeker also learned to abuse a similar physics issue and launch itself exactly onto the top of the hiders. Man, what a paper. This system can be extended and modded for many other tasks too, so expect to see more of these fun experiments in the future. We get to do this for a living, and we are even being paid for this. I can’t believe it. In this series, my mission is to showcase beautiful works that light a fire in people. And this is, no doubt, one of those works. Great idea, interesting, unexpected results, crisp presentation. Bravo OpenAI! Love it. So, did you enjoy this? What do you think? Make sure to leave a comment below. Also, if you look at the paper, it contains comparisons to an earlier work we covered about intrinsic motivation, shows how to implement circular convolutions for the agents to detect their environment around them, and more. Thanks for watching and for your generous support, and I'll see you next time!"
361,AIs Are Getting Too Smart - Time For A New IQ Test,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In a world where learning-based algorithms are rapidly becoming more capable, I increasingly find myself asking the question: “so, how smart are these algorithms, really?”. I am clearly not alone with this. To be able to answer this question, a set of tests were proposed, and many of these tests shared one important design decision: they are very difficult to solve for someone without generalized knowledge. In an earlier episode, we talked about DeepMind’s paper where they created a bunch of randomized mind-bending, or in the case of an AI, maybe silicon-bending questions that looked quite a bit like a nasty, nasty IQ test. And even in the presence of additional distractions, their AI did extremely well. I noted that on this test, finding the correct solution around 60% of the time would be quite respectable for a human, where their algorithm succeeded over 62% of the time, and upon removing the annoying distractions, this success rate skyrocketed to 78%. Wow. More specialized tests have also been developed. For instance, scientists at DeepMind also released a modular math test with over 2 million questions, in which their AI did extremely well at tasks like interpolation, rounding decimals, integers, whereas they were not too accurate at detecting primality and factorization. Furthermore, a little more than a year ago, the Glue benchmark appeared that was designed to test the natural language understanding capabilities of these AIs. When benchmarking the state of the art learning algorithms, they found that they were approximately 80% as good as the fellow non-expert human beings. That is remarkable. Given the difficulty of the test, they were likely not expecting human-level performance, which you see marked with the black horizontal line, which was surpassed within less than a year. So, what do we do in this case? Well, as always, of course, design an even harder test. In comes SuperGLUE, the paper we’re looking at today, which is meant to provide an even harder challenge for these learning algorithms. Have a look at these example questions here. For instance, this time around, reusing general background knowledge gets more emphasis in the questions. As a result, the AI has to be able to learn and reason with more finesse to successfully address these questions. Here you see a bunch of examples, and you can see that these are anything but trivial little tests for a baby AI not all, but some of these are calibrated for humans at around college-level education. So, let’s have a look at how the current state of the art AIs fared in this one! Well, not as good as humans, which is good news, because that was the main objective. However, they still did remarkably well. For instance, the BoolQ package contains a set of yes and no questions, in these, the AIs are reasonably close to human performance, but on MultiRC, the multi-sentence reading comprehension package, they still do OK, but humans outperform them by quite a bit. Note that you see two numbers for this test, the reason for this is that there are multiple test sets for this package. Note that in the second one, even humans seem to fail almost half the time, so I can only imagine the revelation we’ll have a couple more papers down the line. I am very excited to see that, and if you are too, make sure to subscribe and hit the bell icon to not miss future episodes. Thanks for watching and for your generous support, and I'll see you next time!"
362,Cubify All The Things!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I apologize for my voice today, I am trapped in this frail human body, and sometimes it falters. But, the papers must go on. This is one of those papers where I find that more time I spend with it, the more I realize how amazing it is. It starts out with an interesting little value proposition that in and of itself, would likely not make it to a paper. So, what is this paper about? Well, as you see here, this one is about cubification of 3D geometry. In other words, we take an input shape, and it stylizes it to look more like a cube. Okay, that’s cute, especially given that there are many-many ways to do this, and it’s hard to immediately put into words what a good end result would be, you can see a comparison to previous works here. These previous works did not seem to preserve a lot of fine details, but if you look at this new one, you see that this one does that really well. Very nice indeed. But still…when I read this paper, at this point, I was thinking…I’d like a little more. Well, I quickly found out that this work has more up its sleeve. So much more. Let’s talk about 7 of these amazing features. For instance, one, we can control the strength of the transformation with this lambda parameter, as you see, the more we increase it, the more heavy-handed the smushing process is going to get. Please remember this part. Two, we can also cubify selectively along different directions, or, select parts of the object that should be cubified differently. Hmm. Okay. Three, we can even use it to fix flaws in the input 3D geometry. Four, this transformation procedure also takes into consideration the orientations this means that we can perform it from different angles, which gives us a large selection of possible outputs for the same model. Five, it is fast and works on high-resolution geometry, and you see different settings for the lambda parameter here that is the same parameter as we have talked about before the strength of the transformation. Six, we can also combine many of these features interactively until a desirable shape is found. Seven is about to come in a moment, but to appreciate what that is, we have to look at …this. To perform what you have seen here so far, we have to minimize this expression. This first term says ARAP, as rigid as possible, which stipulates that whatever we do in terms of smushing, it should preserve the fine local features. The second part is called a regularization term that encourages sparser, more axis-aligned solutions so we don’t destroy the entire model during this process. The stronger this term is, the bigger say it has in the final results, which, in return, become more cube-like. So, how do we do that? Well, of course, with our trusty little lambda parameter. Not only that, but if we look at the appendix, it tells us that we can generalize this second regularization term for many different shapes. So here we are, finally, seven, it doesn’t even need to be cubification, we can specify all kinds of polyhedra. Look at those gorgeous results. I love this paper. It is playful, it is elegant, it has utility, and, it generalizes well. It doesn’t care in the slightest what the current mainstream ideas are and invites us into its own little world. In summary, this will serve all your cubification needs, and turns out, it might even fix your geometry, and more. I would love to see more papers like this. In this series, I try to make people feel how I feel when I read these papers. I hope I have managed this time, but you be the judge. Let me know in the comments. Thanks for watching and for your generous support, and I'll see you next time!"
363,Can an AI Learn The Concept of Pose And Appearance?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I apologize for my voice today, I am trapped in this frail human body, and sometimes it falters. But you remember from the previous episode, the papers must go on. In the last few years, we have seen a bunch of new AI-based techniques that were specialized in generating new and novel images. This is mainly done through learning-based techniques, typically a Generative Adversarial Network, a GAN in short, which is an architecture where a generator neural network creates new images, and passes it to a discriminator network, which learns to distinguish real photos from these fake, generated images. These two networks learn and improve together, so much so that many of these techniques have become so realistic that we sometimes can’t even tell they are synthetic images unless we look really closely. You see some examples here from BigGAN, a previous technique that is based on this architecture. Now, normally, if we are looking to generate a specific human face, we have to generate hundreds and hundreds of these images, and our best bet is to hope that sooner or later, we’ll find something that we were looking for. So, of course, scientists were interested in trying to exert control over the outputs, and with followup works, we can now kind of control the appearance, but, in return, we have to accept the pose in which they are given. And, this new project is about teaching a learning algorithm to separate pose from identity. Now, that sounds kind of possible with proper supervision. What does this mean exactly? Well, we have to train these GANs on a large number of images so they can learn what a human face looks like, what landmarks to expect and how to form them properly when generating new images. However, when the input images are given with different poses, we will normally need to add additional information to the discriminator that describes the rotations of these people and objects. Well, hold on to your papers, because that is exactly what is not happening in this new work. This paper proposes an architecture that contains a 3D transform and a projection unit, you see them here with red and blue, and, these help us in separating pose and identity. As a result, we have much finer artistic control over these during image generation. That is amazing. So as you see here, it enables a really nice workflow where we can also set up the poses. Don’t like the camera position for this generated bedroom? No problem. Need to rotate the chairs? No problem. And we are not even finished yet, because when we set up the pose correctly, we’re not stuck with these images we can also choose from several different appearances. And all this comes from the fact that this technique was able to learn the intricacies of these objects. Love it. Now, it is abundantly clear that as we rotate these cars, or change the camera viewpoint for the bedroom, a flickering effect is still present. And this, is how research works. We try to solve a new problem, one step at a time. Then, we find flaws in the solution, and improve upon that. As a result, we always say, two more papers down the line, and we’ll likely have smooth and creamy transitions between the images. The Lambda sponsorship spot is coming in a moment, and I don’t know if you have noticed at the start, but they were also part of this research project as well. I think that is as relevant of a sponsor as it gets. Thanks for watching and for  your generous support, and I'll see you next time!"
364,"Ken Burns Effect, Now In 3D!","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Have you heard of the Ken Burns effect? If you have been watching this channel, you have probably seen examples where a still image is shown, and a zooming and panning effect is added to it. It looks something like this. Familiar, right? The fact that there is some motion is indeed pleasing for the eye, but something is missing. Since we are doing this with 2D images, all the depth information is lost, so we are missing out on the motion parallax effect that we would see in real life when moving around a camera. So, this is only 2D. Can this be done in 3D? Well, to find out, have a look at this. Wow, I love it. Much better, right? Well, if we would try to perform something like this without this paper, we’ll be met with bad news. And that bad news is that we have to buy an RGBD camera. This kind of camera endows the 2D image with depth information, which is specialized hardware that is likely not available in our phones as of the making of this video. Now, since depth estimation from these simple, monocular 2D images without depth data is a research field of its own, the first step sounds simple enough: take one of those neural networks then, ask it to try to guess the depth of each pixel. Does this work? Well, let’s have a look! As we move our imaginary camera around, uh-oh. This is not looking good. Do you see what the problems are here? Problem number one is the presence geometric distortions, you see it if you look here. Problem number two is referred to as semantic distortion in the paper, or in other words, we now have missing data. Not only that, but this poor tiny human’s hand is also…ouch. Let’s look at something else instead. If we start zooming in into images, which is a hallmark of the Ken Burns effect, it gets even worse. Artifacts. So how does this new paper address these issues? After creating the first, coarse depth map, an additional step is taken to alleviate the semantic distortion issue, and then, this depth information is upsampled to make sure that we have enough fine details to perform the 3D Ken Burns effect. Let’s do that! Unfortunately, we are still nowhere near done yet. Previously occluded parts of the background suddenly become visible, and, we have no information about those. So, how can we address that? Do you remember image inpainting? I hope so, but if not, no matter, I’ll quickly explain what it is. Both learning-based, and traditional handcrafted algorithms exist to try to fill in this missing information in images with sensible data by looking at its surroundings. This is also not as trivial as it might seem first, for instance, just filling in sensible data is not enough, because this time around, we are synthesizing videos, it has to be temporally coherent, which means that there mustn’t be too much of a change from one frame to another, or else we’ll get a flickering effect. As a result, we finally have these results that are not only absolutely beautiful, but the user study in the paper shows that they stack up against handcrafted results made by real artists. How cool is that! It also opens up really cool workflows that would normally be very difficult, if not impossible to perform. For instance, here you see that we can freeze this lightning bolt in time, zoom around and marvel at the entire landscape. Love it. Of course, limitations still apply. If we have really thin objects, such as this flagpole, it might be missing entirely from the depth map, or there are also cases where the image inpainter cannot fill in useful information. I cannot wait to see how this work evolves a couple papers down the line. One more interesting tidbit. If you have a look at the paper, make sure to open it in Adobe Reader you will likely be very surprised to see that many of these things that you think are still images…are actually animations. Papers are not only getting more mind-blowing by the day, but also more informative, and beautiful as well. What a time to be alive! This video has been supported by you on Patreon. If you wish to support the series and also pick up cool perks in return, like early access to these episodes, or getting your name immortalized in the video description, make sure to visit us through Patreon.com/TwoMinutePapers. Thanks for watching and for your generous support, and I'll see you next time!"
365,Google's AI Clones Your Voice After Listening for 5 Seconds!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we are going to listen to some amazing improvements in the area of AI-based voice cloning. For instance, if someone wanted to clone my voice, there are hours and hours of my voice recordings on Youtube and elsewhere, they could do it with previously existing techniques. But the question today is, if we had even more advanced methods to do this, how big of a sound sample would we really need for this? Do we need a few hours? A few minutes? The answer is no. Not at all. Hold on to your papers because this new technique only requires 5 seconds. Let’s listen to a couple examples. Absolutely incredible. The timbre of the voice is very similar, and it is able to synthesize sounds and consonants that have to be inferred because they were not heard in the original voice sample. This requires a certain kind of intelligence and quite a bit of that. So, while we are at that, how does this new system work? Well, it requires three components. One, the speaker encoder is a neural network that was trained on thousands and thousands of speakers and is meant to squeeze all this learned data into a compressed representation. In other words, it tries to learn the essence of human speech from many many speakers. To clarify, I will add that this system listens to thousands of people talking to learn the intricacies of human speech, but this training step needs to be done only once, and after that, it was allowed just 5 seconds of speech data from someone they haven’t heard of previously, and later, the synthesis takes place using this 5 seconds as an input. Two, we have a synthesizer that takes text as an input, this is what we would like our test subject to say, and it gives us a Mel Spectrogram, which is a concise representation of someone’s voice and intonation. The implementation of this module is based on DeepMind’s Tacotron 2 technique, and here you can see an example of this Mel spectrogram built for a male and two female speakers. On the left, we have the spectrograms of the reference recordings, the voice samples if you will, and on the right, we specify a piece of text that we would like the learning algorithm to utter, and it produces these corresponding synthesized spectrograms. But, eventually, we would like to listen to something, and for that, we need a waveform as an output. So, the third element is thus a neural vocoder that does exactly that, and this component is implemented by DeepMind’s WaveNet technique. This is the architecture that led to these amazing examples. So how do we measure exactly how amazing it is? When we have a solution, evaluating it is also anything but trivial. In principle, we are looking for a result that is both close to the recording that we have of the target person, but says something completely different, and all this in a natural manner. This naturalness and similarity can be measured, but we’re not nearly done yet, because the problem gets even more difficult. For instance, it matters how we fit the three puzzle pieces together, and then, what data we train on, of course, also matters a great deal. Here you see that if we train on one dataset and test the results against a different one, and then, swap the two, and…the results in naturalness and similarity will differ significantly. The paper contains a very detailed evaluation section that explains how to deal with these difficulties. The mean opinion score is measured in this section, which is a number that describes how well a sound sample would pass as genuine human speech. And we haven’t even talked about the speaker verification part, so make sure to have a look at the paper. So, indeed, we can clone each other’s voice by using a sample of only 5 seconds. What a time to be alive! This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. They also wrote a guide on the fundamentals of neural networks where they explain in simple terms how to train a neural network properly, what are the most common errors you can make, and how to fix them. It is really great, you got to have a look. So make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
366,This AI Learned To Animate Humanoids!🚶,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If we have an animation movie or a computer game with quadrupeds, and we are yearning for really high-quality, lifelike animations, motion capture is often the go-to tool for the job. Motion capture means that we put an actor, in our case, a dog in the studio, we ask it to perform sitting, trotting, pacing and jumping, record its motion, and transfer it onto our virtual character. In an earlier work, a learning-based technique was introduced by the name Mode-Adaptive Neural Network, and it was able to correctly weave together these previously recorded motions, and not only that, but it also addressed these unnatural sliding motions that were produced by previous works. As you see here, it also worked well on more challenging landscapes. We talked about this paper approximately a hundred videos, or in other words, a little more than a year ago, and I noted that it was scientifically interesting, it was evaluated well, it had all the ingredients for a truly excellent paper. But one thing was missing. So what is that one thing? Well, we haven’t seen the characters interacting with the scene itself. If you liked this previous paper, you are going to be elated by this one because this new work is from the very same group, and goes by the name Neural State Machine, and introduces character-scene interactions for bipeds. Now, we suddenly jumped from a quadruped paper to a biped one, and the reason for this is because I was looking to introduce the concept of foot sliding, which will be measured later for this new method too. Stay tuned! So, in this new problem formulation, we need to guide the character to a challenging end state, for instance, sitting in a chair, while being able to maneuver through all kinds of geometry. We’ll use the chair example a fair bit in the next minute or two, so I’ll stress that this can do a whole lot more, the chair is just used as a vehicle to get a taste of how this technique works. But the end state needn’t just be some kind of chair. It can be any chair! This chair may have all kinds of different heights and shapes, and the agent has to be able to change the animations and stitch them together correctly regardless of the geometry. To achieve this, the authors propose an interesting new data augmentation model. Since we are working with neural networks, we already have a training set to teach it about motion, and data augmentation means that we extend this dataset with lots and lots of new information to make the AI generalize better to unseen, real-world examples. So, how is this done here exactly? Well, the authors proposed a clever idea to do this. Let’s walk through their five prescribed steps. One, let’s use motion capture data, have the subject sit down and see what the contact points are when it happens. Two, we then record the curves that describe the entirety of the motion of sitting down. So far so good, but we are not interested in one kind of chair, we want it to sit into all kinds of chairs, so three, generate a large selection of different geometries and adjust the location of these contact points accordingly. Four, change the motion curves so they indeed end at the new, transformed contact points. And five, move the joints of the character to make it follow this motion curve and compute the evolution of the character pose. We then pair up this motion with the chair geometry and chuck it into the new, augmented training set. Now, make no mistake, the paper contains much, much more than this, so make sure to have a look in the video description. So what do we get for all this work? Well, have a look at this trembly character from a previous paper, and look at the new synthesized motions. Natural, smooth, creamy, and I don’t see artifacts. Also, here you see some results that measure the amount of foot sliding during these animations, which is subject to minimization. That means that the smaller the bars are, the better. With NSM, you see how this Neural State Machine method produces much less than previous methods, and now we see how cool it is that we talked about the quadruped paper as well, because we see that it even beats the MANN, the mode-adaptive neural networks from the previous paper. That one had very little foot sliding, and apparently, it can still be improved by quite a bit. The positional and rotational errors in the animation it offers are also by far the lowest of the bunch. Since it works in real time, it can also be used for computer games and virtual reality applications. And all this improvement within a year of work. What a time to be alive! If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Thanks for watching and for your generous support, and I'll see you next time!"
367,OpenAI’s Robot Hand Won't Stop Rotating The Rubik’s Cube,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, we’re going to talk about OpenAI’s robot hand that dexterously manipulates and solves a Rubik’s cube. Here you can marvel at this majestic result. Now, why did I use the term dexterously manipulate a Rubik’s cube? In this project, there are two problems to solve. One, finding out what kind of rotation we need to get closer to a solved cube, and adjusting the finger positions to be able to execute these prescribed rotations. And this paper is about the latter, which means the rotation sequences are given by a previously existing algorithm, and OpenAI’s method manipulates the hand to be able to follow this algorithm. To rephrase it, the robot hand doesn’t really know how to solve the cube and is told what to do, and the contribution lies in the robot figuring out how to execute these rotations. If you take only one thing from this video, let it be this thought. Now, to perform all this, we have to first solve a problem in a computer simulation where we can learn and iterate quickly, and then, transfer everything the agent learned there to the real world, and hope that it obtained general knowledge that indeed can be applied there. This task is one of my favorites. However, no simulation is as detailed as the real world, and as every experienced student knows very well, things that are written in the textbook might not always work exactly the same in practice. So the problem formulation naturally emerges our job is to prepare this AI in this simulation so it becomes good enough to perform well in the real world. Well, good news, first, let’s think about the fact that in a simulation, we can train much faster as we are not bound by the physical limits of the robot hand in a simulation, we are bound by our processing power, which is much, much more vast and is growing every day. So, this means that the simulated environments can be as grueling as we can make them be, what’s even more, we can do something that OpenAI refers to as Automatic Domain Randomization. This is one of the key contributions of this paper. The domain randomization part means that it creates a large number of random environments, each of which are a little different, and the AI is meant to learn how to account for these differences and hopefully, as a result, obtain general knowledge about our world. The automatic part is responsible for detecting how much randomization the neural network can shoulder, and hence, the difficulty of these random environments is increased over time. So, how good are the results? Well, spectacular. In fact, hold on to your papers, because it can not only dexterously manipulate and solve the cube, but we can even hamstring the hand in many different ways and it will still be able to do well. And I am telling you, scientists at OpenAI got very creative in tormenting this little hand. They added a rubber glove, tied multiple fingers together, threw a blanket on it, and pushed it around with a plush giraffe and a pen. It still worked. This is a testament to the usefulness of the mentioned automatic domain randomization technique. What’s more, if you have a look at the paper, you will even see how well it was able to recover from a randomly breaking joint. What a time to be alive! As always, some limitations apply. The hand is only able to solve the cube about 60% of the time for simpler cases, and the success rate drops to 20% for the most difficult ones. If it gets stuck, it typically does in the first few rotations. But so far, we have been able to do this 0% of the time, and given that the first steps towards cracking the problem are almost always the hardest, I have no doubt that two more papers down the line, this will become significantly more reliable. But you know what, we are talking about OpenAI, make it one paper. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Here you see a write-up of theirs where they explain how to visualize the gradients running through your models, and illustrate it through the example of predicting protein structure. They also have a live example that you can try! Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
368,This AI Captures Your Hair Geometry...From Just One Photo!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we talk about research on all kinds of physics simulations, including fluids, collision physics, and we have even ventured into hair simulations. If you look here at this beautiful footage, you may be surprised to know how many moving parts a researcher has to get right to get something like this. For instance, some of these simulations have to go down to the level of computing the physics between individual hair strands. If it is done well, like what you see here from our earlier episode, these simulations will properly show us how things should move, but that’s not all there is also an abundance of research works out there on how they should look. And even then, we’re not done, because before that, we have to take a step back and somehow create these digital 3D models that show us the geometry of these flamboyant hairstyles. Approximately 300 episodes ago, we talked about a technique that took a photograph as an input, and created a digital 3D model that we can use in our simulations and rendering systems. It had a really cool idea where it initially predicted a coarse result, and then, this result was matched with the hairstyles found in public data repositories, and the closest match was presented to us. Clearly, this often meant that we got something that was similar to the photograph, but often not exactly the hairstyle we were seeking. And now, hold on to your papers because this work introduces a learning-based framework that can create a full reconstruction by itself, without external help, and now, squeeze that paper, because it works not only only for images, but for videos too! Woohoo! It works for shorter hairstyles, long hair, and even takes into consideration motion and external forces as well. The heart of the architecture behind this technique is this pair of neural networks, where the one above creates the predicted hair geometry for each frame, while the other tries to look backwards in the data and try to predict the appropriate motions that should be present. Interestingly, it only needs two consecutive frames to make these predictions, and adding more information does not seem to improve its results. That is very little data. Quite remarkable. Also, note that there are a lot of moving parts here in the full paper, for instance, this motion is first predicted in 2D, and is then extrapolated to 3D afterwards. Let’s have a look at this comparison indeed, it seems to produce smoother and more appealing results than this older technique. But if we look here, this other method seems even better, so what about that? Well, this method had access to multiple views of the model, which is significantly more information than what this new technique has that only needs a simple monocular 2D video from our phone, or from the internet. The fact that they are even comparable is absolutely amazing. If you have a look at the paper, you will see that it even contains a hair growing component in this architecture. And as you see, the progress in computer graphics research is also absolutely amazing. And we are even being paid  for doing this. Unreal. Thanks for watching and for your generous support, and I'll see you next time!"
369,This AI Makes The Mona Lisa Speak…And More!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In an earlier episode, we covered a paper by the name Everybody Dance Now. In this stunning work, we could take a video of a professional dancer, then record a video of our own, let’s be diplomatic less beautiful moves, and then, transfer the dancer's performance onto our own body in the video. We called this process motion transfer. Now, look at this new, also learning-based technique, that does something similar…where in goes a description of a pose, just one image of the target person, and on the other side, out comes a proper animation of this character according to our prescribed motions. Now, before you think that it means that we would need to draw and animate stick figures to use this, I will stress that this is not the case. There are many techniques that perform pose estimation, where we just insert a photo, or even a video, and it creates all these stick figures for us that represent the pose that people are taking in these videos. This means that we can even have a video of someone dancing, and just one image of the target person, and the rest is history. Insanity. That is already amazing and very convenient, but this paper works with a video to video problem formulation, which is a concept that is more general than just generating movement. Way more. For instance, we can also specify the input video of us, then add one, or at most a few images of the target subject, and we can make them speak and behave using our gestures. This is already absolutely amazing, however, the more creative minds out there are already thinking that if we are thinking about images, it can be a painting as well, right? Yes, indeed, we can make the Mona Lisa speak with it as well. It can also take a labeled image, this is what you see here, where the colored and animated patches show the object boundaries for different object classes, then, we take an input photo of a street scene, and we get photorealistic footage with all the cars, buildings, and vegetation. Now, make no mistake, some of these applications were possible before, many of which we showcased in previous videos, some of which you can see here, what is new and interesting here is that we have just one architecture here that can handle many of these tasks. Beyond that, this architecture requires much less data than previous techniques as it often needs just one or at most a few images of the target subject to do all this magic. The paper is ample in comparisons to these other methods, for instance, the FID measures the quality and the diversity of the generated output images, and is subject to minimization, and you see that it is miles beyond these previous works. Some limitations also apply, if the inputs stray too far away from topics that the neural networks were trained on, we shouldn’t expect results of this quality, and we are also dependent on proper inputs for the poses and segmentation maps for it to work well. The pace of progress in machine learning research is absolutely incredible, and we are getting very close to producing tools that can be actively used to empower artists working in the industry. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
370,DeepMind’s AlphaStar: A Grandmaster Level StarCraft 2 AI!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The paper that we are going to cover today in my view, is one of the more important things that happened in AI research lately. In the last few years, we have seen DeepMind’s AI defeat the best Go players in the world, and after OpenAI’s venture in the game of DOTA2, DeepMind embarked on a journey to defeat pro players in Starcraft 2, a real-time strategy game. This is a game that requires a great deal of mechanical skill, split-second decision making and we have imperfect information as we only see what our units can see. A nightmare situation for any AI. The previous version of AlphaStar we covered in this series was able to beat at least mid-grandmaster level players, which is truly remarkable, but, as with every project of this complexity, there were limitations and caveats. In our earlier video, the paper was still pending, and now, it has finally appeared, so my sleepless nights have officially ended, at least for this work, and now, we can look into some more results. One of the limitations of the earlier version was that DeepMind needed to further tune some of the parameters and rules to make sure that the AI and the players play on an even footing. For instance, the camera movement and the number of actions the AI can make per minute has been limited some more and are now more human-like. TLO, a professional StarCraft 2 player noted that this time around, it indeed felt very much like playing another human player. The second limitation was that the AI was only able to play Protoss, which is one of the three races available in the game. This new version can now play all three races, and here you see its MMR ratings, a number that describes the skill level of the AI, and for non-experts, win percentages for each individual race. As you see, it is still the best with Protoss, however, all three races are well over the 99% winrate mark. Absolutely amazing. In this version, there is also more emphasis on self-play, and the goal is to create a learning algorithm that is able to learn how to play really well by playing against previous versions of itself millions and millions of times. This is, again, one of those curious cases where the agents train against themselves in a simulated world, and then, when the final AI was deployed on the official game servers, it played against human players for the very first time. I promise to tell you about the results in a moment, but for now, please note that relying more on self-play is extremely difficult. Let me explain why. Self-play agents have the well-known drawback of forgetting, which means that as they improve, they might forget how to win against a previous version of themselves. Since StarCraft 2 is designed in a way that every unit and strategy has an antidote, we have a rock-paper-scissors kind of situation where the agent plays rock all the time because it encountered a lot of scissors lately. Then, when a lot of papers appear, it will start playing scissors more often, and completely forget about the olden times when the rock was all the rage. And, on and on this circle goes without any real learning or progress. This doesn’t just lead to suboptimal results this leads to disastrously bad learning, if any learning at all. But it gets even worse. This situation opens up the possibility for an exploiter to take advantage of this information and easily beat these agents. In concrete StarCraft terms, such an exploit could be trying to defeat the AlphaStar AI early by rushing it with workers and warping in photon cannons to their base. This strategy is also known as a cannon rush, and as you can see here the red agent performing this, it can quickly defeat the unsuspecting blue opponent. So, how do we defend against such exploits? DeepMind used a clever idea here, by trying to turn the whole thing around and use these exploits to its advantage. How? Well, they proposed a novel self-play method where they additionally insert these exploiter AIs to expose the main AI’s flaws and create an overall, more knowledgeable and robust agent. So, how did it go? Well, as a result, you can see how the green agent has learned to adapt to this by pulling its worker line and successfully defended the cannon rush of the red AI. This is proper machine learning progress happening right before our eyes. Glorious! This is just one example of using exploiters to create a better main AI, but the training process continually creates newer and newer kinds of exploiters, for instance, you will see in a moment that it later came up with a nasty strategy including attacking the main base with cloaking units. One of the coolest parts of the work, in my opinion, is that this kind of exploitation is a general concept that will surely come useful for completely different test domains as well. We noted earlier that it finally started playing humans for the first time on the official servers. So, how did that go? In my opinion, given the difficulty and the vast search space we have in StarCraft 2, creating a self-learning AI that has the skills of an amateur player is already incredible. But that’s not what happened. Hold on to your papers, because it quickly reached grandmaster level with all three races and ranked above 99.8% of the officially ranked human players. Bravo, DeepMind. Stunning work. Later, it also played Serral, a decorated, world champion Zerg player, one of the most dominant players of our time. I will not spoil the results, especially given there were limitations as Serral wasn’t playing on his equipment, but I will note that Artosis, a well-known and beloved Starcraft player and commentator analyzed these matches and said “The results are so impressive and I really feel like we can learn a lot from it. I would be surprised if a non-human entity could get this good and there was nothing to learn”. His commentary is excellent and is tailored towards people who don’t know anything about the game. He’ll often pause the game and slowly explain what is going on. In these matches, I loved the fact that so many times it makes so many plays that we consider to be very poor and somehow, overall, it still plays outrageously well. It has unit compositions that nobody in their right minds would play. It is kind of like a drunken kung fu master, but in StarCraft 2. Love it. But no more spoilers I think you should really watch these matches and, of course, I put a link to his analysis videos in the video description. Even though both this video and the paper appears to be laser focused on playing StarCraft 2, it is of utmost importance to note that this is still just a testbed to demonstrate the learning capabilities of this AI. As amazing as it sounds, DeepMind wasn’t just looking to spend millions and millions of dollars on research to just play video games. The building blocks of AlphaStar are meant to be reasonably general, which means that parts of this AI can be reused for other things, for instance, Demis Hassabis mentioned weather prediction and climate modeling as examples. If you take only one thought from this video, let it be this one. There is really so much to talk about, so make sure to head over to the video description, watch the matches and check out the paper as well. The evaluation section is as detailed as it can possibly get. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
371,This AI Creates A Moving Digital Avatar Of You,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we talk about research on all kinds of physics simulations, including fluids, collision physics, and we have even ventured into hair simulations. We mostly talk about how the individual hair strands should move, and how they should look, in terms of color and reflectance. Creating these beautiful videos takes getting many-many moving parts right, for instance, before all of that, the very first step is not any of those steps. First, we have to get the 3D geometry of these hairstyles into our simulation system. In a previous episode, we have seen an excellent work that does this well for human hair. But what if we would like to model not human hair, but something completely different? Well, hold on to your papers, because this new work is so general, that it can look at an input image or video, and give us not only a model of the human hair, but human skin, garments, and of course, my favorite, smoke plumes, and more. But if you look here, this part begs the following question the input is an image, and the output also looks like an image, and we need to make them similar so what’s the big deal here? A copying machine can do that, no? Well, not really. Here’s why. On the output, we are working with something that indeed looks like an image, but it is not an image. It is a 3 dimensional cube, in which we have to specify color and opacity values everywhere. After that, we simulate rays of light passing through this volume, which is a technique that we call ray marching, and this process has to produce the same 2D image through ray marching as what was given as an input. That’s much, much harder than building a copying machine. As you see here, normally, this does not work well at all, because, for instance, a standard algorithm sees lights in the background, and it assumes that these are really bright and dense points. That is kind of true, but they are usually not even part of the data we would like to reconstruct. To solve this issue, the authors propose learning to tell the foreground and background images apart, so they can be separated before we start the reconstruction of the human. And this is a good research paper, which means that if it contains multiple new techniques, each of them are tested separately to know how much they contribute to the final results. We get the previously seen, dreadful results without the background separation step, here are the results with the learned backgrounds, we can still see the lights due to the way the final image is constructed, and the fact that we have so little of this halo effect is really cool. Here you see the results with the true background data where the background learning step is not present. Note that this is cheating, because this data is not available for all cameras and backgrounds, however, it is a great way to test the quality of this learning step. The comparison of the learned method against this reveals that the two are very close, which is exactly what we are looking for. And finally, the input footage is also shown for reference. This is ultimately what we are trying to achieve, and as you see, the output is quite close to it. As you see here, the final algorithm excels at reconstructing volume data for toys, smoke plumes, and humans alike. And the coolest part is that it works for not only stationary inputs, but for animations as well. Wait, actually, there is something that is perhaps even cooler, with the magic of neural networks and latent spaces, we can even animate this data. Here you see an example of that where an avatar is animated in real-time by moving around this magenta dot. A limiting factor here is the resolution of this reconstruction if you look closely, you see that some fine details are missing, but you know the saying…given the rate of progress in machine learning research, two more papers down the line, and this will likely be orders of magnitude better. If you feel that you always need to take your daily dose of papers, my statistics show that many of you are subscribed, but didn’t use the bell icon. If you click this bell icon, you will never miss a future episode and can properly engage in your paper addiction. This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
372,OpenAI Safety Gym: A Safe Place For AIs To Learn,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Reinforcement learning is a technique in the field of machine learning to learn how to navigate in a labyrinth, play a video game, or to teach a digital creature to walk. Usually, we are interested in a series of actions that are in some sense, optimal in a given environment. Despite the fact that many enormous tomes exist to discuss the mathematical details, the intuition behind the algorithm itself is remarkably simple. Choose an action, and if you get rewarded for it, try to find out which series of actions led to this and keep doing it. If the rewards are not coming, try something else. The reward can be, for instance, our score in a computer game or how far our digital creature could walk. Approximately a 300 episodes ago, OpenAI published one of their first major works by the name Gym, where anyone could submit their solutions and compete against each other on the same games. It was like Disneyworld for reinforcement learning researchers. A moment ago, I noted that in reinforcement learning, if the rewards are not coming we have to try something else. Hmm..is that so? Because there are cases where trying crazy new actions is downright dangerous. For instance, imagine that during the training of this robot arm, initially, it would try random actions and start flailing about, where it may damage itself, some other equipment, or even worse, humans may come to harm. Here you see an amusing example of DeepMind’s reinforcement learning agent from 2017 that liked to engage in similar flailing activities. So, what could be a possible solution for this? Well, have a look at this new work from OpenAI by the name Safety Gym. In this paper, they introduce what they call the constrained reinforcement learning formulation, in which these agents can be discouraged from performing actions that are deemed potentially dangerous in an environment. You can see an example here where the AI has to navigate through these environments and achieve a task, such as reaching the green goal signs, push buttons, or move a box around to a prescribed position. The constrained part comes in whenever some sort of safety violation happens, which are, in this environment, collisions with the boxes or blue regions. All of these events are highlighted with this red sphere and a good learning algorithm should be instructed to try to avoid these. The goal of this project is that in the future, for reinforcement learning algorithms, not only the efficiency, but the safety scores should also be measured. This way, a self-driving AI would be incentivized to not only drive recklessly to the finish line, but respect our safety standards along the journey as well. While noting that clearly, self-driving cars may be achieved with other kinds of algorithms, many of which have been in the works for years, there are many additional applications for this work: for instance, the paper discusses the case of incentivizing recommender systems to not show psychologically harmful content to its users, or to make sure that a medical question answering system does not mislead us with false information. This episode has been supported by Linode. Linode is the world’s largest independent cloud computing provider. They offer you virtual servers that make it easy and affordable to host your own app, site, project, or anything else in the cloud. Whether you’re a Linux expert or just starting to tinker with your own code, Linode will be useful for you. A few episodes ago, we played with an implementation of OpenAI’s GPT-2 where our excited viewers accidentally overloaded the system. With Linode's load balancing technology, and instances ranging from shared nanodes all the way up to dedicated GPUs you don't have to worry about your project being overloaded. To get 20 dollars of free credit, make sure to head over to linode.com/papers and sign up today using the promo code “papers20”. Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
373,These Natural Images Fool Neural Networks (And Maybe You Too),"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In the last few years, neural network-based learning algorithms became so good at image recognition tasks that they can often rival, and sometimes even outperform humans in these endeavors. Beyond making these neural networks even more accurate in these tasks, interestingly, there is also plenty of research work on how to attack and mislead these neural networks. I think this area of research is extremely exciting and I’ll now try to show you why. One of the first examples of an adversarial attack can be performed as follows. We present such a classifier with an image of a bus, and it will successfully tell us that yes, this is indeed a bus. Nothing too crazy here. Now, we show it not an image of a bus, but a bus plus some carefully crafted noise that is barely perceptible, that forces the neural network to misclassify it as an ostrich. I will stress that this is not any kind of noise, but the kind of noise that exploits biases in the neural network, which is, by no means easy or trivial to craft. However, if we succeed at that, this kind of adversarial attack can be pulled off on many different kinds of images. Everything that you see here on the right will be classified as an ostrich by the neural network these noise patterns were crafted for. In a later work, researchers of the Google Brain team found that we can not only coerce the neural network into making some mistake, but we can even force it to make exactly the kind of mistake we want! This example here reprograms an image classifier to count the number of squares in our images. However, interestingly, some adversarial attacks do not need carefully crafted noise, or any tricks for that matter. Did you know that many of them occur naturally in nature! This new work contains a brutally hard dataset with such images that throw off even the best neural image recognition systems. Let’s have a look at an example. If I were the neural network, I would look at this squirrel and claim that “with high confidence, I can tell you that this is a sea lion”. And you human, may think that this is a dragonfly, but you would be wrong. I am pretty sure that this is a manhole cover! Except that it’s not. The paper shows many of these examples, some of which don’t really occur in my brain. For instance, I don’t see this mushroom as a pretzel at all, but there was something about that dragonfly that, upon a cursory look, may get registered as a manhole cover. If you look quickly, you see a squirrel here…just kidding, it’s a bullfrog. I feel that if I look at some of these with a fresh eye, sometimes I get a similar impression as the neural network. I’ll put up a bunch of more examples for you here, let me know in the comments which are the ones that got you. Very cool project, I love it. What’s even better, this dataset by the name ImageNet-A is now available for everyone, free of charge. And if you remember, at the start of the video, I said that it is brutally hard for neural networks to identify what is going on here. So what kind of success rates can we expect? 70%? Maybe 50%? Nope. 2%. Wow. In a world where some of these learning-based image classifiers are better than us at some datasets, they are vastly outclassed by us humans on these natural adversarial examples. If you have a look at the paper, you will see that the currently known techniques to improve the robustness of training show little to no improvement to this. I cannot wait to see some followup papers on how to crack this nut. We can learn so much from this paper, and will likely learn even more from these followup works. Make sure to subscribe and also hit the bell icon to never miss future episodes. What a time to be alive! This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. In this post, they show you how to train a state of the art machine learning model with over 99% accuracy on classifying squiggly handwritten numbers and how to use their tools to get a crystal clear understanding of what your model exactly does and what part of the letters it is looking at. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
374,This Robot Arm Learned To Assemble Objects It Hasn’t Seen Before,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Have a look and marvel at this learning-based assembler robot that is able to put together simple contraptions. Since this is a neural-network based learning method, it needs to be trained to be able to do this. So, how is it trained? Normally, to train such an algorithm, we would have to show it a lot of pairs of the same contraption, and tell is that this is what it looks like when it’s disassembled, and what you see here is the same thing, assembled. If we did this, this method would be called supervised learning. This would be very time consuming, and potentially expensive as it would require the presence of a human as well. A more convenient way would be to go for unsupervised learning, where we just chuck a lot of things on the table and say, “well, robot, you figure it out”. However, this would be very inefficient, if at all possible because we would have to provide it many-many contraptions that wouldn’t fit on the table. But this paper went for none of these solutions, as they opted for a really smart self-supervised technique. So what does that mean? Well, first, we give the robot an assembled contraption, and ask it to disassemble it. And therein lies the really cool idea, because disassembling it is easier, and by rewinding the process, it also gets to know how to assemble it later. And, the training process takes place by assembling, disassembling, and doing it over and over again, several hundred times per object. Isn’t this amazing? Love it. However, what is the point of all this? Instead of all this, we could just add explicit instructions to a non-learning based robot to assemble the objects. Why not just do that? And the answer lies in one of the most important aspects within machine learning generalization. If we program a robot to be able to assemble one thing, it will be able to do exactly that assemble one thing. And whenever we have a new contraption on our hands, we’ll need to reprogram it. However, with this technique, after the learning process took place, we will be able to give it a new, previously unseen object and it will have a chance to assemble it. This requires intelligence to perform. So, how good is it at generalization? Well, get this, the paper reports that when showing it new objects, it was able to successfully assemble them 86% of the time. Incredible. So what about the limitations? This technique works on a 2D planar surface, for instance, this table, and while it is able to insert most of these parts vertically it does not deal well with more complex assemblies that require inserting screws and pegs in a 45 degree angle. As we always say, two more papers down the line, and this will likely be improved significantly. I you have ever bought a bed or a cupboard and said, well, it just looks like a block, how hard can it be to assemble? Wait, does this thing have more than a 100 screws and pegs? I wonder why? And then, 4.5 hours later, you find out yourself. I hope techniques like these will help us save time by enabling us to buy many of these contraptions pre-assembled, and it can be used for much, much more. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
375,MuZero: DeepMind’s New AI Mastered More Than 50 Games,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Some papers come with an intense media campaign and a lot of nice videos, and some other amazing papers are at the risk of slipping under the radar because of the lack of such a media presence. This new work from DeepMind is indeed absolutely amazing, you’ll see in a moment why, and is not really talked about. So in this video, let’s try to reward such a work. In many episodes, you get ice cream for your eyes, but today, you get ice cream for your mind. Buckle up. In the last few years, we have seen DeepMind’s AI defeat the best Go players in the world, and after OpenAI’s venture in the game of DOTA2, DeepMind embarked on a journey to defeat pro players in Starcraft 2, a real-time strategy game. This is a game that requires a great deal of mechanical skill, split-second decision making and we have imperfect information as we only see what our units can see. A nightmare situation for any AI. You see some footage of its previous games here on the screen. And, in my opinion, people seem to pay too much attention to how good a given algorithm performs, and too little to how general it is. Let me explain. DeepMind has developed a new technique that tries to rely more on its predictions of the future, and generalizes to many many more games than previous techniques. This includes AlphaZero, a previous technique also from them that was able to play Go, Chess, and Japanese Chess or Shogi as well and beat any human player at these games confidently. This new method is so general, that it does as well as AlphaZero at these games, however, it can also play a wide variety of Atari games as well. And that is the key here: writing an algorithm that plays chess well has been a possibility for decades. For instance, if you wish to know more, make sure to check out Stockfish, which is an incredible open-source project and a very potent algorithm. However, Stockfish cannot play anything else whenever we look at a new game, we have to derive a new algorithm that solves it. Not so much with these learning methods, that can generalize to a wide variety of games! This is why I would like to argue that the generalization capability of these AIs is just as important as their performance. In other words, if there were a narrow algorithm that is the best possible Chess algorithm that ever existed, or a somewhat below world-champion level AI that can play any game we can possibly imagine, I would take the latter in a heartbeat. Now, speaking about generalization, let’s see how well it does at these Atari games, shall we? After 30 minutes of time on each game, it significantly outperforms humans on nearly all of these games, the percentages show you here what kind of outperformance we are talking about. In many cases, the algorithm outperforms us several times, and up to several hundred times. Absolutely incredible. As you see, it has a more than formidable score on almost all of these games, and therefore it generalizes quite well. I’ll tell you in a moment about the games it falters at, but for now, let’s compare it to three other competing algorithms. You see one bold number per row, which always highlights the best performing algorithm for your convenience. The new technique beats the others on about 66% of the games, including the Recurrent Experience Replay technique, in short, R2D2. Yes, this is another one of those crazy paper names. And even when it falls short, it is typically very close. As a reference, humans triumphed on less than 10% of the games. We still have a big fat zero on Pitfall and Montezuma’s Revenge games. So why is that? Well, these games require long-term planning, which is one of the more difficult cases for reinforcement learning algorithms. In an earlier episode, we discussed how we can infuse an AI agent with a curiosity to go out there and explore some more with success. However, note that these algorithms are more narrow than the one we’ve been talking about today. So there is still plenty of work to be done, but I hope you see that this is incredibly nimble progress on AI research. Bravo DeepMind! What a time to be alive! This episode has been supported by Linode. Linode is the world’s largest independent cloud computing provider. They offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. Exactly the kind of works you see here in this series. If you feel inspired by these works and you wish to run your experiments or deploy your already existing works through a simple and reliable hosting service, make sure to join over 800,000 other happy customers and choose Linode. To spin up your own GPU instance and receive a $20 free credit, visit linode.com/papers or click the link in the description and use the promo code “papers20” during signup. Give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
376,Baking And Melting Chocolate Simulations Are Now Possible!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is one of those simulation papers where you can look at it for three seconds and immediately know what it’s about. Let’s try that! Clearly, expansion and baking is happening. And now, let’s look inside. Mmmm! Yup, this is done. Clearly, this is a paper on simulating the process of baking! Loving the idea. So how comprehensive is it? Well, other than these phenomena, for a proper baking procedure, the simulator also has to be able to deal with melting, solidification, dehydration, coloring and much, much more. This requires developing a proper thermomechanical model where these materials are modeled as a collection of solids, water, and gas. Let’s have a look at some more results. And we have to stop right here, because I’d like to tell you that the information density on this deceivingly simple scene is just stunning. In the x axis, from the left to the right we have a decreasing temperature in the oven, left being the hottest, and chocolate chip cookies above are simulated with an earlier work from 2014. The ones in the bottom row are made with the new technique. You can see a different kind of shape change as we increase the temperature, discoloration if we crank the oven up even more, and…look there! Even the chocolate chips are melting. Oh my goodness! What a paper! Talking about information density, you can also see here how these simulated pieces of dough of different viscosities react to different amounts of stress. Viscosity means the amount of resistance against deformation, therefore, as we go up, you can witness this kind of resistance increasing. Here you can see a cross section of the bread which shows the amount of heat everywhere. This not only teaches us why crust forms on the outside layer, but you can see how the amount of heat diffuses slowly into the inside. This is a maxed out paper. By this, I mean the execution quality is through the roof, and the paper is considered done not when it looks alright, but when the idea is being pushed to the limit and the work is as good as it can be without trivial ways to improve it. And the results are absolute witchcraft. Huge congratulations to the authors. In fact, double congratulations because it seems to me that this is only the second paper of Mengyuan Ding, the lead author, and it has been accepted to the SIGGRAPH ASIA conference, which is one of the greatest achivements a computer graphics researcher can dream of. A paper of such quality for the second try. Wow. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. They have excellent tutorial videos, in this one, the CEO himself teaches you how to build your own neural network, and more. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
377,This Neural Network Performs Foveated Rendering,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As humans, when looking at the world, our eyes and brain does not process the entirety of the image we have in front of us, but plays an interesting trick on us. We can only see fine details in a tiny, tiny foveated region that we are gazing at, while our peripheral or indirect vision only sees a sparse, blurry version of the image, and the rest of the information is filled in by our brain. This is a very efficient system, because our vision system only has to process a tiny fraction of the visual data that is in front of us, and it still enables us to interact with the world around us. So what if we would take a learning algorithm that does something similar for digital videos? Imagine that we would need to render a sparse video only every tenth pixel filled with information, and some kind of neural network-based technique would be able to reconstruct the full image similarly to what our brain does. Yes, but that is very little information to reconstruct an image from. So, is it possible? Well, hold on to your papers, because this new work can reconstruct a near-perfect image by looking at less than 10% of the input pixels. So we have this as an input, and we get this. Wow. What is happening here is called a neural reconstruction of foveated rendering data, or you are welcome to refer to it as foveated reconstruction in short during your conversations over dinner. The scrambled text part here is quite interesting, one might think that, well, it could be better, however, given the fact that if you look at the appropriate place in the sparse image, I not only cannot read the text, I am not even sure if I see anything that indicates that there is text there at all! So far, the example assumed that we are looking at a particular point in the middle of the screen, and the ultimate question is, how does this deal with a real-life case where the user is looking around? Let’s see! This is the input….and the reconstruction. Witchcraft. Let’s have a look at some more results. Note that this method is developed for head-mounted displays, where we have information on where the user is looking over time, and this can make all the difference in terms of optimization. You see a comparison here against a method labeled as “Multiresolution”, this is from a paper by the name “Foveated 3D Graphics”, and you can see that the difference in the quality of the reconstruction is truly remarkable. Additionally, it has been trained on 350 thousand short natural video sequences, and the whole thing runs in real time! Also, note that we often discuss image inpainting methods in this series, for instance, what you see here is the legendary PatchMatch algorithm that is one of these, and it is able to fill in missing parts of an image. However, in image inpainting, most of the image is intact, with smaller regions that are missing. This is even more difficult than image inpainting, because the vast majority of the image is completely missing. The fact that we can now do this with learning-based methods is absolutely incredible. The first author of the paper is Anton Kaplanyan, who is a brilliant and very rigorous mathematician, so of course, the results are evaluated in detail, both in terms of mathematics, and with a user study. Make sure to have a look at the paper for more on that! We got to know each other with Anton during the days when all we did was light transport simulations, all day, every day, and were always speculating about potential projects, and to my great sadness, somehow, unfortunately we never managed to work together for a full project. Again, congratulations Anton! Stunning, beautiful work. What a time to be alive! This episode has been supported by Linode. Linode is the world’s largest independent cloud computing provider. They offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. Exactly the kind of works you see here in this series. If you feel inspired by these works and you wish to run your experiments or deploy your already existing works through a simple and reliable hosting service, make sure to join over 800,000 other happy customers and choose Linode. To spin up your own GPU instance and receive a $20 free credit, visit linode.com/papers or click the link in the description and use the promo code “papers20” during signup. Give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
378,Is a Realistic Water Bubble Simulation Possible?,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If we study the laws of fluid motion from physics, and write a computer program that contains these laws, we can create beautiful fluid simulations like the one you see here. The amount of detail we can simulate with these programs is increasing every year, not only due to the fact that hardware improves over time, but also, the pace of progress in computer graphics research is truly remarkable. However, when talking about fluid simulations, we often see a paper produce a piece of geometry that evolves over time, and of course, the more detailed this geometry is, the better. However, look at this. It is detailed, but something is really missing here. Do you see it? Well, let’s look at the revised version of this simulation to find out what it is. Yes, foam, spray and bubble particles are now present, and the quality of the simulation just got elevated to the next level. Also, if you look at the source text, you see that this is a paper from 2012, and it describes how to add these effects to a fluid simulation. So, why are we talking about a paper that’s about 8 years old? Not only that, but this work was not published at one of the most prestigious journals. Not even close. So, why? Well, you’ll find out in a moment, but I have to tell you that I just got to know about this paper a few days ago, and it is so good it has singlehandedly changed the way I think about research. Note that a variant of this paper has been implemented in a Blender plugin called FLIP Fluids. Blender is a free and open-source modeler program, which is a complete powerhouse, I love it. And this plugin embeds this work into a modern framework, and boy, does it come to life in there. I have rerun one of their simulations and rendered a high-resolution animation with light transport. The fluid simulation took about 8 hours, and as always, I went a little overboard with the light transport, that took about 40 hours. Have a look. It is unreal how good it looks. My goodness. It is one of the miracles of the world that we can take a piece of silicon in our machines, and through the power of science, explain fluid dynamics to it so well that such a simulation can come out of it. I have been working on these for many years now, and I am still shocked by the level of progress in computer graphics research. So, let’s talk about three important aspects of this work. First, it proposes one unified technique to add foam, spray and bubbles in one go to the fluid simulation. One technique to model all three. In the paper, they are collectively called diffuse particles, and if these particles are deeply underwater, they will be classified as bubbles. If they are on the surface of the water, they will be foam particles, and if they are further above the surface, we will call them spray particles. With one method, we get all three of those. Lovely. Two, when I had shown you this footage with and without the diffuse particles, normally I would need to resimulate the whole fluid domain to add these advanced effects, but this is not the case at all. These particles can be added as a post-processing step, which means that I was able to just run the simulation once, and then decide whether to use them or not. Just one click, and here it is, with the particles removed. Absolutely amazing. And three, perhaps the most important part, the technique is so simple I could hardly believe the paper when I saw it. You see, normally, to be able to simulate the formation of bubbles or foam, we would need to compute the Weber numbers, which requires expensive surface tension computations and more. Instead, the paper forfeits that and goes with the notion that bubbles and foam appear at regions where air gets trapped within the fluid. On the back of this knowledge, they note that wave crests are an example of that, and propose a method to find these wave crests by looking for regions where the curvature of the fluid geometry is high and locally convex. Both of these can be found through very simple expressions. Finally, air is also trapped when fluid particles move rapidly towards each other, which is also super simple to compute and evaluate. The whole thing can be implemented in a day and it leads to absolutely killer fluid animations. You see, I have a great deal of admiration for a 20-page long technique that models something very difficult perfectly, but I have at least as much admiration for an almost trivially simple method that gets us to 80% of the perfect solution. This paper is the latter. I love it. This really changed my thinking not only about fluid simulation papers, but this paper is so good, it challenged how I think about research in general. It is an honor to be able to talk about beautiful works like this to you, so thank you so much for coming and listening to these videos. Note that the paper does more than what we’ve talked about here it also proposes a method to compute the lifetime of these particles, tells us how they get advected by water and more. Make sure to check out the paper in the description for more on that. If you are interested, go and try Blender, that tool is completely free for everyone to use, I have been using it for around a decade now and it truly is incredible that something like this exists as a community effort. The FLIP Fluids plugin is a paid addition. If one pays for it, it can be used immediately, or, if you spend a little time, you can compile it yourself, and this way, you can get it for free. Respect for the plugin authors for making such a gentle business model. If you don’t want to do any of those, even Blender has a usable built-in fluid simulator. You can do incredible things with it, but it can’t produce diffuse particles. I am still stunned by how simple and powerful this technique is. You can really find gems anywhere, not just around the most prestigious research venues. I hope you got inspired by this, and if you wish to understand how these fluids work some more, or write your own simulator, I put a link to my Master’s thesis where I try to explain the whole thing as intuitively as possible, and it also comes with the full source code free of charge for a simulator that runs on your graphics card. If you feel so voracious that even that’s not enough, I’ll also highly recommend Doyub Kim’s book on Fluid Engine Development. That one also comes with free source code. This episode has been supported by Weights & Biases. Here you see their beautiful final report on a point cloud classification project of theirs and see how using different learning rates and other parameters influences the final results. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
379,This Neural Network Combines Motion Capture and Physics,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we often talk about computer animation and physical simulations, and these episodes are typically about one or the other. You see, it is possible to teach a simulated AI agent to lift weights and jump really high using physical simulations to make sure that the movements and forces are accurate. The simulation side is always looking for correctness. However, let’s not forget that things also have to look good. Animation studios are paying a fortune to record motion capture data from real humans and sometimes even dogs to make sure that these movements are visually appealing. So, is it possible to create something that reacts to our commands with the controller, looks good, and also adheres to physics? Well, have a look! This work was developed at Ubisoft La Forge. It responds to our input via the controller and the output animations are fluid and natural. Since it relies on a technique called deep reinforcement learning, it requires training. You see that early on, the blue agent is trying to imitate the white character, and it is not doing well at all. It basically looks like me when going to bed after reading papers all night. The white agent’s movement is not physically simulated and was built using a motion database with only 10 minutes of animation data. This is the one that is in the “looks good” category. Or, it would look really good if it wasn’t pacing around like a drunkard, so the question naturally arises, who in their right minds would control a character like this? Well, of course, no one! This sequence was generated by an artificial, worst-case player which is a nightmare situation for any AI to reproduce. Early on, it indeed is a nightmare. However…after 30 hours of training, the blue agent learned to reproduce the motion of the white character, while being physically simulated. So, what is the advantage of that? Well, for instance, it can interact with the scene better, and is robust against perturbations. This means that it can rapidly recover from undesirable positions. This can be validated via something that the paper calls impact testing. Are you thinking what I am thinking? I hope so, because I am thinking about throwing blocks at this virtual agent, one of our favorite pastimes at Two Minute Papers, and it will be able to handle them. Whoops! Well, most of them anyway. It also reacts to a change in direction much quicker than previous agents. If all that was not amazing enough, the whole control system is very light, and takes only a few microseconds, most of which is spent by not even the control part, but the physics simulation. So, with the power of computer graphics and machine learning research, animation and physics can now be combined beautifully, it does not limit controller responsiveness, looks very realistic, and it is very likely that we’ll see this technique in action in future Ubisoft games. Outstanding. This video was supported by you on Patreon. If you wish to watch these videos in early access, or get your name immortalized in the video description, make sure to go to Patreon.com/TwoMinutePapers and pick up one of those cool perks, or, we are also test driving the early access program here on YouTube, just go ahead and click the join button, or use the link in the description. Thanks for watching and for your generous support, and I'll see you next time!"
380,"Finally, Differentiable Physics is Here!","Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A few episodes ago, we discussed a new research work that performs something that they call differentiable rendering. The problem formulation is the following: we specify a target image that is either rendered by a computer program, or even better, a photo. The input is a pitiful approximation of it, and now, because it progressively changes the input materials, textures, and even the geometry of this input in a 3D modeler system, it is able to match this photo. At the end of the video, I noted that I am really looking forward for more differentiable rendering and differentiable everything papers. So, fortunately, here we go, this new paper introduces differentiable programming for physical simulations. So what does that mean exactly? Let’s look at a few examples and find out together! Imagine that we have this billiard game, where we would like to hit the white ball with just the right amount of force and from the right direction, such that the blue ball ends up close to the black spot. Let’s try it. Well, this example shows that this doesn’t happen by chance, and we have to engage in a fair amount of trial and error to make this happen. What this differentiable programming system does for us is that we can specify an end state, which is the blue ball on the black dot, and it is able to compute the required forces and angles to make this happen. Very close. But the key point here is that this system is general, and therefore can be applied to many-many more problems. We’ll have a look at a few that are much more challenging than this example. For instance, it can also teach this gooey object to actuate itself in a way so that it would start to walk properly within only 2 minutes. The 3D version of this simulation learned so robustly, so that it can even withstand a few extra particles in the way. The next example is going to be obscenely powerful. I’ll try to explain what this is to make sure we can properly appreciate it. Many years ago, I was trying to solve a problem called fluid control, where we would try to coerce a smoke plume or a piece of fluid to take a given shape. Like a bunny, or a logo with letters. You can see some footage of this project here. The key difficulty of this problem is that this is not what typically happens in reality, of course, a glass of spilled water is very unlikely to suddenly take the shape of a human face, so we have to introduce changes to the simulation itself, but at the same time, it still has to look as if it could happen in nature. If you wish to know more about my work here, the full thesis and the source code is available in the video description, and one of my kind students has even implemented it in Blender. So, this problem is obscenely difficult. So you can now guess what’s next for this differentiable technique…it starts out with a piece of simulated ink with a checkerboard pattern, and it exerts just the appropriate forces so that it forms exactly the Yin-Yang symbol shortly after. I am shocked by how such a general system can perform something of this complexity. Having worked on this problem for a while, I can tell you that this is immensely difficult. Amazing. And hold on to your papers, because it can do even more. In this example, it adds carefully crafted ripples to the water, to make sure that it ends up in a state that distorts the image of the squirrel in a way that a powerful and well-known neural network sees it not as a squirrel, but as a goldfish. This thing is basically a victory lap in the paper. It is so powerful, it’s not even funny. You can just make up some problems that sound completely impossible and it rips right through them. The full source code of this work is also available. By the way, the first author of this paper is Yuanming Hu, his work was showcased several times in this series, in one of the earlier videos, we showcased his amazing Jello simulation that was implemented in so few lines of code, it almost fits on a business card. I said it in a previous episode, and I will say it again. I can’t wait to see more and more papers in differentiable rendering and simulations. And as this work leaves plenty of room for creativity for novel problem definitions, I’d love to hear what you think about it. What else could this be used for? Solving video games faster than other learning-based techniques? Anything else? Let me know in the comments below. What a time to be alive! This episode has been supported by Weights & Biases. Here you see a beautiful final report on one of their projects on classifying parts of street images, and see how these learning algorithms evolve over time. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. It is really easy to set up, so much so that they have made an instrumentation for this exact paper we have talked about in this episode. Have a look here! Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
381,StyleGAN2: Near-Perfect Human Face Synthesis...and More,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Neural network-based learning algorithms are on the rise these days, and even though it is common knowledge that they are capable of image classification, or in other words, looking at an image and saying whether it depicts a dog or a cat, nowadays, they can do much, much more. In this series, we covered a stunning paper that showcased a system that could not only classify an image, but write a proper sentence on what is going on, and could cover even highly non-trivial cases. You may be surprised, but this thing is not recent at all. This is 4 year old news! Insanity. Later, researchers turned this whole problem around, and performed something that was previously thought to be impossible. They started using these networks to generate photorealistic images from a written text description. We could create new bird species by specifying that it should have orange legs and a short yellow bill. Later, researchers at NVIDIA recognized and addressed two shortcomings: one was that the images were not that detailed, and two, even though we could input text, we couldn’t exert too much artistic control over the results. In came StyleGAN to the rescue, which was then able to perform both of these difficult tasks really well. These images were progressively grown, which means that we started out with a coarse image, and go over it over and over again, adding new details. This is what the results look like and we can marvel at the fact that none of these people are real. However, some of these images were still contaminated by unwanted artifacts. Furthermore, there are some features that are highly localized as we exert control over these images, you can see how this part of the teeth and eyes are pinned to a particular location and the algorithm just refuses to let it go, sometimes to the detriment of its surroundings. This new work is titled StyleGAN2 and it addresses all of these problems in one go. Perhaps this is the only place on the internet where we can say that finally, teeth and eyes are now allowed to float around freely, and mean it with a positive sentiment. Here you see a few hand-picked examples from the best ones, and I have to say, these are eye-poppingly detailed and correct looking images. My goodness! The mixing examples you have seen earlier are also outstanding. Way better than the previous version. Also, note that as there are plenty of training images out there for many other things beyond human faces, it can also generate cars, churches, horses, and of course, cats. Now that the original StyleGAN 1 work has been out for a while, we have a little more clarity and understanding as to how it does what it does, and the redundant parts of architecture have been revised and simplified. This clarity comes with additional advantages beyond faster and higher-quality training and image generation. Interestingly, despite the fact that the quality has improved significantly, images made with the new method can be detected more easily. Note that the paper does much, much more than this, so make sure to have a look in the video description! In this series, we always say that two more papers down the line, and this technique will be leaps and bounds beyond the first iteration. Well, here we are, not two, only one more paper down the line. What a time to be alive! The source code of this project is also available. What’s more, it even runs in your browser. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Here you see a beautiful final report on one of their projects on classifying parts of street images, and see how these learning algorithms evolve over time. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
382,Simulating Breaking Bread,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In a recent video, we showcased a computer graphics technique that simulated the process of baking, and now, it’s time to discuss a paper that is about simulating how we can tear this loaf of bread apart. This paper aligns well with the favorite pastimes of a computer graphics researcher, which is, of course, destroying virtual objects in a spectacular fashion. Like the previous work, this new paper also builds on top of the Material Point Method, a hybrid simulation technique that uses both particles and grids to create these beautiful animations, however, it traditionally does not support simulating cracking and tearing phenomena. Now, have a look at this new work, and marvel at how beautifully this phenomenon is simulated. With this, we can smash oreos, candy crabs, pumpkins, and much, much more. This jelly fracture scene is my absolute favorite. Now, when an artist works with these simulations, the issue of artistic control often comes up. After all, this method is meant to compute these phenomena by simulating physics, and we can’t just instruct physics to be more beautiful…or can we? Well, this technique offers us plenty of parameters to tune the simulation to our liking, two that we’ll note today are alpha, which means the hardening, and beta is the cohesion parameter. So what does that mean exactly? Well, beta was cohesion, which is the force that holds matter together, so as we go to the right, the objects stay more intact, and as we go down, the objects shatter into more and more pieces. The method offers us more parameters than these, but even with these two, we can really make the kind of simulation we are looking for. Ah, what the heck, let’s do two more. We can even control the way the cracks form with the Mc parameter, which is the speed of crack propagation, and G is the energy release which, as we look to the right, increases the object’s resistance to damage. So how long does this take? Well, the technique takes its sweet time, the execution timings range from 17 seconds to about 10 minutes per frame. This is one of those methods that does something that wasn’t possible before, and it is about doing things correctly. And after a paper appears on something that makes the impossible possible, followup research works get published later that further refine and optimize it. So, as we say, two more papers down the line, this will run much faster. Now, a word about the first author of the paper, Joshuah Wolper. Strictly speaking, it is his third paper, but only the second within computer graphics, and my goodness, did he come back with guns blazing. This paper was accepted to the SIGGRAPH conference, which is one of the biggest honors a computer graphics researcher can get, perhaps equivalent to the olympic gold medal for an athlete. It definitely is worthy of a gold medal. Make sure to have a look at the paper in the video description, it is an absolutely beautifully crafted piece of work. Congratulations Joshuah! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
383,This Neural Network Restores Old Videos,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we often discuss a class of techniques by the name image inpainting. Image inpainting methods are capable of filling in missing details from a mostly intact image. You see the legendary PatchMatch algorithm at work here, which is more than 10 years old, and it is a good old computer graphics method with no machine learning in sight, and after so much time, 10 years is an eternity in research years, it still punches way above its weight. However, with the ascendancy of neural network-based learning methods, I am often wondering whether it would be possible to take a more difficult problem, for instance, inpainting not just still images, but movies as well. For instance, let’s take and old old black and white movie that suffers from missing data, flickering, blurriness, and interestingly, even the contrast of the footage has changed as it faded over time. Well, hold on to your papers, because this learning-based approach fixes all of these, and even more! Step number one is restoration, which takes care of all of these artifacts and contrast issues. You can not only see how much better the restored version is, but it is also reported what the technique did exactly. However, it does more. What more could we possibly ask for? Well, colorization! What it does is that it looks at only 6 colorized reference images that we have to provide, and uses this as art direction and propagate it to the remainder of the frames. And it does an absolutely amazing work at that. It even tells us which reference image it is looking at when colorizing some of these frames, so if something does not come out favorably, we know which image to recolor. The architecture of the neural network that is used for all this also has to follow the requirements appropriately. For instance, beyond the standard spatial convolution layers, it also makes ample use of these blue temporal convolution layers, which helps “smearing out” the colorization information from one reference image to multiple frames. However, in research, a technique is rarely the very first at doing something, and sure enough, this is not the first technique that does this kind of restoration and colorization. So how does it compare to previously published methods? Well, quite favorably. With previous methods, in some cases, the colorization just appears and disappears over time, while it is much more stable here. Also, fewer artifacts make it to the final footage, and since cleaning these up is one of the main objectives of these methods, that’s also great news. If we look at some quantitative results, or in other words, numbers that describe the difference, you see here that we get a 3-4 decibels cleaner image, which is outstanding. Note that the decibel scale is not linear, but a logarithmic scale, therefore if you read 28 instead of 24, it does not mean that it’s just approximately 15% better. It is a much, much more pronounced difference than that. I think these results are approaching a state where they are becoming close to good enough so that we can revive some of these old masterpiece movies and give them a much-deserved facelift. What a time to be alive! This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. They also wrote a guide on the fundamentals of neural networks where they explain in simple terms how to train a neural network properly, what are the most common errors you can make, and how to fix them. It is really great, you got to have a look. So make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
384,Neural Portrait Relighting is Here!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In computer graphics, when we are talking about portrait relighting, we mean a technique that is able to look at an image and change the lighting, and maybe even the materials or geometry after this image has been taken. This is a very challenging endeavor. So can neural networks put a dent into this problem and give us something new and better? You bet! The examples that you see here are done with this new work that uses a learning-based technique, and is able to change the lighting for human portraits, and only requires one input image. You see, normally, using methods in computer graphics to relight these images would require trying to find out what the geometry of the face, materials, and lighting is from the image, and then, we can change the lighting or other parameters, run a light simulation program, and hope that the estimations are good enough to make it realistic. However, if we wish to use neural networks to learn the concept of portrait relighting, of course, we need quite a bit of training data. Since this is not trivially available, the paper contains a new dataset with over 25 thousand portrait images that are relit in 5 different ways. It also proposes a neural network structure that can learn this relighting operation efficiently. It is shaped a bit like an hourglass and contains an encoder and decoder parts. The encoder part takes an image as an input and estimates what lighting could have been used to produce it, while the decoder part is where we can play around with changing the lighting, and it will generate the appropriate image that this kind of lighting would produce. However, there is more to it. What you see here are skip connections that are useful to save insights from different abstraction levels and transfer them from the encoder to the decoder network. So what does this mean exactly? Intuitively, it is a bit like using the lighting estimator network to teach the image generator what it has learned. So, do we really lose a lot if we skip the skip connections? Well, quite a bit, have a look here. The image on the left shows the result using all skip connections, while as we traverse to the right, we see the results omitting them. These connections indeed make a profound difference. Let’s be thankful for the authors of the paper as putting together such a dataset and trying to get an understanding as to what network architectures it would require to get great results like these takes quite a bit of work. However, I’d like to make a note about modeling subsurface light transport. This is a piece of footage from our earlier paper that we wrote as a collaboration with the Activision Blizzard company, and you can see here that including this indeed makes a profound difference in the looks of a human face. I cannot wait to see some followup papers that take more advanced effects like this into consideration for relighting as well. If you wish to find out more about this work, make sure to click the link in the video description. This episode has been supported by Weights & Biases. Here you see a write-up of theirs where they explain how to visualize the gradients running through your models, and illustrate it through the example of predicting protein structure. They also have a live example that you can try! Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
385,This Neural Network Turns Videos Into 60 FPS!,"Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. With today's camera and graphics technology, we can enjoy smooth and creamy videos on our devices that were created with 60 frames per second. I also make each of these videos using 60 frames per second, however, it almost always happens that I encounter the paper videos that have anything from 24 to 30 frames per second. In this case, I put them in my video editor that has a 60 fps timeline, so half or even more of these frames will not provide any new information. As we try to slow down the videos for some nice slow-motion action, this ratio is even worse, creating an extremely choppy output video because we have huge gaps between these frames. So, does this mean that there is nothing we can do and have to put up with this choppy footage? No, not at all! Earlier, we discussed two potential techniques to remedy this issue. One was frame blending, which simply computes the average of two consecutive images and presents that as a solution. This helps a little for simpler cases, but this technique is unable to produce new information. Optical flow is a much more sophisticated method that is very capable as it tries to predict the motion that takes place between these frames. This can kind of produce new information and I use this in the video series on a regular basis, but the output footage also has to be carefully inspected for unwanted artifacts. Which are a relatively common occurrence. Now, our seasoned Fellow Scholars will immediately note that we have a lot of high-framerate videos on the internet, why not delete some of the in-between frames, give the choppy and the smooth videos to a neural network, and teach it to fill in the gaps! After the lengthy training process, it should be able to complete these choppy videos properly. So, is that true? Yes, but note that there are plenty of techniques out there that already do this, so what is new in this paper? Well, this work does that, …and… much more! We will have a look at the results, which are absolutely incredible, but to be able to appreciate what is going on, let me quickly show you this. The design of this neural network tries to produce four different kinds of data to fill in these images. One is optical flows, which is part of previous solutions too, but two, it also produces a depth map that tells us how far different parts of the image are from the camera. This is of utmost importance, because if we rotate the camera around, previously occluded objects suddenly become visible, and we need proper intelligence to be able to recognize this and to fill in this kind of missing information. This is what the contextual extraction step is for, which drastically improves the quality of the reconstruction, and finally, the interpolation kernels are also learned, which gives it more knowledge as to what data to take from the previous and the next frame. Since it also has a contextual understanding of these images, one would think that it needs a ton of neighboring frames to understand what is going on, which, surprisingly, is not the case at all! All it needs is just the two neighboring images. So, after doing all this work, it better be worth it, right? Let’s have a look at some results! Hold on to your papers, and in the meantime, look at how smooth and creamy the outputs are! Love it! Because it also deals with contextual information, if you wish to feel like real Scholar, you can gaze at regions where the occlusion situation changes rapidly and see how well it fills in this kind of information. Unreal. So how does one show that the technique is quite robust? Well, by producing and showing it off on tons and tons of footage and that is exactly what the authors did! I put a link to a huge playlist with 33 different videos in the description so you can have a look at how well this works on a wide variety of genres. Now, of course, this is not the first technique for learning-based frame interpolation, so let’s see how it stacks up against the competition! Wow, this is quite a value proposition, because depending on the dataset, it comes out first and second place on most examples. The PSNR is the peak signal to noise ratio, while the SSIM is the structural similarity metric, both of which measure how well the algorithm reconstructs these details compared to the ground truth, and both are subject to maximization. Note that none of them are linear, therefore even a small difference in these numbers can mean a significant difference. I think we are now at a point where these tools are getting so much better than their handcrafted optical flow rivals that I think they will quickly find their way to production software. I cannot wait. What a time to be alive! This episode has been supported by Weights & Biases. In this post, they show you which hyperparameters to tweak to improve your model performance. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. They don’t lock you in, and if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
386,The Story of Light!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Whenever we look at these amazing research papers on physical simulations, it is always a joy seeing people discussing them in the comments section. However, one thing that caught my attention is that some people comment about how things look, and not on how things move in these papers. Which is fair enough, and to this end, I will devote this episode to talk a little about a few amazing techniques used in light transport simulations. But first things first: when talking about physics simulations, we are talking about a technique that computes how things move. Then, we typically run a light simulation program that computes how things look. The two are completely independent, which means that it is possible that the physical behavior of bread breaking here is correct, but the bread itself does not look perfectly realistic. This second part depends on the quality of the light simulation and the materials used there. We can create such an image by simulating the path of millions and millions of light rays. And initially, this image will look noisy, and as we add more and more rays, this image will slowly clean up over time. If we don’t have a well-optimized program, this can take from hours to days to compute. We can speed up this process by carefully choosing where to shoot these rays, and this is a technique that is called importance sampling. But then, around 1993, an amazing paper appeared by the name Bidirectional Path Tracing, that proposed that we don’t just start building light paths from one direction, but two instead one from the camera, and one from a light source and then, connect them. This significantly improved the efficiency of these light simulations, however, it opened up a new can of worms. There are many different ways of connecting these paths which leads to mathematical difficulties. For instance, we have to specify the probability of a light path forming, but what do we do if there are multiple ways of producing this light path? There will be multiple probabilities. What do we do with all this stuff? To address this, Eric Veach described a magical algorithm in this thesis, and thus, multiple importance sampling was born. I can say without exaggeration that this is one of the most powerful techniques in all photorealistic rendering research. What multiple importance sampling, or from now on, MIS in short does, is combine these multiple sampling techniques in a way that accentuates the strengths of each of them. For instance, you can see the image created by one sampling technique here, and the image from a different one here. Both of them are quite noisy, but if we combine them with MIS, we get this instead in the same amount of time. A much smoother, less noisy image. In many cases, this can truly bring down the computation times from several hours to several minutes. Absolute witchcraft. Later, even more advanced techniques appeared to accelerate the speed of these light simulation programs. For instance, it is now not only possible to compute light transport between points in space, but between a point and a beam instead. You see the evolution of an image using this photon beam-based technique. This way, we can get rid of the point-based noise and get a much, much more appealing rendering process. The lead author of this beam paper is Wojciech Jarosz, who, three years later, ended up being the head of the Rendering group at the amazing Disney Reseach lab. Around that time he also hired me to work with him at Disney on a project I can’t talk about, which was an incredible and life-changing experience and I will be forever grateful for his kindness. By the way, he is now a professor at the Darthmouth University and just keeps pumping out one killer paper after another. So, as you might have guessed, if it is possible to compute light transport between two points, a point and a beam, later, it became possible to do this between two beams. None of these are for the faint of the heart, but it works really well. But, there is a huge problem. These techniques work with different dimensionalities, or in other words, they estimate the final result so differently, that they cannot be combined with multiple importance sampling. That is indeed a problem, because all of these have completely different strengths and weaknesses. And now, hold on to your papers, because we have finally arrived to the main paper of this episode. It bears the name UPBP, which stands for unifying points, beams and paths, and it formulates multiple importance sampling between all of these different kinds of light transport simulations. Basically, what we can do with this is throw every advanced simulation program we can think of together, and out comes a super powerful version of them that combines all their strengths and nullifies nearly all of their weaknesses. It is absolutely unreal. Here you see four completely different algorithms running, and as you can see, they are noisy and smooth at very different places. They are good at computing different kinds of light transport. And now, hold on to your papers, because the final result with the UPBP technique is this. Wow! Light transport on steroids. While we look at some more results, I will note that in my opinion, this is one of the best papers ever written in light transport research. The crazy thing is that I hardly ever hear anybody talk about it. If any paper would deserve a bit more attention, it is this one, so I hope this video will help with that. I would like to dedicate this video to Jaroslav Krivanek, the first author of this absolutely amazing paper, who has tragically passed away a few months ago. In my memories, I think of him as the True King of Multiple Importance Sampling and I hope that now, you do too. Note that MIS is not limited to light transport algorithms, it is a general concept that can be used together with a mathematical technique called Monte Carlo integration, which is used pretty much everywhere, from finding out what an electromagnetic field looks like, to financial modeling, and much, much more. If you have anything to do with Monte Carlo integration, please read Eric Veach’s thesis and this paper, and if you feel that it is a good fit, try to incorporate Multiple Importance Sampling into your system. You’ll be glad you did. Also, we have recorded my lectures of a Master-level course on light transport simulations at the Technical University of Vienna. In this course, we write write such a light simulation program from scratch, and it is available free of charge for everyone, no strings attached, so make sure to click the link in the video description to get started. Additionally, I have implemented a small 1 dimensional example of MIS, if you wish to pick it up and try it, that’s also available in the video description. While talking about the Technical University of Vienna we are hiring for a PhD and a PostDoc position. The call here about “Lighting Simulation For Architectural Design” is advised by my PhD advisor, Michael Wimmer, who I highly recommend. Apply now if you feel qualified, the link is in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
387,This Neural Network Creates 3D Objects From Your Photos,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In computer graphics research, we spend most of our time dealing with images. An image is a bunch of pixels put onto a 2D plane, which is a tiny window into reality, but reality is inherently 3D. This is easy to understand for us, because if we look at a flat image, we see the geometric structures that it depicts. If we look at this image, we know that this is not a sticker, but a three dimensional fluid domain. If I would freeze an image and ask a human to imagine rotating around this fluid domain, that human would do a pretty good job at that. However, for a computer algorithm, it would be extremely difficult to extract the 3D structure out from this image. So, can we use these shiny new neural network-based learning algorithms to accomplish something like this? Well, have a look at this new technique that takes a 2D image as an input, and tries to guess three things. The cool thing is that the geometry problem we talked about is just the first one. Beyond that, two, it also guesses what the lighting configuration is that leads to an appearance like this, and three, it also produces the texture map for an object as well. This would already be great, but wait, there is more. If we plug all this into a rendering program, we can also specify a camera position, and this position can be different from the one that was used to take this input image. So what does that mean exactly? Well, it means that maybe, it can not only reconstruct the geometry, light and texture of the object, but even put this all together and make a photo of it from a novel viewpoint! Wow. Let’s have a look at an example! There is a lot going on in this image, so let me try to explain how to read it. This image is the input photo, and the white silhouette image is called a mask, which can either be given with the image, or be approximated by already existing methods. This is the reconstructed image by this technique, and then, this is a previous method from 2018 by the name category-specific mesh reconstruction, CMR in short. And, now, hold on to your papers, because in the second row, you see this technique creating images of this bird from different, novel viewpoints! How cool is that! Absolutely amazing. Since we can render this bird from any viewpoint, we can even create a turntable video of it. And all this from just one input photo. Let’s have a look at another example! Here, you see how it puts together the final car rendering in the first column from the individual elements, like geometry, texture, and lighting. The other comparisons in the paper reveal that this technique is indeed a huge step up from previous works. Now, this all sounds great, but what is all this used for? What are some example applications of this 3D object from 2D image thing? Well, techniques like this can be a great deal of help in enhancing the depth perception capabilities of robots, and of course, whenever we would like to build a virtual world, creating a 3D version of something we only have a picture of can get extremely laborious. This could help a great deal with that too. For this application, we could quickly get a starting point with some texture information, and get an artist to fill in the fine details. This might get addressed in a followup paper. And if you are worried about the slight discoloration around the beak area of this bird, do not despair. As we always say, two more papers down the line, and this will likely be improved significantly. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
388,Can We Detect Neural Image Generators?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, we have an abundance of neural network-based image generation techniques. Every image that you see here and throughout this video is generated by one of these learning-based methods. These can offer high-fidelity synthesis, and not only that, but we can often even exert artistic control over the outputs. We can truly do so much with these. And if you are wondering, there is a reason why we will be talking about an exact set of techniques, and you will see that in a moment. So the first one is a very capable technique by the name CycleGAN! This was great at image translation, or in other words, transforming apples into oranges, zebras into horses, and more. It was called CycleGAN because it introduced a cycle consistency loss function. This means that if we convert a summer image to a winter image, and then back to a summer image, we should get the same input image back. If our learning system obeys to this principle, the output quality of the translation is going to be significantly better. Later, a technique by the name BigGAN appeared, which was able to create reasonably high quality images and not only that, but it also gave us a little artistic control over the outputs. After that, StyleGAN and even its second version appeared, which, among many other crazy good features, opened up the possibility to lock in several aspects of these images, for instance, age, pose, some facial features and more, and then, we could mix them with other images to our liking, while retaining these locked-in aspects. And of course, DeepFake creation provides fertile grounds for research works, so much so that at this point, it seems to be a subfield of its own where the rate of progress is just stunning. Now that we can generate arbitrarily many beautiful images with these learning algorithms, they will inevitably appear in many corners of the internet, so an important new question arises can we detect if an image was made by these methods? This new paper argues that the answer is a resounding yes. You see a bunch of synthetic images above, and real images below here, and if you look carefully for the labels, you’ll see many names that ring a bell to our Scholarly minds. CycleGAN, BigGAN, StyleGAN…nice! And now, you know that this is exactly why we briefly went through what these techniques do at the start of the video. So, all of these can be detected by this new method. And now, hold on to your papers, because I kind of expected that, but what I didn’t expect is that this detector was trained on only one of these techniques, and leaning on that knowledge, it was able to catch all the others! Now that’s incredible. This means that there are foundational elements that bind together all of these techniques. Our seasoned Fellow Scholars know that this similarity is none other than the that the fact that they are all built on convolutional neural networks. They are vastly different, but they use very similar building blocks. Imagine the convolutional layers as lego pieces, and think of the techniques themselves to be the objects that we build using them. We can build anything, but what binds these all together is that they are all but a collection of lego pieces. So, this detector was only trained on real images and synthetic ones created by the ProGAN technique, and you see with the blue bars that the detection ratio is quite close to perfect for a number of techniques, save for these two. The AP label means average precision. If you look at the paper in the description, you will get a lot more insights as to how robust it is against compression artifacts, a little frequency analysis of the different synthesis techniques, and more. Let’s send a huge thank you to the authors of the paper, who also provide the source code and training data for this technique. For now, we can all breathe a sigh of relief that there are proper detection tools that we can train ourselves at home. In fact, you will see such an example in a second. What a time to be alive! Good news! We now have an unofficial discord server where all of you Fellow Scholars are welcome to discuss ideas and learn together in a kind and respectful environment. Look, some connections and discussions are already being made thank you so much for our volunteering Fellow Scholars for making this happen! The link is available in the video description, it is completely free, if you have joined, make sure to leave a short introduction! Meanwhile, what you see here is an instrumentation of this exact paper we have talked about, which was made by Weights and Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
389,Transferring Real Honey Into A Simulation,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. It’s time for some fluid simulations again! Writing fluid simulations is one of the most fun things we can do within computer graphics, because we can create a virtual scene, add the laws of physics for fluid motion, and create photorealistic footage with an absolutely incredible amount of detail and realism. Note that we can do this ourselves, so much so, that for this scene, I ran the fluid and light simulation myself here at the Two Minute Papers studio, and, on consumer hardware. However, despite this amazing looking footage, we are not nearly done yet! There is still so much to explore! For instance, a big challenge these days is trying to simulate fluid-solid interactions. This means that the sand is allowed to have an effect on the fluid, but at the same time, as the fluid sloshes around, it also moves the sand particles within. This is what we refer to as two-way coupling. We also know that there are different kinds of two-way coupling, and only the more advanced ones can correctly simulate how real honey supports the dipper and there is barely any movement. This may be about the only place on the internet where we are super happy that nothing at all is happening. However, many of you astute Fellow Scholars immediately ask, okay, but what kind of honey are we talking about? We can buy tens, if not hundreds of different kinds of honey at the market. If we don’t know what kind of honey we are using, how do we know if this simulation is too viscous, or not viscous enough? Great question! Just to make sure we don’t get lost, viscosity means the amount of resistance against deformation, therefore, as we go up, you can witness this kind of resistance increasing. And now, hold on to your papers because this new technique comes from the same authors as the previous one with the honey dipper, and enables us to import our real-world honey into our simulation. That sounds like science fiction. Importing real-world materials into a computer simulation? How is that even possible? Well, with this solution, all we need to do is point a consumer smartphone camera at the phenomenon and record it. The proposed technique does all the heavy lifting by first, extracting the silhouette of the footage, and then, creating a simulation that tries to reproduce this behavior. The closer it is, the better. However, at first, of course, we don’t know the exact parameters that would result in this, however, now we have an objective we can work towards. The goal is to re-run this simulation with different parameter sets, in a way to minimize the difference between the simulation and reality. This is not just working by trial and error but through a technique that we refer to as mathematical optimization. As you see, later, the technique was able to successfully identify the appropriate viscosity parameter. And when evaluating these results, note that this work does not deal with how things look for instance, whether the honey has the proper color or translucency is not the point here. What we are trying to reproduce is not how it looks, but how it moves. It works on a variety of different fluid types. I have slowed down some of these videos to make sure we can appreciate together how amazingly good these estimations are. And we’re not even done yet! If we wish to, we can even set up a similar scene as the real-world one with our simulation as a proxy for the real honey or caramel flow. After that, we can perform anything we want with this virtual piece of fluid, even including putting it into novel scenarios, like this scene which would otherwise be difficult to control, and quite wasteful, or even creating the perfect honey dipper experiment. Look at how perfect the symmetry is there down below! Yum! Normally, in a real-world environment, we cannot pour the honey and apply forces this accurately, but in a simulation, we can do anything we want! And now, we can also import the exact kind of materials from our real world repertoire. If you can buy it, you can simulate it. What a time to be alive! This episode has been supported by Linode. Linode is the world’s largest independent cloud computing provider. Unlike entry-level hosting services, Linode gives you full backend access to your server, which is your step up to powerful, fast, fully configurable cloud computing. Linode also has One-Click Apps that streamline your ability to deploy websites, personal VPNs, game servers, and more. If you need something as small as a personal online portfolio, Linode has your back, and if you need to manage tons of client’s websites and reliably serve them to millions of visitors, Linode can do that too. What’s more, they offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. If only I had access to a tool like this while I was working on my last few papers! To receive $20 in credit on your new Linode account, visit linode.com/papers or click the link in the description and give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
390,Deformable Simulations…Running In Real Time!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. With the power of modern computer graphics and machine learning techniques, we are now able to teach virtual humanoids to walk, sit, manipulate objects, and we can even make up new creature types and teach them new tricks…if we are patient enough, that is. But, even with all this knowledge, we are not done yet, are we? Should we just shut down all the research facilities, because there is nothing else to do? Well, if you have spent any amount of time watching Two Minute Papers, you know that the answer is, of course not! There is so much to do I don’t even know where to start! For instance, let’s consider the case of deformable simulations. Not so long ago, we talked about Yuanming Hu’s amazing paper with which, we can engage in the favorite pastime of a computer graphics researcher, which is, of course, destroying virtual objects in a spectacular manner. It can also create remarkably accurate jello simulations, where we can even choose our physical parameters. Here you see how we can drop in blocks of different densities into the jello, and as a result, they sink in deeper and deeper. Amazing. However, note that this is not for real-time applications and computer games because the execution time is measured not in frames per second, but in seconds per frame. If we are looking for somewhat coarse results, but in real time, we have covered a paper approximately 300 episodes ago, which performed something that is called a Reduced Deformable Simulation. Leave a comment if you were already a Fellow Scholar back then! This technique could be trained on a number of different representative cases, which, in computer graphics research, is often referred to as precomputation, which means that we have to do a ton of work before starting a task, but only once, and then, all our subsequent simulations can be sped up. Kind of like a student studying before an exam, so when the exam itself happens, the student, in the ideal case, will know exactly what to do. Imagine trying to learn the whole subject during the exam! Note that this training in this technique is not the same kind of training we are used to see with neural networks, and its generalization capabilities were limited, meaning that if we strayed too far from the training examples, the algorithm did not work so reliably. And now, hold on to your papers, because this new method contains a ton of optimizations, runs on your graphics card, and hence, can perform these deformable simulations at close to 40 frames per second. And in the following examples in a moment, you will see something even better. A killer advantage of this method is that this is also scalable. This means that the resolution of the object geometry can be changed around, here, the upper left is a coarse version of the object, where the lower right is the most refined version of it. Of course, the number of frames we can put out per second depends a great deal on the resolution of this geometry, and if you have a look, this looks very close to the one below it, but is still more than 3 to 6 times faster than real time. Wow. And whenever we are dealing with collisions, lots of amazing details appear. Just look at this! Let’s look at a little more formal measurement of the scalability of this method. Note that this is a log-log plot, since the number of tetrahedra used for the geometry and the execution time spans many orders of magnitude. In other words, we can see how it works from the coarsest piece of geometry to the most detailed models we can throw at it. If we look at something like this, we are hoping that the lines are not too steep, which is the case for both the memory and execution timings. So, finally, real-time deformable simulations, here we come! What a time to be alive! This episode has been supported by Weights & Biases. Here, they show you how to make it to the top of Kaggle leaderboards by using their tool to find the best model faster than everyone else. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
391,This Neural Network Learned The Style of Famous Illustrators,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In the last few years, we have seen a bunch of new AI-based techniques that were specialized in generating new and novel images. This is mainly done through learning-based techniques, typically a Generative Adversarial Network, a GAN in short, which is an architecture where a generator neural network creates new images, and passes it to a discriminator network, which learns to distinguish real photos from these fake, generated images. These two networks learn and improve together, and generate better and better images over time. What you see here is a set of results created with the technique by the name CycleGAN. This could even translate daytime into nighttime images, reimagine a picture of a horse as if it were a zebra, and more. We can also use it for style transfer, a problem where we have two input images, one for content, and one for style, and as you see here, the output would be a nice mixture of the two. However, if we use CycleGAN for this kind of style transfer, we’ll get something like this. The goal was to learn the style of a select set of famous illustrators of children’s books by providing an input image with their work. So, what do you think about the results? Well, the style is indeed completely different, but the algorithm seems a little too heavy-handed and did not leave the content itself intact. Let’s have a look at another result with a previous technique. Maybe this will do better. This is DualGAN, which refers to a paper by the name Unsupervised dual learning for image-to-image translation. This uses two GANs to perform image translation, where one GAN learns to translate, for instance, from day to night, while the other one learns the opposite, night to day translation. This, among other advantages, makes things very efficient, but as you see here, in these cases, it preserves the content of the image, but perhaps a little too much, because the style itself does not appear too prominently in the output images. So CycleGAN is good at transferring style, but a little less so for content, and DualGAN is good at preserving the content, but sometimes adds too little of the style to the image. And now, hold on to your papers, because this new technique by the name GANILLA offers us these results. The content is intact, checkmark, the style goes through really well, checkmark. It preserves the content and transfers the style at the same time! Excellent! One of the many key reasons as to why this happens is the usage of skip connections, which help preserve the content information as we travel deeper into the neural network. So, finally, let’s put our money where our mouth is and take a bunch of illustrators, marvel at their unique style, and then, apply it to photographs and see how the algorithm stacks up against other previous works. Wow. I love these beautiful results. These comparisons really show how good the new GANILLA technique is at preserving content. And note that these are distinct artistic styles that are really difficult to reproduce, even for humans. It is truly amazing that we can perform such a thing algorithmically. Don’t forget that the first style transfer paper appeared approximately 3-3.5 years ago, and now, we have come a long-long way! The pace of progress in machine learning research is truly stunning! While we are looking at some more amazing results, this time around, only from GANILLA, I will note that the authors also made a user study with 48 people who favored this against previous techniques. And, perhaps leaving the best for last, it can even draw in the style of Hayao Miyazaki. I bet there are a bunch of Miyazaki fans watching, so let me know in the comments what you think about these results! What a time to be alive! This episode has been supported by Weights & Biases. In this post they show you how to easily iterate on models by visualizing and comparing experiments in real time. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
392,This Neural Network Regenerates…Kind Of,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to play with a cellular automaton. You can imagine these automata as small games where we have a bunch of cells, and a set of simple rules that describe when a cell should be full, and when it should be empty. These rules typically depend on the state of the neighboring cells. For instance, perhaps the most well-known form of this cellular automaton is John Horton Conway’s Game of Life, which a simulates a tiny world where each cell represents a little life form. The rules, again, depend on the neighbors of this cell if there are too many neighbors, they will die due to overpopulation, if too few, they will die due to underpopulation, and if they have just the right amount of neighbors, they will thrive, and reproduce. So why is this so interesting? Well, this cellular automaton shows us that a small set of simple rules can give rise to remarkably complex life forms, such as gliders, spaceships, and even John von Neumann’s universal constructor, or in other words, self-replicating machines. I hope you think that’s quite something, and in this paper today, we are going to take this concept further. Way further! This cellular automaton is programmed to evolve a single cell to grow into a prescribed kind of life form. Apart from that, there are many other key differences from other works, and we will highlight two of them them today. One, the cell state is a little different because it can either be empty, growing, or mature, and even more importantly, two, the mathematical formulation of the problem is written in a way that is quite similar to how we train a deep neural network to accomplish something. This is absolutely amazing. Why is that? Well, because it gives rise to a highly-useful feature, namely that we can teach it to grow these prescribed organisms. But wait, over time, some of them seem to decay, some of them can’t stop growing…and, some of them will be responsible for your nightmares, so, from this point on, proceed with care. In the next experiment, the authors describe an additional step in which it can recover from these undesirable states. And now, hold on to your papers, because this leads to the one of the major points of this paper. If it can recover from undesirable states, can it perhaps..regenerate when damaged? Well, here, you will see all kinds of damage…and then, this happens. Wow! The best part is that this thing wasn’t even trained to be able to perform this kind of regeneration! The objective for training was that it should be able to perform its task of growing and maintaining shape, and it turns out, some sort of regeneration is included in that. It can also handle rotations as well, which will give rise to a lot of fun, as noted a moment ago, some nightmarish experiments. And, note that this is a paper in the Distill journal, which not only means that it is excellent, but also interactive, so you can run many of these experiments yourself right in your web browser. If Alexander Mordvintsev, the name of the first author rings a bell, he worked on Google’s Deep Dreams approximately 5 years ago. How far we can some since, my goodness. Loving these crazy, non-traditional research papers and am looking forward to seeing more of these. This episode has been supported by Weights & Biases. Here, they show you how you can visualize the training process for your boosted trees with XGBoost using their tool. If you have a closer look, you’ll see that all you need is one line of code. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
393,Google’s Chatbot: Almost Perfect,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. When I was growing up, IQ tests were created by humans to test the intelligence of other humans. If someone told me just 10 years ago that algorithms will create IQ tests to be taken by other algorithms, I wouldn’t have believed a word of it. Yet, just a year ago, scientists at DeepMind created a program that is able to generate a large amount of problems that test abstract reasoning capabilities. They are inspired by human IQ-tests with all the these questions about sizes, colors and progressions. And then, they wrote their own neural network to take these tests, which performed remarkably well. How well exactly? In the presence of nasty distractor objects, it was able to find out the correct solution about 62% of the time, and, if we removed these distractors, which, I will note that are good at misdirecting humans too, the AI was correct 78% of the time! Awesome. But today, we are capable of writing even more sophisticated learning algorithms that can even complete our sentences! Not so long ago, the OpenAI lab published GPT-2, a technique that they unleashed to read the internet, and it learned our language by itself. A few episodes ago, we gave it a spin, and I almost fell out of the chair when I saw that it could finish my sentences about fluid simulations in such a scholarly way, that I think, could easily fool a layperson. Have a look here and judge for yourself! This GPT-2 technique was a neural network variant that was trained using 1.5 billion parameters. At the risk of oversimplifying what that means, it roughly refers to the internal complexity of the networks, or in other words, how many weights and connections are there. And now, the Google Brain team has released Meena, an open-domain chatbot that uses 2.6 billion parameters, and shows remarkable human-like properties. The chatbot part means a piece of software or a machine that we can talk to, and the open-domain part refers to the fact that we can try any topic, hotels, movies, the ocean, favorite movie characters, or pretty much anything we can think of and expect the bot to do well. So how do we know that it’s really good? Well, let’s try to evaluate it in two different ways. First, let’s try the super fun, but less scientific way, or, in other words, what we are already doing, looking at chat logs! You see a Meena writing on the left, and a human being on the right, and it not only answers questions sensibly and coherently, but is even capable of cracking a joke. Of course, if you consider a pun to be a joke, that is. You see a selection of topics here, where the user talks with Meena about movies, and the bot expresses the desire to see The Grand Budapest Hotel, which is indeed a very humanlike quality. It can also try to come up with a proper definition of philosophy. And now, since we are scholars, we would also like to measure how humanlike this is in a more scientific manner as well! Now is a good time to hold on to your papers, because this is measured with the Sensibleness and Specificity Average score, from now on, SSA in short, in which, humans are here, previous chatbots are down there, and Meena is right there, close by, which means that it is easy to be confused for a real human. That already sounds like science fiction, however, let’s be a little nosy here and also ask, how do we know that this SSA is any good in predicting what is humanlike and what isn’t? Excellent question. When measuring human-likeness for these chatbots, plugging in this SSA, again, the Sensibleness and Specificity Average, we see that they correlate really strongly, which means that the two seem to measure very similar things, and in this case, SSA can indeed be used as a proxy for human likeness. The coefficient of determination is 0.96. To put this into perspective, this is a several times stronger correlation than we can measure between the intelligence and the grades of a student, which is already a great correlation. This is a remarkable result. Now, what we get out of this is that the SSA is much easier and precise to measure than human likeness, and is hence, used throughout the paper. So, chatbots eh? What are all these things useful for? Well, remember Google’s technique that would automatically use an AI to talk to your callers and screen your calls? Or even make calls on your behalf. When connected to a text to speech synthesizer, something that Google already does amazingly well, Meena could really come alive in our daily lives soon. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
394,Can Self-Driving Cars Learn Depth Perception?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. When we, humans look at an image, or a piece of video footage, such as this one, we all understand that this is just a 2D projection of the world around us. So much so, that if we have the time and patience, we could draw a depth map that describes the distance of each object from the camera. This information is highly useful, because we can use it to create real-time defocus effects for virtual reality and computer games, or even perform this Ken Burns effect in 3D, or in other words, zoom and pan around in a photograph, but, with a beautiful twist, because in the meantime, we can reveal the depth of the image. However, when we show the same images to a machine, all it sees is a bunch of numbers. Fortunately, with the ascendancy of neural network-based learning algorithms, we now have a chance to do this reasonably well. For instance, we discussed this depth perception neural network in an earlier episode, which was trained using large number input-output pairs, where the inputs are a bunch of images, and the outputs are their corresponding depth maps for the neural network to learn from. The authors implemented this with a random scene generator, which creates a bunch of these crazy configurations with a lot of occlusions and computes via simulation the appropriate depth map for them. This is what we call supervised learning, because we have all these input-output pairs. The solutions are given in the training set to guide the training of the neural network. This is supervised learning, machine learning with crutches. We can also use this depth information to enhance the perception of self-driving cars, but this application is not like previous two I just mentioned. It is much, much harder, because in the earlier, supervised learning example, we have trained the network in a simulation, and then, we also use it later in a computer game, which is, of course, another simulation. We control all the variables and the environment here. However, self-driving cars need to be deployed in the real world. These cars also generate a lot of video footage with their sensors, which could be fed back to the neural networks as additional training data…if we had the depth maps for them, which, of course, unfortunately, we don’t. And now, with this, we have arrived to the concept of unsupervised learning. Unsupervised learning is proper machine learning, where no crutches are allowed. We just unleash the algorithm on a bunch of data, with no labels, and if we do it well, the neural network will learn something useful from it. It is very convenient, because any video we have may be used as training data. That would be great. But we have a tiny problem, and that tiny problem is that that this sounds impossible. Or it may have sounded impossible, until this paper appeared. This work promises us no less than unsupervised depth learning from videos. Since this is unsupervised, it means that during training, all it sees is unlabeled videos from different viewpoints, and somehow, figures out a way to create these depth maps from it. So how is this even possible? Well, it is possible by adding just one ingenious idea. The idea is that since we don’t have the labels, we can’t teach the algorithm how to be right, but instead, we can teach it to be consistent. That doesn’t sound like much, does it? Well, it makes all the difference, because if we ask the algorithm to be consistent, it will find out that a good way to be consistent is to be right! While we are looking at some results, to make this clearer, let me add one more real-world example that demonstrates how cool this idea is. Imagine that you are a university professor overseeing an exam in mathematics, and someone tells you that for one of the problems, most of the students gave the same answer. If this is the case, there is good chance that this was the right answer. It is not a 100% chance that this is the case, but if most of the students have the same answer, it is much more unlikely that they all failed the same way. There are many different ways to fail, but there is only one way to succeed. Therefore, if there is consistency, often there is success. And this simple, but powerful thought leads to far-reaching conclusions. Let’s have a look at some more results! Wo-hoo! Now this is something. Let me explain why I am so excited for this. This is the input image, and this is the perfect depth map that is concealed from our beloved algorithm and is there for us to be able to evaluate its performance. These are two previous works, both use crutches, the first was trained via supervised learning by showing it input-output image pairs with depth maps, and does reasonably well, while the other one gets even less supervision, a worse crutch if you will, and it came up with this. Now, the unsupervised new technique was not given any crutches and came up with this. Holy mother of papers. It looks like a somewhat coarser, but still, very accurate version of the true depth maps. So what do you know! This neural network-based method just looks at unlabeled videos, and finds a way to create depth maps by not trying to be right, but trying to be consistent. This is one of those amazing papers where one simple, brilliant idea can change everything and make the impossible possible. What a time to be alive! What you see here is an instrumentation of this depth learning paper we have talked about, which was made by Weights and Biases. I think organizing these experiments really showcases the usability of their system. Also, Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
395,Everybody Can Make Deepfakes Now!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. It is important for you to know that everybody can make deepfakes now. You can turn your head around, mouth movements are looking great, and eye movements are also translated into the target footage. And, of course, as we always say, two more papers down the line, and it will be even better and cheaper than this. As you see, some papers are so well done, and are so clear, that they just speak for themselves. This is one of them. To use this technique, all you need to do is record a video of yourself, add just one image of the target subject, run this learning-based algorithm, and there you go. If you stay until the end of this video, you will see even more people introducing themselves as me. As noted, many important gestures are being translated, such as head, mouth and eye movement, but what’s even better, is that even full-body movement works. Absolutely incredible. Now, there are plenty of techniques out there that can create DeepFakes, many of which we have talked about in this series, so what sets this one apart? Well, one, most previous algorithms required additional information, for instance, facial landmarks or a pose estimation of the target subject. This one requires no knowledge of the image. As a result, this technique becomes so much more general. We can create high quality DeepFakes with just one photo of the target subject, make ourselves dance like a professional, and what’s more, hold on to your papers, because it also works on non-humanoid and cartoon models, and even that’s not all, we can even synthesize an animation of a robot arm by using another one as a driving sequence. So, why is it that it doesn’t need all this additional information? Well, if we look under the hood, we see that it is a neural-network based method that generates all this information by itself! It identifies what kind of movements and transformations are taking place in our driving video. You can see that the learned keypoints here follow the motion of the videos really well. Now, we pack up all this information, and send it over to the generator to warp the target image appropriately, taking into consideration possible occlusions that may occur. This means that some parts of the image may now be uncovered where we don’t know what the background should look like. Normally, we would do this by hand, with an image inpainting technique, for instance, you see the legendary PatchMatch algorithm here that does it, however, in this case, the neural network does it automatically, by itself! If you are seeking for flaws in the output, these will be important regions to look at. And it not only requires less information than previous techniques, but it also outperforms them…significantly. Yes, there is still room to improve this, for instance, the sudden head rotation here seems to generate an excessive amount of visual artifacts. The source code and even an example Colab notebook is available, I think it is one of the most accessible papers in this area. Don’t miss out and make sure to have a look in the video description, and try to run your own experiments! Let me know in the comments how they went or feel free to drop by at our discord server, where all of you Fellow Scholars are welcome to discuss ideas and learn together in a kind and respectful environment. The link is available in the video description, it is completely free, if you have joined, make sure to leave a short introduction! Now, of course, beyond the many amazing use-cases of this in reviving deceased actors, creating beautiful visual art, redubbing movies, and more, unfortunately, there are people around the world who are rubbing their palms together in excitement to use this to their advantage. So, you may ask, why make these videos on DeepFakes? Why spread this knowledge, especially now, with the source codes? Well, I think step number one is to make sure to inform the public that these DeepFakes can now be created quickly and inexpensively, and they don’t require a trained scientist anymore. If this can be done, it is of utmost importance that we all know about it! Then, beyond that, step number two, as a service to the public, I attend to EU and NATO conferences, and inform key political and military decision makers about the existence and details of these techniques to make sure that they also know about these and using that knowledge, they can make better decisions for us. You see me doing it here. …and again, you see this technique in action here to demonstrate that it works really well for video footage in the wild. Note that these talks and consultations all happen free of charge, and if they keep inviting me, I’ll keep showing up to help with this in the future as a service to the public. The cool thing is that later, over dinner, they tend to come back to me with a summary of their understanding of the situation, and I highly appreciate the fact that they are open to what we, scientists have to say. And now, please enjoy the promised footage. Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. It is important for you to know that everybody can make deepfakes now. You can turn your head around, mouth movements are looking great, and eye movements are also translated into the target footage. And, of course, as we always say, two more papers down the line, and it will be even better and cheaper than this. This episode has been supported by Weights & Biases. Here, they show you how you can use Sweeps, their tool to search through high-dimensional parameter spaces and find the best performing model. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
396,Is Visualizing Light Waves Possible?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Have you heard the saying that whenever we look into the mirror, strictly speaking, we don’t really see ourselves, but we see ourselves from the past…from a few nanoseconds ago. Is that true? If so, why? This is indeed true, and the reason for this is that the speed of light is finite, and it has to travel back from the mirror to our eyes. If you feel that this is really hard to imagine, you are in luck, because a legendary paper from 2013 by the name Femto-photography captured this effect. I would say it is safe to start holding on to your papers from this point basically until the end of this video. Here you can see a super high-speed camera capturing how a wave of light propagates through a bottle, most makes it through, and some gets absorbed by the bottle cap. But this means that this mirror example we talked about shall not only be a thought experiment, but we can even witness it ourselves. Yup, toy first, mirror image second. Approximately a nanosecond apart. So if someone says that you look old, you have an excellent excuse now. The first author of this work was Andreas Velten, who worked on this at MIT, and he is now a professor leading an incredible research group at the University of Wisconsin, Madison. But wait…since it is possible to create light transport simulations, in which we simulate the path of many-many millions of light rays to create a beautiful, photorealistic image, Adrián Jarabo thought that he would create a simulator that wouldn’t just give us the final image, but he would show us the propagation of light in a digital, simulated environment. As you see here, with this, we can create even crazier experiments because we are not limited to the real-world light conditions and the limitations of the camera. The beauty of this technique is just unparalleled. He calls this method transient rendering, and this particular work is tailored to excel at rendering caustic patterns. A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. I hope that you are not surprised when I say that it is the favorite phenomenon of most light transport researchers. Now, a word about these caustics. We need a super efficient technique to be able to pull this off. For instance, back in 2013, we showcased a fantastic scene made by Vlad Miller that was a nightmare to compute, and it took a community effort and more than a month to accomplish it. Beyond that, the transient renderer only uses very little memory, builds on the photon beams technique we talked about a few videos ago, and always arrives to a correct solution, given enough time. Bravo! And we can do all this, through the power of science. Isn’t it incredible? And if you feel a little stranded at home and are yearning to learn more about light transport, I held a Master-level course on light transport simulations at the Technical University of Vienna. Since I was always teaching it to a handful of motivated students, I thought that the teachings shouldn’t only be available for the privileged few who can afford a college education, but the teachings should be available for everyone. So, the course is now available free of charge for everyone, no strings attached, so make sure to click the link in the video description to get started. We write a full light simulation program from scratch there, and learn about physics, the world around us, and more. This episode has been supported by Weights & Biases. In this post, they show you how to build and track a simple neural network in Keras to recognize characters from the Simpsons series. You can even fork this piece of code and start right away. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
397,Muscle Simulation...Now In Real Time!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. We have showcased this paper just a few months ago, which was about creating virtual characters with a skeletal system, adding more than 300 muscles and teaching them to use these muscles to kick, jump, move around, and perform other realistic human movements. It came with really cool insights as it could portray how increasing the amount of weight to be lifted changes what muscles are being trained during a workout. These agents also learned to jump really high and you can see a drastic difference between the movement required for a mediocre jump and an amazing one. Beyond that, it showed us how these virtual characters would move if they were hamstrung by bone deformities, a stiff ankle, or muscle deficiencies and watch them learn to walk despite these setbacks. We could even have a look at the improvements after a virtual surgery takes place. So now, how about an even more elaborate technique that focuses more on the muscle simulation part? The ropes here are simulated in a way that the only interesting property of the particles holding them together is position. Cosserat rod simulations are an improvement because they also take into consideration the orientation of the particles, and hence, can simulate twists as well. And this new technique is called VIPER, and adds a scale property to these particles, and hence, takes into consideration stretching and compression. What does that mean? Well, it means that this can be used for a lot of muscle-related simulation problems that you will see in a moment. However, before that, an important part is inserting these objects into our simulations. The cool thing is that we don’t need to get an artist to break up these surfaces into muscle fibers. That would not only be too laborious, but of course, would also require a great deal of anatomical knowledge. Instead, this technique does all this automatically, a process that the authors call…viperization. So, in goes the geometry, and out comes a nice muscle model. This really opens up a world of really cool applications. For instance, one such application is muscle movement simulation. When attaching the muscles to bones, as we move the character, the muscles move and contract accurately. Two, it can also perform muscle growth simulations. And three, we get more accurate soft body physics. Or, in other words, we can animate gooey characters, like this octopus. Okay, that all sounds great, but how expensive is this? Do we have to wait a few seconds to minutes to get this? No, no, not at all! This technique is really efficient and runs in milliseconds, so we can throw in a couple more objects. And by couple, a computer graphics researcher always means a couple dozen more, of course. And in the meantime, let’s look carefully at the simulation timings! It starts from around 8-9 milliseconds per frame, and with all these octopi, we’re still hovering around 10 milliseconds per frame. That’s a hundred frames per second, which means that the algorithm scales with the complexity of these scenes really well. This is one of those rare papers that is written both very precisely, and it is absolutely beautiful. Make sure to have a look in the video description. The source code of the project is also available. With this, I hope that we’ll get even more realistic characters with real muscle models in our computer games and real-time applications. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
398,This AI Makes Audio Deepfakes!,"Dear Fellow Scholars, this is Two Minute Papers with this guy's name that is impossible to pronounce. My name is Dr. Károly Zsolnai-Fehér, and indeed, it seems that pronouncing my name requires some advanced technology. So what was this? I promise to tell you in a moment, but to understand what happened here, first, let’s have a look at this deepfake technique we showcased a few videos ago. As you see, we are at the point where our mouth, head, and eye movements are also realistically translated to a chosen target subject, and perhaps the most remarkable part of this work was that we don’t even need a video of this target person, just one photograph. However, these deepfake techniques mainly help us in transferring video content. So what about voice synthesis? Is it also as advanced as this technique we’re looking at? Well, let’s have a look at an example, and you can decide for yourself. This is a recent work that goes by the name Tacotron 2, and it performs AI-based voice cloning. All this technique requires is a 5-second sound sample of us, and is able to synthesize new sentences in our voice, as if we uttered these words ourselves. Let’s listen to a couple examples. Wow, these are truly incredible. The timbre of the voice is very similar, and it is able to synthesize sounds and consonants that have to be inferred because they were not heard in the original voice sample. And now, let’s jump to the next level, and use a new technique that takes a sound sample and animates the video footage as if the target subject said it themselves. This technique is called Neural Voice Puppetry, and even though the voices here are synthesized by this previous Tacotron 2 method that you heard a moment ago, we shouldn’t judge this technique by its audio quality, but how well the video follows these given sounds. Let’s go! If you decide to stay until the end of this video, there will be another fun video sample waiting for you there. Now, note that this is not the first technique to achieve results like this, so I can’t wait to look under the hood and see what’s new here. After processing the incoming audio, the gestures are applied to an intermediate 3D model, which is specific to each person since each speaker has their own way of expressing themselves. You can see this intermediate 3D model here, but we are not done yet, we feed it through a neural renderer, and what this does is apply this motion to the particular face model shown in the video. You can imagine the intermediate 3D model as a crude mask that models the gestures well, but does not look like the face of anyone, where the neural renderer adapts this mask to our target subject. This includes adapting it to the current resolution, lighting, face position and more, all of which is specific to what is seen in the video. What is even cooler is that this neural rendering part runs in real time. So, what do we get from all this? Well, one, superior quality, but at the same time, it also generalizes to multiple targets. Have a look here! And the list of great news is not over yet, you can try it yourself, the link is available in the video description. Make sure to leave a comment with your results! To sum up, by combining multiple existing techniques, it is important that everyone knows about the fact that we can both perform joint video and audio synthesis for a target subject. This episode has been supported by Weights & Biases. Here, they show you how to use their tool to perform faceswapping and improve your model that performs it. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
399,This Neural Network Learned To Look Around In Real Scenes! (NERF),"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. About two years ago, we worked on a neural rendering system, which would perform light transport on this scene and guess how it would change if we would change the material properties of this test object. It was able to closely match the output of a real light simulation program, and, it was near instantaneous as it took less than 5 milliseconds instead of the 40-60 seconds the light transport algorithm usually requires. This technique went by the name Gaussian Material Synthesis, and the learned quantities were material properties. But this new paper sets out to learn something more difficult, and also, more general. We are talking about a 5D neural radiance field representation. So what does this mean exactly? What this means is that we have 3 dimensions for location and two for view direction, or in short, the input is where we are in space and what are we looking at, and the resulting image of this view. So here, we take a bunch of this input data, learn it, and synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. And here, we are talking not only digital environments, but also, real scenes as well! Now that’s quite a value proposition, let’s see if it can live up to this promise! Wow! So good. Love it! But, what is it really that we should be looking at? What makes a good output here? The most challenging part is writing an algorithm that is able to reproduce delicate, high-frequency details while having temporal coherence. So what does that mean? Well, in simpler words, we are looking for sharp and smooth image sequences. Perfectly matte objects are easier to learn here because they look the same from all directions, while glossier, more reflective materials are significantly more difficult, because they change a great deal as we move our head around, and this highly variant information is typically not present in the learned input images. If you read the paper, you’ll see these referred to as non-Lambertian materials. The paper and the video contains a ton of examples of these view-dependent effects to demonstrate that these difficult scenes are handled really well by this technique. Refractions also look great. Now, if we define difficulty as things that change a lot when we change our position or view direction a little, not only the non-Lambertian materials are going to give us headaches, occlusion can be challenging as well. For instance, you can see here how well it handles the complex occlusion situation between the ribs of the skeleton here. It also has an understanding of depth, and this depth information is so accurate, that we can do these nice augmented reality applications where we put a new, virtual object in the scene and it correctly determines whether it is in front of, or behind the real objects in the scene. Kind of what these new iPads do with their LiDAR sensors, but without the sensor. As you see, this technique smokes the competition. So what do you know, entire real-world scenes can be reproduced from only a few views by using neural networks. And the results are just out of this world. Absolutely amazing. What you see here is an instrumentation of this exact paper we have talked about, which was made by Weights and Biases. I think organizing these experiments really showcases the usability of their system. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
400,"Sure, DeepFake Detectors Exist - But Can They Be Fooled?","Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. With the ascendancy of neural network-based learning algorithms, we are now able to take on, and defeat problems that sounded completely impossible just a few years ago. For instance, now, we can create deepfakes, or, in other words, we can record a short video of ourselves, and transfer our gestures to a target subject, and this particular technique is so advanced, that we don’t even need a video of our target, just one still image. So, we can even use paintings, images of sculptures, so yes, even the Mona Lisa works! However, don’t despair, it’s not all doom and gloom. A paper by the name FaceForensics contains a large dataset of original and manipulated video pairs. As this offered a ton of training data for real and forged videos, it became possible to use these to train a deepfake detector. You can see it here in action as these green to red colors showcase regions that the AI correctly thinks were tampered with. However, if we have access to a deepfake detector, we can also use it to improve our deepfake creating algorithms. And with this, an arms race has begun. The paper we are looking at today showcases this phenomenon. If you look here, you see this footage which is very visibly fake, and the algorithm correctly concludes that. Now, if you look at this video, which, for us, looks like if it were the same video, yet it suddenly became real, at least the AI thinks that, of course, incorrectly. This is very confusing. So what really happened here? To understand what is going on here, we first have to talk about ostriches. So what do ostriches have to do with this insanity? Let me try to explain that. An adversarial attack on a neural network can be performed as follows. We present such a classifier network with an image of a bus, and it will successfully tell us that yes, this is indeed a bus. Nothing too crazy here. Now, we show it not an image of a bus, but a bus plus some carefully crafted noise that is barely perceptible, that forces the neural network to misclassify it as an ostrich. I will stress that this is not any kind of noise, but the kind of noise that exploits biases in the neural network, which is, by no means trivial to craft. However, if we succeed at that, this kind of adversarial attack can be pulled off on many different kinds of images. Everything that you see here on the right will be classified as an ostrich by the neural network these noise patterns were crafted for. And, this can now be done not only on images, but videos as well, hence, what happened a minute ago is that the deepfake video has been adversarially modified with noise to bypass such a detector. If you look here, you see that the authors have chosen excellent examples, because some of these are clearly forged videos, which is initially recognized by the detector algorithm, but after adding the adversarial noise to it, the detector fails spectacularly. To demonstrate the utility of their technique, they have chosen the other examples to be much more subtle. Now, let’s talk about one more question. We are talking about a detector algorithm. But there is not one detector out there, there are many, and we can change the wiring of these neural networks to have even more variation. So what does it mean to fool a detector? Excellent question. The success rate of these adversarial videos indeed depends on the deepfake detector we are up against, but, hold on to your papers, because this success rate on uncompressed videos is over 98%, which is amazing, but, note that when using video compression, this success rate may drop to 58 to 92% depending on the detector. This means that video compression and some other tricks involving image transformations still help us in defending against these adversarial attacks. What I also really like about the paper is that it discusses white and black-box attacks separately. In the white-box case, we know everything about the inner workings of the detector, including the neural network architecture and parameters, this is typically the easier case. But, the technique also does really well in the black-box case, where we are not allowed to look under the hood of the detector, but we can show it a few videos and see how it reacts to them. This is a really cool work that gives us a more nuanced view about the current state of the art around deepfakes and deepfake detectors. I think it is best if we all know about the fact that these tools exist. If you wish to help us with this endeavor, please make sure to share this with your friends. Thank you. This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
401,This AI Learned to Summarize Videos,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Neural network-based learning algorithms are making great leaps in a variety of areas. And many of us are wondering whether it is possible that one day we’ll get a learning algorithm, show it a video, and ask it to summarize it, and we can then decide whether we wish to watch it or not? Or just describe what we are looking for and it would fetch the appropriate videos for us. I think today’s paper has a good pointer whether we can expect this to happen, and in a few moments, we’ll find out together why. A few years ago, these neural networks were mainly used for image classification, or in other words, they would tell us what kinds of objects are present in an image. But they are capable of so much more, for instance, these days, we can get a recurrent neural network write proper sentences about images, and it would work well for even highly non-trivial cases. For instance, it is able to infer that work is being done here, or that a ball is present in this image even if the vast majority of the ball itself is concealed. The even crazier thing about this is that this work is not recent at all, this is from a more than 4 year old paper! Insanity. The first author of this paper was Andrej Karpathy, one of the best minds in the game who is currently the director of AI at Tesla and works on making these cars able to drive themselves. So, as amazing as this work was, the progress in machine learning research keeps on accelerating, so let’s have a look at this newer paper that takes it a step further, and has a look at not an image, but a video, and explains what happens therein. Very exciting. Let’s have a look at an example! This was the input video, and let’s stop right at the first statement. The red sphere enters the scene. So, it was able to correctly identify not only what we are talking about in terms of color and shape, but also knows what this object is doing as well. That’s a great start. Let’s proceed further. Now, it correctly identifies the collision event with the cylinder, then this cylinder hits another cylinder, very good… and look at that. It identifies that the cylinder is made of metal, I like that a lot, because this particular object is made of a very reflective material, which shows us more about the surrounding room than the object itself. But we shouldn’t only let the AI tell us what is going on on its own terms let’s ask questions and see if it can answer them correctly. So, first, let’s ask what is the material of the last object that hit the cyan cylinder? And it correctly finds that the answer is Metal. Awesome. Now let’s take it a step further and stop the video here can it predict what is about to happen after this point? Look, it indeed can! This is remarkable because of two things. If we look under the hood, we see that to be able to pull this off, it not only has to understand what objects are present in the video and predict how they will interact, but also has to parse our questions correctly, put it all together, and form an answer based on all this information. If any of these tasks works unreliably, the answers will be incorrect. And two, there are many other techniques that are able to do some of these tasks, so why is this one particularly interesting? Well, look here! This new method is able to do all of these tasks at the same time. So there we go, if this improves further, we might become able to search Youtube videos by just typing something that happens in the video and it would be able to automatically find it for us. That would be absolutely amazing. What a time to be alive! This episode has been supported by Linode. Linode is the world’s largest independent cloud computing provider. Unlike entry-level hosting services, Linode gives you full backend access to your server, which is your step up to powerful, fast, fully configurable cloud computing. Linode also has One-Click Apps that streamline your ability to deploy websites, personal VPNs, game servers, and more. If you need something as small as a personal online portfolio, Linode has your back, and if you need to manage tons of client’s websites and reliably serve them to millions of visitors, Linode can do that too. What’s more, they offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. If only I had access to a tool like this while I was working on my last few papers! To receive $20 in credit on your new Linode account, visit linode.com/papers or click the link in the description and give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
402,This AI Creates Beautiful Time Lapse Videos,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. A few years ago, we have mainly seen neural network-based techniques being used for image classification. This means that they were able to recognize objects, for instance, animals and traffic signs in images. But today, with the incredible pace of machine learning research, we now have a selection of neural network-based techniques for not only classifying images, but, also, synthesizing them! The images that you see here and throughout this video is generated by one of these learning-based methods. But of course, in this series, we are always obsessed with artistic control, or, in other words, how much of a say we have in the creation of these images. After all, getting thousands and thousands of images without any overarching theme or artistic control is hardly useful for anyone. One way of being able to control the outputs is to use a technique that is capable of image translation. What you see here is a work by the name CycleGAN! It could transform apples into oranges, zebras into horses, and more. It was called CycleGAN because it introduced a cycle consistency loss function. This means that if we convert a summer image to a winter image, and then back to a summer image, we should get the same input image back, or at least, something very similar. If our learning system obeys this principle, the output quality of the translation is going to be significantly better. Today, we are going to study a more advanced image translation technique that takes this further. This paper is amazingly good at daytime image translation. It looks at a selection of landscape images, and then, as you see here, it learns to reimagine our input photos as if they were taken at different times of the day. I love how clouds form, and move over time in the synthesized images, and the night sky with the stars is also truly a sight to behold. But wait, CycleGAN and many other followup works did image translation, this also does image translation, so, what’s really new here? Well, one, this work proposes a novel upsampling scheme that helps creating output images with lots and lots of detail. Two, it can also create not just a bunch of images a few hours apart, but it can also make beautiful timelapse videos, where the transitions are smooth. Oh my goodness. I love this. And three, the training happens by shoveling 20 thousand landscape images into the neural network, and it becomes able to perform this translation task without labels. This means that we don’t have to explicitly search for all the daytime images and tell the learner that these are daytime images, and these other images are not. This is amazing because the algorithm is able to learn by itself, without labels, but it is also easier to use because we can feed in lots and lots more training data without having to label these images correctly. As a result, we now know that this daytime translation task is used as a testbed to demonstrate that this method can be reused for other kinds of image translation tasks. The fact that it can learn on its own and still compete with other works in this area is truly incredible. Due to this kind of generality, it can also perform other related tasks, for instance, it can perform style transfer, or in other words, not just change the time of day, but reimagine our pictures in the style of famous artists. I think with this paper, we have a really capable technique on our hands that is getting closer and closer to the point where they can see use in mainstream software packages and image editors. That would be absolutely amazing. If you have a closer look at the paper, you will see that it tries to minimize 7 things at the same time. What a time to be alive! This episode has been supported by Weights & Biases. Here, they show you how to build a proper Convolutional Neural Network for image classification, and how to visualize the performance of your model. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
403,Is Simulating Soft and Bouncy Jelly Possible?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. After reading a physics textbook on the laws of fluid motion, with a little effort, we can make a virtual world come alive by writing a computer program that contains these laws, resulting in beautiful fluid simulations like the one you see here. The amount of detail we can simulate with these programs is increasing every year, not only due to the fact that hardware improves over time, but also, the pace of progress in computer graphics research is truly remarkable. To simulate all these, many recent methods build on top of a technique called the Material Point Method. This is a hybrid simulation technique that uses both particles and grids to create these beautiful animations, however, when used by itself, we can come up with a bunch of phenomena that it cannot simulate properly. One such example is cracking and tearing phenomena, which has been addressed in a previous paper that we covered a few videos ago. With this, we can smash oreos, candy crabs, pumpkins, and much, much more. In a few minutes, I will show you how to combine some of these aspects of a simulation. It is going to be glorious…or, maybe, not so much! Just give me a moment and you’ll see! Beyond that, when using this material point method, coupling problems frequently arise. This means that the sand is allowed to have an effect on the fluid, but at the same time, as the fluid sloshes around, it also moves the sand particles within. This is what we refer to as two-way coupling. If it is implemented correctly, our simulated honey will behave as real honey in the footage here, and support the dipper. These are also not trivial to compute with the Material Point Method and require specialized extensions to do so. So what else is there to do? This amazing new paper provides an extension to handle simulating elastic objects, such as hair, rubber, and you will see that it even works for skin simulations, and it can handle their interactions with other materials. So, why is this useful? Well, we know that we can pull off simulating a bunch of particles, and a jello simulation separately, so it’s time for some experimentation! This is the one I promised earlier, so let’s try to put these two things together and see what happens. It seems to start our okay, particles are bouncing off of the jello…and then…uh-oh…look! Many of them seem to get stuck. So can we fix this somehow? Well, this, is where this new paper comes into play. Look here! It starts out somewhat similarly, most of the particles get pushed away from the jello, and then…look! Some of them indeed keep bouncing for a long-long time, and, none of them are stuck to the jello. Glorious! We can see the same phenomenon here with three jello blocks of different stiffness values. With this, we can also simulate more than 10 thousand bouncy hair strands, and to the delight of a computer graphics researcher, we can even throw snow into it and expect it to behave correctly. Braids work well too. And if you remember, I also promised some skin simulation. And this demonstration is not only super fun, for instance, the ones around this area are perhaps the most entertaining, but the information density of this screen is just absolutely amazing. As we go from bottom to top, you can see the effect of the stiffness parameters, or in other words, the higher we are, the stiffer things become, and as we go from left to right, the effect of damping increases. And you can see not only a bunch of combinations of these two parameters, but you can also compare many configurations against each other at a glance, on the same screen. Loving it. So how long does it take to simulate all this? Well, given that we are talking about an off-line simulation technique, this is not designed to run in real-time games, as the execution time is typically not measured in frames per second, but seconds per frame, and sometimes even minutes per frame. However, having run simulations that contained much fewer interactions than this that took me several days to compute, I would argue that these numbers are quite appealing for a method of this class. Also, note that this is one of those papers that makes the impossible possible for us, and of course, as we always say around here, two more papers down the line, and it will be significantly improved. For now, I am very impressed. Time to fire up some elaborate jello simulations! What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
404,What’s Inside a Neural Network?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This paper is not your usual paper, but it does something quite novel. It appeared in the Distill journal, one of my favorites, which offers new and exciting ways of publishing beautiful, but unusual works aiming for exceptional clarity and readability. And of course, this new paper is no different. It claims that despite the fact that these neural network-based learning algorithms look almost unfathomably complex inside, if we look under the hood, we can often find meaningful algorithms in there. Well, I am quite excited for this, so, sign me up! Let’s have a look at an example! At the risk of oversimplifying the explanation, we can say that a neural network is given as a collection of neurons and connections. If you look here, you see the visualization of three neurons. At first glance, they look like an absolute mess, don’t they? Well, kind of, but upon closer inspection, we see that there is quite a bit of structure here! For instance, the upper part looks like a car window, the next one resembles a car body, and the bottom of the third neuron clearly contains a wheel detector. However, no car looks exactly like these neurons, so what does the network do with all this? Well, in the next layer, the neurons arise as a combination of neurons in the previous layers, where we cherry pick parts of each neuron that we wish to use. So here, with red, you see that we are exciting the upper part of this neuron to get the window, use roughly the entirety of the middle one, and use the bottom part of the third one to assemble…this! And now, we have a neuron in the next layer that will help us detect whether we see a car in an image or not. So cool! I love this one! Let’s look at another example. Here you see a dog head detector, but it kind of looks like a crazy Picasso painting where he tried to paint a human from not one angle like everyone else, but from all possible angles on one image. But this is a neural network, so why engage in this kind of insanity? Well, if we have a picture of a dog, the orientation of the head of the dog can be anything. It can be a frontal image, look from the left to right, right to left, and so on. So this, is a pose invariant dog head detector! What this means is that it can detect many different orientations, and look here! You see that it gets very excited by all of these good boys. I think we even have a squirrel in here. Good thing this is not the only neuron we have in the network to make a decision! I hope that it already shows that this is truly an ingenious design. If you have a look at the paper in the video description, which you should absolutely do, you’ll see exactly how these neurons are built from the neurons in the previous layers. The article contains way, way more than this, you’ll see a lot more dog snouts, curve detectors, and even a followup article that you can have a look at and even comment on before it gets finished! A huge thank you to Chris Olah, who devotes his time away from research and uses his own money to run this amazing journal, I cannot wait to cover more of these articles in future episodes, so make sure to subscribe and hit the bell icon to never miss any of those. So, finally, we understand a little more how neural networks do all the amazing things they are able to do. What a time to be alive! This episode has been supported by Weights & Biases. Here, they show you how you can visualize the training process for your boosted trees with XGBoost using their tool. If you have a closer look, you’ll see that all you need is one line of code. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
405,Neural Network Dreams About Beautiful Natural Scenes,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In the last few years, the pace of progress in machine learning research has been staggering. Neural network-based learning algorithms are now able to look at an image and describe what’s seen in this image, or even better, the other way around, generating images from a written description. You see here a set of results from BigGAN, a state of the art image generation technique and marvel at the fact that all of these images are indeed synthetic. The GAN part of this technique abbreviates the term Generative Adversarial Network this means a pair of neural networks that battle each other over time to master a task, for instance, to generate realistic looking images when given a theme. After that, StyleGAN and even its second version appeared, which, among many other crazy good features, opened up the possibility to lock in several aspects of these images, for instance, age, pose, some facial features and more, and then, we could mix them with other images to our liking, while retaining these locked-in aspects. I am loving the fact that these newer research works are moving in the direction of more artistic control, and the paper we’ll discuss today also takes a step in this direction. With this new work, we can ask to translate our image into different seasons, weather conditions, time of day, and more! Let’s have a look! Here, we have our input, and imagine that we’d like to add more clouds, and translate it into a different time of the day, and…there we go! Wow. Or, we can take this snowy landscape image and translate it into a blooming flowery field. This truly seems like black magic, so I can’t wait to look under the hood and see what is going on! The input is our source image, and, a set of attributes where we can describe our artistic vision. For instance, here, let’s ask the AI to add some more vegetation to this scene. That will do! Step number one, this artistic description is routed to a scene generation network, which hallucinates an image that fits our description. Well, that’s great, as you see here, it kind of resembles the input image, but still, it is substantially different! So, why is that? If you look here, you see that it also takes the layout of our image as an input, or in other words, the colors and the silhouettes describe what part of the image contains a lake, vegetation, clouds, and more. It creates the hallucination according to that, so we have more clouds, that’s great, but the road here has been left out. So now, are we stuck with an image that only kind of resembles what we want. What do we do now? Now, step number two, let’s not use this hallucinated image directly, but, apply its artistic style to our source image. Brilliant! Now we have our content, but, with more vegetation. However, remember that we have the layout of the input image. That is a gold mine of information! So, are you thinking what I am thinking? Yes, including this indeed opens up a killer application. We can even change the scene around by modifying the labels on this layout, for instance, by adding some mountains, make it a grassy field, and add a lake. Making a scene from scratch from a simple starting point is also possible. Just add some mountains, trees, a lake, and you are good to go! And then, you can use the other part of the algorithm to transform it into a different season, time of day, or make it foggier. What a time to be alive! Now, as with every research work, there is still room for improvements! For instance, I find that it is hard to define what it means to have a cloudier image. For instance, the hallucination here works according to the specification, it indeed has more clouds than this. But, for instance, here, I am unsure if we have more clouds in the output you see that perhaps it is even less than in the input. It seems that not all of them made it to the final image. Also, do fewer, but denser clouds qualify as cloudier? Nonetheless, I think this is going to be an awesome tool as is, and I can only imagine how cool it will become two more papers down the line. This episode has been supported by Weights & Biases. In this post they show you how to easily iterate on models by visualizing and comparing experiments in real time. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
406,"Finally, A Blazing Fast Fluid Simulator!","Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. With the nimble progress we are seeing in computer graphics research, it is now not only possible to perform beautiful fluid simulations, but we can also simulate more advanced effects, such as honey coiling, ferrofluids climbing up on other objects, and a variety of similar advanced effects. However, due to the complexity of these techniques, we often have to wait for several seconds, or even minutes for every single one of these images, which often means that we have to leave our computer crunching these scenes overnight. Or even wait several days for the results to appear. But what about real-time applications? Can we perform these fluid simulations in a more reasonable timeframe? Well, this technique offers detailed fluid simulations, like the one here, and is blazing fast. The reason for this is that one, it uses a sparse volume representation and two, it supports parallel computation and runs on your graphics card. So what do these terms really mean? Let’s start with the sparse part. With classical fluid simulation techniques, the simulation domain has to be declared in advance and is typically confined to a cube. This comes with several disadvantages. For instance, if we wish to have a piece of fluid or smoke coming out of this cube, we are out of luck. The simulation domain stays, so we would have to know in advance how the simulation pans out, which we don’t. Now, the first thing you are probably thinking about, well, of course, make the simulation domain bigger! Yes, but unless special measures are taken, the bigger the domain, the more we have to compute. Even the empty parts take some computation! Ouch. This means that we have to confine the simulation to as small a domain as we can. So, this is where this technique comes into play the sparse representation that it uses means that the simulation domain can take any form, as you see here, it just starts altering the shape of the simulation domain as the fluid splashes out of it. Furthermore, we are not only not doing work in the empty parts of the domain, which is a huge efficiency increase, but we don’t need to allocate too much additional memory for these regions, which, you will see in a minute, will be a key part of the value proposition of this technique. We noted that it supports parallel computation and runs on your graphics card. The graphics card part is key, because otherwise, it would run on your processor, like most of the techniques that require “minutes per frame”. The more complex the technique is, typically, the more likely that it runs on your processor, which has a few cores to a few tens of cores. However, your graphics card, comparably, is almost a supercomputer as it has up to a few hundred, or, even a few thousand cores to compute on. So why not use that? Well, it’s not that simple, and here is where the word “parallel” is key. If the problem can be decomposed into smaller, independent problems, they can be allocated to many-many cores that can work independently, and much more efficiently. This, is exactly what this paper does with the fluid simulation. It runs it on your graphics card, and hence, it is typically 10 to 20 times faster than the equivalent techniques running on your processor. Let me try to demonstrate this with an example. Let’s talk about coffee. Making coffee is not a very parallel task. If you ask a person to make coffee, it can typically be done in a few seconds. However, if you suddenly put 30 people in the kitchen and ask them to make coffee, it not only will not be a faster process, but may even be slower than one person because of two reasons: one, it is hard to coordinate 30 people and there will be miscommunication, and two, there are very few tools, and lots of people, so they won’t be able to help each other, or much worse, will just hold each other up. If we could formulate the coffee making problem such that we need 30 units of coffee and we have 30 kitchens, we could just place one person into each kitchen and then, they could work efficiently and independently. At the risk of oversimplifying the situation, this is an intuition of what this technique does, and hence, it runs on your graphics card, and is incredibly fast. Also, note that your graphics card typically has a limited amount of memory, and, remember, we noted that the sparse representation makes it very gentle on memory usage, making this the perfect algorithm for creating detailed, large-scale fluid simulations quickly. Excellent design. I plan to post slowed down versions of some of the footage that you see here to our Instagram page, if you feel that it is something that you would enjoy, make sure to follow us there. Just search for Two Minute Papers on Instagram to find it, or also, as always, the link is in the video description. And, finally! Hold on to your papers, because if you look here, you see that the dam break scene can be simulated with about 5 frames per second, not seconds per frame, while the water drop scene can run at about 7 frames per second with a few million particles. We can, of course, scale up the simulation, and then, we are back at seconds per frame land, but it is still blazing fast. If you look here, we can go up to 27 times faster, so in one all-nighter simulation, I can simulate what I could simulate in nearly a month. Sign me up! What a time to be alive! Now, note that in the early days of Two Minute Papers, about 3-400 episodes ago, I covered plenty of papers of fluid simulations, however, nearly no one really showed up to watch them. Before publishing any of these videos, I was like “here we go again”, I knew that almost nobody would watch it, but this is a series where I set out to share the love for these papers. I believe we can learn a lot from these works, and if no one watches them, then so be it, I still love doing this. But, I was surprised to find out that over the years, something has changed. You Fellow Scholars somehow started to love the fluids, and I am delighted to see that. So, thank you so much for trusting the process, showing up and watching these videos. I hope you are enjoying watching these as much as I enjoyed making them. This episode has been supported by Weights & Biases. Here, they show you how to make it to the top of Kaggle leaderboards by using Weights & Biases to find the best model faster than everyone else. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
407,This AI Does Nothing In Games…And Still Wins!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, it is almost taken for granted that neural network-based learning algorithms are capable of identifying objects in images, or even write full, coherent sentences about them, but fewer people know that there is also parallel research on trying to break these systems. For instance, some of these image detectors can be fooled by adding a little noise to the image, and in some specialized cases, we can even perform something that is called the one pixel attack. Let’s have a look at some examples. Changing just this one pixel can make a classifier think that this ship is a car, or that this horse is a frog, and amusingly, be quite confident about its guess. Note that the choice of this pixel and the color is by no means random and it needs solving a mathematical optimization problem to find out exactly how to perform this. Trying to build better image detectors, while other researchers are trying to break them is not the only arms race we’re experiencing in machine learning research. For instance, a few years ago, DeepMind introduced an incredible learning algorithm that looked at the screen, much like a human would, but was able to reach superhuman levels in playing a few Atari games. It was a spectacular milestone in AI research. They also have just published a followup paper on this that we’ll cover very soon, so make sure to subscribe and hit the bell icon to not miss it when it appears in the near future. Interestingly, while these learning algorithms are being improved at a staggering pace, there is a parallel subfield where researchers endeavor to break these learning systems by slightly changing the information they are presented with. Let’s have a look at OpenAI’s example. Their first method adds a tiny bit of noise to a large portion of the video input, where the difference is barely perceptible, but it forces the learning algorithm to choose a different action than it would have chosen otherwise. In the other one, a different modification was used, that has a smaller footprint, but is more visible. For instance, in pong, adding a tiny fake ball to the game can coerce the learner into going down when it was originally planning to go up. It is important to emphasize that the researchers did not do this by hand. The algorithm itself is able to pick up game-specific knowledge by itself and find out how to fool the other AI using it. Both attacks perform remarkably well. However, it is not always true that we can just change these images or the playing environment to our desire to fool these algorithms. So, with this, an even more interesting question arises. Is it possible to just enter the game as a player, and perform interesting stunts that can reliably win against these AIs? And with this, we have arrived to the subject of today’s paper. This is the “You Shall Not Pass” game, where the red agent is trying to hold back the blue character and not let it cross the line. Here you see two regular AIs duking it out, sometimes the red wins, sometimes the blue is able to get through. Nothing too crazy here. This is the reference case which is somewhat well balanced. And now, hold on to your papers, because this adversarial agent that this new paper proposes, does this. You may think this was some kind of glitch, and I put the incorrect footage here by accident. No, this is not an error, you can believe your eyes, it basically collapses and does absolutely nothing. This can’t be a useful strategy, can it? Well, look at that! It still wins the majority of the time. This is very confusing. How can that be? Let’s have a closer look. This red agent is normally a somewhat competent player, as you can see here, it can punch the blue victim and make it fall. We now replaced this red player with the adversarial agent, which collapsed, and it almost feels like it hypnotized the blue agent to also fall. And now, squeeze your papers, because the normal red opponent’s winrate was 47% percent, and this collapsing chap wins 86% of the time. It not only wins, but it wins much, much more reliably than a competent AI. What is this wizardry? The answer is that the adversary induces off-distribution activations. To understand what that exactly means, let’s have a look at this chart. This tells us how likely it is that the actions of the AI against different opponents are normal. As you see, when this agent named Zoo plays against itself, the bars are in the positive region, meaning that normal things are happening. Things go as expected. However, that’s not the case for the blue lines, which are the actions when we play against this adversarial agent, in which case, the blue victim’s actions are not normal in the slightest. So, the adversarial agent is really doing nothing, but it is doing nothing in a way that reprograms its opponent to make mistakes and behave close to a completely randomly acting agent! This paper is absolute insanity. I love it! And if you look here, you see that the more the blue curve improves, the better this scheme works for a given game. For instance, it is doing real good on Kick and Defend, fairly good on Sumo Humans, and that there is something about the Sumo Ants game that prevents this interesting kind of hypnosis from happening. I’d love to see a followup paper that can pull this off a little more reliably. What a  time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
408,Can We Teach a Robot Hand To Keep Learning?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In 2019, researchers at OpenAI came up with an amazing learning algorithm that they deployed on a robot hand that was able to dexterously manipulate a Rubik’s cube…even when it was severely hamstrung. A good game plan to perform such a thing, is to first, solve the problem in a computer simulation where we can learn and iterate quickly, and then, transfer everything the agent learned there to the real world, and hope that it obtained general knowledge that indeed can be applied to real tasks. Papers like these are some of my favorites. If you are one of our core Fellow Scholars, you may also remember that we talked about walking robots about 200 episodes ago. In this amazing paper, we witnessed a robot not only learning to walk, but, it could also adjust its behavior and keep walking, even if one or multiple legs lose power, or get damaged. In this previous work, the key idea was to allow the robot to learn tasks such as walking not only in one, optimal way, but to explore and build a map of many alternative motions relying on different body parts. Both of these papers teach us that working in the real world often shows us new, unexpected challenges to overcome. And, this new paper offers a technique to adapt a robot arm to these challenges after it has been deployed into the real world. It is supposed to be able to pick up objects, which sounds somewhat simple these days…until we realize that new, previously unseen objects may appear in the bin with different shapes, or material models. For example, reflective and refractive objects are particularly perilous because they often show us more about their surroundings than about themselves, lighting conditions may also change after deployment, the gripper’s length or shape may change, and many, many other issues are likely to arise. Let’s have a look at the lighting conditions part. Why would that be such an issue? The objects are the same, the scene looks nearly the same, so, why is this a challenge? Well, if the lighting changes, the reflections change significantly, and since the robot arm sees its reflection and thinks that it is a different object, it just keeps trying to grasp it. After some fine-tuning, this method was able to increase the otherwise not too pleasant 32% success rate to 63%. Much, much better. Also, extending the gripper used to be somewhat of a problem, but as you see here, with this technique, it is barely an issue anymore. Also, if we have a somewhat intelligent system, and we move position of the gripper around, nothing really changes, so we would expect it to perform well. Does it? Well, let’s have a look! Unfortunately, it just seems to be rotating around without too many meaningful actions. And now, hold on to your papers, because after using this continual learning scheme, yes! It improved substantially and makes very few mistakes, and can even pick up these tiny objects that are very challenging to grasp with this clumsy hand. This fine-tuning step typically takes an additional hour, or at most a few hours of extra training, and can used to help these AIs learn continuously after they are deployed in the real world, thereby updating and improving themselves. It is hard to define what exactly intelligence is, but an important component of it is being able to reuse knowledge and adapt to new, unseen situations. This is exactly what this paper helps with. Absolute witchcraft. What a time to be alive! This episode has been supported by Linode. Linode is the world’s largest independent cloud computing provider. Unlike entry-level hosting services, Linode gives you full backend access to your server, which is your step up to powerful, fast, fully configurable cloud computing. Linode also has One-Click Apps that streamline your ability to deploy websites, personal VPNs, game servers, and more. If you need something as small as a personal online portfolio, Linode has your back, and if you need to manage tons of client’s websites and reliably serve them to millions of visitors, Linode can do that too. What’s more, they offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. If only I had access to a tool like this while I was working on my last few papers! To receive $20 in credit on your new Linode account, visit linode.com/papers or click the link in the description and give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
409,Two Shots of Green Screen Please!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. When shooting feature-length movies, or just trying to hold meetings from home through Zoom or Skype, we can make our appearance a little more professional by hiding the mess we have in the background by changing it to something more pleasing. Of course, this can only happen if we have an algorithm at hand that can detect what the foreground and background is, which, typically, is easiest when we have a green screen behind us that is easy to filter for even the simpler algorithms out there. However, of course, not everyone has a green screen at home, and even for people who do, may need to hold meetings out there in the wilderness. Unfortunately, this would mean that the problem statement is the exact opposite of what we’ve said, or in other words, the background is almost anything else but a green screen. So, it is possible to apply some of these newer neural-network based learning methods to tackle this problem? Well, this technique promises to make this problem much, much easier to solve. All we need to do is take two photographs, one with, and one without the test subject, and it will automatically predict an alpha matte that isolates the test subject from the background. If you have a closer look, you’ll see the first part of why this problem is difficult. This matte is not binary, so the final compositing process is not given as only foreground or only background for every pixel in the image, but there are parts, typically around the silhouettes and hair that need to be blended together. This blending information is contained in the grey parts of the image, and are especially difficult to predict. Let’s have a look at some results! You see the captured background here, and the input video below, and you see that it is truly a sight to behold. It seems that this person is really just casually hanging out in front of a place that is definitely not a whiteboard. It even works in cases where the background or the camera itself is slightly in motion. Very cool! It really is much, much better than these previous techniques, where you see that temporal coherence is typically a problem. This is the flickering that you see here, which arises from the vastly different predictions for the alpha matte between neighboring frames in the video. Opposed to previous methods, this new technique shows very little of that. Excellent! Now, we noted that a little movement in the background is permissible, but it really means just a little…if things get too crazy back there, the outputs are also going to break down. This wizardry all works through a generative adversarial network, in which one neural network generates the output results. This, by itself, didn’t work all that well, because images used to train this neural network can differ greatly from the backgrounds that we record out there in the wild. In this work, the authors bridged the gap by introducing a detector network, that tries to find faults in the output and tell the generator if it has failed to fool it. As the two neural networks duke it out, they work and improve together, yielding these incredible results. Note that there are plenty of more contributions in the paper, so please make sure to have a look for more details! What a time to be alive! What you see here is an instrumentation of this exact paper we have talked about, which was made by Weights and Biases. I think organizing these experiments really showcases the usability of their system. This episode has been supported by Weights & Biases. Here, they show you how you can use Sweeps, their tool to search through high-dimensional parameter spaces and find the best performing model. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
410,"Now We Can Relight Paintings…and Turns Out, Photos Too!","Dear fellow scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. When I was a bachelor student and took on my first bigger undertaking in computer graphics in 2011 it was a research project for a feature length movie where the goal was to be able to learn the brush strokes of an artist; you see the sample brush stroke here. And what it could do is change the silhouette of a digital 3D object to appear, as if it were drawn with this style. This way, we could use an immense amount of perfectly modeled geometry, and make them look as if they were drawn by an artist. The project was a combination of machine learning and computer graphics and got me hooked on this topic for life. So this was about silhouettes but what about being able to change the lighting? To address this problem, this new work promises something that sounds like science fiction. The input is a painting which is thought of as a collection of brush strokes. First, the algorithm is trying to break down the image into these individual strokes. Here on the left, with (a), you see the painting itself; and (b), is the real collection of strokes that were used to create it. This is what the algorithm is trying to estimate it with. And this colorful image visualizes the difference between the two. The blue color denotes regions where these brush strokes are estimated well; and we can find more differences as we transition into the red colored regions. So, great, now that we have a bunch of these brush strokes, but what do we do with them? Well let's add one more assumption into this system, which is: that the densely packed regions are going to be more effected by the lighting effects, while the sparser regions will be less impacted. This way, we can make the painting change as if we were to move our imaginary light source around. No painting skills or manual labor required. Wonderful. But, some of the skeptical fellow scholars out there would immediately ask the question: It looks great, but how do we know if this really is good enough to be used in practice? The authors thought of that too, and asked an artist to create some of these views by hand; and what do you know, they are extremely good. Very close to the real deal, and all this comes for free. Insanity. Now, we noted that the input for this algorithm is just one image. So, what about a cheeky experiment where we would add, not a painting, but a photo; and pretend that it is a painting, can it relight it properly? Well hold onto your papers and let's have a look. Here is the photo; the break down of the brush strokes, if this were a painting; and, wow! here are the lighting effects. It worked! and if you enjoyed these results, and would like to see more, make sure to have a look at this beautiful paper in the video description. For instance here, you see a comparison against previous works, and it seems quite clear that it smokes the competition, on a variety of test cases. And these papers they're comparing to are also quite recent. The pace of progress in computer graphics research is absolutely incredible; more on that in a moment. Also, just look at the information density here. This tiny diagram shows you exactly where the light source positions are; I remember looking at a paper on a similar topic that did not have this thing, and it made the entirety of the work a great deal more challenging to evaluate properly. This kind of attention to detail might seem like a small thing, but it makes all the difference for a great paper; which, this one is. The provided user study shows that these outputs can be generated within a matter of seconds, and reinforces our hunch that most people prefer the outputs of the new technique to the previous ones; so much improvement in so little time. With this, we can now create digital lighting effects from a single image, for paintings; and even photographs in a matter of seconds. What a time to be alive! What you see here, is an instrumentation of this exact paper we have talked about, which was made by Weights & Biases. I think organizing these experiments really showcases the usability of their system. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it's actively used in projects at prestigious labs, such as: OpenAI, Toyota Research, GitHub, and more. And, the best part is, that if you're an academic, or have an open source project, you can use their tools for free. (wandb.com/papers) It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long standing support, and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time."
411,DeepMind Made A Superhuman AI For 57 Atari Games!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Between 2013 and 2015, DeepMind worked on an incredible learning algorithm by the name Deep Reinforcement Learning. This technique looked at the pixels of the game, was given a controller and played much like a human would… with the exception that it learned to play some Atari games on a superhuman level. I have tried to train it a few years ago and would like to invite you for a marvelous journey to see what happened. When it starts learning to play an old game, Atari breakout, at first, the algorithm loses all of its lives without any signs of intelligent action. If we wait a bit, it becomes better at playing the game, roughly matching the skill level of an adept player. But here's the catch, if we wait for longer, we get something absolutely spectacular. Over time, it learns to play like a pro, and finds out that the best way to win the game is digging a tunnel through the bricks and hit them from behind. This technique is combination of a neural network that processes the visual data that we see on the screen, and a reinforcement learner that comes up with the gameplay-related decisions. This is an amazing algorithm, a true breakthrough in AI research. However, it had its own issues. For instance, it did not do well on Montezuma’s revenge or Pitfall because these games require more long-term planning. Believe it or not, the solution in a followup work was to infuse these agents with a very human-like property… curiosity. That agent was able to do much, much better at these games…and then got addicted to the TV. But that’s a different story. Note that this has been remedied since. And believe it or not, as impossible as it may sound, all of this has been improved significantly. This new work is called Agent57, and it plays better than humans, on all 57 Atari games. Absolute insanity. Let’s have a look at it in action and then in a moment, I’ll try to explain how it does what it does. You see Agent57 doing really well at the Solaris game here. This space battle game is one of the most impressive games on the Atari as it contains 16 quadrants, 48 sectors, space battles, warp mechanics, pirate ships, fuel management, and much more, you name it. This game is not only quite complex, but it also is a credit assignment nightmare for an AI to play. This credit assignment problem means that it can happen that we choose an action, and we only win or lose hundreds of actions later, leaving us with no idea as to which of our actions led to this win or loss, thus, making it difficult to learn from our actions. This Solaris game is a credit assignment nightmare. Let me try to bring this point to life by talking about school. In school, when we take an exam, we hand it in, and the teacher gives us feedback for every single one of our solutions and tells us whether we were correct or not. We know exactly where we did well, and what we need to practice to do better next time. Clear, simple, easy. Solaris, on the other hand, not so much! If this were a school project, the Solaris game would be a brutal, merciless teacher. Would you like to know your grade? No grades, but he tells you that you failed. Well, that’s weird, okay. Where did we fail? He won’t say. What should we do better next time to improve? You’ll figure it out bucko! Also, we wrote this exam 10 weeks ago, why do we only get to know about the results now? No answer. I think in this case, we can conclude that this would be a challenging learning environment even for a motivated human, so just imagine how hard it is for an AI! Hopefully this puts into perspective how incredible it is that Agent57 performs well on this game. It truly looks like science fiction. To understand what Agent57 adds to this, it was given something called a meta-controller that can decide when to prioritize short and long term planning. On the short term, we typically have mechanical challenges, like avoiding a skull in Montezuma’s revenge or dodging the shots of an enemy ship in Solaris. The long term part is also necessary to explore new parts of the game, and have a good strategic plan to eventually win the game. This is great because this new technique can now deal with the brutal and merciless teacher who we just introduced. Alternatively, this agent can be thought of someone who has a motivation to explore the game and do well at mechanical tasks at the same time and can also prioritize these tasks. With this, for the first time, scientists at DeepMind found a learning algorithm that exceeds human performance on all 57 Atari games. And please, do not forget about the fact that DeepMind tries to solve general intelligence, and then, use general intelligence to solve everything else. This is their holy grail. In other words, they are seeking an algorithm that can learn by itself and achieve human-like performance on a wide variety of tasks. There is still plenty to do, but, we are now one step closer to that. If you learn only one thing from this video, let it be the fact that there are not 57 different methods, but one general learning algorithm that plays 57 games better than humans. What a time to be alive! I would like to show you a short message from a few days ago that melted my heart. This I got from Nathan, who has been inspired by these incredible works and he decided to turn his life around, and go back to study more. I love my job, and reading messages like this is one of the absolute best parts of it. Congratulations Nathan and note that you can take this inspiration and greatness can materialize in every aspect of life, not only in computer graphics or machine learning research. Good luck! If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
412,Can We Make An Image Synthesis AI Controllable?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Not so long ago, we talked about a neural image generator that was able to dream up beautiful natural scenes. It had a killer feature where it would take as an input, not only the image itself, but the labeled layout of this image as well. That is a gold mine of information, and including this indeed opens up a killer application. Look! We can even change the scene around by modifying the labels on this layout, for instance, by adding some mountains, make a grassy field, and add a lake. Making a scene from scratch from a simple starting point was also possible with this technique. This is already a powerful, learning-based tool for artists to use as-is, but can we go further? For instance, would it be possible to choose exactly what to fill these regions with? And this, is what today’s paper excels at, and it turns out, it can do, much, much more. Let’s dive in. One, we can provide it this layout, which they refer to as a semantic mask, and it can synthesize clothes, pants, and hair in many, many different ways. Heavenly. If you have a closer look, you see that fortunately, it doesn’t seem to change any other part of the image. Nothing too crazy here, but please remember this, and now would be a good time to hold on to your papers, because two, it can change the sky or the material properties of the floor. And…wait! Are you seeing what I am seeing? We cannot just change the sky because we have a lake there, reflecting it, therefore, the lake has to change too. Does it? Yes, it does! It indeed changes other parts of the image when it is necessary, which is the hallmark of a learning algorithm that truly understands what it is synthesizing. You can see this effect especially clearly at the end of the looped footage when the sky is the brightest. Loving it. So what about the floor? This is one of my favorites! It doesn’t just change the color of the floor itself, but it performs proper material modeling. Look, the reflections also became glossier over time. A proper light transport simulation for this scenario would take a very, very long time, we are likely talking from minutes to hours. And this thing has never been taught about light transport and learned about these materials by itself! Make no mistake, these may be low-resolution, pixelated images, but this still feels like science fiction. Two more papers down the line, and we will see HD videos of this I am sure. The third application is something that the authors refer to as appearance mixture, where we can essentially select parts of the image to our liking and fuse these aspects together into a new image. This could, more or less be done with traditional, handcrafted methods too, but four, it can also do style morphing, where we start from image A, change it until it looks like image B, and back. Now, normally, this can be done very easily with a handcrafted method called image interpolation, however, to make this morphing really work, the tricky part is that all of the intermediate images have to be meaningful. And as you can see, this learning method does a fine job at that. Any of these intermediate images can stand on their own. I’ll try to stop the morphing process at different points so you can have a look and decide for yourself. Let me know in the comments if you agree. I am delighted to see that these image synthesis algorithms are improving at a stunning pace, and I think these tools will rapidly become viable to aid the work of artists in the industry. Thanks for watching and for your generous support, and I'll see you next time!"
413,DeepMind’s New AI Helps Detecting Breast Cancer,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. These days, we see so many amazing uses for learning-based algorithms, from enhancing computer animations, teaching virtual animals to walk, to teaching self-driving cars depth perception, and more. It truly feels like no field of science is left untouched by these new techniques, including the medical sciences. You see, in medical imaging, a common problem is that we have so many diagnostic images out there in the wild that it makes it more and more infeasible for doctors to look at all of them. What you see here is a work from scientists at DeepMind Health that we covered a few hundred episodes ago. The training part takes about 14 thousand optical coherence tomography scans, this is the OCT label you see on the the left, these images are cross sections of the human retina. We first start out with this OCT scan, then, a manual segmentation step follows, where a doctor marks up this image to show where the most relevant parts, like the retinal fluids or the elevations of retinal pigments are. After the learning process, this method can reproduce these segmentations really well by itself, without the doctor’s supervision, and you see here that the two images are almost identical in these tests. Now that we have the segmentation map, it is time to perform classification. This means that we look at this map and assign a probability to each possible condition that may be present. Finally, based on these, a final verdict is made whether the patient needs to be urgently seen, or just a routine check, or perhaps no check is required. This was an absolutely incredible piece of work. However, it is of utmost importance to evaluate these tools together with experienced doctors, and hopefully, on international datasets. Since then, in this new work, DeepMind has knocked the evaluation out of the park for a system they developed to detect breast cancer as early as possible. Let’s briefly talk about the technique, and then I’ll try to explain why it is sinfully difficult to evaluate it properly. So, onto the new problem. These mammograms contain four images that show the breasts from two different angles, and the goal is to predict whether the biopsy taken later will be positive for cancer or not. This is especially important because early detection is key for treating these patients. And the key question is, how does it compare to the experts? Have a look here. This is a case of cancer that was missed by all six experts in the study, but was correctly identified by the AI. And what about this one? This case didn’t work so well — it was caught by all six experts but was missed by the AI. So, one reassuring sample, and one failed sample. And with this, we have arrived to the central thesis of the paper, which asks the question, “what does it really take to say that an AI system surpassed human experts”? To even have a fighting chance in tackling this, we have to measure false positives and false negatives. A false positive means that the AI mistakenly predicts that the sample is positive, when in reality, it is negative. A false negative means that the AI thinks that the sample is negative, whereas it is positive in reality. The key is that in every decision domain, the permissible rates for false negatives and positives is different. Let me try to explain this through this example. In cancer detection, if we have a sick patient who gets classified as healthy is a grave mistake that can lead to serious consequences. But, if we have a healthy patient who is misclassified as sick, the positive cases get a second look from a doctor, who can easily identify the mistake. The consequences, in this case, are much less problematic, and can be remedied by spending a little time checking the samples that the AI was less confident about.The bottom line is that there are many different ways to interpret the data, and, it is by no means trivial to find out which one is the right way to do so. And now, hold on to your papers because here comes the best part. If we compare the predictions of the AI to the human experts, we see that the false positive cases in the US have been reduced by 5.7 percent, while the false negative cases have been reduced by 9.7%. That is the holy grail! That is the holy grail! We don’t need to consider the cost of false positives or negatives here, because it reduced false positives and false negatives at the same time. Spectacular! Another important detail is that these numbers came out of an independent evaluation. It means that the results did not come from the scientists who wrote the algorithm and have been thoroughly checked by independent experts who have no vested interest in this project. This is the reason why you see so many authors on this paper. Excellent. Another interesting tidbit is that the AI was trained on subjects from the UK, and the question was how well does this knowledge generalize for subjects from other places, for instance, the United States. Is this UK knowledge reusable in the US? I have been quite surprised by the answer, because it never saw a sample from anyone in the US, and still did better than the experts on US data. This is a very reassuring property, and I hope to see some more studies that show how general the knowledge is that these systems are able to obtain through training. And, perhaps the most important. If you remember one thing from this video, let it be the following. This work, much like most other AI-infused medical solutions are not made to replace human doctors. The goal is, instead, to empower them, and take off as much weight from their shoulders as possible. We have hard numbers for this, as the results concluded that this work reduces this workload of the doctors by 88%, which is an incredible result. Among other far-reaching consequences, I would like to mention that this would substantially help not only the work of doctors in wealthier, more developed countries, but it may single-handedly enable proper cancer detection in more developing countries who can not afford to check these scans. And note that in this video, we truly have just scratched the surface, whatever we talk about here in a few minutes cannot be a description as rigorous and accurate as the paper itself, so make sure to check it out in the video description. And with that, I hope you now have a good feel of the pace of progress in machine learning research. The retina fluid project was the state of the art in 2018, and now, less than two years later, we have a proper, independently evaluated AI-based detection for breast cancer. Bravo DeepMind. What a time to be alive! This episode has been supported by Linode. Linode is the world’s largest independent cloud computing provider. Unlike entry-level hosting services, Linode gives you full backend access to your server, which is your step up to powerful, fast, fully configurable cloud computing. Linode also has One-Click Apps that streamline your ability to deploy websites, personal VPNs, game servers, and more. If you need something as small as a personal online portfolio, Linode has your back, and if you need to manage tons of client’s websites and reliably serve them to millions of visitors, Linode can do that too. What’s more, they offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. If only I had access to a tool like this while I was working on my last few papers! To receive $20 in credit on your new Linode account, visit linode.com/papers or click the link in the description and give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
414,Is Style Transfer For Fluid Simulations Possible?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Style transfer is a technique in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to the super fun results that you see here. An earlier paper had shown that the more sophisticated ones can sometimes make even art curators think that they are real. This previous work blew me away as it could perform style transfer for smoke simulations. I almost fell out of the chair when I have first seen these results. It could do fire textures, starry night, you name it. It seems that it is able to do anything we can think of. Now let me try to explain two things. One, why is this so difficult, and two, the results are looking really good, so, are there any shortcomings? Doing this for smoke simulations is a big departure from 2D style transfer, because that one takes an image, where this works in 3D, and does not deal with images, but with density fields. A density field means a collection of numbers that describe how dense a smoke plume is at a given spatial position. It is a physical description of a smoke plume, if you will. So how could we possibly apply artistic style from an image to a collection of densities? The solution in this earlier paper was to first, downsample the density field to a coarser version, perform the style transfer there, and upsample this density field again with already existing techniques. This technique was called Transport-based Neural Style Transfer, TNST in short, please remember this. Now, let’s look at some results from this technique. This is what our simulation would look like normally, and then, all we have to do is show this image to the simulator, and, what does it do with it? Wow. My goodness. Just look at those heavenly patterns! So what does today’s new, followup work offer to us that the previous one doesn’t? How can this seemingly nearly perfect technique be improved? Well, this new work takes an even more brazen vantage point to this question. If style transfer on density fields is hard, then try a different representation. The title of the paper says Lagrangian style neural style transfer. So what does that mean? It means particles! This was made for particle-based simulations, which comes with several advantages. One, because the styles are now attached to particles, we can choose different styles for different smoke plumes, and they will remember what style they are supposed to follow. Because of this advantageous property, we can even ask the particles to change their styles over time, creating these heavenly animations. In these 2D examples, you also see how the texture of the simulation evolves over time, and that the elements of the style are really propagated to the surface and the style indeed follows how the fluid changes. This is true, even if we mix these styles together. Two, it not only provides us these high-quality results, but, it is fast. And by this, I mean blazing fast. You see, we talked about TNST, the transport-based technique approximately 7 months ago, and in this series, I always note that two more papers down the line, and it will be much, much faster. So here is the Two Minute Papers Moment of Truth, what do the timings say? For the previous technique, it said more than 1d. What could that 1D mean? Oh, goodness! This thing took an entire day to compute. So, what about the new one? What, really? Just one hour? That is insanity. So, how detailed of a simulation are we talking about? Let’s have a look together. M slash f means minutes per frame, and as you see, if we have tens of thousands of particles, we have 0.05, or in other words, three seconds per frame, and we can go up to hundreds of thousands, or even millions of particles, and end up around thirty seconds per frame. Loving it. Artists are going to do miracles with this technique, I am sure. The next step is likely going to be a real-time algorithm, which may appear as soon as one or two more works down the line, and you can bet your papers that I’ll be here to cover it, so make sure to subscribe, and hit the bell icon to not miss it when it appears. The speed of progress in computer graphics research is nothing short of amazing. Also, make sure to have a look at the full paper in the video description, not only because it is a beautiful paper and a lot of fun to read, but because you will also know what this regularization step here does exactly to the simulation. Thanks for watching and for your generous support, and I'll see you next time!"
415,This AI Controls Virtual Quadrupeds!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. If we have an animation movie or a computer game with quadrupeds, and we are yearning for really high-quality, lifelike animations, motion capture is often the go-to tool for the job. Motion capture means that we put an actor, in our case, a dog in the studio, we ask it to perform sitting, trotting, pacing and jumping, record its motion, and transfer it onto our virtual character. There are two key challenges with this approach. One, we have to try to weave together all these motions, because we cannot record all the possible transitions between sitting and pacing, jumping and trotting, and so on. We need some filler animations to make these transitions work. This was addressed by this neural network-based technique here. The other one is trying to reduce these unnatural foot sliding motions. Both of these have been addressed by learning-based algorithms in the previous works that you see here. Later, bipeds were also taught to maneuver through complex geometry, and sit in not one kind of chair, but any chair, regardless of geometry. This already sounds like science fiction. So, are we done, or can these amazing techniques be further improved? Well, we are talking about research, so the answer is, of course, yes! Here, you see a technique that reacts to its environment in a believable manner. It can accidentally step on the ball, stagger a little bit, and then flounder on this slippery surface, and it doesn’t fall! And it can do much, much more. The goal is that we would be able to do all this without explicitly programming all of these behaviors by hand. But, unfortunately, there is a problem. If we write an agent that behaves according to physics, it will be difficult to control properly. And this is where this new technique shines it gives us physically appealing motion, and we can grab a controller and play with the character like in a video game. The first step we need to perform is called imitation learning. This means looking at real, reference movement data and trying to reproduce it. This is going to be motion that looks great, is very natural, however, we are nowhere near done, because we still don’t have any control over this agent. Can we improve this somehow? Well, let’s try something and see if it works! This paper proposes that in step number two, we try an architecture by the name generative adversarial network. Here, we have a neural network that generates motion, and a discriminator that looks at these motions and tries to tell what is real, and what is fake. However, to accomplish this, we need lots of real and fake data, that we then use to train the discriminator to be able to tell which one is which. So how do we do that? Well, let’s try to label the movement that came from the user controller inputs as fake, and the reference movement data from before as real. Remember that this makes sense as we concluded that the reference motion looked natural. If we do this, over time, we will have a discriminator neural network that is able to look at a piece of animation data and tell whether it is real or fake. So, after doing all this work, how does this perform? Does this work? Well, sort of… but it does not react well if we try to control the simulation. If we let it run undisturbed, it works beautifully, and now, when we try to stop it with the controller…well, this needs some more work, doesn’t it? So, how do we adapt this architecture to the animation problem we have here? And here comes one of the key ideas of the paper. In step number three, we can rewire this whole thing to originate from the controller, and introduce a deep reinforcement learning-based fine tuning stage. This was the amazing technique that DeepMind used to defeat Atari Breakout. So what good does all this for us? Well, hold on to your papers, because it enables true user control, while synthesizing motion that is very robust against tough, previously unseen scenarios. And if you have been watching this series for a while, you know what is coming…of course, throwing blocks at it and see how well it can take the punishment. As you see, the AI is taking it like a champ. We can also add pathfinding to the agent, and, of course, being computer graphics researchers, throw some blocks into the mix for good measure. It performs beautifully. This is so realistic. We can also add sensors to the agent to allow them to navigate in this virtual world in a realistic manner. Just a note on how remarkable this is. So this quadruped behaves according to physics, lets us control it with a controller, which is already somewhat of a contradiction. And, it is robust against these perturbations at the same time. This is absolute witchcraft, and no doubt, it has earned to be accepted to SIGGRAPH, which is perhaps the most prestigious research venue in computer graphics. Congratulations. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
416,OpenAI’s Jukebox AI Writes Amazing New Songs,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, I will attempt to tell you a glorious tale about AI-based music generation. You see, there is no shortage of neural network-based methods that can perform physics simulations, style transfer, deepfakes, and a lot more applications where the training data is typically images, or video. If the training data for a neural network is in pure text, it can learn about that. If the training data is waveforms and music, it can learn that too! Wait, really? Yes! In fact, let’s look at two examples and then, dive into today’s amazing paper. In this earlier work by the name Look, Listen and Learn, two scientists at DeepMind set out to look at a large number of videos with sound. You see here that there is a neural network for processing the vision, and one for the audio information. That sounds great, but what are these heatmaps? These were created by this learning algorithm, and they show us, which part of the image is responsible for the sounds that we hear in the video. The hotter the color, the more sounds are expected from a given region. It was truly amazing that it didn’t automatically look for humans and colored them red in the heatmap there are cases where the humans are expected to be the source of the noise, for instance, in concerts, whereas in other cases, they don’t emit any noise at all. It could successfully identify these cases. This still feels like science fiction to me, and we covered this paper in 2017, approximately 250 episodes ago. You will see that we have come a long, long way since. We often say that these neural networks should try embody general learning concepts. That’s an excellent, and in this case, testable statement, so let’s go ahead and have a look under the hood of these vision and audio processing neural networks…and…yes, they are almost identical! Some parameters are not same because they have been adapted to the length and dimensionality of the incoming data, but the key algorithm that we run for the learning is the same. Later, in 2018 DeepMind published a followup work that looks at performances on the piano from the masters of the past and learns play in their style. A key differentiating factor here was that it did not do what most previous techniques do, which was looking at the score of the performance. These older techniques knew what to play, but not how to play these notes, and these are the nuances that truly make music come alive. This method learned from raw audio waveforms and thus, could capture much, much more of the artistic style. Let’s listen to it, and in the meantime, you can look at the composers it has learned from to produce these works. However, in 2019, OpenAI recognized that text-based music synthesizers can not only look at a piece of score, but can also continue it, thereby composing a new piece of music, and what’s more, they could even create really cool blends between genres. Listen as their AI starts out from the first 6 notes of a Chopin piece and transitions into a pop style with a bunch of different instruments entering a few seconds in. Very cool! The score-based techniques are a little lacking in nuance, but can do magical genre mixing and more, whereas the waveform-based techniques are more limited, but can create much more sophisticated music. Are you thinking what I am thinking? Yes, you have guessed right, hold on to your papers, because in OpenAI’s new work, they tried to fuse the two concepts together, or, in other words, take a genre, an artist, and even lyrics as an input, and it would create a song for us. Let’s marvel at these few curated samples together. The genre, artist and lyrics information will always be on the screen. Wow, I am speechless. Loved the AI-based lyrics too. This has the nuance of waveform-based techniques, with the versatility of the score-based methods. Glorious! If you look in the video description, you will find a selection of uncurated music samples as well. It does what it does by compressing the raw audio waveform into a compact representation. In this space, it is much easier to synthesize new patterns, after which, we can decompress it to get the output waveforms. It has also learned to group up and cluster a selection of artists which reflects how the AI thinks about them. There is so much cool stuff in here that it would be worthy of a video of its own. Note that it currently takes 9 hours to generate one minute of music, and the network was mainly trained on Western music and only speaks English, but you know, as we always say around here, two more papers down the line, and it will be improved significantly. I cannot wait to report on them should any followup works appear, so make sure to subscribe and hit the bell icon to not miss it. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
417,How Well Can DeepMind's AI Learn Physics?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. If you have been watching this series for a while, you know very well that I love learning algorithms and fluid simulations. But do you know what I like even better? Learning algorithms applied to fluid simulations, so I couldn’t be happier with today’s paper. We can create wondrous fluid simulations like the ones you see here by studying the laws of fluid motion from physics, and write a computer program that contains these laws. As you see, the amount of detail we can simulate with these programs is nothing short of amazing. However, I just mentioned neural networks. If we can write a simulator that runs the laws of physics to create these programs, why would we need learning-based algorithms? The answer is in this paper that we have discussed about 300 episodes ago. The goal was to show a neural network video footage of lots and lots of fluid and smoke simulations, and have it learn how the dynamics work, to the point that it can continue and guess how the behavior of a smoke puff would change over time. We stop the video and it would learn how to continue it, if you will. This definitely is an interesting take as normally, we use neural networks to solve problems that are otherwise close to impossible to tackle. For instance, it is very hard, if not impossible to create a handcrafted algorithm that detects cats reliably because we cannot really write down the mathematical description of a cat. However, these days, we can easily teach a neural network to do that. But this task here is fundamentally different. Here, the neural networks are applied to solve something that we already know how to solve. Especially given that if we use a neural network to perform this task, we have to train it, which is a long and arduous process. I hope to have convinced you that this is a bad, bad idea. Why would anyone bother to do that? Does this make any sense? Well, it does make a lot of sense! And the reason for that is that this training step only has to be done once, and afterwards, querying the neural network, that is, predicting what happens next in the simulation runs almost immediately. This takes way less time than calculating all the forces and pressures in the simulation while retaining high quality results. So, we suddenly went from thinking that an idea is useless to being amazing. What are the weaknesses of the approach? Generalization. You see, these techniques, including a newer variant that you see here can give us detailed simulations in real time or close to real time, but if we present them with something that is far outside of the cases that they had seen in the training domain, they will fail. This does not happen with our handcrafted techniques, only to AI-based methods. So, onwards to this new technique, and you will see in just a moment that a key differentiator here is that its generalization capabilities are just astounding. Look here. The predicted results match the true simulation quite well. Let’s look at it in slow motion too so we can evaluate it a little better. Looking great. But, we have talked about superior generalization, so what about that? Well, it can also handle sand and goop simulations, so that’s a great step beyond just water and smoke. And now, have a look at this one. This is a scene with the boxes it has been trained on. And now, let’s ask it to try to simulate the evolution of significantly different shapes. Wow. It not only does well with these previously unseen shapes, but it also handles their interactions really well. But there is more! We can also train it on a tiny domain with only a few particles, and then, it is able to learn general concepts that we can reuse to simulate a much bigger domain, and also, with more particles. Fantastic! But there is even more! We can train it by showing how water behaves on these water ramps, and then, let’s remove the ramps and see if it understands what it has to do with all these particles? Yes, it does! Now, let’s give it something more difficult. I want more ramps! Yes! And now, even more ramps! Yes! I love it! Let’s see if it can do it with sand too. Here is the ramp for training, and let’s try an hourglass now. Absolute witchcraft. And we are even being paid to do this. I can hardly believe this! The reason why you see so many particles in many of these views, is because if we look under the hood, we see that the paper proposes a really cool graph-based method that represents the particles and they can pass messages to each other over these connections between them. This leads to a simple, general and accurate model that truly is a force to be reckoned with. Now, this is a great leap in neural network-based physics simulations, but of course, not everything is perfect here. Its generalization capabilities have their limits. For instance, over longer timeframes, solids may get incorrectly deformed. However, I will quietly note that during my college years, I was also studying the beautiful Navier-Stokes equations and even as a highly motivated student, it took several months to understand the theory and write my first fluid simulator. You can check out the thesis and the source code in the video description if you are interested. And to see that these neural networks could learn something very similar in a matter of days… every time I think about this, shivers run down my spine. Absolutely amazing. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!"
418,Surprise Video With Our New Paper On Material Editing!,"Dear Fellow Scholars, this is not Two Minute Papers with Dr. Károly Zsolnai-Fehér, due to popular demand, this is a surprise video with the talk of our new paper that we just published. This was the third and last paper in my PhD thesis and hence, this is going to be a one-off video that is longer and a tiny bit more technical, I am keenly aware of it, but I hope you’ll enjoy it. Let me know in the comments when you have finished the video. And worry not, all the upcoming videos are going to be in the usual Two Minute Papers format. The paper and the source code are all available in the video description. And now, let’s dive in. In a previous paper, our goal was to populate this scene with over a hundred materials with a learning-based technique, and create a beautiful planet with rich vegetation. The results looked like this. One of the key elements to accomplish this was to use a neural renderer, or in other words, a decoder network that you see here which took a material shader description as an input and predicted its appearance, thereby replacing the renderer we used in the project. It had its own limitations, for instance, it was limited to this scene with a fixed lighting setup, and only the material properties were subject to change. But in return, it mimiced the global illumination renderer rapidly and faithfully. And, in this new work, our goal was to take a different vantage point, and help artists with general image processing knowledge to perform material synthesis. Now, this sounds a little nebulous, so let me explain. One of the key ideas is to achieve this with a system that is meant to take images from its own renderer, like the ones you see here. But, of course, we produced these ourselves, so obviously, we know how to do it, so this is not very useful yet. However, the twist is that we only start out with an image of this source material, and then, load it into a raster image editing program like Photoshop, and edit it to our liking, and just pretend that this is achievable with our renderer. As you see, many of these target images in the middle are results of poorly-executed edits. For instance, the stitched specular highlight in the first example isn’t very well done, and neither is the background of the gold target image in the middle. However, in the next step, our method proceeds to find a photorealistic material description that, when rendered, resembles this target image, and works well even in the presence of these poorly executed edits. The whole process executes in 20 seconds To produce a mathematical formulation for this problem, we started with this. We have an input image t, and edit it to our liking to get the target image, t with a tilde. Now, we are looking for a shader parameter set x, that, when rendered with the phi operator, approximates the edited image. The constraint below stipulates that we should remain within the physical boundaries for each parameter, for instance, albedos between 0 and 1, proper indices of refraction and so on. So how do we deal with phi? We use the previously mentioned neural renderer to implement it, otherwise this optimization process would take 25 hours. Later, we made an equivalent, unconstrained reformulation of this problem to be able to accommodate a greater set of optimizers. This all sounds great on paper, and works reasonably well for materials that can be exactly matched with this shader, like this one. This optimizer-based solution can achieve it reasonably well. But, unfortunately, for more challenging cases, as you see the target image on the lower right, the optimizer’s output leaves much to be desired. Note again that the result on the upper right is achievable with the shader, while the lower right is a challenging imaginary material that we are trying to achieve. The fact that this is quite difficult is not a surprise because we have non-linear and non-convex optimization problem, which is also high-dimensional. So, this optimization solution is also quite slow, but it can start inching towards the target image. As an alternative solution, we also developed something that we call an inversion network, this addresses the adjoint problem of neural rendering, or in other words, we show it the edited input image, and out comes the shader that would produce this image. We have trained 9 different neural network architectures for this problem, which sounds great, so how well did it work? Well, we found out that none of them are really satisfactory for more difficult edits because all of the target images are far-far outside of the training domain. We just cannot prepare the networks to be able to handle the rich variety of edits that come from the artists. However, some of them are, one could say, almost usable, for instance, number one and five are not complete failures. And note that these solutions are provided instantly. So, we have two techniques, none of them are perfect for our task: a fast and approximate solution with the inversion networks, and a slower optimizer that can slowly inch towards the target image. Our key insight here is that we can produce a hybrid method that fuses the two solutions together. The workflow goes as follows: we take an image of the initial source material, and edit it to our liking to get this target image. Then, we create a coarse prediction with a selection of inversion networks, to initialize the optimizer with the prediction of one of these neural networks. Preferably a good one so the optimizer can start out from a reasonable initial guess. So, how well does this hybrid method work? I’ll show you in a moment. Here, we start out with an achievable target image, and then, try two challenging image editing operations. This image can be reproduced perfectly as long as the inversion process works reliably. Unfortunately, as you see here, this is not the case. In the first row, using the optimizer and the inversion networks separately, we get results that fail to capture the specular highlight properly. In the second row, we have deleted the specular highlight on the target image on the right and replaced it with a completely different one. I like to call this the FrankenBRDF and it would be amazing if we could do this, but unfortunately, both the optimizer and the inversion networks flounder. Another thing that would be really nice to do is deleting the specular highlight and filling the image via image inpainting. This kind of works with the optimizer, but you’ll see in a moment that it’s not nearly as good as it could be. And now, if you look carefully, you see that our hybrid method outperforms both of these techniques in each of the three cases. In the paper, we report results on a dozen more cases as well. We make an even stronger claim in the paper, where we say that these results are close to the global optimum. You see the results of this hybrid method if you look at the intersection of Nelder-Mead and NN, they are highlighted with the red ellipses. The records in the table show the RMS errors and are subject to minimization. With this, you see that this goes neck and neck with a global optimizer, which is highlighted with green. In summary, our technique runs in approximately 20 seconds, works for specular highlight editing, image blending, stitching, inpainting and more. The proposed method is robust, works even in the presence of poorly edited images, and can be easily deployed in already existing rendering systems and allows for rapid material prototyping for artists working in the industry. It is also independent of the underlying principled shader, so you can also add your own and expect it to work well as long as the neural renderer works reliably. A key limitation of the work is that it only takes images in this canonical scene with the carved sphere material sample, but we conjecture that it can be extended to be more general and propose a way to do it in the paper. Make sure to have a closer look if you are interested. The teaser image of this paper is showcased in the 2020 Computer Graphics Forum cover page. The whole thing is also quite simple to implement, and we provide the source code, pre-trained networks on our website, all of them are under a permissive license. Thank you so much for watching this and a big thanks to Peter Wonka and Michael Wimmer for advising this work."
419,NVIDIA’s AI Recreated PacMan!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Neural network-based learning methods are capable of wondrous things these days. They can do classification, which means that they look at an image, and the output is a decision, whether we see a dog or a cat, or a sentence that describes an image. In the case of DeepMind’s AI playing Atari games, the input is the video footage of the game, and the output is an action that decides what we do with our character next. In OpenAI’s amazing Jukebox paper, the input was a style of someone, a music genre, and lyrics, and the output was a waveform, or in other words, a song we can listen to. But, a few hundred episodes ago we covered a paper from 2015, where scientists at DeepMind asked the question, what if we would get these neural networks to output not sentences, decisions, waveforms or any of that sort… what if the output would be a computer program? Can we teach a neural network programming? I was convinced that the answer is no, until I saw these results. So what is happening here? The input is a scratch pad where we are performing multi-digit addition in front of the curious eyes of the neural network. And if it has looked for long enough, it was indeed able to produce a computer program that could, eventually, perform addition. It could also perform sorting, and would even be able to rotate the images of these cars into a target position. It was called a neural programmer-interpreter, and of course, it was slow and a bit inefficient, but no matter, because it could finally make something previously impossible possible. That is an amazing leap. So, why are we talking about this work from 2015? Well, apart from the fact that there are many amazing works that are timeless, and this is one of them, in this series, I always say, two more papers down the line, and it will be improved significantly. So here is the Two Minute Papers Moment of Truth. How has this area improved with this followup work? Let’s have a look at this paper from scientists at NVIDIA that implements a similar concept for computer games. So how is that even possible? Normally, if we wish to write a computer game, we first, envision the game in our mind, then, we sit down and do the programming. But this new paper does this completely differently. Now, hold on to your papers, because this is a neural network-based method, that first, looks at someone playing the game, and then, it is able to implement the game so that it not only looks like it, but it also behaves the same way to our keypresses. You see it at work here. Yes, this means that we can even play with it and it learns the internal rules of the game and the graphics, just by looking at some gameplay. Note that the key part here is that we are not doing any programming by hand the entirety of the program is written by the AI. We don’t need access to the source code or the internal workings of the game, as long as we can just look at it, it can learn the rules. Everything truly behaves as expected, we can even pick up the capsule and eat the ghosts as well. This sounds like science fiction, and we are not nearly done yet! There are additional goodies. It has memory and uses it consistently, or, in other words, things don’t just happen arbitrarily. If we return to a state of the game that we visited before, it will remember to present us with very similar information. It also has an understanding of foreground and background, dynamic and static objects as well, so we can experiment with replacing these parts, thereby reskinning our games. It still needs quite a bit of data to perform all this as it has looked at approximately 120 hours of footage of the game being played, however, now, something is possible that was previously impossible. And of course, two more papers down the line, this will be improved significantly I am sure. I think this work is going to be one of those important milestones that remind us that many of the things that we had handcrafted methods for will, over time, be replaced with these learning algorithms. They already know the physics of fluids, or in other words, they are already capable of looking at videos of these simulations and learn the underlying physical laws, and they can demonstrate having learned general knowledge of the rules by being able to continue these simulations, even if we change the scene around quite a bit. In light transport research, we also have decades of progress in simulating how rays of light interact with scenes and we can create these beautiful images. Parts of these algorithms, for instance, noise filtering are already taken over by AI-based techniques, and I can’t help but feel that a bigger tidal wave is coming. This tidal-wave will be an entirely AI-driven technique that will write the code for the entirety of the system. Sure the first ones will be limited, for instance, this is a neural renderer from one of our papers that is limited to this scene and lighting setup, but you know the saying, two more papers down the line, and it will be an order of magnitude better. I can’t wait to tell all about it to you with a video when this happens. Make sure to subscribe and hit the bell icon to not miss any followup works. Goodness, I love my job. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
420,An AI Made All of These Faces!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today I am going to try to tell you the glorious tale of AI-based human face generation and showcase an absolutely unbelievable new paper in this area. Early in this series, we covered a stunning paper that showcased a system that could not only classify an image, but write a proper sentence on what is going on, and could cover even highly non-trivial cases. You may be surprised, but this thing is not recent at all. This is 4 year old news! Insanity. Later, researchers turned this whole problem around, and performed something that was previously thought to be impossible. They started using these networks to generate photorealistic images from a written text description. We could create new bird species by specifying that it should have orange legs and a short yellow bill. Then, researchers at NVIDIA recognized and addressed two shortcomings: one was that the images were not that detailed, and two, even though we could input text, we couldn’t exert too much artistic control over the results. In came StyleGAN to the rescue, which was then able to perform both of these difficult tasks really well. Furthermore, there are some features that are highly localized as we exert control over these images, you can see how this part of the teeth and eyes were pinned to a particular location and the algorithm just refuses to let it go, sometimes to the detriment of its surroundings. A followup work titled StyleGAN2 addresses all of these problems in one go. So, StyleGAN2 was able to perform near-perfect synthesis of human faces, and remember, none of these people that you see here really exist. Quite remarkable. So, how can we improve this magnificent technique? Well, this new work can do so many things, I don’t even know where to start. First, and most important, we now have much much more intuitive artistic control over the output images. We can add or remove a beard, make the subject younger or older, change their hairstyle, make their hairline recede, put a smile on their face, or even make their nose pointier. Absolute witchcraft. So why can we do all this with this new method? The key idea is that it is not using a Generative Adversarial Network, a GAN, in short. A GAN means two competing neural networks, where one is trained to generate new images, and the other one is used to tell whether the generated images are real or fake. GANs dominated this field for a long while because of their powerful generation capabilities, but, on the other hand, they are quite difficult to train and we have only limited control over its output. Among other changes, this work disassembles this generator network into F and G, and the discriminator network into E and D, or in other words, adds an encoder and decoder network here. Why? The key idea is that the encoder compresses the image data down into a representation that we can edit more easily. This is the land of beards and smiles, or in other words, all of these intuitive features that we can edit exist here, and when we are done, we can decompress the output with the decoder network and produce these beautiful images. This is already incredible, but what else can we do with this new architecture? A lot more. For instance, two, if we add a source and destination subjects, their coarse, middle, or fine styles can also be mixed. What does that mean exactly? The coarse part means that high-level attributes, like pose, hairstyle and face shape will resemble the source subject, in other words, the child will remain a child and inherit some of the properties of the destination subjects. However, as we transition to the “fine from source” part, the effect of the destination subject will be stronger, and the source will only be used to change the color scheme and microstructure of this image. Interestingly, it also changes the background of the subject. Three, it can also perform image interpolation. This means that we have these four images as starting points, and it can compute intermediate images between them. You see here that as we slowly become Bill Gates, somewhere along the way, glasses appear. Now note that interpolating between images is not difficult in the slightest and has been possible for a long-long time — all we need to do is just compute average results between these images. So what makes a good interpolation process? Well, we are talking about good interpolation, when each of the intermediate images make sense and can stand on their own. I think this technique does amazingly well at that. I’ll stop the process at different places, you can see for yourself and let me know in the comments if you agree or not. I also kindly thank the authors for creating more footage just for us to showcase in this series. That is a huge honor, thank you so much! And note that StyleGAN2 appeared around December of 2019, and now, this paper by the name “Adversarial Latent Autoencoders” appeared only four months later. Four months later. My goodness! This is so much progress in so little time it truly makes my head spin. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
421,How Do Neural Networks Learn?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. We have recently explored a few new neural network-based learning algorithms that could perform material editing, physics simulations, and more. As some of these networks have hundreds of layers, and often thousands of neurons within these layers, they are almost unfathomably complex. At this point, it makes sense to ask, can we understand what is going on inside these networks? Do we even have a fighting chance? Luckily, today, visualizing the inner workings of neural networks is a research subfield of its own, and the answer is, yes, we learn more and more every year. But there is also plenty more to learn. Earlier, we talked about a technique that we called activation maximization, which was about trying to find an input that makes a given neuron as excited as possible. This gives us some cues as to what the neural network is looking for in an image. A later work that proposes visualizing spatial activations gives us more information about these interactions between two, or even more neurons. You see here with the dots that it provides us a dense sampling of the most likely activations, and, this leads to a more complete bigger-picture view of the inner workings of the neural network. This is what it looks like if we run it on one image. It also provides us with way more extra value, because so far, we have only seen how the neural network reacts to one image, but this method can be extended to see its reaction to not one, but one million images! You can see an example of that here. Later, it was also revealed that some of these image detector networks can assemble something that we call a pose invariant dog head detector! What this means is that it can detect a dog head in many different orientations, and…look! You see that it gets very excited by all of these good boys…plus, this squirrel. Today’s technique offers us an excellent tool to look into the inner workings of a convolutional neural network, a learning method that is very capable of image-related operations, for instance, image classification. The task here is that we have an input image of a mug or a red panda, and the output should be a decision from the network that yes, what we are seeing is indeed a mug or a panda or not. They apply something that we call a convolutional filter over an image which tries to find interesting patterns that differentiate objects from each other. You can see how the outputs are related to the input image here. As you see, the neurons in the next layer will be assembled as a combination of the neurons from the previous layer. When we use the term deep learning, we typically refer to neural networks that have two or more of these inner layers. Each subsequent layer is built by taking all the neurons in the previous layer, which select for the features relevant to what the next neuron represents, for instance, the handle of the mug and inhibits everything else. To make this a little clearer, this previous work tried to detect whether we have a car in an image by using these neurons. Here, the upper part looks like a car window, the next one resembles a car body, and the bottom of the third neuron clearly contains a wheel detector. This is the information that the neurons in the next layer are looking for. In the end, we make the final decision as to whether this is a panda or a mug by adding up all the intermediate results, the bluer this part is, the more relevant this neuron is in the final decision. Here, the neural network concludes that this doesn’t look like a lifeboat or a ladybug at all, but it looks like pizza. If we look at the other sums, we see that the school bus and orange are not hopeless candidates, but still, the neural network does not have much doubt whether this is a pizza or not. And, the best part is that you can even try it yourself in your browser if you click the link in the video description, run these simulations, and even upload your own image. Make sure that you upload or link something that belongs to one of these classes on the right to make this visualization work. So, clearly, there is plenty more work to do for us to properly understand what is going on under the hood of neural networks, but I hope this quick rundown showcased how many facets there are to this neural network visualization subfield and how exciting it is. Make sure to post your experience in the comments section whether the classification worked well for you or not. And if you wish to see more videos like this, make sure to subscribe and hit the bell icon to not miss future videos. Thanks for watching and for your generous support, and I'll see you next time!"
422,Amazing AR Effects Are Coming!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. When we, humans look at an image, or a piece of video footage, we understand the geometry of the objects in there so well, that if we have the time and patience, we could draw a depth map that describes the distance of each object from the camera. This goes without saying. However, what does not go without saying, is that if we could teach computers to do the same, we could do incredible things. For instance, this learning-based technique creates real-time defocus effects for virtual reality and computer games, and this one performs this Ken Burns effect in 3D, or in other words, zoom and pan around in a photograph, but, with a beautiful twist, because in the meantime, it also reveals the depth of the image. With this data, we can even try to teach self-driving cars about depth perception to enhance their ability to navigate around safely. However, if you look here, you see two key problems: one, it is a little blurry, and there are lots of fine details that it couldn’t resolve, and, it is flickering. In other words, there are abrupt changes from one image to the next one, which shouldn’t be there as the objects in the video feed are moving smoothly. Smooth motion should mean smooth depth maps, and it is getting there, but it still is not the case here. So, I wonder, if we could teach a machine to perform this task better? And more importantly, what new wondrous things can we do if we pull this off? This new technique is called Consistent Video Depth Estimation, and it promises smooth and detailed depth maps that are of much higher quality than what previous works offer. And, now, hold on to your papers, because finally, these maps contain enough detail to open up the possibility of adding new objects to the scene, or even flood the room with water, or add many other, really cool video effects. All of these will take the geometry of the existing real-world objects, for instance, cats into consideration. Very cool! The reason why we need such a consistent technique to pull this off, is because if we have this flickering in time that we’ve seen here, then, the depth of different objects suddenly bounces around over time, even for a stationary object. This means, that in one frame, the ball would be in front of the person, when in the next one, it would suddenly think that it has to put the ball behind them, and then, in the next one, front again, creating a not only jarring, but quite unconvincing animation. What is really remarkable is that due to the consistency of the technique, none of that happens here. Love it! Here are some more results where you can see that the outlines of the objects in the depth map are really crisp, and follow the changes really well over time. The snowing example here is one of my favorites, and it is really convincing. However, there are still a few spots where we can find some visual artifacts. For instance, as the subject is waving, there is lots of fine, high-frequency data around the fingers there, and if you look at the region behind the head closely, you find some more issues, or you can find that some balls are flickering on the table as we move the camera around. Compare that to previous methods that could not do nearly as good as this, and now, we have something that is quite satisfactory. I can only imagine how good this will get to more papers down the line. And for the meantime, we’ll be able to run these amazing effects even without having a real depth camera. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
423,This is Geometry Processing Made Easy!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. I think this might be it. This paper is called Monte Carlo Geometry Processing, and in my opinion, this may one of the best, if not the best computer graphics paper of the year. There will be so many amazing results, I first thought this one paper could be the topic of ten Two Minute Papers videos, but I will attempt to cram everything into this one video. It is quite challenging to explain, so please bear with me, I’ll try my best. To even have a fighting chance at understanding what is going on here, first, we start out one of the most beautiful topics in computer graphics, which is none other than light transport. To create a beautiful light simulation, we have to solve something that we call the rendering equation. For practical cases, it is currently impossible to solve it like we usually solve any other equation. However, what we can do, is that we can choose a random ray, simulate its path as it bounces around in the scene and compute how it interacts with the objects and material within this scene. As we do it with more and more rays, we get more information about what we see in the scene, that’s good, but, look, it is noisy. As we add even more rays, this noise slowly evens out, and in the end, we get a perfectly clean image. This takes place with the help of a technique that we call Monte Carlo integration, which involves randomness. At the risk of oversimplifying the situation, in essence, this technique says that we cannot solve the problem, but we can take samples from the problem, and if we do it in a smart way, eventually, we will be able to solve it. In light transport, the problem is the rendering equation, which we cannot solve, but we can take samples, one sample is simulating one ray. If we have enough rays, we have a solution. However, it hasn’t always been like this. Before Monte Carlo integration, light transport was done through a technique called Radiosity. The key issue with Radiosity was that the geometry of the scene had to be sliced up into many small pieces, and the light scattering events had to be evaluated between these small pieces. It could not handle all light transport phenomena and the geometry processing part was a major headache, and Monte Carlo integration was a revelation that breathed new life into this field. However, there are still many geometry processing problems that include these headaches, and, hold on to your papers, because this paper shows us that we can apply Monte Carlo integration to many of these problems too. For instance, one, it can resolve the rich external and internal structure of this ant. With traditional techniques, this would normally take more than 14 hours and 30 gigabytes of memory, but, if we apply Monte Carlo integration to this problem, we can get a somewhat noisy preview of the result in less than one minute. Of course, over time, as we compute more samples, the noise clears up and we get this beautiful final result. And the concept can be used for so much more, it truly makes my head spin. Let’s discuss six amazing applications, while noting that there are so many more in the paper, which you can and should check out in the video description. For instance, two, it can also compute a CT scan of the infamous shovelnose frog that you see here, and instead of creating the full 3D solution, we only have to compute a 2D slice of it, which is much, much cheaper. Three, it can also edit these curves, and note that the key part is that we can do that without the major headache of creating an intermediate triangle mesh geometry for it. Four, it also supports denoising techniques, so we don’t have to compute too many samples to get a clear image or piece of geometry. Five, performing a Helmholtz-Hodge decomposition with this method is also possible. This is a technique that is used widely in many domains, for instance, it is responsible to ensure the stability of many fluid simulation programs, and this technique can also produce these decompositions. And interestingly, here it is used to represent 3D objects without the usual triangle meshes that we use in computer graphics. Six, it supports multiple importance sampling as well. This means that if we have multiple sampling strategies, multiple ways to solve a problem that have different advantages and disadvantages, it combines them in a way that we get the best of all of them. We had a mega-episode on multiple importance sampling, it has lots of amazing uses in light transport simulations, so if you would like to hear more, make sure to check that video out in the video description. But wait, these are all difficult problems. One surely needs a PhD and years of experience in computer graphics to implement this, right? When seeing a work like this, we often ask, okay, it does something great, but how complex is it? How many days do I have to work to reimplement it? Please take a guess and let me know what the guess was in the comments section. And now, what you see here is none other than the source code for the core of the method, and what’s even more, a bunch of implementations of it already exist. And, if you see that a paper has been reimplemented around day 1, you know it’s good. So, no wonder, this paper has been accepted to SIGGRAPH, perhaps the most prestigious computer graphics conference. It is immensely difficult to get a paper accepted there, and I would say this one more than earned it. Huge congratulations to the first author of the paper, Rohan Sawhney, he is currently a PhD student, and note that this was his second published paper. Unreal. Such a great leap for the field in just one paper. Also congratulations to professor Keenan Crane who advised this project and many other wonderful works in the last few years. I cannot wait to see what they will be up to next and I hope that now, you are just as excited. Thanks for watching and for your generous support, and I'll see you next time!"
424,Can an AI Learn Lip Reading?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. When watching science fiction movies, we often encounter crazy devices and technologies that don’t really exist, or sometimes, ones that are not even possible to make. For instance, reconstructing sound from vibrations would be an excellent example of that, and could make a great novel with the secret service trying to catch dangerous criminals. Except that it has already been done in real life research. I think you can imagine how surprised I was when I saw this paper in 2014 that showcased a result where a camera looks at this bag of chips, and from these tiny-tiny vibrations, it could reconstruct the sounds in the room. Let’s listen. Yes, this indeed sounds like science fiction. But 2014 was a long-long time ago, and since then, we have a selection of powerful learning algorithms, and the question is, what’s the next idea that sounded completely impossible a few years ago, which is now possible? Well, what about looking at silent footage from a speaker and trying to guess what they were saying? Checkmark, that sounds absolutely impossible to me, yet, this new technique is able to produce the entirety of this speech after looking at the video footage of the lip movements. Let’s listen. Wow. So the first question is, of course, what was used as the training data? It used a dataset with lecture videos and chess commentary from 5 speakers, and make no mistake, it takes a ton of data from these speakers, about 20 hours from each, but it uses video that was shot in a natural setting, which is something that we have in abundance on Youtube and other places on the internet. Note that the neural network works on the same speakers it was trained on and was able to learn their gestures and lip movements remarkably well. However, this is not the first work attempting to do this, so let’s see how it compares to the competition. The new one is very close to the true spoken sentence. Let’s look at another one. Note that there are gestures, a reasonable amount of head movement and other factors at play and the algorithm does amazingly well. Potential applications of this could be video conferencing in zones where we have to be silent, giving a voice to people with the inability to speak due to aphonia or other conditions, or, potentially fixing a piece of video footage where parts of the speech signal are corrupted. In these cases, the gaps could be filled in with such a technique. Look! Now, let’s have a look under the hood. If we visualize the activations within this neural network, we see that it found out that it mainly looks at the mouth of the speaker. That is, of course, not surprising. However, what is surprising is that the other regions, for instance, around the forehead and eyebrows are also important to the attention mechanism. Perhaps this could mean that it also looks at the gestures of the speaker, and uses that information for the speech synthesis. I find this aspect of the work very intriguing and would love to see some additional analysis on that. There is so much more in the paper, for instance, I mentioned giving a voice to people with aphonia, which should not be possible because we are training these neural networks for a specific speaker, but with an additional speaker embedding step, it is possible to pair up any speaker with any voice. This is another amazing work that makes me feel like we are living in a science fiction world. I can only imagine what we will be able to do with this technique two more papers down the line. If you have any ideas, feel free to speculate in the comments section below. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
425,This AI Creates Beautiful 3D Photographs!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. We hear more and more about RGBD images these days. These are photographs that are endowed with depth information, which enable us to do many wondrous things. For instance, this method was used to endow self-driving cars with depth information and worked reasonably well, and this other one provides depth maps that are so consistent, we can even add some AR effects to it, and today’s paper is going to show what 3D photography is. However, first, we need not only color, but depth information in our images to perform these. You see, phones with depth scanners already exist, and even more are coming as soon as this year, but even if you have a device that only gives you 2D color images, don’t despair. There is plenty of research on how we can estimate these depth maps, even if we have very limited information. And, with proper depth information, we can now create these 3D photographs, where we get even more information out of one still image. We can look behind objects and see things that we wouldn’t see otherwise. Beautiful parallax effects appear as objects at different distances move different amounts as we move the camera around. You see that the foreground changes a great deal, the buildings in the background, less so, and the hills behind them, even less so. These photos truly come alive with this new method. An earlier algorithm, the legendary PatchMatch method from more than a decade ago could perform something that we call image inpainting. Image inpainting means looking at what we see in these images, and trying to fill in missing information with data that makes sense. The key difference here is that this new technique uses a learning method, and does this image inpainting in 3D, and it not only fills in color, but depth information as well. What a crazy, amazing idea. However, this is not the first method to perform this, so how does it compare to other research works? Let’s have a look together. Previous methods have a great deal of warping and distortions on the bathtub here, and if you look at the new method, you see that it is much cleaner. There is still a tiny bit of warping, but, it is significantly better. The dog head here with this previous method seems to be bobbing around a great deal, while the other methods also have some problems with it…look at this too. And if you look at how the new method handles it…it is significantly more stable. And you see that these previous techniques are from just one or two years ago. It is unbelievable how far we have come since. Bravo. So this was a qualitative comparison, or in other words, we looked at the results, what about the quantitative differences? What do the numbers say? Look at the PSNR column here, this means the Peak Signal to Noise Ratio, this is subject to maximization, as the up arrow denotes here. The higher, the better. The difference is between one half to 2.5 points when compared to previous methods, which does not sound like a lot at all. So, what happened here? Note that PSNR is not a linear, but a logarithmic scale, so this means that a small numeric difference typically translates to a great deal of difference in the images, even if the numeric difference is just 0.5 points on the PSNR scale. However, if we look at SSIM, the structural similarity metric, all of them are quite similar, and a previous technique appears to be winning here. But this was the method that warped the dog head, and in the visual comparisons, the new method came out significantly better than this. So what is going on here? Well, have a look at this metric, LPIPS, which was developed at UC Berkeley, OpenAI and Adobe research. At the risk of simplifying the situation, this uses a neural network to look at an image, and uses its inner representation to decide how close the two images are to each other. Loosely speaking, it kind of thinks about differences as we, humans do, and is an excellent tool to compare images. And, sure enough, this also concludes that the new method performs best. However, this method is still not perfect. There is some flickering going on behind these fences, the transparency of the glass here isn’t perfect, but witnessing this huge a leap in the quality of results in such little time is truly a sight to behold. What a time to be alive! I started this series to make people feel how I feel when I read these papers, and I really hope that it goes through with this paper. Absolutely amazing. What is even more amazing is that with a tiny bit of technical knowledge, you can run the source code in your browser, so make sure to have a look at the link in the video description. Let me know in the comments how it went! Thanks for watching and for your generous support, and I'll see you next time!"
426,This AI Creates Dogs From Cats…And More!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, we have a selection of learning-based techniques that can generate images of photorealistic human faces for people that don’t exist. These techniques have come a long way over the last few years, so much so that we can now even edit these images to our liking, by, for instance, putting a smile on their faces, making them older or younger, adding or removing a beard, and more. However, most of these techniques are still lacking in two things. One is diversity of outputs, and two, generalization to multiple domains. Typically, the ones that work on multiple domains don’t perform too well on most of them. This new technique is called StarGAN 2 and addresses both of these issues. Let’s start with the humans. In the footage here, you see a lot of interpolation between test subjects, which means that we start out from a source person, and generate images that morph them into the target subjects, not in any way, but in a way that all of the intermediate images are believable. In these results, many attributes from the input subject, such as pose, nose type, mouth shape and position are also reflected on the output. I like how the motion of the images on the left reflects the state of the interpolation. As this slowly takes place, we can witness how the reference person grows out a beard. But we're not nearly done yet. We noted that another great advantage of this technique is that it works for multiple domains, and this means, of course, none other than us looking at cats morphing into dogs and other animals. In these cases, I see that the algorithm picks up the gaze direction, so this generalizes to even animals. That's great. What is even more great is that the face shape of the tiger appears to have been translated to the photo of this cat, and, if we have a bigger cat as an input, the output will also give us… this lovely, and a little plump creature. And...look! Here, the cat in the input is occluded in this target image, but that is not translated to the output image. The AI knows that this is not part of the cat, but an occlusion. Imagine what it would take to prepare a handcrafted algorithm to distinguish these features. My goodness. And now, onto dogs. What is really cool is that in this case, bendy ears have their own meaning and we get several versions of the same dog breed, with, or without them. And it can handle a variety of other animals too. I could look at these all day. And now, to understand why this works so well, we first have to understand what a latent space is. Here you see an example of a latent space that was created to be able to browse through fonts, and even generate new ones. This method essentially tries to look at a bunch of already existing fonts and tries to boil them down into the essence of what makes them different. It is a simpler, often incomplete, but, more manageable representation for a given domain. This domain can be almost anything, for instance, you see another technique that does something similar with material models. Now, the key difference in this new work compared to previous techniques, is that it creates not one latent space, but several of these latent spaces for different domains. As a result, it can not only generate images in all of these domains, but can also translate different features, for instance, ears, eyes, noses from a cat to a dog or a cheetah in a way that makes sense. And the results look like absolute witchcraft. Now, since the look on this cheetah’s face indicates that it has had enough of this video, just one more example before we go. As a possible failure case, have a look at the ears of this cat. It seems to be in a peculiar midway-land between a pointy and a bent ear, but it doesn’t quite look like any of them. What do you think? Maybe some of you cat people can weigh in on this. Let me know in  the comments. Thanks  for watching and for your generous support, and I'll see you next time!"
427,An AI Learned To See Through Obstructions!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Approximately two years ago, we covered a work where a learning-based algorithm was able to read the wifi signals in a room to not only locate a person in a building, but even estimate their pose. An additional property of this method was that, as you see here, it does not look at images, but radio signals, which also traverse in the dark, and therefore, this pose estimation also works well in poor lighting conditions. Today’s paper offers a learning-based method for a different, more practical data completion problem, and it mainly works on image sequences. You see, we can give this one a short image sequence with obstructions, for instance, the fence here. And it is able to find and remove this obstruction, and not only that, but it can also show us what is exactly behind the fence! How is that even possible? Well, note that we mentioned that the input is not an image, but an image sequence, a short video if you will. This contains the scene from different viewpoints, and is one of the typical cases where if we would give all this data to a human, this human would take a long-long time, but would be able to reconstruct what is exactly behind the fence, because this data was visible from other viewpoints. But of course, clearly, this approach would be prohibitively slow and expensive. The cool thing here is that this learning-based method is capable of doing this, automatically! But it does not stop there. I was really surprised to find out that it even works for video outputs as well, so if you did not have a clear sight of that tiger in the zoo, do not despair! Just use this method, and there you go. When looking at the results of techniques like this, I always try to only look at the output, and try to guess where the fence was obstructing it. With many simpler, image inpainting techniques, this is easy to tell if you look for it, but here, I can’t see a trace. Can you? Let me know in the comments. Admittedly, the resolution of this video is not very high, but the results look very reassuring. It can also perform reflection removal, and some of the input images are highly contaminated by these reflected objects. Let’s have a look at some results! You can see here how the technique decomposes the input into two images, one with the reflection, and one without. The results are clearly not perfect, but they are easily good enough to make my brain focus on the real background without being distracted by the reflections. This was not the case with the input at all. Bravo! This use case can also be extended for videos, and I wonder how much temporal coherence I can expect in the output. In other words, if the technique solves the adjacent frames too differently, flickering is introduced to the video, and this effect is the bane of many techniques that are otherwise really good on still images. Let’s have a look! There is a tiny bit of flickering, but the results are surprisingly consistent. It also does quite well when compared to previous methods, especially when we are able to provide multiple images as an input. Now note that it says “ours, without online optimization”. What could that mean? This online optimization step is a computationally expensive way to further improve separation in the outputs, and with that, the authors propose a quicker and a slower version of the technique. The one without the on-line optimization step runs in just a few seconds, and if we add this step, we will have to wait approximately 15 minutes. I had to read the table several times, because researchers typically bring the best version of their technique to the comparisons, and it is not the case here. Even the quicker version smokes the competition! Loving it. Note if you have a look at the paper in the video description, there are, of course, more detailed comparisons against other methods as well. If these AR glasses that we hear so much about come to fruition in the next few years, having an algorithm for real-time glare, reflection and obstruction removal would be beyond amazing. We truly live in a science fiction world. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
428,These AI-Driven Characters Dribble Like Mad!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In computer games and all kinds of applications where we are yearning for realistic animation, we somehow need to tell our computers how the different joints and body parts of these virtual characters are meant to move around over time. Since the human eye is very sensitive to even the tiniest inaccuracies, we typically don’t program these motions by hand, but instead, we often record a lot of real-life motion capture data in a studio, and try to reuse that in our applications. Previous techniques have tackled quadruped control, and we can even teach bipeds to interact with their environment in a realistic manner. Today we will have a look at an absolutely magnificent piece of work, where the authors carved out a smaller subproblem, and made a solution for it that is truly second to none. And this subproblem is simulating virtual characters playing basketball. Like with previous works, we are looking for realism in the movement, and for games, it is also a requirement that the character responds to our controls well. However, the key challenge is that all we are given, is 3 hours of unstructured motion capture data. That is next to nothing, and from this next to nothing, a learning algorithm has to learn to understand these motions so well, that it can weave them together, even when a specific movement combination is not present in this data. That is quite a challenge. Compared to many other works, this data is really not a lot, so I am excited to see what value we are getting out of these three hours. At first I thought we’d get only very rudimentary motions, and boy, was I dead wrong on that one. We have control over this character and can perform these elaborate maneuvers, and it remains very responsive even if we mash the controller like a madman, producing these sharp turns. As you see, it can handle these cases really well. And not only that, but it is so well done, we can even dribble through a set of obstacles, leading to a responsive, and enjoyable gameplay. About these dribbling behaviors. Do we get only one, boring motion, or not? Not at all, it was able to mine out not just one, but many kinds of dribbling motions, and is able to weave them into other moves as soon as we interact with the controller. This is already very convincing, especially from just three hours of unstructured motion capture data. But this paper is just getting started. Now, hold on to your papers, because we can also shoot and catch the ball, move it around, that is very surprising, because it has looked at so little shooting data, let’s see…yes, less than 7 minutes. My goodness. And it keeps going, what I have found even more surprising is that it can handle unexpected movements, which I find to be even more remarkable given the limited training data. These crazy corner cases are typically learnable when they are available in abundance in the training data, which is not the case here. Amazing. When we compare these motions to a previous method, we see that both the character and the ball’s movement is much more lively. For instance, here, you can see that the Phase-Functioned Neural Network, PFNN in short, almost makes it seem like the ball has to stick to the hand of the player for an unhealthy amount of time to be able to create these motions. It doesn’t happen at all with the new technique. And remember, this new method is also much more responsive to the player’s controls, and thus, more enjoyable not only to look at, but to play with. This is an aspect that hard is hard to measure, but it is not to be underestimated in the general playing experience. Just imagine what this research area will be capable of not in a decade, but just two more papers down the line. Loving it. Now, at the start of the video, I noted that the authors carved out a small use-case, which is training an AI to weave together basketball motion capture data in a manner that is both realistic and controllable. However, many times in research, we look at a specialized problem, and during that journey we learn general concepts that can be applied to other problems as well. That is exactly what happened here, as you see, parts of this technique can be generalized for quadruped control as well. This good boy is pacing and running around beautifully. And…you guessed right, our favorite biped from the previous paper is also making an introduction. I am absolutely spellbound by this work, and I hope that now, you are too. Can’t wait to see this implemented in newer games and other real-time applications. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
429,OpenAI GPT-3 - Good At Almost Everything!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In early 2019, a learning-based technique appeared that could perform common natural language processing operations, for instance, answering questions, completing text, reading comprehension, summarization, and more. This method was developed by scientists at OpenAI, and they called it GPT-2. The goal was to be able to perform this task with as little supervision as possible. This means that they unleashed this algorithm to read the internet, and the question is, what would the AI learn during this process? That is a tricky question. And to be able to answer it, have a look at this paper from 2017, where an AI was given a bunch of Amazon product reviews and the goal was to teach it to be able to generate new ones, or continue a review when given one. Then, something unexpected happened. The finished neural network used surprisingly few neurons to be able to continue these reviews, and upon closer inspection, they noticed that the neural network has built up a knowledge of not only language, but also built a sentiment detector as well. This means that the AI recognized that in order to be able to continue a review, it not only needs to learn English, but also needs to be able to detect whether the review seems positive or not. If we know that we have to complete a review that seems positive from a small snippet, we have a much easier time doing it well. And now, back to GPT-2. As it was asked to predict the next character in sentences of not reviews, but of any kind, we asked what this neural network would learn? Well, now we know that of course, it learns whatever it needs to learn to perform the sentence completion properly. And to do this, it needs to learn English by itself, and that’s exactly what it did! It also learned about a lot of topics to be able to discuss them well. What topics? Let’s see. We gave it a try, and I was somewhat surprised when I saw that it was able to continue a Two Minute Papers script, even though it seems to have turned into a history lesson. What was even more surprising is that it could shoulder the Two Minute Papers test, or in other words, I asked it to talk about the nature of fluid simulations, and it was caught cheating red handed. But then, it continued in a way that was not only coherent, but had quite a bit of truth to it. Note that there was no explicit instruction for the AI apart from it being unleashed on the internet and reading it. And now, the next version appeared, by the name GPT-3. This version is now more than a 100 times bigger, so our first question is, how much better can an AI get if we increase the size of a neural network? Let’s have a look together. These are the results on a challenging reading comprehension test as a function of the number of parameters. As you see, around 1.5 billion parameters, which is roughly equivalent to GPT-2, it learned a great deal, but its understanding is nowhere near the level of human comprehension. However, as we grow the network, something incredible happens. Non-trivial capabilities start to appear as we approach a hundred billion parameters. Look! It nearly matched the level of humans. My goodness! This was possible before, but only with neural networks that are specifically designed for a narrow task. In comparison, GPT-3 is much more general. Let’s test that generality and have a look at 5 practical applications together! One, OpenAI made this AI accessible to a lucky few people, and it turns out, it has read a lot of things on the internet, which contains a lot of code, so it can generate website layouts from a written description. Two, it also learned how to generate properly formatted plots from a tiny prompt written in plain English. Not just one kind many kinds! Perhaps to the joy of technical PhD students around the world, three, it can properly typeset mathematical equations from a plain English description as well. Four, it understands the kind of data we have in a spreadsheet, in this case, population, and fills the missing parts correctly. And five, it can also translate a complex legal text into plain language, or, the other way around, in other words, it can also generate legal text from our simple descriptions. And as you see here, it can do much, much more, I left a link to all of these materials in the video description. However, of course, this iteration of GPT also has its limitations. For instance, we haven’t seen the extent to which these examples are cherrypicked, or in other words, for every good output that we marvel at, there might have been one, or a dozen tries that did not come out well. We don’t exactly know. But the main point is that working with GPT-3 is a really peculiar process where we know that a vast body of knowledge lies within, but it only emerges if we can bring it out with properly written prompts. It almost feels like a new kind of programming that is open to everyone, even people without any programming or technical knowledge. If a computer is a bicycle for the mind, then GPT-3 is a fighter jet. Absolutely incredible. And to say that the paper is vast would be an understatement we only scratched the surface of what it can do here, so make sure to have a look if you wish to know more about it. The link is available in the video description. I can only imagine what we will be able to do with GPT-4 and GPT-5 in the near future! What a  time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
430,Can We Simulate Tearing Meat?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Perhaps the best part of being a computer graphics researcher is creating virtual worlds on a daily basis and computing beautiful simulations in these worlds. And what you see here is not one, but two kinds of simulations. One is a physics simulation that computes how these objects move, and a light transport simulation that computes how these objects look. In this video, we will strictly talk about the physics simulation part of what you see on the screen. To simulate these beautiful phenomena, many recent methods build on top of a technique called the Material Point Method. This is a hybrid simulation technique that uses both particles and grids to create these beautiful animations, however, when used by itself, we can come up with a bunch of cases that it cannot simulate properly. One such example is cracking and tearing phenomena, which has been addressed in this great paper that we discussed earlier this year. With this method, we could smash oreos, candy crabs, pumpkins, and much, much more. It even supported tearing this piece of bread apart. This already looks quite convincing, and in this series, I always say, two more papers down the line, and it will be improved significantly. Today, we are going to have a Two Minute Papers moment of truth, because this is from a followup work by Joshuah Wolper, the first author of the previous bread paper, and you can immediately start holding on to your papers, because this work is one of the finest I have seen as of late. With this, we can enrich our simulations with anisotropic damage and elasticity. So what does that mean exactly? This means that it supports more extreme topological changes in these virtual objects. This leads to better material separation when the damage happens. For instance, if you look here on the right, this was done by a previous method. For the first sight, it looks good, there is some bouncy behavior here, but the separation line is a little too clean. Let’s have a look at the new method! Woo-hoo! Now that’s what I am talking about! Let’s have another look. I hope you now see what I meant by the previous separation line being a little too clean. Remarkably, it also supports changing a few intuitive parameters, like eta, the crack propagation speed, which we can use to further tailor the simulation to our liking. Artists are going to love this. We can also play with the Young modulus, which describes the material’s resistance against fractures. On the left, it is quite low, and makes the material tear apart easily, much like a sponge. As we increase it bit, we get a stiffer material, which gives us this glorious floppy behavior. Let’s increase it even more, and see what happens! Yes, it is more resistant against damage, however, in return, it gives us some more vibrations after the break. It is not only realistic, but it also gives us the choice with these parameters to tailor our simulation results to our liking. Absolutely incredible. Now then, if you have been holding on to your papers so far, now squeeze that paper, because previous methods were only capable of tearing off a small piece, or only a strip of this virtual pork, let’s see what this new work will do. Yes, it can also simulate peeling off an entire layer. Glorious! But that’s not the only thing we can peel. It can also deal with small pieces of this mozzarella cheese. I must admit that I have never done this myself, so this will be the official piece of homework for me, and for the more curious minds out there after watching this video. Let me know in the comments if it went the same way in your kitchen as it did in the simulation here! You get extra credit if you post a picture too. And finally, if we tear this piece of meat apart, you see that it takes into consideration the location of the fibers, and the tearing takes place not in an arbitrary way, but much like in reality, it tears along the muscle fibers. So, how fast is it? We still have to wait a few seconds for each frame in these simulations. None of them took too long, there is a fish tearing experiment in the paper that went very quickly, half a second for each frame is a great deal, the pork experiment took nearly 40 seconds for each frame, and the most demanding experiments involved a lance and bones. Frankly, they were a little too horrific to be included here, even for virtual bodies, but if you wish to have a look, make sure to click the paper in the video description. But wait, are you seeing what I am seeing? Those examples took more than 1000 times longer to compute! Goodness! How can that be? Look! As you see here, in these cases, the delta-t-step is extremely tiny, which means that we have to advance the simulation with tiny-tiny time steps that takes much longer to compute. How tiny? Quite! In this case, we have to advance the simulation one millionth of a second at a time. The reason for this is that bones have an extremely high stiffness, which makes this method much less efficient. And of course, you know the drill, two more papers down the line, and this may run interactively on a consumer machine at home. So what’s the verdict? Algorithm design. A+. Exposition, A+. Quality of presentation A double plus. And it’s still Mr. Wolper’s third paper in computer graphics. Unreal. And we, researchers even get paid to create beautiful works like this. I also couldn’t resist creating a slow-motion version of some of these videos, so if this is something that you wish to see, make sure to visit our Instagram page in the video description for more. Thanks for watching and for your generous support, and I'll see you next time!"
431,From Video Games To Reality…With Just One AI!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Approximately three years ago, a magical learning-based algorithm appeared that was capable of translating a photorealistic image of a zebra into a horse or the other way around, could transform apples into oranges, and more. Later, it became possible to do this even without the presence of a photorealistic image, where all we needed was a segmentation map. This segmentation map provides labels on what should go where, for instance, this should be the road, here will be trees, traffic signs, other vehicles, and so on. And the output was a hopefully photorealistic video, and you can see here that the results were absolutely jaw-dropping. However, … look! As time goes by, the backside of the car morphs and warps over time, creating unrealistic results that are inconsistent, even on the short term. In other words, things change around from second to second, and the AI does not appear to remember what it did just a moment ago. This kind of consistency was solved surprisingly well in a followup paper from NVIDIA, in which an AI would look at the footage of a video game, for instance, Pacman, and after it has looked for approximately 120 hours, we could shut down the video game, and the AI would understand the rules so well that it could recreate the game that we could even play with. It had memory and used it well, and therefore, it could enforce a notion of world consistency, or in other words, if we return to a state of the game that we visited before, it will remember to present us with very similar information. So, the question naturally arises, would it be possible to create a photorealistic video from these segmentation maps that is also consistent? And in today’s paper, researchers at NVIDIA proposed a new technique, that requests some additional information, for instance, a depth map that provides a little more information on how far different parts of the image are from the camera. Much like the Pacman paper, this also has memory, and I wonder if it is able to use it as well as that one did. Let’s test it out. This previous work is currently looking at a man with a red shirt, we slowly look away, disregard the warping, and when we go back…hey! Do you see what I see here? The shirt became white. This is not because the person is one of those artists who can change their clothes in less than a second, but because this older technique did not have a consistent internal model of the world. And now, let’s see the new one. Once again, we start with the red shirt, look away, and then…yes! Same red to blue gradient. Excellent! So it appears that this new technique also reuses information from previous frames efficiently, and therefore, it is finally able to create a consistent video, with much less morphing and warping, and even better, we have this advantageous consistency property where if we look at something that we looked at before, we will see very similar information there. But there is more. Additionally, it can also generate scenes from new viewpoints, which we also refer to as neural rendering. And as you see, the two viewpoints show similar objects, so the consistency property holds here too. And now, hold on to your papers, because we do not necessarily have to produce these semantic maps ourselves. We can let the machines do all the work by firing up a video game that we like, request that the different object classes are colored differently, and get this input for free. And then, the technique generated a photorealistic video from the game graphics. Absolutely amazing. Now note that it is not perfect, for instance, it has a different notion of time as the clouds are changing in the background rapidly. And, look! At the end of the sequence, we got back to our starting point, and the first frame that we saw is very similar to the last one. The consistency works here too. Very good. I have no doubt that two more papers down the line, and this will be even better. And for now, we can create consistent, photorealistic videos even if all we have is freely obtained video game data. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
432,How Can We Simulate Water Droplets?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. A computer graphics paper from approximately 3 years ago was able to simulate the motion of these bubbles, and even these beautiful collision events between them in a matter of milliseconds. This was approximately 300 episodes ago, and in this series, we always say that two more papers down the line, and this will be improved significantly. So now, once again, here goes another one of those Two Minute Papers moments of truth. Now, three years later, let’s see how this field evolved. Let’s fire up this new technique that will now, simulate the evolution of two cube-shaped droplets for us. In reality, mother nature would make sure that the surface area of these droplets is minimized, let’s see…yes, droplets form immediately, so the simulation program understands surface tension, and the collision event is also simulated beautifully. A+. However, this was possible with previous methods, for instance, a paper by the name Surface-Only Liquids could also pull it off, so what’s new here? Well, let’s look under the hood and find out. Oh yes. This is different. You see, normally, if we do this breakdown, we get triangle meshes, this is typically how these surfaces are represented. But I don’t see any meshes here, I see particles! Great, but what does this enable us to do? Look here. If we break down the simulation of this beautiful fluid polygon, we see that there is not only one kind of particle here! There are three kinds! With light blue, we see sheet particles, the yellow ones are filament particles, and if we look inside, with dark blue here, you see volume particles. With these building blocks, and the proposed new simulation method, we can create much more sophisticated surface tension-related phenomena! So let’s do exactly that! For instance, here, you see soap membranes stretching due to wind flows. They get separated, lots of topological changes take place, and the algorithm handles it correctly. In an other example, this soap bubble has been initialized with a hole, and you can see it cascading through the entire surface. Beautiful work! And, after we finish the simulation of these fluid chains, we can look under the hood, and see how the algorithm thinks about this piece of fluid. Once again, with dark blue, we have the particles that represent the inner volume of the water chains, and on the outside, there is a thin layer of sheet particles holding them together. What a clean and beautiful visualization. So, how much do we have to wait to get these results? A bit. Simulating this fluid chain example took roughly 60 seconds per frame. This droplet on a plane example runs approximately ten times faster than that, it needs only 6.5 seconds for each frame. This was one of the cheaper scenes in the paper, and you may be wondering, which one was the most expensive? This water bell took almost two minutes for each frame, and here, when you see this breakdown, from the particle color coding, you know exactly what we are looking at. Since part of this algorithm runs on your processor, and a different part on your graphics card, there is plenty of room for improvements in terms of the computation time for a followup paper. I cannot wait to see these beautiful simulations in real time two more papers down the line. What a time to be alive! I also couldn’t resist creating a slow-motion version of some of these videos, if this is something that you wish to see, make sure to click our Instagram page link in the video description for more. Thanks for watching and for your generous support, and I'll see you next time!"
433,This AI Removes Shadows From Your Photos!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. When we look at the cover page of a magazine, we often see lots of well-made, but also idealized photos of people. Idealized here means that the photographer made them in a studio, where they can add or remove light sources and move them around to bring out the best from their models. But most photos are not made in the studio, they are made out there in the wild where the lighting is what it is, and we can’t control it too much. So, with that, today, our question is what if we could change the lighting after the photo has been made? This work proposes a cool technique to perform exactly that by enabling us to edit the shadows on a portrait photo that we would normally think of deleting. Many of these have to do with the presence of shadows, and you can see here that we can really edit these after the photo has been taken. However, before we start taking a closer look at the editing process, we have to note that there are different kinds of shadows. One, there are shadows cast on us by external objects, let’s call those foreign shadows, and there is self-shadowing, which comes from the model’s own facial features. Let’s call those facial shadows. So why divide them into two classes? Simple, because we typically seek to remove foreign shadows, and edit facial shadows. The removal part can be done with a learning algorithm, provided that we can teach it with a lot of training data. Let’s think about ways to synthesize such a large dataset! Let’s start with the foreign shadows. We need image pairs of test subjects with and without shadows to have the neural network learn about their relations. Since removing shadows is difficult without further interfering with the image, the authors opted to do it the other way around. In other words, they take a clean photo of the subject, that’s the one without the shadows, and then, and add shadows to it algorithmically. Very cool! And, the results are not bad at all, and get this, they even accounted for subsurface scattering, which is the scattering of light under our skin. That makes a great deal of a difference. This is a reference from a paper we wrote with scientists at the University of Zaragoza and the Activision Blizzard company to add this beautiful effect to their games. Here is a shadow edge without subsurface scattering, quite dark, and with subsurface scattering, you see this beautiful glowing effect. Subsurface scattering indeed makes a great deal of difference around hard shadow edges, so huge thumbs up for the authors for including an approximation of that. However, the synthesized photos are still a little suspect. We can still tell that they are synthesized. And that is kind of the point. Our question is “can the neural network still learn the difference between a clean and a shadowy photo” despite all this? As you see, the problem is not easy previous methods did not do too well on these examples when you compare them to the reference solution. And let’s see this new method. Wow, I can hardly believe my eyes. Nearly perfect. And it did learn all this on not real, but synthetic images. And believe it or not, this was only the simpler part. Now comes the hard part. Let’s look at how well it performs at editing the facial shadows! We can pretend to edit both the size and the intensity of these light sources. The goal is to have a little more control over the shadows in these photos, but, whatever we do with them, the outputs still have to remain realistic. Here are the before and after results. The facial shadows have been weakened, and depending on our artistic choices, we can also soften the image a great deal. Absolutely amazing. As a result, we now have a two-step algorithm, that first, removes foreign shadows, and is able to soften the remainder of the facial shadows, creating much more usable portrait photos of our friends, and all this after the photo has been made. What a time to be alive! Now, of course, even though this technique convincingly beats previous works, it is still not perfect. The algorithm may fail to remove some highly detailed shadows, you can see how the shadow of the hair remains in the output. In this other output, the hair shadows are handled a little better, there is some dampening, but the symmetric nature of the facial shadows here put the output results in an interesting no man’s land where the opacity of the shadow has been decreased, but the result looks unnatural. I can’t wait to see how this method will be improved two more papers down the line. I will be here to report on it to you, so make sure to subscribe and hit the bell icon to not miss out on that. Thanks for watching and for your generous support, and I'll see you next time!"
434,TecoGAN: Super Resolution Extraordinaire!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Let’s talk about video super resolution. The problem statement is simple, in goes a coarse video, the technique analyzes it, guesses what’s missing, and out comes a detailed video. However, of course, reliably solving this problem is anything but simple. When learning-based algorithms were not nearly as good as they are today, this problem was mainly handled by handcrafted techniques, but they had their limits after all, if we don’t see something too well, how could we tell what’s there? And this is where new learning-based methods, especially this one, come into play. This is a hard enough problem for even a still image, yet this technique is able to do it really well even for videos. Let’s have a look. The eye color for this character is blurry, but we see that it likely has a green-ish, blue-ish color. And if we gave this problem to a human, this human would know that we are talking about the eye of another human, and we know roughly what this should look like in reality. A human would also know that this must a bridge, and finish the picture. What about computers? The key is that if we have a learning algorithm that looks at the coarse and fine version of the same video, it will hopefully learn what it takes to create a detailed video when given a poor one, which is exactly what happened here. As you see, we can give it very little information, and it was able to add a stunning amount of detail to it. Now of course, super resolution is a highly studied field these days, therefore it is a requirement for a good paper to compare to quite a few previous works. Let’s see how it stacks up against those! Here, we are given given a blocky image of this garment, and this is the reference image that was coarsened to create this input. The reference was carefully hidden from the algorithms, and only we have it. Previous works could add some details, but the results were nowhere near as good as the reference. So what about the new method? My goodness, it is very close to the real deal! Previous methods also had trouble resolving the details of this region, where the new method, again, very close to reality. It is truly amazing how much this technique understands the world around us from just this training set of low and high-resolution videos. Now, if you have a closer look at the author list, you see that Nils Thuerey is also there. He is a fluid and smoke person, so I thought there had to be an angle here for smoke simulations. And, yup, there we go. To even have a fighting chance of understanding the importance of this sequence, let’s go back to one of Nils’s earlier works, which is one the best papers ever written, Wavelet Turbulence. That’s a paper from twelve years ago. Now, some of the more seasoned Fellow Scholars among you know that I bring this paper up every chance I get, but especially now that it connects to this work we’re looking at. You see, Wavelet Turbulence was an algorithm that could take a coarse smoke simulation after it has been created, and added fine details to it. In fact, so many fine details that creating the equivalently high resolution simulation would have been near impossible at the time. However, it did not work with images, it required knowledge about the inner workings of the simulator. For instance, it would need to know about velocities and pressures at different points in this simulation. Now, this new method can do something very similar, and all it does is just look at the image itself, and improves it, without even looking into the simulation data! Even though the flaws in the output are quite clear, the fact that it can add fine details to a rapidly moving smoke plume is still an incredible feat! If you look at the comparison against CycleGAN, a technique from just 3 years ago, this is just a few more papers down the line, and you see that this has improved significantly. And the new one is also more careful with temporal coherence, or in other words, there is no flickering arising from solving the adjacent frames in the video differently. Very good. And if we look a few more papers further down the line, we may just get a learning-based algorithm that does so well at this task, that we would be able to rewatch any old footage in super high quality. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
435,This AI Creates Images Of Nearly Any Animal!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. The research field of image translation with the aid of learning algorithms has been on fire lately. For instance, this earlier technique would look at a large number of animal faces, and could interpolate between them, or in other words, blend one kind of dog into another breed. But that’s not all, because it could even transform dogs into cats, or create these glorious plump cats and cheetahs. The results were absolutely stunning, however, it would only work on the domains it was trained on. In other words, it could only translate to and from species that it took the time to learn about. This new method offers something really amazing: it can handle multiple domains, or multiple breeds, if you will, even ones that it hadn’t seen previously. That sounds flat out impossible, so let’s have a look at some results. This dog will be used as content, therefore the output should have a similar pose, but its breed has to be changed to this one. But there is one little problem. And that problem is that the AI has never seen this breed before. This will be very challenging because we only see the head of the dog used for style. Should the body of the dog also get curly hair? You only know if you know this particular dog breed, or if you are smart and can infer missing information by looking at other kinds of dogs. Let’s see the result. Incredible. The remnants of the leash also remain there in the output results. It also did a nearly impeccable job with this bird, where again, the style image is from a previously unseen breed. Now, this is, of course, a remarkably difficult problem domain, translating into different kinds of animals that you know nothing about apart from a tiny, cropped image, this would be quite a challenge, even for a human. However, this one is not the first technique to attempt to solve it, so let’s see how it stacks up against a previous method. This one is from just a year ago, and you will see in a moment how much this field has progressed since then. For instance, in this output, we get two dogs, which seems to be a mix of the content and the style dog. And, while the new method still seems to have some structural issues, the dog type and the pose is indeed correct. The rest of the results also appear to be significantly better. But what do you think? Did you notice something weird? Let me know in the comments below. And now, let’s transition into image interpolation. This will be a touch more involved than previous interpolation efforts. You see, in this previous paper, we had a source and a target image, and the AI was asked to generate intermediate images between them. Simple enough. In this case, however, we have not two, but three images as an input. There will be a content image, this will provide the high-level features, such as pose, and its style is going to transition from this to this. The goal is that the content image remains intact while transforming one breed or species into another. This particular example is one of my favorites. Such a beautiful transition, and surely not all, but many of the intermediate images could stand on their own. Again, the style images are from unseen species. Not all cases do this well with the intermediate images, however. We start with one eye because the content and this style image have one eye visible, while the target style of the owl has two. How do we solve that? Of course, with nuclear fission. Look! Very amusing. Loving this example, especially how impossible it seems because the owl is looking into the camera with both eyes, while, we see its backside below the head. If it looked to the side, like the input content image, this might be a possibility, but with this contorted body posture, I am not so sure, so I’ll give it a pass on this one. So there you go, transforming one known animal into a different one that the AI has never seen before. And it is already doing a more than formidable job at that. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
436,OpenAI’s Image GPT Completes Your Images With Style!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In early 2019, a learning-based technique appeared that could perform common natural language processing operations, for instance, answering questions, completing text, reading comprehension, summarization, and more. This method was developed by scientists at OpenAI, and they called it GPT-2. A followup paper introduced a more capable version of this technique called GPT-3, and among many incredible examples, it could generate website layouts from a written description. The key idea, in both cases was, that we would provide it an incomplete piece of text, and it would try to finish it. However, no one said that these neural networks have to only deal with text information. And sure enough, in this work, scientists at OpenAI introduced a new version of this method that tries to complete not text, but…images! The problem statement is simple: we give it an incomplete image, and we ask the AI to fill in the missing pixels. That is, of course, an immensely difficult task, because these images may depict any part of the world around us. It would have to know a great deal about our world to be able to continue the images, so how well did it do? Let’s have a look! This is undoubtedly a cat. But look! See that white part that is just starting? The interesting part has been cut out of the image. What could that be? A piece of paper? Something else? Now, let’s leave the dirty work to the machine and ask it to finish it! Wow. A piece of paper indeed, according to the AI, and it even has text on it. The text has a heading section and a paragraph below it too. Truly excellent. You know what is even more excellent? Perhaps the best part. It also added the indirect illumination on the fur of the cat, meaning that it sees that a blue room surrounds it and therefore some amount of color bleeds onto the fur of the cat, making it bluer. I am a light transport researcher by trade, so I spend the majority of my life calculating things like this, and I have to say that this looks quite good to me. Absolutely amazing attention to detail. But it had more ideas. What’s this? The face of the cat has been finished, quite well in fact, but the rest, I am not so sure. If you have an idea what this is supposed to be, please let me know in the comments. And here go the rest of the results. All quite good! And, the true, real image that was concealed for the algorithm. This is the reference solution. Let’s see the next one. Oh, my, scientists at OpenAI pulled no punches here, this is also quite nasty. How many stripes should this continue with? Zero? Maybe! In any case, this solution is not unreasonable. I appreciate the fact that it continued the shadows of the humans. Next one. Yes, more stripes, great! But likely a few too many. Here are the remainder of the solutions, and, the true reference image again. Let’s have a look at this water droplet example too. We humans, know that since we see the remnants of some ripples over there too, there must be a splash, but does the AI know? Oh yes, yes it does! Amazing! And the true image. Now, what about these little creatures? The first continuation finishes them correctly, and puts them on a twig. The second one involves a stone. The third is my favorite. Hold on to your papers, and look at this. They stand in the water and we can even see their mirror images. Wow! The fourth is a branch, and finally, the true reference image. This is one of its best works I have seen so far. Here are some more results, and note that these are not cherrypicked, or in other words, there was no selection process for the results, nothing was discarded, these came out from the AI as you see them. There is a link to these and to the paper in the video description, so make sure to have a look and let me know in the comments if you have found something interesting! So what about the size of the neural network for this technique? Well, it contains from 1.5 to about 7 billion parameters. Let’s have a look together and find out what that means. These are the results from the GPT-2 paper, the previous version of the text processor on a challenging reading comprehension test as a function of the number of parameters. As you see, around 1.5 billion parameters, which is roughly similar to GPT-2, it learned a great deal, but its understanding was nowhere near the level of human comprehension. However, as they grew the network, something incredible happened. Non-trivial capabilities started to appear as we approached a hundred billion parameters. Look! It nearly matched the level of humans. And all this was measured on a nasty reading comprehension test. So, this Image GPT has the number of parameters that is closer to GPT-2 than GPT-3, so we can maybe speculate that the next version could be, potentially, another explosion in capabilities. I can’t wait to have a look at that. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
437,Can We Simulate Merging Bubbles?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. If we write the laws of fluid motion into a computer program, we can create beautiful water simulations like the one you see here. However, with all the progress in computer graphics research, we can not only simulate the water volume itself, but there are also efficient techniques to add foam, spray, and bubbles to this simulation. The even crazier thing is that this paper from 8 years ago can do all three in one go, and is remarkably simple for what it does. Just look at this heavenly footage, all simulated on a computer by using Blender, a piece of free and open source software and the FLIP Fluids plugin. But all this has been possible for quite a while now, so what happened in the 8 years since this paper has been published? How has this been improved? Well, it’s good to have bubbles in our simulation, however, in real life, bubbles have their individual densities, and can coalesce at a moment’s notice. This technique is able to simulate these events, and you will see that offers much, much more. Now, let’s marvel at three different phenomena in this simulation. First, the bubbles here are less dense than the water, and hence, start to rise, then, look at the interaction with the air! Now, after this, the bubbles that got denser than the water start sinking again. And all this can be done on your computer today! What a beautiful simulation! And now, hold on to your papers, because this method also adds simulating air pressure, which opens up the possibility for an interaction to happen at a distance. Look. First, we start pushing the piston here. The layer of air starts to push the fluid, which weighs on the next air pocket, which gets compressed, and so on. Such a beautiful phenomenon. And let’s not miss the best part! When we pull the piston back, the emerging negative flux starts drawing the liquid back. Look! One more time. Simulating all this efficiently is quite a technical marvel. When reading through the paper, I was very surprised to see that it is able to incorporate this air compression without simulating the air gaps themselves. A simulation without simulation, if you will. Let’s simulate pouring water through the neck of the water cooler with a standard, already existing technique. For some reason, it doesn’t look right, does it? So what’s missing here? We see a vast downward flow of liquid, therefore, there also has to be a vast upward flow of air at the same time, but I don’t see any of that here. Let’s see how the new simulation method handles this…we start the down flow, and yes, huge air bubbles are coming up, creating this beautiful glugging effect! I think I now have a good guess as to what scientists are discussing over the watercooler in Professor Christopher Batty’s research group. So, how long do we have to wait to get these results? You see, the quality of the outputs is nearly the same as the reference simulation, however, it takes less than half the amount of time to produce it! Admittedly, these simulations still take a few hours to compute, but it is absolutely amazing that these beautiful, complex phenomena can be simulated in a reasonable amount of time, and you know the drill, two more papers down the line, and it will be improved significantly. But we don’t necessarily need a bubbly simulation to enjoy the advantages of this method. In this scene, you see a detailed splash, where the one on the right here was simulated with the new method, it also matches the reference solution and it was more than 3 times faster. If you have a look at the paper in the video description, you will see how it simplifies the simulation by finding a way to identify regions of the simulation domain where not a lot is happening and coarsen the simulation there. These are the green regions that you see here and the paper refers to them as affine regions. As you see, the progress in computer graphics and fluid simulation research is absolutely stunning, and these amazing papers just keep coming out year after year. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
438,This AI Creates Human Faces From Your Sketches!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In 2017, so more than 300 episodes ago, we talked about an algorithm that took a 3D model of a complex object, and would give us an easy to follow, step by step breakdown on how to draw it. Automated drawing tutorials, if you will! This was a handcrafted algorithm that used graph theory to break these 3D objects into smaller, easier to manage pieces, and since then, learning algorithms have improved so much that we started looking more and more to the opposite direction. And that opposite direction would be giving a crude drawing to the machine, and getting a photorealistic image. Now that sounds like science fiction, until we realize that scientists at NVIDIA already had an amazing algorithm for this around 1.5 years ago. In that work, the input was a labeling which we can draw ourselves, and the output is a hopefully photorealistic landscape image that adheres to these labels. I love how first, only the silhouette of the rock is drawn, so we have this hollow thing on the right that is not very realistic, and then, it is now filled in with the bucket tool, and, there you go. And next thing you know, you have an amazing-looking landscape image. It was capable of much, much more, but what it couldn’t do is synthesize human faces this way. And believe it or not, this is what today’s technique is able to do. Look! In goes our crude sketch as a guide image, and out comes a nearly photorealistic human face that matches it. Interestingly, before we draw the hair itself, it gives us something as a starting point, but if we choose to, we can also change the hair shape and the outputs will follow our drawing really well. But it goes much further than this as it boasts a few additional appealing features. For instance, it not only refines the output as we change our drawing, but since one crude input can be mapped to many-many possible people, these output images can also be further art-directed with these sliders. According to the included user study, journeymen users mainly appreciated the variety they can achieve with this algorithm, if you look here, you can get a taste of that, while professionals were more excited about the controllability aspect of this method. That was showcased with the footage with the sliders. Another really cool thing that it can do is called face copy-paste, where we don’t even need to draw anything, and just take a few aspects of human faces that we would like to combine, and there you go. Absolutely amazing. This work is not without failure cases, however. You have probably noticed, but the AI is not explicitly instructed to match the eye colors, where some asymmetry may arise in the output. I am sure this will be improved just one more paper down the line, and I am really curious where digital artists will take these techniques in the near future. The objective is always to get out of the way, and help the artist spend more time bringing their artistic vision to life, and spend less time on the execution. This is exactly what these techniques can help with. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
439,Can We Simulate a Rocket Launch?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In this series, we often talk about smoke and fluid simulations, and sometimes, the examples showcase a beautiful smoke plume, but not much else. However, in real production environments, these simulations often involve complex scenes with many objects that interact with each other, and therein lies the problem. Computing these interactions is called coupling, and it is very difficult to get right, but is necessary for many of the scenes you will see throughout this video. This new graphics paper builds on a technique called the Lattice Boltzmann Method and promises a better way to compute this two-way coupling. For instance, in this simulation, two-way coupling is required to compute how this fiery smoke trail propels the rocket upward. So, coupling means interaction between different kinds of objects, but what about the two-way part? What does that mean exactly? Well, first, let’s have a look at one way coupling. As the box moves here, it has an effect on the smoke plume around it. This example also showcases one-way coupling, where the falling plate stirs up the smoke around it. The parts with the higher Reynolds numbers showcase more turbulent flows. Typically, that’s the real good stuff if you ask me. And now, on to two-way coupling. In this case, similarly to the previous ones, the boxes are allowed to move the smoke, but the added two-way coupling part means that now, the smoke is also allowed to blow away the boxes. What’s more, the vortices here on the right were even able to suspend the red box in the air for a few seconds. An excellent demonstration of a beautiful phenomenon. Now let’s look at the previous example with the dropping plate and see what happens. Yes, indeed, as the plate drops, it moves the smoke, and as the smoke moves, it also blows away the boxes. Woo-hoo! Due to improvements in the coupling computation, it also simulates these kinds of vortices much more realistically than previous works. Just look at all this magnificent progress in just 2 years. So, what else can we do with all this? What are some typical scenarios that require accurate two-way coupling? Well, for instance, it can perform an incredible tornado simulation, that you see here, and there is an alternative view where we only see the objects moving about. So, all this looks good, but really, how do we know how accurate this technique is? And now comes my favorite part, and this is when we let reality be our judge and compare the simulation results with real-world experiments. Hold on to your papers while you observe the real experiment here on the left. And now, the algorithmic reproduction of the same scene here. How close are they? Goodness…very, very close. I will stop the footage at different times so we can both evaluate it better. Love it. The technique can also undergo the wind tunnel test, here is the real footage, and here is the simulation. And it is truly remarkable how close this is able to match it, and I was wondering that even though as someone who has been doing fluids for a while now, if someone cropped this part of the image and told me that it is real-world footage, I would have believed it in a second. Absolute insanity. So, how much do we have to wait to compute a simulation like this? Well, great news, it uses your graphics card, which is typically the case for the more rudimentary fluid simulation algorithms out there, but the more elaborate ones typically don’t support it, or at least, not without a fair amount of additional effort. The quickest example was this, as it was simulated in less than 6 seconds, which I find to be mind blowing. A smoke simulation with box movements in a few seconds, I am truly out of words. The rocket launch scene took the longest with 16 hours, while the falling plate example with the strong draft that threw the boxes around was 4.5 hours of computation time. The results depend greatly on delta t, which is the size of time steps, or in others words, in how small increments we can advance time when creating these simulations to make sure we don’t miss any important interactions. You see here that in the rocket example, we have to simulate roughly a hundred thousand steps for every second of video footage. No wonder it takes so long! We have an easier time with this scene where these time steps can be 50 times larger without losing any detail, and hence, it goes much faster. The grid resolution also matters a great deal, which specifies how many spatial points the simulation has to take place in. The higher the resolution the grid, the larger region we can cover, and the more details we can simulate. As most research works, this technique doesn’t come without limitations, however. It is less accurate if we have simulations involving thin rods and shells and typically uses two to three times more memory than a typical simulator program. If these are the only tradeoffs to create all this marvelous footage, sign me up this very second! Overall, this paper is extraordinarily well written and presented, and of course, it has been accepted to the SIGGRAPH conference, one of the most prestigious scientific venues in computer graphics research. Huge congratulations to the authors, and if you wish to see something beautiful today, make sure to have a look at the paper itself in the video description. Truly stunning work. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
440,Can An AI Create Original Art?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Approximately 7 months ago, we discussed an AI-based technique called StyleGAN2, which could synthesize images of human faces for us. As a result, none of the faces that you see here are real, all of them were generated by this technique. The quality of the images and the amount of detail therein is truly stunning. We could also exert artistic control over these outputs by combining aspects of multiple faces together. And as the quality of these images improve over time, we think more and more about new questions to ask about them. And one of those questions is, for instance, how original are the outputs of these networks? Can these really make something truly unique? And believe it or not, this paper gives us a fairly good answer to that. One of the key ideas in this work is that in order to change the outputs, we have to change the model itself. Now that sounds a little nebulous, so let’s have a look at an example. First, we choose a rule that we wish to change, in our case, this will be the towers. We can ask the algorithm to show us matches to this concept, and indeed, it highlights the towers on the images we haven’t marked up yet, so it indeed understands what we meant. Then, we highlight the tree as a goal, place it accordingly onto the tower, and a few seconds later, there we go! The model has been reprogrammed such that instead of towers, it would make trees. Something original has emerged here, and, look, not only on one image, but on multiple images at the same time. Now, have a look at these human faces. By the way, none of them are real and were all synthesized by StyleGAN2, the method that you saw at the start of this video. Some of them do not appear to be too happy about the research progress in machine learning, but I am sure that this paper can put a smile on their faces. Let’s select the ones that aren’t too happy, then copy a big smile and paste it onto their faces. See if it works! It does, wow! Let’s flick between the before and after images and see how well the changes are adapted to each of the target faces. Truly excellent work. And now, on to eyebrows. Hold on to your papers while we choose a few of them, and now, I hope you agree that this mustache would make glorious replacement for them. And there we go. Perfect! And note that with this, we are violating Betteridge's law of headlines again in this series, because the answer to our central question is a resounding yes, these neural networks can indeed create truly original works, and what’s more, even entire datasets that haven’t existed before. Now, at the start of the video, we noted that instead of editing images, it edits the neural network’s model instead. If you look here, we have a set of input images created by a generator network. Then, as we highlight concepts, for instance, the watermark text here, we can look for the weights that contain this information, and rewrite the network to accommodate these user requests, in this case, to remove these patterns. Now that they are gone, by selecting humans, we can again, rewrite the network weights to add more of them, and finally, the signature tree trick from earlier can take place. The key here is that if we change one image, then we have a new and original image, but if we change the generator model itself, we can make thousands of new images in one go. Or even a full dataset. Loving the idea. And perhaps the trickiest part of this work is minimizing the effect on other weights while we reprogram the ones we wish to change. Of course, there will always be some collateral damage, but the results, in most cases still seem to remain intact. Make sure to have a look at the paper to see how it’s done exactly. Also, good news, the authors also provided an online notebook where you can try this technique yourself. If you do, make sure to read the instructions carefully and regardless of whether you get successes or failure cases, make sure to post them in the comments section here! In research, both are useful information. So, after the training step has taken place, neural networks can be rewired to make sure they create truly original works, and all this on not one image, but on a mass scale. What  a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
441,AI Makes Video Game After Watching Tennis Matches!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Approximately a year ago, we talked about an absolutely amazing paper by the name vid2game, in which we could grab a controller and become video game characters. It was among the first, introductory papers to tackle this problem, and in this series, we always say that two more papers down the line, and it will be improved significantly, so let’s see what’s in store, and this time, just one more paper down the line. This new work offers an impressive value proposition, which is to transform a real tennis match into a realistic looking video game that is controllable. This includes synthesizing not only movements, but also what effect the movements have on the ball as well. So, how do we control this? And now, hold on to your papers, because we can specify where the next shot would land with just one click. For instance, we can place this red dot here. And now, just think about the fact that this doesn’t just change where the ball should go, but the trajectory of the ball should be computed using a physical model, and, the kind of shot the tennis players has to perform for the resulting ball trajectory to look believable. This physical model even contains the ball’s spin velocity, and the Magnus effect created by this spin. The entire chain of animations has to be correct, and that’s exactly what happens here. Bravo! With blue, we can also specify the position the player has to await in to hit the ball next. And these virtual characters don’t just look like their real counterparts, they also play like them. You see, the authors analyzed the playstyle of these athletes and built a heatmap that contains information about their usual shot placements for the forehand and backhand shots separately, the average velocities of these shots, and even their favored recovery positions. If you have a closer look at the paper, you will see that they not only include this kind of statistical knowledge into their system, but they really went the extra mile and included common tennis strategies as well. So, how does it work? Let’s look under the hood. First, it looks at broadcast footage, from which, annotated clips are extracted that contain the movement of these players. If you look carefully, you see this red line on the spine of the player, and some more, these are annotations that tell the AI about the pose of the players. It builds a database from these clips and chooses the appropriate piece of footage for the action that is about to happen, which sounds great in theory, but in a moment, you will see that this is not nearly enough to produce a believable animation. For instance, we also need a rendering step, which has to adjust this footage to the appropriate perspective as you see here. But we have to do way more to make this work. Look! Without additional considerations, we get something like this. Not good. So, what happened here? Well, given the fact that the source datasets contain matches that are several hours long, they therefore contain many different lighting conditions. With this, visual glitches are practically guaranteed to happen. To address this, the paper describes a normalization step that can even these changes out. How well does this do its job? Let’s have a look. This is the unnormalized case. This short sequence appears to contain at least 4 of these glitches, all of which are quite apparent. And now, let’s see the new system after the normalization step. Yup. That’s what I am talking about! But these are not the only considerations the authors had to take to produce these amazing results. You see, oftentimes, quite a bit of information is missing from these frames. Our seasoned Fellow Scholars know not to despair, because we can reach out to image inpainting methods to address this. These can fill in missing details in images with sensible information. You can see NVIDIA’s work from two years ago that could do this reliably for a great variety of images. This new work uses a learning-based technique called image to image translation to fill in these details. Of course, the advantages of this new system are visible right away, and so are its limitations. For instance, temporal coherence could be improved, meaning that the tennis rackets can appear or disappear from one frame to another. The sprites are not as detailed as they could be, but, none of this really matters. What matters is that now, what’s been previously impossible is now possible, and two more papers down the line, it is very likely that all of these issues will be ironed out. What  a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
442,This AI Creates Real Scenes From Your Photos!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Approximately 5 months ago, we talked about a technique called Neural Radiance Fields, or NERF in short, that worked on a 5D neural radiance field representation. So what does this mean exactly? What this means is that we have 3 dimensions for location and two for view direction, or in short, the input is where we are in space give it to a neural network to learn it, and synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. In short, it can learn and reproduce entire real-world scenes from only a few views by using neural networks. And the results were just out of this world. Look. It could deal with many kinds of matte and glossy materials, and even refractions worked quite well. It also understood depth so accurately that we could use it for augmented reality applications where we put a new, virtual object in the scene and it correctly determined whether it is in front of, or behind the real objects in the scene. However, not everything was perfect. In many cases, it had trouble with scenes with variable lighting conditions and lots of occluders. You might ask, is that a problem? Well, imagine a use case of a tourist attraction that a lot of people take photos of, and we then have a collection of photos taken during a different time of the day, and of course, with a lot of people around. But, hey, remember that this means an application where we have exactly these conditions: a wide variety of illumination changes and occluders. This is exactly what NERF was not too good at! Let’s see how it did on such a case. Yes, we see both abrupt changes in the illumination, and the remnants of the folks occluding the Brandenburg Gate as well. And this is where this new technique from scientists at Google Research by the name NERF-W shines. It takes such a photo collection, and tries to reconstruct the whole scene from it, which we can, again, render from new viewpoints. So, how well did the new method do in this case? Let’s see. Wow. Just look at how consistent those results are. So much improvement in just 6 months of time. This is unbelievable. This is how it did in a similar case with the Trevi fountain. Absolutely beautiful. And what is even more beautiful is that since it has variation in the viewpoint information, we can change these viewpoints around as the algorithm learned to reconstruct the scene itself. This is something that the original NERF technique could also do, however, what it couldn’t do, is the same, with illumination. Now, we can also change the lighting conditions together with the viewpoint. This truly showcases a deep understanding of illumination and geometry. That is not trivial at all! For instance, when loading this scene into this neural re-rendering technique from last year, it couldn’t tell whether we see just color variations on the same geometry, or if the geometry itself is changing. And, look, this new method does much better on cases like this. So clean! Now that we have seen the images, let’s see what the numbers say for these scenes. The NRW is the neural re-rendering technique we just saw, and the other one is the NERF paper from this year. The abbreviations show different ways of comparing the output images, the up and down arrows show whether they are subject to maximization or minimization. They are both relatively close, but, when we look at the new method, we see one of the rare cases where it wins decisively regardless of what we are measuring. Incredible. This paper truly is a great leap in just a few months, but of course, not everything is perfect here. This technique may fail to reconstruct regions that are only visible on just a few photos in the input dataset. The training still takes from hours to days, I take this as an interesting detail more than a limitation since this training only has to be done once, and then, using the technique can take place very quickly. But, with that, there you go, a neural algorithm that understands lighting, geometry, can disentangle the two, and reconstruct real-world scenes from just a few photos. It truly feels like we are living in a science fiction world. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
443,Elon Musk’s Neuralink Puts An AI Into Your Brain!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Due to popular request, today we will talk about Neuralink, Elon Musk’s neural engineering company that he created to develop brain-machine interfaces. And your first question likely is, why talk about Neuralink now? There was a recent event, and another one last year as well, why did I not cover that? Well, the launch event from last year indeed promised a great deal. In this series, we often look at research works that are just one year apart, and marvel at the difference scientists have been able to make in that tiny-tiny timeframe. So first, let’s talk about their paper from 2019, which will be incredible, and then, see how far they have come in a year, which, as you will see, is even more incredible. The promise is to be able to read and write information to and from the brain. To accomplish this, as of 2019, they used this robot to insert the electrodes into your brain tissue. You can see the insertion process here. From the close-up image you might think that this is a huge needle, but in fact, this needle is extremely tiny, you can see a penny for scale here. This is the story of how this rat got equipped with a USB port. As this process is almost like inserting microphones into its brain, now, we are able to read the neural signals of this rat. Normally, these are analog signals, which are read and digitized by Neuralink’s implant, and now this brain data is represented as a digital signal. Well, at first, this looks a bit like gibberish. Do we really have to undergo a brain surgery to get a bunch of these squiggly curves? What do these really do for us? Well, now that they are digitized, we can have the Neuralink chip analyze these signals and look for action potentials in them. These are also referred to as “spikes” because of their shape. That sounds a bit better, but still, what does this do for us? Let’s see. Here we have a person with a mouse in their hand. This is an outward movement with the mouse, and then, reaching back. Simple enough. Now, what you see here below is the activity of an example neuron. When nothing is happening, there is some firing, but not much activity, and now, look! When reaching out, this neuron fires a great deal, and suddenly, when reaching back, again, nearly no activity. This means that this neuron is tuned for an outward motion, and this other one is tuned for the returning motion. And all this is now information that we can read in real time, and the more neurons we can read, the more complex motion we can read. Absolutely incredible. However, this is still a little difficult to read, so let’s order them by what kind of motion makes them excited. And there we go! Suddenly, this is a much more organized way to present all this neural activity, and now we can detect what kind of motion the brain is thinking about. This was the reading part, and that’s just the start. What is even cooler is that we can invert this process, read this spiking activity, and just by looking at these, we can reconstruct the motion the human wishes to perform. With this, brain-machine interfaces can be created for people with all kinds of disabilities where the brain can still think about the movements, but the connection to the rest of the body is severed. Now, these people only have to think about moving, and then, the Neuralink device will read it and perform the cursor movement for them. It really feels like we live in a science fiction world. And all this signal processing is now possible automatically and in real time, and all we need for this is this tiny-tiny chip that takes just a few square millimeters. And don’t forget, that is just version one from 2019. Now, onwards to the 2020 event, where, it gets even better. The Neuralink device has been placed into Gertrude, the pig’s brain, and here, you see it in action. We see the raster view here, and luckily, we already know what it means, this lays bare the neural action potentials before our eyes, or in other words, which neuron is spiking and exactly when. Below with blue, you see these activities summed up for our convenience, and this way, you will not only see, but hear it too, as these neurons are tuned for snout boops. In other words, you will see and hear that the more the snout is stimulated, the more neural activity it will show. Let’s listen. And all this is possible today, and in real time. That was one of the highlights of the 2020 progress update event, but it went further. Much further! Look! This is a pig on a treadmill, and here you see the brain signal readings. This signal marked with the circle shows where a joint or limb is about to move, where the other, dimmer colored signal is the chip’s prediction as to what is about to happen. It takes into consideration periodicity, and predicts higher-frequency movement, like these sharp turns really well. The two are almost identical, and that means exactly what you think it means today, we can not only read and write, but even predict what the pig’s brain is about to do. And that was the part where I fell off the chair when I watched this event live. You can also see the real and predicted world-space positions for these body parts as well. Very close. Now note that there is a vast body of research in brain-machine interfaces, and many of these things were possible in lab conditions, and Neuralink’s quest here is to make them accessible for a wider audience within the next decade. If this project further improves at this rate, it could help many paralyzed people around the world live a longer, and more meaningful life, and the neural enhancement aspect is also not out of question. Just thinking about summoning your Tesla might also summon it, which sounds like science fiction, and based on these results, you see that it may even be one of the simplest tasks for a Neuralink chip in the future. And who knows, one day, maybe, with this device, these videos could be beamed into your brain much quicker, and this series would have to be renamed from Two Minute Papers to Two Second Papers, or maybe even Two Microsecond Papers. They might actually fit into two minutes like the title says, now that would truly be a miracle. Huge thanks to scientists at Neuralink for our discussions about the concepts descriped in this video and ensuring that you get accurate information. This is one of the reasons why our coverage of the 2020 event is way too late compared to many mainstream media outlets, which leads to a great deal less views for us, but it doesn’t matter. We are not maximizing views here, we are maximizing learning. Note that they are also hiring, if you wish to be a part of their vision and work with them, make sure to apply! The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
444,AI-Based Style Transfer For Video…Now in Real Time!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Style transfer is an interesting problem in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to super fun, and really good looking results. We have seen plenty of papers doing variations of style transfer, but I always wonder, can we can push this concept further? And the answer is, yes! For instance, few people know that style transfer can also be done for video! First, we record a video with our camera, then, take a still image from the video and apply our artistic style to it. Then, our style will be applied to the entirety of the video! The main advantage of this new method, compared to previous ones is that they either take too long, or we have to run an expensive pre-training step. With this new one, we can just start drawing and see the output results right away. But it gets even better. Due to the interactive nature of this new technique, we can even do this live! All we need to do is change our input drawing and it transfers the new style to the video as fast as we can draw! This way, we can refine our input style for as long as we wish, or until we find the perfect way to stylize the video. And there is even more. If this works interactively, then it has to be able to offer an amazing workflow where we can capture a video of ourselves live, and mark it up as we go. Let’s see! Oh wow, just look at that. It is great to see that this new method also retains temporal consistency over a long time frame, which means that even if the marked up keyframe is from a long time ago, it can still be applied to the video and the outputs will show minimal flickering. And note that we can not only play with the colors, but with the geometry too! Look, we can warp the style image, and it will be reflected in the output as well. I bet there is going to be a followup paper on more elaborate shape modifications as well! And, this new work improves upon previous methods in even more areas. For instance, this is a method from just one year ago, and here you see how it struggled with contour-based styles. Here is a keyframe of the input video, and here is the style that we wish to apply to it. Later, this method from last year seems to lose not only the contours, but a lot of visual detail is also gone. So, how did the new method do in this case? Look! It not only retains the contours better, but a lot more of the sharp details remain in the outputs. Amazing. Now note that this technique also comes with some limitations. For instance, there is still some temporal flickering in the outputs, and in some cases, separating the foreground and the background is challenging. But really, such incredible progress in just one year! And I can only imagine what this method will be capable of two more papers down the line. What a time to be alive! Make sure to have a look at the paper in the video description, and you will see many additional details, for instance, how you can just partially fill in some of they keyframes with your style, and still get an excellent result. Thanks for watching and for your generous support, and I'll see you next time!"
445,Beautiful Results From 30 Years Of Light Transport Simulation!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. I have been yearning for a light transport paper, and goodness, was I ecstatic when reading this one. And by the end of the video, I hope you will be too. And now, we only have to go through just about 30 years of light transport research. As some of you know, if we immerse ourselves into the art of light transport simulations, we can use our computer to simulate millions and millions of light rays, and calculate how they get absorbed or scattered off of our objects in a virtual scene. Initially, we start out with a really noisy image, and as we add more rays, the image gets clearer and clearer over time. The time it takes for these images to clean up depends on the complexity of the geometry and our material models, but it typically takes a while. This microplanet scene mostly contains vegetation, which are matte objects, these we also refer to as diffuse materials, these typically converge very quickly. As you see, we get meaningful progress on the entirety of the image within the first two minutes of the rendering process. And remember, in light transport simulations, noise is public enemy number one. This used a technique called path tracing. Let’s refer to it as the okay technique. And now, let’s try to use path tracing, this okay technique to render this torus in a glass enclosure. This is the first two minutes of the rendering process, and it does not look anything like the previous scene. The previous one was looking pretty smooth after two minutes, whereas here, you see, this is indeed looking grim. We have lots of these fireflies, which will take us up to a few days of computation time to clean up, even if we have a modern, powerful machine. So, why did this happen? The reason for this is that there are tricky cases for specular light transport that take many-many millions, if not billions of light rays to compute properly. Specular here means mirror-like materials, those can get tricky, and this torus that has been enclosed in there is also not doing too well. So this was path tracing, the okay technique. Now let’s try a better technique called Metropolis Light Transport! This method is the result of a decade of research and is much better in dealing with difficult scenes. This particular variant is a proud Hungarian algorithm by a scientist called Csaba Kelemen and his colleagues at the Technical University of Budapest. For instance, here is a snippet of our earlier paper on a similarly challenging scene. This is how the okay path tracer did, and in the same amount of time, this is what Metropolis Light Transport, the better technique could do! This was a lot more efficient, so let’s see how it does with the torus. Now that’s unexpected! This is indeed a notoriously difficult scene to render, even for Metropolis Light Transport, the better technique. As you see, the reflected light patterns that we also refer to as caustics on the floor are much cleaner, but the torus is still not giving up. Let’s jump another 15 years of light transport research, and use a technique that goes by the name Manifold Exploration. Let’s call this the best technique. Wow. Look at how beautifully it improves the image, it is not only much cleaner, but also converges much more gracefully. It doesn’t go from a noisy image to a slightly less noisy image, but almost immediately gives us a solid baseline, and new, cleaned up light paths appear over time. This technique is from 2012, and it truly is mind boggling how good it is. This technique is so difficult to understand and implement, that to the best of my knowledge, the number of people who can and have implemented it properly is exactly one. And that one person is Wenzel Jakob, one of the best minds in the game, and believe it or not, he wrote this method as a PhD student in light transport research. And today, as a professor at EPFL Switzerland, he and his colleagues set out to create a technique that is as good as manifold exploration, the best technique, but is much simpler. Well, good luck with that, I thought when skimming through the paper. Let’s see how it did. For instance, we have some caustics at the bottom of a pool of water, as expected, lots of firefly noise with the okay path tracer, and now, hold on to your papers, and here is the new technique. Just look at that! It can do so much better in the same amount of time! Let’s also have a look at this scene with lots and lots of specular microgeometry, or in other words, glints. This is also a nightmare to render. With the okay path tracer, we have lots of flickering from one frame to the next, and here you see the result with the new technique. Perfect. So it is indeed possible to take the best technique, manifold exploration, and reimagine it in a way that ordinary humans can also implement. Huge congratulations to the authors on this work, that I think, is a crown achievement in light transport research. And that’s why I was ecstatic when I first read through this incredible paper. Make sure to have a look at the paper and you will see how they borrowed a nice little trick from a recent work in nuclear physics to tackle this problem. The presentation of the paper and the talk video with the details is also brilliant and I urge you to have a look at it in the video description. This whole thing got me so excited I was barely able to fall asleep for several days now. What a time to be alive! Now, while we look through some more results from this paper, if you feel a little stranded at home and are thinking that this light transport thing is pretty cool, and you would like to learn more about it, I held a Master-level course on this topic at the Technical University of Vienna. Since I was always teaching it to a handful of motivated students, I thought that the teachings shouldn’t only be available for the privileged few who can afford a college education, but the teachings should be available for everyone. Free education for everyone, that’s what I want. So, the course is available free of charge for everyone, no strings attached, so make sure to click the link in the video description to get started. We write a full light simulation program from scratch there, and learn about physics, the world around us, and more. Also, note that my former PhD advisor, Michael Wimmer is looking to hire a postdoctoral researcher in this area, which is an amazing opportunity to push this field forward. The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
446,This AI Can Deal With Body Shape Variation!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This glorious paper from about 7 years ago was about teaching digital creatures to walk, the numbers here showcase the process of learning over time, and it is clear that the later generations did much better than the earlier ones. These control algorithms are not only able to teach these creatures to walk, but they are quite robust against perturbations as well, or, more simply put, we can engage in one of the favorite pastimes of a computer graphics researcher, which is, of course, throwing boxes at the character and seeing how well it can take it. This one has done really well! Well, kind of. Now we just noted that this is a computer graphics paper, and an amazing one at that, but it does not yet use these incredible new machine learning techniques that just keep getting better year by year. You see, these agents could learn to inhabit a given body, but I am wondering what would happen if we would suddenly change their bodies on the fly? Could previous methods handle it? Unfortunately, not really. And I think it is fair to say that an intelligent agent should have the ability to adapt when something changes. Therefore, our next question is, how far have we come in these 7 years? Can these new machine learning methods help us create a more general agent that could control not just one body, but a variety of different bodies? Let’s have a look at today’s paper, and with that, let the fun begin. This initial agent was blessed with reasonable body proportions, but of course, we can’t just leave it like that. Yes, much better. And, look, all of these combinations can still walk properly, and all of them use the same one reinforcement learning algorithm. But do not think for a second that this is where the fun ends, no, no, no. Now, hold on to your papers, and now let’s engage in these horrific asymmetric changes. There is no way that the same algorithm could be given this body and still be able to walk. Goodness! Look at that! It is indeed still able to walk. If you have been holding on to your papers, good now squeeze that paper, because after adjusting the height and thickness of this character, it could still not only walk, but even dance. But it goes further. Do you remember the crazy asymmetric experiment for the legs? Let’s do something like that with thickness. And as a result, they can still not only walk, but even perform gymnastic moves. Wo-hoo! Now it’s great that one algorithm can adapt to all of these body shapes, but it would be reasonable to ask, how much do we have to wait for it to adapt? Have a look here. Are you seeing what I am seeing? We can make changes to the body on the fly, and the AI adapts to it immediately. No re-training, or parameter tuning is required. And that is the point where I fell off the chair when I read this paper. What a time to be alive! And now, Scholars, bring in the boxes. Ha-haa! It can also inhabit dogs and fish, and we can also have some fun with them as we grab a controller and control them in real time. The technique is also very efficient as it requires very little memory and computation, not to mention that we only have to train one controller for many body types instead of always retraining after each change to the body. However, of course, this algorithm isn’t perfect, one of its key limitations is that it will not do well if the body shapes we are producing strays too far away from the ones contained in the training set. But let’s leave some space for the next followup paper too. And just one more thing that didn’t quite fit into this story. Every now and then I get these heartwarming messages from you Fellow Scholars noting that you’ve been watching the series for a while and decided to turn your lives around, and go back to study more and improve. Good work Mr. Munad! That is absolutely amazing and reading these messages are a true delight to me. Please keep them coming. Thanks for watching and for your generous support, and I'll see you next time!"
447,Enhance! Neural Supersampling is Here!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Let’s talk about video super resolution. The problem statement is simple, in goes a coarse video, the technique analyzes it, guesses what’s missing, and out comes a detailed video. You know, the CSI thing. Enhance! However, of course, reliably solving this problem is anything but simple. This previous method is called TecoGAN, and it is able to give us results that are often very close to reality. It is truly amazing how much this technique understands the world around us from just this training set of low and high-resolution videos. However, as amazing as super resolution is, it is not the most reliable way of delivering high-quality images in many real-time applications, for instance, video games. Note that in these applications, we typically have more data at our disposal, but in return, the requirements are also higher: we need high-quality images at least 60 times per second, temporal coherence is a necessity, or in other words, no jarring jumps and flickering is permitted. And hence, one of the techniques often used in these cases is called supersampling. At the risk of simplifying the term, supersampling means that we split every pixel into multiple pixels to compute a more detailed image, and then, display that to the user. Does this work? Yes it does, it works wonderfully, but it requires a lot more memory and computation, therefore it is generally quite expensive. So our question today is, is it possible to use these amazing learning-based algorithms to do it a little smarter? Let’s have a look at some results from a recent paper that uses a neural network to perform supersampling at a more reasonable computational cost. Now, in goes the low-resolution input, and, my goodness, like magic, out comes this wonderful, much more detailed result. And here is the reference, which is the true, higher-resolution image. Of course, the closer the neural supersampling is to this, the better. And as you see, this is indeed really close, and much better than the pixelated inputs. Let’s do one more. Wow. This is so close I feel that we are just a couple papers away from the result being indistinguishable from the real reference image. Now, we noted that this method has access to more information than the previously showcased super resolution method. It looks at not just one frame, but a few previous frames as well, can use an estimation of the motion of each pixel over time, and also gets depth information. These can be typically produced inexpensively with any major game engine. So, how much of this data is required to train this neural network? Hold on to your papers, because only 80 videos were used, and the training took approximately 1.5 days on one TITAN V, which is an expensive, but commercially available graphics card. And no matter, because this step only has to be done once, and depending on how high the resolution of the output should be, with the faster version of the technique, the upsampling step takes from 8 to 18 milliseconds, so this runs easily in real time! Now, of course, this is not the only modern supersampling method, this topic is subject to a great deal of research, so let’s see how it compares to others. Here you see the results with TAAU, the temporal upsampling technique used in Unreal Engine, an industry-standard game engine. And, look! This neural supersampler is significantly better at antialiasing, or in other words, smoothing these jagged edges, and not only that, but it also resolves many more of the intricate details of the image. Temporal coherence has also improved a great deal, as you see, the video output is much smoother for the new method. The paper also contains a ton more comparisons against recent methods, so make sure to have a look! Like many of us, I would love to see a comparison against NVIDIA’s DLSS solution, but I haven’t been able to find a published paper on the later versions of this method. I remain very excited about seeing that too. And for now, with techniques like this, the future of video game visuals and other real-time graphics applications is looking as exciting as it’s ever been. What  a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
448,Remove This! AI-Based Video Completion is Amazing!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Have you ever had a moment where you took the perfect photo, but upon closer inspection, there was this one annoying thing that ruined the whole picture? Well, why not just take a learning algorithm to erase those cracks in the facade of a building, or a photobombing sheep? Or, to even reimagine ourselves with different eye colors, we can try one of the many research works that are capable of something that we call image inpainting. What you see here is the legendary PatchMatch algorithm at work, which, believe it or not, is a handcrafted technique from more than 10 years ago. Later, scientists at NVIDIA published a more modern inpainter that uses a learning-based algorithm to do this more reliably, and for a greater variety of images. These all work really well, but the common denominator for these techniques is that they all work on inpainting still images. Would this be a possibility for video? Like, removing a moving object or person from a video? Is this possible, or is it science fiction? Let’s see if these learning-based techniques can really do more. And now, hold on to your papers, because this new work can really perform proper inpainting for video. Let’s give it a try by highlighting this human. And pro tip: also highlight the shadowy region for inpainting to make sure that not only the human, but its silhouette also disappears from the footage. And, look! Wow. Let’s look at some other examples. Now that’s really something because video is much more difficult due to the requirement of temporal coherence, which means that it’s not nearly enough if the images are inpainted really well individually, they also have to look good if we weave them together into a video. You will hear and see more about this in a moment. Not only that, but if we highlight a person, this person not only needs to be inpainted, but we also have to track the boundaries of this person throughout the footage and then inpaint a moving region. We get some help with that, which I will also talk about in a moment. Now, as you see here, these all work extremely well, and believe it or not, you have seen nothing yet, because so far, another common denominator in these examples was that we highlighted regions inside the video. But that’s not all. If you have been holding on to your papers so far, now squeeze that paper, because we can also go outside, and expand our video spatially with even more content. This one is very short so I will keep looping it. Are you ready? Let’s go. Wow! My goodness! The information from inside of the video frames is reused to infer what should be around the video frame, and all this in a temporally coherent manner. Now, of course, this is not the first technique to perform this, so let’s see how it compares to the competition by erasing this bear from the video footage. The remnants of the bear are visible with a wide selection of previously published techniques from the last few years. This is true even for these four methods from last year. And, let’s see how this new method did one the same case. Yup, very good, not perfect, we still see some flickering. This is the temporal coherence example, or the lack thereof that I have promised earlier. But now, let’s look at this example with the BMX rider. We see similar performance with the previous techniques, and now, let’s have a look at the new one. Now that’s what I’m talking about! Not a trace left from this person, the only clue that we get in reconstructing what went down here is the camera movement. It truly feels like we are living in a science fiction world. What a time to be alive! Now these were the qualitative results, and now, let’s have a look at the quantitative results. In other words, we saw the videos, now let’s see what the numbers say. We could talk all day about the peak signal to noise ratios or structural similarity or other ways to measure how good these techniques are, but you will see in a moment that it is completely unnecessary. Why is that? Well, you see here that the second best results are underscored and highlighted with blue. As you see, there is plenty of competition, as the blues are all over the place. But there is no competition at all for the first place, because this new method smokes the competition in every category. This was measured on a dataset by the name Densely Annotated Video Segmentation, DAVIS in short, this contains 150 video sequences and it is annotated, which means that many of the objects are highlighted throughout this video, so for the cases in this dataset, we don’t have to deal with the tracking ourselves. I am truly out of ideas as to what I should wish for two more papers down the line. Maybe not only removing the tennis player, but putting myself in there as a proxy? We can already grab a controller and play as if we were real characters in real broadcast footage, so who really knows. Anything is possible. Let me know in the comments what you have in mind for potential applications and what you would be excited to see two more papers down  the line! Thanks for watching and for your generous support, and I'll see you next time!"
449,This Is What Simulating a 100 Million Particles Looks Like!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. If we study the laws of fluid motion and implement them in a computer program, we can create and enjoy these beautiful fluid simulations. And not only that, but today, with the amazing progress in computer graphics research, we can even enrich our physics simulations with anisotropic damage and elasticity. So what does that mean exactly? This means that we can simulate more extreme topological changes in these virtual objects. This leads to better material separation when the damage happens. So it appears that today we can do a great deal, but these techniques are pretty complex, and take quite a while to compute, and they typically run on your processor. That’s a pity, because many powerful consumer computers also have a graphics card, and if we could restate some of these algorithms to be able to run them on those, they would run significantly faster. So, do we have any hope for that? Well, today’s paper promises a new particle data structure that is better suited for number crunching on the graphics card, and is, hence, much faster than its predecessors. As a result, this runs the Material Point Method, the algorithm that is capable of simulating these wondrous things you are seeing here, not only on your graphics card, but the work on one problem can be also be distributed between many-many graphics cards. This means that we get crushing concrete, falling soil, candy bowls, sand armadillos, oh my! You name it. And all this, much faster than before. Now, since these are some devilishly detailed simulations, please do not expect several frames per second kind of performance, we are still in the seconds per frame region, but we are not that far away. For instance, hold on to your papers because this candy bowl example contains nearly 23 million particles, and despite that, it runs in about 4 seconds per frame on a system equipped with four graphics cards. Four seconds per frame! My goodness! If somebody told this to me today without showing me this paper, I would have not believed a word of it. But there is more. You know what, let’s double the number of particles and pull up this dam break scene. What you see here is 48 million particles that runs in 15 seconds per frame. Let’s do even more. These sand armadillos contain a total of 55 million particles, and take about 30 seconds per frame. And in return, look at that beautiful mixture of the two sand materials. And with half a minute per frame, that’s a great deal, I’ll take this any day of the week. And if we wish to simulate crushing this piece of concrete with a hydraulic press, that will take nearly a hundred million particles. Just look at that footage. This is an obscene amount of detail. And the price to be paid for this is nearly four minutes of simulation time for every frame that you see on the screen here. Four minutes you say? Hmm, that’s a little more than expected. Why is that? We had several seconds per frame for the others, not minutes per frame. Well, it is because the particle count matters a great deal, but that’s not the only consideration for such a simulation. For instance, here, you see with delta t something that we call time step size. The smaller this number is, the tinier the time steps with which we can advance the simulation when computing every interaction, and hence, the more steps there are to compute. In simpler words, generally, time step size is also an important factor in the computation time, and the smaller this is, the slower the simulation will be. As you see, we have to simulate 5 times more time steps to make sure that we don’t miss any particle interactions, and hence, this takes much longer. Now, this one appears to be perhaps the simplest simulation of the bunch, isn’t it? No, no, no. Quite the opposite! If you have been holding on to your papers so far, now squeeze that paper, because, and watch carefully. They wiggle. They… wiggle! Why do they wiggle? These are just around 6 thousand bombs, which is not a lot, however, wait a minute…each bomb is a collection of particles, giving us a total of not a measly 6 thousand, but, a whopping 134 million particle simulation, and hence, we may think that it’s nearly impossible to perform in a reasonable amount of time. The time steps are not that far apart for this one, so we can do it in less than one minute per frame. This was nearly impossible when I started my PhD, and today, less than a minute for one frame. It truly feels like we are living in a science fiction world. What a time to be alive! I also couldn’t resist creating a slow-motion version of some of these videos, so if this is something that you wish to see, make sure to visit our Instagram page in the video description for more. Thanks for watching and for  your generous support, and I'll see you next time!"
450,What is De-Aging?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, we are living the advent of neural network-based image generation algorithms. What you see here is some super high quality results from a technique developed by scientists at NVIDIA called StyleGAN2. Right, all of these were generated by a learning algorithm. And, while generating images of this quality is a great achievement, but if we have an artistic vision, we wonder, can we bend these images to our will? Can we control them? Well, kind of, and one of the methods that enables us to do that is called image interpolation. This means that we have a reference image for style, and a target image, and with this, we can morph one human face into another. This is sufficient for some use cases, however, if we are looking for more elaborate edits, we hit a wall. Now it’s good that we already know what StyleGAN is, because this new work builds on top of that, and shows exceptional image editing and interpolation abilities. Let’s start with the image editing part! With this new work, we can give anyone glasses, and a smile, or even better, transform them into a variant of the Mona Lisa. Beautiful. The authors of the paper call this process semantic diffusion. Now, let’s have a closer look at the expression and pose change possibilities. I really like that we have fine-grained control over these parameters, and what’s even better, we don’t just have a start and endpoint, but all the intermediate images make sense, and can stand on their own. This is great for pose and expression because we can control how big of a smile we are looking for, or even better, we can adjust the age of the test subject with remarkable granularity. Let’s go all out! I like how Mr. Cumberbatch looks nearly the same as a baby, we might have a new mathematical definition for baby face right there, and apparently Mr. DiCaprio scores a bit lower on that. And I would say that both results are quite credible! Very cool! And now, onto image interpolation. What does this new work bring to the table in this area? Previous techniques are also pretty good at morphing…until we take a closer look at them. Let’s continue our journey with three interpolation examples, with increasing difficulty. Let’s see the easy one first. I was looking for morphing example with long hair, you will see why right away. This is how the older method did. Uh-oh. One more time. Do you see what I see? If I stop the process here, you see that this is an intermediate image that doesn’t make sense. The hair over the forehead just suddenly vanishes into the ether. Let’s see how the new method deals with this issue! Wow, much cleaner, and I can stop nearly anywhere and leave the process with a usable image. Easy example, checkmark. Now let’s see an intermediate-level example! Let’s go from an old black and white Einstein photo to a recent picture with colors and stop the process at different points, and…yes, I prefer the picture created with the new technique close to every single time! Do you agree? Let me know in the comments below! Intermediate example, checkmark. And now, onwards to the let’s hardest, nastiest example. This is going to sound impossible, but we are going to transform the Eiffel Tower into the Tower Bridge. Yes, that sounds pretty much impossible. So let’s see how a conventional interpolation technique did here. Well…that’s not good. I would argue that nearly none of the images showcased here would be believable if we stopped the process and took them out. And let’s see the new method. Hmm, that makes sense, we start with one tower, then, two towers grow from the ground, and…look! Wow! The bridge slowly appears between them. That was incredible. While we look at some more results, what really happened here? At the risk of simplifying the contribution of this new paper, we can say that during interpolation, it ensures that we remain within the same domain for the intermediate images. Intuitively, as a result, we get less nonsense in the outputs, and can pull off morphing not only between human faces, but even go from a black and white photo to a colored one, and what’s more, it can even deal with completely different building types. Or, you know, just transform people into Mona Lisa variants. Absolutely amazing. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
451,"Beautiful Elastic Simulations, Now Much Faster!","Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. It is time for some fluids. Hm-hmm! As many of you know, in this series, we often talk about fluid simulations, and sometimes, the examples showcase a fluid splash, but not much else. However, in real production environments, these simulations often involve complex scenes with many objects that interact with each other, and therein lies the problem. Computing these interactions is called coupling, and it is very difficult to get right, but is necessary for many of the beautiful scenes you will see throughout this video. Getting this right is of utmost importance if we wish to create a realistic simulation where fluids and solids interact. So first question would be, as many of these techniques build upon The Material Point Method or MPM in short, why not just use that? Well, let’s do exactly that and see how it does on this scene. Let’s drop the liquid ball on the bunny…and. Uh-oh..a lot of it is now stuck to the bunny. This is not supposed to happen. So, what about the improved version of MPM? Yup, still too sticky. And now, let’s have a look at how this new technique handles this situation! I want to see dry, and floppy bunny ears. Yes, now that’s what I’m talking about! Now then. That’s great, but what else can this do? A lot more. For instance, we can engage in the favorite pastimes of the computer graphics researcher, which is, of course, destroying objects in a spectacular manner. This is going to be a very challenging scene. Ouch! And now, let physics take care of the rest. This was a harrowing, but beautiful simulation. And we can try to challenge the algorithm even more. Here, we have three elastic spheres filled with water, and now, watch how they deform as they hit the ground, and how the water gushes out exactly as it should. And now hold on to your papers, because there is a great deal to be seen in this animation, but the most important part remains invisible. Get this all three spheres use a different hyperelasticity model to demonstrate that this new technique can be plugged into many existing techniques. And, it works so seamlessly that I don’t think anyone would be able to tell the difference. And, it can do even more. For instance, it can also simulate wet sand. Wow! And I say wow not only because of this beautiful result, but there is more behind it. If you are one of our hardcore, long time Fellow Scholars, you may remember that three years ago, we needed an entire paper to pull this off. This algorithm is more general and can simulate this kind of interaction between liquids and granular media as an additional side effect. We can also simulate dropping this creature into a piece of fluid, and as we increase the density of the creature, it sinks in in a realistic manner. While we are lifting frogs and help an elastic bear take a bath, let’s look at why this technique works so well. The key to achieving these amazing results in a reasonable amount of time is that this new method is able to find these interfaces where the fluids and solids meet and handles their interactions in a way that we can advance the time in our simulation in larger steps than previous methods. This leads to not only these amazingly general and realistic simulations, but they also run faster. Furthermore, I am very happy about the fact that now, we can not only simulate these difficult phenomena, but we don’t even have to implement a technique for each of them, but we can take this one, and simulate a wide variety of fluid-solid interactions. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
452,"Finally, Deformation Simulation... in Real Time!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today, with the power of computer  graphics research, we can use our   computers to run fluid simulations, simulate  immersing a selection of objects into jelly,   or tear meat in a way that much like in  reality, it tears along the muscle fibers. If we look at the abstract of this amazing new  paper, we see this, quoting: “This allows us to   trace high speed motion of objects colliding  against curved geometry, to reduce the number   of constraints, to increase the robustness of  the simulation, and to simplify the formulation   of the solver.” What! This sounds impossible,  but at the very least, outrageously good. Let’s   look at three examples of what it can do and see  for ourselves if it can live up to its promise. One. It can simulate a steering  mechanism full of joints and contact.   Yup, an entire servo steering mechanism is  simulated with a prescribed mass ratio. Loving   it. I hereby declare that it passes inspection,  and now, we can take off for some off-roading.   All of the movement is simulated really well,  and… wait a minute. Hold on to your papers!   Are you seeing what I am seeing? Look. Even the  tire deformations are part of the simulation!   Beautiful. And now, let’s do a stress test,  and race through a bunch of obstacles and see   how well those tires can take it. At the end of  the video, I will tell you how much time it takes   to simulate all this…and note that I had to look  three times because I could not believe my eyes. Two. Restitution! Or in other words, we can smash  an independent marble into a bunch of others   and their combined velocity will be correctly  computed. We know for a fact that the computations   are correct, because when I stop the video  here, you can see that the marbles themselves   are smiling. The joys of curved geometry and  specular reflections. Of course, this is not true,   because if we attempt to do the same with  a classical earlier technique by the name   position based dynamics, this would happen.  Yes, the velocities become erroneously large   and the marbles jump off of the wire. And  they still appear to be very happy about it.   Of course, with the new technique, the  simulation is much more stable and realistic. Talking about stability. Is it stable  only in a small-scale simulation,   or can it take a huge scene with lots  of interactions? Would it still work?   Let’s run a stress test and find out.  Ha-haa! This animation can run all day long   and not one thing appears to  behave incorrectly. Loving this. Three. It can also simulate these beautiful  high-frequency rolls that we often experience   when we drop a coin on a table. This kind of  interaction is very challenging to simulate   correctly because of the high-frequency nature of  the motion and the curved geometry that interacts   with the table. I would love to see a technique  that algorithmically generates the sound for this.   I could almost hear its sound  in my head! Believe it or not,   this should be possible and is subject to  some research attention in computer graphics.   The animation was given, but the sounds  were algorithmically generated. Listen!   Let me know in the comments if you are one  of our OG Fellow Scholars who were there   when this episode was published  hundreds of videos ago. So, how much do we have to wait to simulate  all of these crazy physical interactions?   We mentioned that the tires are stiff and take a  great deal of computation to simulate properly.   So, as always… all nighters, right? Nope! Look at  that! Holy mother of papers! The car example takes   only 18 milliseconds to compute per frame,  which means 55 frames per second. Goodness!   Not only do we not need an all-nighter, we  don’t even need to leave for a coffee break!   And the rolling marbles took even less, and,  wo-hoo! The high-frequency coin example needs   only one third of a millisecond, which means that  we can generate more than 3000 frames with it per   second. We not only don’t need an all nighter or  a coffee break, we don’t even need to wait at all! Now, at the start of the video, I noted that the  claim in the abstract sounds almost outrageous.   It is because it promises to be able to do more  than previous techniques, simplify the simulation   algorithm itself, make it more robust, and do all  this while being blazing fast. If someone told me   that there is a work that does all this at the  same time, I would say that give me that paper   immediately because I do not believe a word of  it. And yet, it really lives up to its promise. Typically, as a research field matures, we see new  techniques that can do more than previous methods,   but the price to be paid for it is in the form  of complexity. The algorithms get more and more   involved over time, and with that, they often  get slower and less robust. The engineers in the   industry have to decide how much complexity they  are willing to shoulder to be able to simulate all   of these beautiful interactions. Don’t forget,  these code bases have to be maintained and   improved for many-many years so choosing a simple  base algorithm is of utmost importance. But here,   none of these factors need to be considered,  because there is nearly no tradeoff here:   it is simpler, more robust,  and better at the same time.   It really feels like we are living in a  science fiction world. What a time to be alive! Huge congratulations to scientists at NVIDIA  and the university of Copenhagen for this.   Don’t forget, they could have kept the results  for themselves, but they chose to share the   details of this algorithm with everyone, free  of charge. Thank you so much for doing this. Thanks for watching and for your generous  support, and I'll see you next time!"
453,Simulating Dragons Under Cloth Sheets!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today, we are going to see a lot of physics  simulations with many-many collisions. In   particular, you will see a lot of beautiful  footage which contains contact between thin   shells and rigid bodies. In this simulation  program, at least one of these objects will   always be represented as a signed distance field.  This representation is useful because it helps us   rapidly compute whether something  is inside or outside of this object. However, a collision here takes two objects, of  course, so the other object will be represented   as a triangle mesh, which is perhaps the  most common way of storing object geometry in   computer graphics. However, with this, we have  a problem. Signed distance fields work great,   triangle meshes also work great for a number  of applications, however, computing where they   overlap when they collide is still slow and  difficult. If that does not sound bad enough,   it gets even worse than that. How so? Let’s  have a look together. Experiment number one.   Let’s try to intersect this cone with this  rubbery sheet using a traditional technique,   and there is only one rule no poking through  is allowed. Well, guess what just happened. This   earlier technique is called point sampling,  and we either have to check too many points   in the two geometries against each other,  which takes way too long and still fails, or,   we skimp on some of them, but then, this happens.  Important contact points go missing. Not good. Let’s see how this new method does with  this case. Now that’s what I am talking   about! No poking through anywhere to  be seen. And, let’s have another look.   Wait a second. Are you seeing what I am seeing?  Look at this part…after the first collision,   the contact points are moving ever so slightly,  many-many times, and the new method is not missing   any of them. Then, things get a little out of  hand, and it still works perfectly. Amazing!   I can only imagine how many of these  interactions the previous point sampling   technique would miss, but we won’t know  because it has already failed long ago. Let’s do another one. Experiment number two.  Dragon versus cloth sheet. This is the previous   point sampling method. We now see that it can  find some of the interactions, but many others   go missing, and due to the anomalies, we can’t  continue the animation by pulling the cloth sheet   off of the dragon because it is stuck. Let’s see  how the new method fared in this case. Oh yeah!   Nothing pokes through, and therefore, now, we can  continue the animation by doing this. Excellent! Experiment number three! Rope curtain. Point  sampling. Oh no! This is a disaster. And now,   hold on to your papers, and marvel at the new  proposed method. Just look at how beautifully   we can pull the rope curtain through this fine  geometry. Loving it. So, of course, if you are   a seasoned fellow scholar, you probably want  to know, how much computation do we have to do   with the old and new methods. How much longer do  I have to wait for the new, improved technique? Let’s have a look together! Each rigid shell here  is a mesh that uses a 129 thousand triangles, and   the old point sampling method took 15 milliseconds  to compute the collisions, and this time, it has   done reasonably well. What about the new one?  How much more computation do we have to perform   to make sure that our simulations are more robust?  Please stop the video and make a guess. I’ll wait.   Alright, let’s see…and, the new one does it in  half a millisecond. Half a millisecond. It is   not slower at all, quite the opposite thirty  times faster. My goodness! Huge congratulations   on yet another masterpiece from scientists  at NVIDIA and the University of Copenhagen. While we look at some more results, in case  you are wondering, the authors used NVIDIA’s   Omniverse platform to create these amazing  rendered worlds. And now, with this new method,   we can infuse our physics simulation programs with  a robust, and blazing fast collision detector, and   I truly can’t wait to see where talented artists  will take these tools. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
454,This AI Creates A 3D Model of You!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, a variety of techniques exist that can take an image that contains humans, and perform pose estimation on it. This gives us these interesting skeletons that show us the current posture of the subjects shown in these images. Having this skeleton opens up the possibility for many cool applications, for instance, it’s great for fall detection and generally many kinds of activity recognition, analyzing athletic performance and much, much more. But that would require that we can do it for not only still images, but animations. Can we? Yes, we already can, this is a piece of footage from a previous episode that does exactly that. But what if we wish for more? Let’s think bigger, for instance, can we reconstruct not only the pose of the model, but the entire 3D geometry of the model itself? You know, including the body shape, face, clothes, and more. That sounds like science fiction, right? Or with today’s powerful learning algorithms, maybe it is finally a possibility, who really knows? Let’s have a look together and evaluate it with three, increasingly more difficult experiments. Let’s start with experiment number one, still images. Nice! I think if I knew these people, I might have a shot at recognizing them solely from the 3D reconstruction. And not only that, but I also see some detail in the clothes, a suit can be recognized, and jeans have wrinkles. This new method uses a different geometry representation that enables higher-resolution outputs, and it immediately shows. Checkmark. It is clearly working quite well on still images. And now, hold on to your papers for experiment number two, because it can not only deal with still images of the front side only, but it can also reconstruct the backside of the person. Look! My goodness, but hold on for a second…that part of the data is completely unobserved. We haven’t seen the backside…so, how is that even possible? Well, we have to shift our thinking a little. An intelligent person would be able to infer some of these details, for instance, we know that this is a suit, or that these are boots, and we know roughly what the backside of these objects should look like. This new method leans on an earlier technique by the name image to image translation to estimate this data. And it truly works like magic! If you take a closer look, you see that we have less detail in the backside than in the front, but the fact that we can do this is truly a miracle. But we can go even further. I know it is not reasonable to ask, but what about video reconstruction? Let’s have a look. Don’t expect miracles, at least not yet, there is obviously still quite a bit of flickering left, but the preliminary results are quite encouraging, and I am fairly certain that two more papers down the line, and these video results will be nearly as good as the ones were for the still images. The key idea here is that the new method performs these reconstructions in a way that is consistent, or in other words, if there is a small change in the input model, there will also be a small change in the output model. This is the property that opens up the possibility to extend this method to videos! So, how does it compare to previous methods? All of these competing techniques are quite recent as they are from 2019. They appear to be missing a lot of detail, and I don’t think we would have a chance of recognizing the target subject from the reconstructions. And now, just a year and a half later, look at that incredible progress! It truly feels like we are living in a science fiction world. What  time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
455,Making Talking Memes With Voice DeepFakes!,"Dear Fellow Scholars, hold on to your papers, because we have an emergency situation. Everybody can make deepfakes now by recording a voice sample, such as this one, and the lips of the target subject will move as if they themselves were saying this. Not bad huh? And now to see what else this new method can do, first, let’s watch this short clip of a speech, and make sure to pay attention to the fact that the louder voice is the English translator, and if you pay attention, you can hear the chancellor’s original voice in the background too. So what is the problem here? Strictly speaking, there is no problem here, this is just the way the speech was recorded, however, what if we could recreate this video in a way that the chancellor’s lips would be synced not to her own voice, but to the voice of the English interpreter? This would give an impression as if the speech was given in English, and the video content would follow what we hear. Now that sounds like something straight out of a science fiction movie, perhaps even with today’s advanced machine learning techniques, but let’s see if it’s possible. This is a state of the art technique from last year that attempts to perform this. Hmm…there are extraneous lip movements which are the remnants of the original video, so much so that she seems to be giving two speeches at the same time. Not too convincing. So, is this not possible to pull off? Well, now, hold on to your papers, let’s see how this new paper does at the same problem. Wow, now that’s significantly better! The remnants of the previous speech are still there, but the footage is much, much more convincing. What’s even better is that the previous technique was published just one year ago by the same research group. Such a great leap in just one year, my goodness! So apparently, this is possible. But I would like to see another example, just to make sure. Checkmark. So far, this is an amazing leap, but believe it or not, this is just one of the easier applications of the new model, so let’s see what else it can do! For instance, many of us are sitting at home, yearning for some learning materials, but the vast majority of these were recorded in only one language. What if we could redub famous lectures into many other languages? Look at that! Any lecture could be available in any language and look as if they were originally recorded in these foreign languages as long as someone says the words. Which can also be kind of automated through speech synthesis these days. So, it clearly works well on real characters…but. Are you thinking what I am thinking? Three, what about lip syncing animated characters? Imagine if a line has to be changed in a Disney movie, can we synthesize new video footage without calling in the animators for a yet another all-nighter? Let’s give it a try! Indeed we can! Loving it! Let’s do one more. Four, of course, we have a lot of these meme gifs on the internet. What about redubbing those with an arbitrary line of our choice? Yup, that is indeed also possible. Well done! And imagine that this is such a leap just one more work down the line from the 2019 paper, I can only imagine what results we will see one more paper down the line. It not only does what it does better, but it can also be applied to a multitude of problems. What a time to be alive! When we look under the hood, we see that the two key components that enable this wizardry are here and here. So what does this mean exactly? It means that we jointly improve the quality of the lip syncing, and the visual quality of the video. These two modules curate the results offered by the main generator neural network, and reject solutions that don’t have enough detail or don’t match the speech that we hear, and thereby they steer it towards much higher-quality solutions. If we continue this training process for 29 hours for the lip-sync discriminator, we get these incredible results. Now, let’s have a quick look at the user study, and, humans appear to almost never prefer the older method compared to this one. I tend to agree. If you consider these forgeries to be deepfakes, then… there you go! Useful deepfakes that can potentially help people around the world stranded at home to study and improve themselves. Imagine what good this could do! Well done! Thanks for watching and for your generous support, and I'll see you next time!"
456,This AI Makes Puzzle Solving Look Easy!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to immerse ourselves into a wonderful art of rigid body disentanglement, or in simpler words, we are going to solve these puzzles! To be more exact, we’ll sit and enjoy our time while a learning-based algorithm is going to miraculously solve some really challenging puzzles. Yes, this is going to be a spicy paper. Now, these puzzles are roughly ordered by difficulty, where the easier ones are on the left, and as we traverse to the right, things get more and more difficult. And get this, this new technique is able to algorithmically solve all of them. If you are like me and you don’t believe a word of what’s been said, let’s see if it lives up to its promise by solving three of them, in increasing difficulty. Let’s start with an easy one. Example number one. Here, the algorithm recognizes that we need to pull the circular part of the red piece through the blue one and apply the appropriate rotations to make sure that we don’t get stuck during this process. While we finish this sequence, please note that this video contains spoilers for some well-known puzzles. If you wish to experience them yourself, pause this video, and, I guess, buy and try them. This one was good enough to warm ourselves up, so let’s hop on to the next one. Example number two, the duet puzzle. Well that was quite a bit of a jump in difficulty because we seem stuck right at the start. Hmm… this seems flat out impossible…until the algorithm recognizes that there are these small notches in the puzzle, and if we rotate the red piece correctly, we may go from one cell to the next one. Great, so now it’s not impossible anymore, it is more like a maze that we have to climb through. But the challenges are still not over. Where is the endpoint? How do we finish this puzzle? There has to be a notch on the side…yes, there is this one or this one, so we ultimately have to end up in one of these places…and, there we go. Bravo! Experiment number three. My favorite, the enigma. Hmm..this is exactly the opposite of the previous one. It looks so easy! Just get the tiny opening of the red piece onto this part of the blue one, and we are done. Uh-oh. The curved part is in the way. Something that seemed so easy now suddenly seems absolutely impossible! But fortunately, this learning algorithm does not get discouraged, and does not know what impossible is, and it finds this tricky series of rotations to go around the entirety of the blue piece, and then, finish the puzzle. Glorious. What a rollercoaster! A hallmark of an expertly designed puzzle. And to experience more of these puzzles, make sure to have a look paper and its website with a super fun interactive app. If you do, you will also learn really cool new things…for instance, if you get back home in the middle of the night after a long day, and two of your keys are stuck together, you will know exactly how to handle it. And all this can be done through the power of machine learning and computer graphics research. What a time to be alive! So how does this wizardry work exactly? The key techniques here are tunnel discovery and path planning. First, a neural network looks at the puzzle and identifies where the gaps and notches are, and specifies the starting position and the goal position that we need to achieve to finish the puzzle. Then, a set of collision-free key configurations are identified, after which, the blooming step can commence. So what does that do? Well, the goal is to be able to go through these narrow tunnels that represent tricky steps in the puzzles that typically require some unintuitive rotations. These are typically the most challenging parts of the puzzles, and the blooming step starts from these narrow tunnels and helps us reach the bigger bubbles of the puzzle. But as you see, not all roads connect, or at least, not easily. The forest connect step tries to connect these roads through collision-free paths, and now, finally, all we have to do is find the shortest path from the start to the endpoint to solve the puzzle. And also, according to my internal numbering system, this is Two Minute Papers episode number 478. And to every single one of you Fellow Scholars who are watching this, thank you so much to all of you for being with us for so long on this incredible journey. Man, I love my job, and I jump out of bed full of energy and joy knowing that I get to read research papers and flip out together with many of you Fellow Scholars on a regular basis. Thank you. Thanks for watching and for your generous support, and I'll see you next time!"
457,Is Videoconferencing With Smart Glasses Possible?,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to have a look at the  state of egocentric videoconferencing. Now   this doesn’t mean that only we get to speak during  a meeting, it means that we are wearing a camera,   which looks like this, and the goal is to use  a learning algorithm to synthesize this frontal   view of us. Now note that what you see here is  the recorded reference footage, this is reality,   and this would need to be somehow synthesized  by the algorithm. If we could pull that off,   we could add a low-cost egocentric camera to smart  glasses, and it could pretend to see us from the   front, which would be amazing for hands-free  videoconferencing. That would be insanity. But wait a second. How is this even possible?   For us to even have a fighting chance, there  are four major problems to overcome here. One, this camera lens is very close to  us, which means that it doesn’t see the   entirety of the face. That sounds extremely  challenging. And if that wasn’t bad enough,   two, we also have tons of distortion  in the images, or in other words,   things don’t look like they look in reality,  we would have to account for that too. Three,   it would also have to take into account  our current expression, gaze, blinking,   and more. Oh boy. And finally, four, the output  needs not be photorealistic, even better,   videorealistic. Remember, we don’t just need one  image, but a continuously moving video output. So the problem is, once  again, input, egocentric view,   output, synthesized frontal view. This is  the reference footage, reality if you will,   and now, let’s see how this learning-based  algorithm is able to reconstruct it.   Um…hello? Is this a mistake? They look  identical, as if they were just copied here.   No, you will see in a moment that it’s not a  mistake, this means that the AI is giving us a   nearly perfect reconstruction of the remainder  of the human face. That is absolutely amazing. Now, it is still not perfect, there are some  differences. So how do we get a good feel   of where the inaccuracies are? The answer is a  difference image, look. Regions with warmer colors   indicate where the reconstruction is inaccurate  compared to the real reference footage.   For instance, with an earlier method by the name  pix2pix, the hair and the beard are doing fine,   while we have quite a bit of reconstruction  error on the remainder of the face.   So, did the new method do better than  this? Let’s have a look together. Oh yeah!   It does much better across the entirety of  the face. It still has some trouble with the   cable and the glasses, but otherwise,  this is a clean, clean image. Bravo! Now, we talked about the challenge of  reconstructing expressions correctly.   To be able to read the other person is of  utmost importance during a video conference.   So how good is it at gestures? Well, let’s put it through an intense  stress test! Well, this is as intense as   it gets without having access to Jim  Carrey as a test subject I suppose,   and I bet there was a lot of fun to be had in the  lab on this day. And the results are outstanding,   especially if we compare it again  to the pix2pix technique from 2017. I love this idea, because if we can overcome  the huge shortcomings of the egocentric camera,   in return, we get an excellent view of subtle  facial expressions and can deal with the tiniest   eye movements, twitches, tongue movements,  and more. And it really shows in the results. Now please note that this technique needs to be  trained on each of these test subjects. About   four minutes of video footage is fine and this  calibration process only needs to be done once.   So, once again, the technique knows  these people and had seen them before. But in return, it can do even more. If all of this  is synthesized, we have a lot of control over this   data and the AI understands what much of this data  means. So with all that extra knowledge, what else   can we do with this footage? For instance, we can  not just reconstruct, but create arbitrary head   movement. We can guess what the real head movement  is because we have a view of the background,   we can simply remove it, or from the movement of  the background, we can infer what kind of head   movement is taking place. And what’s even better,  we can not only get control over the head movement   and change it, but even remove the movement  from the footage altogether. And, we can also   remove the glasses and pretend to have dressed  properly for an occasion. How cool is that? Now make no mistake, the paper contains a  ton of comparisons against a variety of other   works as well, here are some, but make sure to  check them all out in the video description. Now, of course, even this new method isn’t  perfect, for instance, it does not work all   that well in low-light situations, but of course,  let’s leave something to improve for the next   paper down the line. And hopefully, in the near  future, we will be able to seamlessly get in   contact with our loved ones through smart glasses  and egocentric cameras. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
458,Near-Perfect Virtual Hands For Virtual Reality!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. The promise of virtual reality, VR is indeed  truly incredible. If one day it comes to fruition,   doctors could be trained to perform surgery in a  virtual environment, we could train better pilots   with better flight simulators, expose astronauts  to virtual zero-gravity simulations, you name it. An important part of doing many of these is  simulating walking in a virtual environment.   You see, we can be located in a small room,  put on a VR headset and enter a wonderful,   expansive virtual world. However, as we  start walking, we immediately experience   a big problem. What is that problem?  Well, we bump into things. As a remedy,   we could make our virtual world smaller,  but that would defeat the purpose.   This earlier technique addresses this  walking problem spectacularly by redirection. So, what is this redirection thing exactly?  Redirection is a simple concept that changes   our movement in the virtual world so it deviates  from our real path in the room in a way that both   lets us explore the virtual world, and not bump  into walls and objects in reality in the meantime.   Here you can see how the blue and orange lines  deviate, which means that the algorithm is at   work. With this, we can wander about in  a huge and majestic virtual landscape or   a cramped bar, even when being confined  to a small physical room. Loving the idea. But there is more to interacting with  virtual worlds than walking, for instance,   look at this tech demo that requires more  precise hand movements. How do we perform these?   Well, the key is here. Controllers!  Clearly, they work, but can we get   rid of them? Can we just opt for a more  natural solution and use our hands instead? Well, hold on to your papers, because this  new work uses a learning-based algorithm   to teach a head-mounted camera to tell the  orientation of our hands at all times. Of course,   the quality of the execution matters a great  deal, so we have to ensure at least three things. One is that the hand tracking  happens with minimal latency,   which means that we see our actions  immediately, with minimal delay. Two, we need low jitter, which means that  the keypoints of the reconstructed hand   should not change too much from frame to frame.  This happens a great deal with previous methods,   and what about the new one? Oh  yes, much smoother. Checkmark! Note that the new method also remembers  the history of the hand movement,   and therefore can deal with difficult occlusion  situations. For instance, look at the pinky here!   A previous technique would not know what’s going  on with it, but, this new one knows exactly what   is going on because it has information  on what the hand was doing a moment ago. And three, this needs to work in all kinds  of lighting conditions. Let’s see if it can   reconstruct a range of mythical creatures in  poor lighting conditions. Yes, these ducks   are reconstructed just as well as the mighty  pokemonster, and, these scissors too. Bravo! So, what can we do with this? A great deal. For  instance, we can type on a virtual keyboard,   or implement all kinds of virtual user  interfaces that we can interact with.   We can also organize imaginary boxes, and  of course, we can’t leave out the the Two   Minute Papers favorite, going into a  physics simulation and playing with it. But of course, not everything  is perfect here, however. Look.   Hand-hand interactions don’t work so well, so  folks who prefer virtual reality applications   that include washing our hands should look  elsewhere. But of course, one step at a time. Thanks for watching and for your generous  support, and I'll see you next time!"
459,These Are Pixels Made of Wood!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Everybody loves style transfer.  This is a task typically done   with neural networks where we have  two images, one for content, and one   for style, and the output is the content image  reimagined with this new style. The cool thing   is that the style can be a different photo,  a famous painting, or even, wooden patterns. Feast your eyes on these majestic images of  this cat reimagined with wooden parquetry   with these previous methods. And now, look at  the result of this new technique, that looks   way nicer. Everything is in order here, except one  thing. And now, hold on to your papers, because   this is not style transfer. Not at all. This is  not a synthetic photo made by a neural network,   this is a reproduction of this cat image  by cutting wood slabs into tiny pieces   and putting them together carefully. This  is computational parquetry. And here,   the key requirement is that if we look from afar,  it looks like the target image, but if we zoom in,   it gets abundantly clear that the puzzle  pieces here are indeed made of real wood. And that is an excellent intuition for this  work. It is kind of like image stylization,   but done in the real world. Now that is extremely challenging. Why is that?  Well, first, there are lots of different kinds   of wood types. Second, if this piece was not a  physical object but an image, this job would not   be that hard because we could add to it, clone it,  and do all kinds of pixel magic to it. However,   these are real, physical pieces of wood,  so we can do exactly none of that. The only   thing we can do is take away from it,  and we have limitations even on that,   because we have to design it in a way that  a CNC device should be able to cut these   pieces. And third, you will see that  initially, nothing seems to work well.   However, this technique does this with flying  colors, so I wonder, how does this really work? First, we can take a photo of the wood panels that  we have our disposal, decide how and where to cut,   give these instructions to the CNC  machine to perform the cutting,   and now, we have to assemble them in a way that  it resembles the target image. Well, still, that’s   easier said than done. For instance, imagine  that we have this target image, and we have   these wood panels. This doesn’t look anything like  that, so how could we possibly approximate it? If we try to match the colors of the two, we  get something that is too much in the middle,   and the colors don’t resemble any  of the original inputs. Not good.   Instead, the authors opted to transform both  of them to grayscale, and match not the colors,   but the intensities of the colors instead. This  seems a little more usable…until we realize   that we still don’t know what  pieces to use and where. Look. Here, on the left, you see how the image  is being reproduced with the wood pieces,   but we have to mind the fact that as  soon as we cut out one piece of wood,   it is not available anymore, so it has to be  subtracted from our wood panel repository here.   As our resources are constrained, depending on  what order we put the pieces together, we may   get a completely different result. But look. There  is still a problem…the left part of the suit gets   a lot of detail, while the right part, not so  much. I cannot judge which solution is better,   less or more detail, but it needs to be  a little more consistent over the image.   Now you see that whatever we do, nothing  seems to work well in the general case. Now, we could get a much better solution  if we would run the algorithm with   every possible starting point in the image, and  with every possible ordering of the wood pieces,   but that would take longer  than our lifetime to finish,   so what do we do? Well, the authors have two  really cool heuristics to address this problem.   First, we can start from the middle, that usually  gives us a reasonably good solution, since the   object of interest is often in the middle of the  image and the good pieces are still available for   it. Or, even better, if that does not work  too well, we can look for salient regions,   these are the places where there is a lot going  on, and try to fill them in first. As you see,   both of these tricks seem to work quite well  most of the time. Finally, something that works. And if you have been holding on to your papers,  now squeeze that paper, because this technique   not only works, but provides us a great deal of  artistic control over the results. Look at that!   And that’s not all, we can even  control the resolution of the output,   or, we can create a hand-drawn geometry  ourselves. I love how the authors took   a really challenging problem,  where nothing really worked well,   and still, they didn’t stop until they  absolutely nailed the solution. Congratulations! Thanks for watching and for your generous  support, and I'll see you next time!"
460,What Is 3D Photography?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This is a standard color photo made with a smartphone. Hence, it contains only a 2D representation of the world, and when we look at it, our brain is able to reconstruct the 3D information from it. And I wonder, would it be possible for an AI to do the same, and go all the way and create a 3D version of this photo that we can rotate around? Well, this new learning-based method promises exactly that, and if that is at all possible, even more. These are big words, so let’s have a look if it can indeed live up to its promise. So, first, we take a photograph, and we’ll find out together in a moment what kind of phone is needed for this. Probably an amazing one, right? For now, this will be the input, and now, let’s see the 3D photo as an output. Let’s rotate this around. And…wow. This is amazing. And you know what is even more amazing? Since pretty much every smartphone is equipped with a gyroscope, these photos can be rotated around in harmony with the rotation of our phones, and wait a second…is this some sort of misunderstanding, or do I see correctly that we can even look behind a human if we wanted to. That content was not even part of the original photo! How does this work? More on that in a moment. Also, just imagine putting on a pair of VR glasses, and looking at a plain 2D photo and get an experience as if we were really there. It truly feels like we are living in a science fiction world. If we grab our trusty smartphone and use these images, we can create a timeline full of these 3D photos and marvel at how beautifully we can scroll such a timeline here. And now, we have piled up quite a few questions here. How is this wizardry possible? What kind of phone do we need for this? Do we need a depth sensor? Maybe even LiDAR? Let’s look under the hood and find out together. This is the input. One colored photograph, that is expected, and let’s continue…goodness! Now this is unexpected… the algorithm creates a depth map by itself. This depth map tells the algorithm how far different parts of the image are from the camera. Just look at how crisp the outlines are. My goodness, so good. Then, with this depth information, it now has an understanding of what is where in this image, and creates these layers. Which is, unfortunately, not much help because, as you remember, we don’t have any information on what is behind the person. No matter, because we can use a technique that implements image inpainting to fill in these regions with sensible data. And now, with this, we can start exploring these 3D photos. So…if it created this depth map from the color information, this means that we don’t even need a depth sensor for this. Just a simple, color photograph. But wait a minute…this means that we can plug in any photo from any phone or camera that we or someone else took, at any time…and I mean at any time, right? Just imagine taking a black and white photo of a historic event, colorizing it with a previous learning-based method, and passing this color image to this new method, and then, this happens. My goodness. So, all this looks and sounds great, but how long do we have to wait for such a 3D photo be generated? Does my phone battery get completely drained by the time all this computation is done? What is your guess? Please stop the video and leave a comment with your guess. I’ll wait. Alright so is this a battery killer? Let’s see. The depth estimation step takes…whoa, a quarter of a second, inpainting, half a second, and after a little housekeeping, we find out that this is not a battery killer at all, because the whole process is done in approximately one second. Holy mother of papers. I am very excited to see this technique out there in  the wild as soon as possible. Thanks for watching and for your generous support, and I'll see you next time!"
461,Painting the Mona Lisa...With Triangles!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. What you see here is a bunch of vector images. Vector images are not like most images that you see on the internet, those are raster images. Those are like photos, and are made of pixels, while vector images are not made of pixels, they are made of shapes. These vector images have lots of advantages, for instance, they have really small file sizes, can be zoomed into as much as we desire and things don’t get pixelated. And hence, vector images are really well suited for logos, maps, user interface icons, and more. Now, if we wish to, we can convert vector images into raster images, so the shapes will become pixels, this is easy, but here is the problem. If we do it once, there is no going back. Or at least, not easily. This method promises to make this conversion a two-way street, so we can take a raster image, a photo if you will, and work with it as if it were a vector image. Now what does that mean? Oh boy, a lot of goodies. For instance, we can perform sculpting, or in other words, manipulating shapes without touching any pixels. We can work with the shapes here instead. Much easier. Or, my favorite, perform painterly rendering. Now what you see here is not the new algorithm performing this. This is a genetic algorithm I wrote a few years ago that takes a target image, which is the Mona Lisa here, takes a bunch of randomly colored triangles and starts reorganizing them to get as close to the target image as possible. The source code and a video explaining how it works is available in the video description. Now, let’s see how this new method performs on a similar task. Oh yeah, it can start with a large number of different shapes, and just look at how beautifully these shapes evolve and start converging to the target image. Loving it. But that’s not all. It also has a nice solution to an old, but challenging problem in computer graphics that is referred to as seam carving. If you ask me, I like to call it image squishing. Why? Well, look here. This gives us an easy way of intelligently squishing an image into different aspect ratios. So good. So can we measure how well it does what it does? How does it compare to, for instance, Adobe’s state of the art method when vectorizing a photo? Well, it can not only do more, but it also does it better. The new method is significantly closer to the target image here, no question about it. And now comes the best part: it not only provides higher-quality results than the previous methods, but it only takes approximately a second to perform all this. Wow. So there you go, finally, with this new technique, we can edit pixels as if they weren’t pixels at all. It feels like we a living in a science fiction world. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
462,Is Simulating Jelly And Bunnies Possible?,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. This new paper fixes many common problems when it  comes to two-way coupling in fluid simulations.   And of course, the first question is, what  is two-way coupling? It means that here,   the boxes are allowed to move the smoke, and  the added two-way coupling part means that now,   the smoke is also allowed to blow away the boxes.   What’s more, the vortices here on the right  are even able to suspend the red box in   the air for a few seconds. An excellent  demonstration of a beautiful phenomenon. However, simulating this effect  properly for water simulations   and for gooey materials is a huge challenge, so  let’s see how traditional methods deal with them! Experiment number one! Water bunnies. Do you  see what I am seeing here? Did you see the magic   trick? Let’s look again. Observe how much water we  are starting with. A full bunny worth of water…and   then, by the end, we have maybe a quarter of  a bunny left. Oh, yes. We have a substantial   amount of numerical dissipation in the  simulator that leads to volume loss. Can this be solved somehow? Well, let’s see  how this new work deals with this. Starting   with one bunny…and ending it with one bunny.  Nice! Just look at the difference of the volume   of water left with the new method compared to  the previous one. Night and day difference. And this was not even the worst volume loss  I’ve seen…make sure to hold on to your papers,   and check out this one. Experiment number two,  gooey dragons and bowls. When using a traditional   technique, whoa, this guy is GONE. And when we try  a different method, it does… this. My goodness.   So let’s see if the new method can deal  with this case. Oh, yes, yes it can! And now, onwards to experiment number  three. If you think that research is   about throwing things at the wall and seeing  what sticks, in the case of this scene…you   are not wrong. So what should happen  here, given these materials? Well,   the bunny should stick to the goo,  and not fall too quickly…hm…none   of which happens here. The previous method does  not simulate viscosity properly, and hence,   this artificial melting phenomenon emerges.  I wonder if the new method can do this too?   And, yes! They stick together and the goo  correctly slows down the fall of the bunny. So how does this magic work? Normally,  in these simulations, we have to compute   pressure, viscosity, and frictional contact  separately, which are three different tasks.   The technique described in this paper is  called Monolith because it has a monolithic   pressure-viscosity-contact solver. Yes, this means  that it does all three of these tasks in one go,   which is a mathematically a tiny bit more  involved, but it gives us a proper simulator   where water and goo can interact with solids.  No volume loss, no artificial melting, no crazy   jumpy behavior. And here comes the punchline…I was  thinking that alright, a more accurate simulator,   that is always welcome, but what is the price  of this accuracy? How much longer do I have to   wait? If you have been holding on to your papers,  now squeeze that paper, because this technique is   not slower, but up to 10 times faster than  previous methods, and that’s where I fell   off the chair when reading this paper. And with  this, I hope that we will be able to marvel at   even more delightful two way-coupled simulations  in the near future! What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
463,NERFIES: The Selfies of The Future!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to get a taste of how insanely quick progress is in machine learning research. In March of 2020, a paper appeared that goes by the name Neural Radiance Fields, NERF in short. With this technique, we could take a bunch of input photos, get a neural network to learn them, and then, synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. And here, we are talking not only digital environments, but also, real scenes as well! Just to make sure, once again, it can learn and reproduce entire real-world scenes from only a few views by using neural networks. However, of course, NERF had its limitations. For instance, in many cases, it had trouble with scenes with variable lighting conditions and lots of occluders. And to my delight, only 5 months later, in August of 2020, a followup paper appeared by the name NERF in the Wild, or NERF-W in short. Its speciality was tourist attractions that a lot of people take photos of, and we then have a collection of photos taken during a different time of the day, and of course, with a lot of people around. And, lots of people, of course means, lots of occlusions. NERF-W improved the original algorithm to excel more in cases like this. And we are still not done yet, because get this, only three months later, on 2020 November 25th, another followup paper appeared by the name Deformable Neural Radiance Fields. D-NERF. The goal here is to take a selfie video, and turn it into a portrait that we can rotate around freely. This is something that the authors call a nerfie. If we take the original NERF technique to perform this, we see that it does not do well at all with moving things. And here is where the deformable part of the name comes into play. And now, hold on to your papers and marvel at the results of the new D-NERF technique. A clean reconstruction. We indeed get a nice portrait that we can rotate around freely and all of the previous NERF artifacts are gone. It performs well even on difficult cases with beards, all kinds of hairstyles, and more. And now, hold on to your papers, because glasses work too, and not only that, but it even computes the proper reflection and refraction off of the lens. And this is just the start of a deluge of new features. For instance, we can even zoom out and capture the whole body of the test subject. Furthermore, it is not limited to people, it also works on dogs too, although in this case, we will have to settle with a lower the resolution output. It can pull off the iconic dolly zoom effect really well. And, amusingly, we can even perform a nerfception, which is recording ourselves as we record ourselves. I hope that now you have a good feel of the pace of progress in machine learning research, which is absolutely incredible. So much progress in just 9 months of research. My goodness. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
464,Light Fields - Videos From The Future!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Whenever we take a photo, we capture a  piece of reality from one viewpoint. Or,   if we have multiple cameras on our smartphone, a  few viewpoints at most. In an earlier episode, we   explored how to upgrade these to 3D photos, where  we could we could kind of look behind the person.   I am saying kind of, because what we see  here is not reality, this is statistical data   that is filled in by an algorithm to match  its surroundings, which we refer to as image   inpainting. So strictly speaking, it is likely  information, but not necessarily true information,   and also, we can recognize the synthetic parts  of the image as they are significantly blurrier.   So the question naturally arises in the mind  of the curious Scholar, how about actually   looking behind the person. Is that somehow  possible or is that still science fiction? Well, hold on to your papers,  because this technique shows us   the images of the future by sticking a  bunch of cameras onto a spherical shell,   and when we capture a video, it will see…something  like this. And the goal is to untangle this mess   and we’re not done yet, we also need to  reconstruct the geometry of the scene   as if the video was captured from many different  viewpoints at the same time. Absolutely amazing.   And yes, this means that we can change  our viewpoint while the video is running. Since it is doing the reconstruction in layers,  we know how far each object is in these scenes,   enabling us to rotate these sparks and  flames and look at them in 3D. Yum. Now, I am a light transport researcher by  trade, so I hope you can tell that I am very   happy about these beautiful volumetric  effects, but I would also love to know   how it deals with reflective surfaces. Let’s see  together…look at the reflections in the sand here,   and I’ll add a lot of camera movement and  wow…this thing works. It really works,   and it does not break a sweat even if  we try a more reflective surface…or   an even more reflective surface! This  is as reflective as it gets I’m afraid,   and we still get a consistent and  crisp image in the mirror. Bravo! Alright, let’s get a little more  greedy, what about seeing through   thin fences…that is quite a challenge.  And…look at the tail wags there. This is still   a touch blurrier here and there,  but overall, very impressive. So what do we do with a video like this?  Well, we can use our mouse to look around   within the photo in our web browser,  you can try this yourself right now   by clicking on the paper in the video description.  Make sure to follow the instructions if you do.   Or we can make the viewing experience even more  immersive with a head-mounted display, where,   of course, the image will follow wherever  we turn our head. Both of these truly feel   like entering a photograph and getting  a feel of the room therein. Loving it. Now, since there is a lot of information in these  Light Field Videos, it also needs a powerful   internet connection to relay them. Even when using  H.265, a powerful video compression standard,   we are talking in the order of hundreds of  megabits. It is like streaming several videos in   4k resolution at the same time. Compression helps,  however, we also have to make sure that we don’t   compress too much, so that compression artifacts  don’t eat the content behind thin geometry,   or at least, not too much. I bet this will  be an interesting topic for a followup paper,   so make sure to subscribe and hit the  bell icon to not miss it when it appears.   And for now, more practical light field photos and  videos will be available that allow us to almost   feel like we are really in the room with the  subjects of the videos. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
465,OpenAI DALL-E: Fighter Jet For The Mind!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In early 2019, a learning-based technique appeared that could perform common natural language processing operations, for instance, answering questions, completing text, reading comprehension, summarization, and more. This method was developed by scientists at OpenAI, and they called it GPT-2. The key idea for GPT-2 was that all of these problems could be formulated as different variants of text completion problems, where all we need to do is provide it an incomplete piece of text, and it would try to finish it. Then in June 2020 came GPT-3 that supercharged this idea, and among many incredible examples, it could generate website layouts from a written description. However, no one said that these neural networks can only deal with text information. And sure enough, a few months later, scientists at OpenAI thought that if we can complete text sentences, why not try to complete images too? They called this project image GPT and the problem statement was simple: we give it an incomplete image, and we ask the AI to fill in the missing pixels. It could identify that the cat here likely holds a piece of paper and finish the picture accordingly, and even understood that if we have a droplet here and we see just a portion of the ripples, then this means a splash must be filled in. And now, right in January 2021, just 7 months after the release of GPT-3, here is their new mind-blowing technique that explores the connection between text and images. But finishing images already kind of works, so what new thing can it do? In just a few moments, you will see that the more appropriate question would be “what can’t it do”? For now, well, it creates images from our written text captions, and you will see in a moment how monumental of a challenge that is. The name of this technique is a mix of Salvador Dalí and Pixar’s Wall-e. So please meet Dall-e. And now, let’s see it through an example. For neural network-based learning methods, it is easy to recognize that this text says OpenAI, and what a storefront it. Images of both of these exist in abundance. Understanding that is simple. However, generating a storefront that says OpenAI is quite a challenge. It is really possible that it can do that? Well, let’s try it. Look, it works! Wow! Now, of course, if you look here, you immediately see that it is by no means perfect, but let’s marvel at the fact that we can get all kinds of 2D and 3D texts, look at the storefronts from different orientations, and it can deal with all of these cases reasonably well. And of course, it is not limited to storefronts, we can request license plates, bags of chips, neon signs, and more. It can really do all that. So, what else? Well, get this, it can also kind of invent new things. So let’s put our entrepreneurial hat on and try to invent something here. For instance, let’s try to create a triangular clock. Or pentagonal. Or, you know, just make it a hexagon. It really doesn’t matter because we can ask for absolutely anything and get a bunch of prototypes in a matter of seconds. Now, let’s make it white, and…look! Now we have a happy, happy Károly. Why is that? It is because I am a light transport researcher by trade, so the first thing when I look at when seeing these generated images is how physically plausible they are. For instance, look at this white clock here on the blue table. And it did not only put it on the table, but it also made sure to generate appropriate glossy reflections that matches the color of the clock. It can do this too! Loving it. Apparently, it understands geometry, shapes and materials. I wonder what else does it understand? Well, get this, for instance, it even understands styles and rendering techniques. Being a graphics person, I am so happy to see that it learned the concept of low polygon count rendering, isometric views, clay objects, and we can even add an X-ray view to the Owl. Kind of. And now, if all that wasn’t enough, hold on to your papers, because we can also commission artistic illustrations for free, and not only that, but even have fine-grained control over these artistic illustrations. I also learned that if manatees wore suits, they would wear them like this, and after a long and strenuous day walking their dogs, they can go for yet another round… in pajamas. But it does not stop there, it can not only generate paintings of nearly anything, but we can even choose the artistic style and the time of day as well. The night images are a little on the nose as most of them have the moon in the background, but I’ll be more than happy to take these. And the best part is that you can try this yourself right now through the link in the video description. In general, not all results are perfect, but it’s hard to even fathom all the things this will enable us to do in the near future when we can get our hands on these pre-trained models. This may be the first technique where the results are not limited by the algorithm, but, by our own imagination. Now this is a quote that I said about GPT-3, and notice that the exact same thing can be said about Dall-e. Quote: “The main point is that working with GPT-3 is a really peculiar process where we know that a vast body of knowledge lies within, but it only emerges if we can bring it out with properly written prompts. It almost feels like a new kind of programming that is open to everyone, even people without any programming or technical knowledge. If a computer is a bicycle for the mind, then GPT-3 is a fighter jet. Absolutely incredible."" I think this kind of programming is going to be more and more common in the future. Now note that these are some amazing preliminary results, but the full paper is not available yet. So this was not two minutes, and it was not about a paper. Welcome to Two Minute Papers! Jokes aside, I cannot wait for the paper to appear, and I’ll be here to have a closer look whenever it happens. Make sure to subscribe and hit the bell icon to not miss it when the big day comes. And until then, let me know in the comments what crazy concoctions you came up with! Thanks for watching and for your generous support, and I'll see you next time!"
466,Building A Liquid Labyrinth!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. You’re in for a real treat today, because today, once again, we’re not going to simulate just plain regular fluids. No! We’re going to simulate ferrofluids! These are fluids that have magnetic properties and respond to an external magnetic field and get this, they are even able even climb things. Look at this footage from a previous paper. Here is a legendary real experiment where with magnetism, we can make a ferrofluid climb up on this steel helix. Look at that. And now, the simulation. Look at how closely it matches the real footage. Marvelous, especially that it is hard to overstate how challenging it is to create an accurate simulation like this. And the paper got even better. This footage could even be used as proper teaching material. Look. On this axis, you can see how the fluid disturbances get more pronounced as a response to a stronger magnetic field. And in this direction, you see how the effect of surface tension smooths out these shapes. What a visualization! The information density here is just out of this world, while it is still so easy to read at a glance. And it is also absolutely beautiful. This paper was a true masterpiece. The first author of this work was Libo Huang and it was advised by Prof. Dominik Michels who has a strong physics background. And here is the punchline: Libo Huang is a PhD student and this was his first paper. Let me say it again: this was Libo Huang’s first paper and it is a masterpiece. Wow! And it gets better, because this new paper is called Surface-Only Ferrofluids…and YES!!! It is from the same authors. So this paper is supposed to be better, but the previous technique set a really high bar. How the heck do you beat that?! What more could we possibly ask for? Well, this new method showcases a surface-only formulation, and a key observation here that for a class of ferrofluids, we don’t have to compute how the magnetic forces act on the entirety of the 3D fluid domain, we only have to compute them on the surface of the model. So what does this give us? One of my favorite experiments! In this case, we squeeze the fluid between two glass planes and start cranking up the magnetic field strength perpendicular to these planes. Of course, we expect that it starts flowing sideways, but not at all how we would expect it. Wow. Look as how these beautiful fluid labyrinths start slowly forming. And we can simulate all this on our home computers today. We are truly living in a science fiction world. Now if you find yourself missing the climbing experiment from the previous paper, don’t despair, this can still do that, look. First, we can control the movement of the fluid by turning on the upper magnet, then, slowly turn it off while turning on the lower magnet to give rise to this beautiful climbing phenomenon. And that’s not all, fortunately, this work is also ample in amazing visualizations, for instance, this one shows how the ferrofluid changes if we crank up the strength of our magnets, and how changing the surface tension determines the distance between the spikes and the overall smoothness of the fluid. What a time to be alive! One of the limitations of this technique is that it does not deal with viscosity well, so if we are looking to create a crazy goo simulation like this one, but, with ferrofluids, we will need something else for that. Perhaps that something will be the next  paper down the line. Thanks for watching and for your generous support, and I'll see you next time!"
467,All Duckies Shall Pass!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In this paper, you will not only see an amazing technique for two-way coupled fluid-solid simulations, but you will also see some of the most creative demonstrations of this new method I’ve seen in a while. But first things first, what is this two way coupling thing? Two-way coupling in a fluid simulation means being able to process contact. You see, the armadillos can collide with the fluids, but the fluids are also allowed to move the armadillos. And this new work can compute this kind of contact. And by this I mean lots and lots of contact. But one of the important lessons of this paper is that we don’t necessarily need a scene this crazy to be dominated by two-way coupling. Have a look at this experiment with these adorable duckies, and below, the propellers are starting up, please don’t be one of those graphics papers. Okay..okay, good. We dodged this one. So the propellers are not here to dismember things, they are here to spin up to 160 rpm, and since they are two-way coupled, they pump water from one tank to the other, raising the water levels, allowing the duckies to pass. An excellent demonstration of a proper algorithm that can compute two-way coupling really well. And simulating this scene is much, much more challenging than we might think. Why is that? Note that the speed of the propellers is quite high, which is a huge challenge to previous methods. If we wish to complete the simulation in a reasonable amount of time, it simulates the interaction incorrectly and no ducks can pass. The new technique can simulate this correctly, and not only that, but it is also 4.5 times faster than the previous method. Also, check out this elegant demonstration of two-way coupling. We start slowly unscrewing this bolt…and…nothing too crazy going on here. However, look! We have tiny cutouts in the bolt, allowing the water to start gushing out. The pipe was made transparent so we can track the water levels slowly decreasing, and finally, when the bolt falls out, we get some more two-way coupling action with the water. Once more, such a beautiful demonstration of a difficult to simulate phenomenon. Loving it. A traditional technique cannot simulate this properly, unless we add a lot of extra computation. At which point, it is still unstable… ouch! And with even more extra computation, we can finally do this, but hold on to your papers, because the new proposed technique can do it about 10 times faster. It also supports contact against rich geometry as well. Look, we have a great deal going on here. You are seeing up to 38 million fluid particles interacting with these walls given with lots of rich geometry, and there will be interaction with mud, and elastic trees as well. This can really do them all. And did you notice that throughout this video, we saw a lot of delta t-s. What are those? Delta t is something that we call time step size. The smaller this number is, the tinier the time steps with which we can advance the simulation when computing every interaction, and hence, the more steps there are to compute. In simpler words, generally, time step size is an important factor in the computation time, and the smaller this is, the slower, but more accurate the simulation will be. This is why we needed to reduce the time steps by more than 30 times to get a stable simulation here with the previous method. And this paper proposes a technique that can get away with time steps that are typically from 10 times to a 100 times larger than previous methods. And it is still stable. That is an incredible achievement. So what does that mean in a practical case? Well, hold on to your papers, because this means that it is up to 58 times faster than previous methods. 58 times! Whoa. With a previous method, I would need to run something for nearly two months, and the new method would be able to compute the same within a day. Witchcraft, I’m telling you. What a time to be alive! Also, as usual, I couldn’t resist creating a slow-motion version of some of these videos, so if this is something that you wish to see, make sure to visit our Instagram page in the video description  for more. Thanks for watching and for your generous support, and I'll see you next time!"
468,This AI Learned To Create Dynamic Photos!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Approximately 5 months ago, we talked about a technique called Neural Radiance Fields, or NERF in short, where the input is the location of the camera and an image of what the camera sees, we take a few of those, give them to a neural network to learn them, and synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. In short, we take a few samples, and the neural network learns what should be there between the samples. In comes a non-continuous data, a bunch of photos, and out goes a continuous video where the AI fills in the data between these samples. With this, we can change the view direction, but only that! This concept can also be used for other variables. For instance, this work is able to change the lighting, but only the lighting. By the way, this is from long ago, from around Two Minute Papers episode number 13, so our seasoned Fellow Scholars know that this was almost 500 episodes ago. Or, the third potential variable is time. With this AI-based physics simulator, we can advance the time, and the algorithm would try to guess how a piece of fluid would evolve over time. This was amazing, but as you might have guessed, we can advance time, but only the time. And this was just a couple of examples from a slew of works that are capable of doing one, or at most, two of these. These are all amazing techniques, but they offer separate features. One can change the view, but nothing else, one for the illumination, but nothing else, and one for time, but nothing else. With the advent of neural network-based learning algorithms, I wonder if it is possible to create an algorithm that does all three? Or is this just science fiction? Well, hold on to your papers, because with this new work that goes by the name X-Fields, we can indeed change the time…and the view direction...and the lighting separately. Or, even better, do all three at the same time. Wo-hoo! Look at how we can play with the time back and forth and set the fluid levels as we desire, that is the time part, and we can also play with the other two parameters as well at the same time. But still, the results that we see here can range from absolutely amazing, to trivial, depending on just one factor. And that factor is, how much training data was available for the algorithm. Neural networks typically require loads of training data to learn a new concept. For instance, if we wish to teach to a neural network what a cat is, we have to show it thousands and thousands of images of cats. So, how much training data is needed for this? And now, hold on to your papers, and…whoa…look at these 5 dots here. Do you know what this means? It means that all the AI saw was five images, that is five samples from the scene with different light positions, and it could fill in all the missing details with such accuracy that we can create this smooth and creamy transition. It almost feels like we have made at least a 100 photographs of the scene. And all this from 5 input photos. Absolutely amazing. Now, here is my other favorite example. I am a light transport simulation researcher by trade, so by definition, I love caustics. A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, and concentrate it to a relatively small area. I hope that you are not surprised when I say that it is the favorite phenomenon of most light transport researchers. And, just look at how beautifully it deals with it. You could take any of these intermediate, AI-generated images and sell them as real ones and I doubt anyone would notice. So, it does three things that previous techniques could do one by one, but really, how does its quality compare to these previous methods? Let’s see how it does on thin geometry, which is a notoriously difficult case for these methods. Here is a previous one. Look. The thick part is reconstructed correctly, however, look at the missing top of the grass blade. Yup, that’s gone. A different previous technique by the name Local Light Field Fusion not only missed the top as well, but also introduced halo-like artifacts to the scene. And, as you see with this footage, the new method solves all of these problems really well, and is quite close to the true reference footage that we kept hidden from the AI. Perhaps the best part is that it also has an online demo that you can try right now, so make sure to click the link in the video description to have a look. Of course, not even this technique is perfect, there are cases where it might confuse the foreground with the background, and we are still not out of the water when it comes to thin geometry. Also, an extension that I would love to see is changing material properties. Here, you see some results from our earlier paper on neural rendering where we can change the material properties of this test object, and get a near-perfect photorealistic image of it in about 5 milliseconds per image. I would love to see it combined with a technique like this one, and while it looks super challenging, it is easily possible that we will have something like that within 2 years. The link to our neural rendering paper and its source code is also available in the video description. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
469,Episode 500 - 8 Years Of Progress In Cloth Simulations!,"Dear Fellow Scholars, this is Two Minute Papers episode number 500 with Dr. Károly Zsolnai-Fehér. And on this glorious day, we are going to simulate the kinematics of yarn and cloth on our computers. We will transition into today’s paper in a moment, but for context, here is a wonderful work to show you what we were able to do in 2012 and we will see how far we have come since. This previous work was about creating these highly detailed cloth geometries for digital characters. Here you see one of its coolest results where it shows how the simulated forces pull the entire piece of garment together. We start out with dreaming up a piece of cloth geometry, and this simulator gradually transforms it into a real-world version of that by subjecting it to real physical forces. This is a step that we call yarn-level relaxation. A few years ago, when I worked at Disney Research, I attended to the talk of the Oscar award winning researcher Steve Marschner who presented this paper. And when I saw these results, shockwaves went through my body. It was one of my truly formative “hold on to your papers” moments that I’ll never forget. Now note that to produce these results, one had to wait for hours and hours to compute all these interactions. So, this paper was published in 2012, and now, nearly 9 years have passed, so I wonder how far have come some since? Well, let’s see together! Today, with this new technique, we can conjure up similar animations where pieces of garments tighten. Beautiful. Now, let’s look under the hood of the simulator, and…well, well, well. Do you see what I see here? Red dots. So, why did I get so excited about a couple red dots? Let’s find out together! These red dots solve a fundamental problem when simulating the movement of these yarns. The issue is that in the mathematical description of this problem, there is a stiffness term that does not behave well when two of these points slide too close to each other. Interestingly, our simulated material gets infinitely stiff in these points. This is incorrect behavior and it makes the simulation unstable. Not good. So what do we to to alleviate this? Now, we can use this new technique that detects these cases and addresses them by introducing these additional red nodes. These are used as a stand-in until things stabilize. Look, we wait until these two points slide off of each other. And now, the distances are large enough so that the mathematical framework can regain its validity and compute the stiffness term correctly, and look, the red dot disappears, and the simulation can continue without breaking. So if we go back to another piece of under the hood footage, we now understand why these red dots come and go. They come when two nodes get too close to each other, and they disappear as they pass each other, keeping the simulation intact. And with this method, we can simulate this beautiful phenomenon when we throw a piece of garment on the sphere, and all kinds of stretching and sliding takes place. Marvelous. So what else can this do? Oh boy, it can even simulate multiple cloth layers, look at the pocket and the stitching patterns here. Beautiful. We can also put a neck tag on this shirt and start stretching and shearing it into oblivion. Pay special attention to the difference in how the shirt and the neck tag reacts to the same forces. We can also stack three tablecloths on top of each other and see how they would behave if we would not simulate friction. And now, the same footage with friction. Much more realistic. And if we look under the hood, you see that the algorithm is doing a ton of work with these red nodes. Look, the table notes that they had to insert tens of thousands of these nodes to keep the simulation intact. Goodness! So, how long do we have to wait for a simulation like this? The 2012 paper took several hours, what about this one? Well, this says we need a few seconds per timestep, and typically, several timesteps correspond to one frame, so where does this put us? Well, it puts us in the domain of not hours per every frame of animation here, but to minutes, and sometimes even seconds per frame. And not only that, but this simulator is also more robust as it can deal with these unpleasant cases where these points get too close to each other. So, I think this was a great testament to the amazing rate of progress in computer graphics research. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
470,This Neural Network Makes Virtual Humans Dance!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Most people think that if we have a piece of camera footage that is a little choppy, then there is nothing we can do with it, we better throw it away. Is that true? No, not at all! Earlier, we discussed two potential techniques to remedy this common problem. The problem statement is simple, in goes a choppy video, something happens, and then, out comes a smooth and creamy video. This process is often referred to as frame interpolation, or frame inbetweening. And of course, it’s easier said than done. If it works well, it really looks like magic much like in the science fiction movies. So what are the potential somethings that we can use to make this happen? One, optical flow. This is an originally handcrafted method that tries to predict the motion that takes place between these frames. This can kind of produce new information and I use this in these videos on a regular basis, but the output footage also has to be carefully inspected for unwanted artifacts. Which are a relatively common occurrence. Two, we can also try to give a bunch of training data to a neural network, and teach it to perform this frame inbetweening. And if we do, the results are magnificent. We can do so much with this! But wait a second…if we can do this for video frames… here is a crazy idea how about a similar kind of inbetweening…for animating humanoids? That would really be something else and it would save us so much time and work! Let’s see what this new method can do in this area! The value proposition of this technique is as simple as it gets: we set up a bunch of keyframes, these are the transparent figures, and the neural network creates realistic motion that transitions from one stage to the next one. Look, it really seems to be able to do it all, it can perform twists and turns, brisk walks and runs, and you will see in a minute, even dance moves. Hmm…this inbetweening for animating humanoid motion idea may not be so crazy after all! What’s more, this could be super useful for artists working in the industry, who can not only do all this, but they can also set up movement variations by moving the keyframes around spatially. Or we can even set up temporal variations to create different timings for the movement. Excellent. Of course, it cannot do everything, if we set up the intermediate stages in a way that uncommon motions would be required to fill in, we might end up with one of these failure cases. And all these results depend on how much training data we have with the kinds of motions we need to fill in. Let’s have a look at a more detailed example! This smooth chap has been given lots of training data with dancing moves, and…look! And when we pull out these dance moves from his training data, he becomes a drunkard. So, talking about training data. How much motion capture footage was given to this algorithm? It used the Ubisoft La Forge Animation Dataset. This contains 5 subjects, 77 sequences, about 4.5 hours of footage in total. Wow, that is not that much. For instance, it only has 8 movement sequences for dancing. That is not that much at all. And we’ve already seen that the model can dance. That is some serious data efficiency, especially given that it can even climb through obstacles. So much knowledge has been extracted from so little data. It truly feels like we are living in a science fiction world. What a time to be alive! So, when we write a paper like this, how do we compare the results to previous techniques? How can we decide which technique is better? Well, the level 1 solution is a user study. We call some folks in, show them the footage, ask which one they liked best the previous method, or this one? That would work, but of course, it is quite laborious, but fortunately, there is a level 2 solution. And this level 2 solution is called the Normalized Power Spectrum Similarity, NPSS in short. This is a number that we can produce with a computer, no humans are required, and it measures how believable these motions are. And the key of NPSS is that it correlates with human judgement, or in other words, if this says that a technique is better, then it is likely that humans would also come to the same conclusion. So let’s see. Here are the previous methods, NPSS is subject to minimization, in other words, the lower, the better. And, let’s see the new method…oh yes, it indeed outpaces the competition. So, there is no wonder that this incredible paper was accepted to the SIGGRAPH ASIA conference. What does that mean exactly? If research were the olympics, a SIGGRAPH or SIGGRAPH ASIA paper would be the gold medal. And, this was Mr. Felix Harvey’s first few papers. Huge congratulations! And as an additional goodie, it can create an animation of me when I lost my papers, and this is me when I found them. Do you have some more ideas on how we could put such an amazing technique to use? Let me know in the comments below. Thanks  for watching and for your generous support, and I'll see you next time!"
471,7 Years of Progress In Snow Simulation!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Let’s talk about snow simulations! Being able to simulate snow on our computers is not new, it’s been possible for a few years now, for instance, this legendary Disney paper from 2013 was capable of performing that. So why do researchers write new papers on this? Well, because the 2013 paper had its limitations. Let’s talk about them while looking at some really sweet footage from a new paper. Limitation number one for previous works is that their snow simulations were sinfully expensive. Most of them took not seconds, and sometimes not even minutes, but half an hour per frame. Yes, that means all nighter simulations. They were also not as good in fracturing interactions, simulating powder snow and avalanches were also out of reach. Until now. You see, simulating snow is quite challenging. It clumps up, deforms, breaks, and hardens under compression. And even my favorite, phase change from fluid to snow! These are all really challenging to simulate properly, and in a moment, you will see that this new method is capable of even more beyond that. Let’s start with friction. First, we turn on the snow machine. And then, engage the wipers! That looks wonderful. And now, may I request seeing some tire marks? There you go! This looks really good, so how about taking a closer look at this phenomenon? Vb here is the boundary friction coefficient and it is a parameter that can be chosen freely by us. So let’s see what that looks like! If we initialize this to a low value, we’ll get very little friction, and if we crank this up, look, things get a great deal more sticky. The big clump of snow also breaks apart in a spectacular manner, also showcasing compression and fracturing beyond the boundary friction effect we just looked at. Oh my! This is beautiful. Okay, now, this is a computer graphics paper. If you are a seasoned Fellow Scholar, you know that this means…it means that it is time to put some virtual bunnies into the oven. This is a great example for rule number one for watching physics simulations, which is that we discuss the physics part, and not the visuals. So why are these bunnies blue? Well, let’s chuck’em in the oven and find out. Aha! They are color coded for temperature. Look, they start from -100C, that’s the blue, and we see the colors change as they approach zero degrees celsius. At this point they don’t yet start melting, but they are already falling apart, so it was indeed a good design decision to show the temperatures, because it tells us exactly what is going on here. Without it, we would be expecting melting. Well, can we see that melting in action too? You bet. Now, hold on to your papers, and bring forth the soft-ice-o-mat! This machine can not only create an exquisite dessert for computer graphics researchers, but also showcases the individual contributions of this new technique one by one. Look! There is the melting, yes! Add a little frosting, and there you go. Bon appetit! Now, as we feared, many of these larger-scale simulations require computing the physics for millions of particles. So how long does that take? When we need millions of particles, we typically have to wait a few minutes per frame, but if we have a smaller scene, we can get away with these computations in a few seconds per frame! Goodness, we went from hours per frame to seconds per frame in just one paper. Outstanding work. And also, wait a second…if we are talking millions of particles, I wonder how much memory it takes to keep track of them? Let’s see. Whoa…this is very appealing. I was expecting a few gigabytes, yet it only asks for a fraction of it…a couple hundred megabytes. So, with this hefty value proposition, it is no wonder that this paper has been accepted to the SIGGRAPH conference. This is the olympic gold medal of computer graphics research, if you will. Huge congratulations to the authors of  this paper… this  was quite an experience. What a  time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
472,Perfect Virtual Hands - But At A Cost!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. The promise of virtual reality, VR is indeed truly incredible. If one day it comes to fruition, doctors could be trained to perform surgery in a virtual environment, we could train better pilots with better flight simulators, expose astronauts to virtual zero-gravity simulations, you name it. This previous work uses a learning-based algorithm to teach a head-mounted camera to tell the orientation of our hands at all times. Okay, so what can we do with this? A great deal. For instance, we can type on a virtual keyboard, or implement all kinds of virtual user interfaces that we can interact with. We can also organize imaginary boxes, and of course, we can’t leave out the the Two Minute Papers favorite, going into a physics simulation and playing with it with our own hands. But of course, not everything is perfect here, however. Look. Hand-hand interactions don’t work so well, so folks who prefer virtual reality applications that include washing our hands should look elsewhere. And in this series, we often say “one more paper down the line, and it will be significantly better”. So now, here is the moment of truth, let’s see that one more paper down the line. Let’s go in guns blazing and give it examples with challenging hand-hand interactions, deformations, lots of self-contact and self-occlusion. Take a look at this footage. This seems like a nightmare for any hand reconstruction algorithm. Who the heck can solve this? And, look, interestingly, they also recorded the hand model with gloves on. How curious! And now, hold on to your papers because these are not gloves…no, no, no. What you see here is the reconstruction of the hand model by a new algorithm. Look. It can deal with all of these rapid hand motions. And what’s more, it also works on this challenging hand massage scene. Look at all those beautiful details! It not only fits like a glove here too, but I see creases, folds, and deformations too this reconstruction is truly out of this world. To be able to do this, the algorithm has to output triangle meshes that typically contain over a hundred thousand faces. Please remember this as we will talk about it later. And now, let’s see how it does all this magic, because there is plenty of magic under the hood. Let’s look at the five ingredients that are paramount to getting an output of this quality. Ingredient number one is the physics term. Without it, we can’t even dream of tracking self-occlusion and contact properly. Two, since there are plenty of deformations going on in the input footage, the deformation term accounts for that. It makes a huge difference in the reconstruction of the thumb here. And if you think “wow, that is horrific”, then you’ll need to hold on to your papers for the next one, which is…three…the geometric consistency term. This one is not for the faint of the heart. You have been warned. Are you ready? Let’s go. Yikes! A piece of advice, if you decide to implement this technique, make sure to include this geometric consistency term so no one has to see this footage ever again. Thank you. With the worst already behind us, let’s proceed to ingredient number four, the photo-consistency term. This ensures that fingernail tips don’t end up sliding into the finger. And five, the collision term fixes problems like this to make sure that the fingers don’t penetrate each other. And this is an excellent paper, so in the evaluation section, these terms are also tested in isolation, and the authors tell us exactly how much each of these ingredients contribute to the solution. Now, these five ingredients are not cheap in terms of computation time, and remember, we also mentioned that many of these meshes have several hundred thousand faces. This means that this technique takes a very long time to compute all this. It is not real time, not even close, for instance, reconstructing the mesh for the hand massage scene takes more than 10 minutes per frame. This means hours, or even days of computation to accomplish this. Now the question naturally arises is that a problem? No, not in the slightest. This is a zero to one paper, which means that it takes a problem that was previously impossible, and now, it makes it possible. That is absolutely amazing. And as always, research is a process, and this is an important stepping stone in this process. I bet that two more good papers down the line, and we will be getting these gloves interactively. I am so happy about this solution, as it could finally give us new ways interact with each other in virtual spaces, add more realism to digital characters, help us better understand human-human interactions, and it may also enable new applications in physical rehabilitation. And these reconstructions indeed fit like a glove. What a time  to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
473,These Neural Networks Have Superpowers!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. I got so excited by the amazing results of this  paper. I will try my best to explain why, and by   the end of this video, there will be a comparison  that blew me away and I hope you will appreciate   it too. With the rise of neural network-based  learning algorithms, we are living the advent   of image generation techniques. What you see  here is a set of breathtaking results created   with a technique called StyleGAN2. This can  generate images of humans, cars, cats, and more. As you see, the progress in machine  learning-based image generation is just stunning. And don’t worry for a second about the progress  in text processing, because that is also   similarly amazing these days. A few months ago,  OpenAI published their GPT-3 model that they   unleashed to read the internet, and learn not just  our language, but much, much more. For instance,   the internet also contains a lot of computer code,  so it learned to generate website layouts from a   written description. But that’s not all, not even  close, to the joy of technical PhD students around   the world, it can properly typeset mathematical  equations from a plain English description as   well. And get this, it can also translate  a complex legal text into plain language,   or, the other way around. And it does many  of these things nearly as well as humans. So what was the key to this  work? One of the keys of GPT-3   was that it uses a neural network architecture  that is called the transformer network. These   really took the world by storm in the last few  years, so our first question is, why transformers?   One, transformer networks can typically learn  on stupendously large datasets, like the whole   internet, and extract a lot of information  from it. That is a very good thing. And two,   transformers are attention-based neural networks,  which means that they are good at learning and   generating long sequences of data. Okay, but  how do we benefit from this? Well, when we ask   OpenAI’s GPT-3 to continue our sentences, it  is able to look back at what we have written   previously. And it looks at not just a couple  of characters, no-no, it looks at up to several   pages of writing backwards to make sure that  it continues what we write the best way it can. This sounds amazing. But what is the lesson here?  Just use transformers for everything and off we   go? Well, not quite. They are indeed good at a lot  of things when it comes to text processing tasks,   but they don’t excel at generating high-resolution  images at all. Can this be improved somehow?   Well, this is what this new  technique does, and much, much more. So let’s dive in and see what it can do! First,   we can give it an incomplete  image and ask it to finish it. Not bad… but! OpenAI’s Image-GPT could do that  too, so what else can it do? Oh boy, a lot more!   And by the way, we will compare the results of  this technique against Image-GPT at the end of   this video, make sure not to miss that, I almost  fell off the chair, you will see in a moment why. Two, it can do one of my favorites, depth  to image generation. We give it a depth map,   which is very easy to produce, and it creates  a photorealistic image that corresponds to it,   which is very hard. We do the easy  part, the AI does the hard part. Great!   And with this, we not only get a selection of  these images, but since we have their depth maps,   we can also rotate them around  as if they were 3D objects. Nice! Three, we can also give it a map of  labels, which is, again, very easy to do,   we just say here goes the sea, put  some mountains here, and the sky here,   and it will create a beautiful landscape  image that corresponds to that.   I can’t wait to see what these amazing artists all  over the world will be able to get out of these   techniques, and these results are already  breathtaking…but research is a process,   and just imagine how good they will become  two more papers down the line. My goodness! Four, it can also perform super resolution.  This is the CSI thing where in goes a blurry   image, and out comes a finer, more  detailed version of it. Witchcraft. And finally, five, we can give it a pose, and  it generates humans that take these poses. Now, the important thing here  is that it can supercharge   transformer networks to do these things  at the same time, with just one technique. So how does it compare to OpenAI’s Image  completion technique? Well, remember,   that technique was beyond amazing, and set a  really high bar. So let’s have a look together!   They were both given the upper half of this image,  and had to fill in the lower half. Remember,   as we just learned transformers are not  great at high-resolution image synthesis.   So here, for OpenAI Image-GPT we expect heavily  pixelated images….and…oh yes, that’s right.   So now, hold on to your papers, and let’s see  how much more detailed the new technique is.   Holy mother of papers! Do you see what I see  here? Image-GPT came out just a few months ago,   and there is already this kind of progress. So  there we go, just imagine what we will be able   to do with these supercharged transformers  just two more papers down the line.   Wow. And that’s where I almost fell off the  chair when reading this paper. Hope you held   on to yours. It truly feels like we are living in  a science fiction world. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
474,This AI Learn To Climb Crazy Terrains!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In 2017, scientists at OpenAI published a paper where virtual humans learned to tackle each other in a sumo competition of sorts, and found out how to rock a stable stance to block others from tackling them. This was a super interesting work because it involved self-play, or in other words, copies of the same AI were playing against each other, and the question was, how do we pair them with each other to maximize their learning. They found something really remarkable when they asked the algorithm to defeat an older version of itself. If it can reliably pull that off, it will lead to a rapid and predictable learning process. This kind of curriculum-driven learning can supercharge many different kinds of AIs. For instance, this robot from a later paper is essentially blind as it only has proprioceptive sensors, which means that the only thing that the robot senses is its own internal state and that’s it. No cameras, depth sensors, no LIDAR, nothing. And at first, it behaves as we would expect it…look, when we start out, the agent is very clumsy and can barely walk through a simple terrain… but as time passes, it grows to be a little more confident, and with that, the terrain also becomes more difficult over time in order to maximize learning. That is a great life lesson right there. So, how potent is this kind of curriculum in teaching the AI? Well, it learned a great deal in the simulation, and as scientists deployed it into the real world, just look at how well it traversed through this rocky mountain, stream, and not even this nightmarish snowy descent gave it too much trouble. This new technique proposes a similar curriculum-based approach where we would teach all kinds of virtual lifeforms to navigate on stepping stones. The examples include a virtual human, a bipedal robot called cassie, and…this sphere with toothpick legs too. The authors call it “monster”, so, you know what, monster it is. So, the fundamental question here is, how do we organize the stepping stones in this virtual environment to deliver the best teaching to this AI? We can freely choose the heights and orientations of the upcoming steps, and…of course, it is easier said than done. If the curriculum is too easy, no meaningful learning will take place, and if gets too difficult too quickly, well…then…in the better case, this happens…and in the worst case, whoops! This work proposes an adaptive curriculum that constantly measures how these agents perform, and creates challenges that progressively get harder, but in a way that they can be solved by the agents. It can even deal with cases where the AI already knows how to climb up and down, and even deal with longer steps. But that does not mean that we are done, because if we don’t build these spirals right, this happens. But, after learning 12 to 24 hours with this adaptive curriculum learning method, they become able to even run, deal with huge step height variations, high step tilt variations, and let’s see if they can pass the hardest exam…look at this mess, my goodness, lots of variation in every parameter. And…Yes! It works! And the key point is that the system is general enough that it can teach different body types to do the same. If there is one thing that you take home from this video, it shouldn’t be that it takes from 12 to 24 hours. It should be that the system is general. Normally, if we have a new body type, we need to write a new control algorithm, but in this case, whatever the body type is, we can use the same algorithm to teach it. Absolutely amazing. What a time to be alive! However, I know what you’re thinking. Why teach them to navigate just stepping stones? This is such a narrow application of locomotion, so why this task? Great question, and the answer is that the generality of this technique we just talked about also means that the stepping stone navigation truly was just a stepping stone, and here it is we can deploy these agents to a continuous terrain and expect them to lean on their stepping stone chops to navigate well here too. Another great triumph for curriculum-based AI training environments. So what do you think? What would you use this technique for? Let me know in the comments, or if you wish to discuss similar topics with other Fellow Scholars in a warm and welcoming environment, make sure to join our Discord channel. The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
475,This is What Abraham Lincoln May Have Looked Like!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to travel in time. With the ascendancy of neural-network based learning techniques, this previous method enables us to take and old old black and white movie that suffers from a lot of problems like missing data, flickering, and more, give it to a neural network, and have it restore it for us. And here, you can not only see how much better this restored version is, but, it did take it one step further. It also performed colorization! Essentially, here, we could produce 6 colorized reference images, and the neural network uses them as art direction and propagates all this information to the remainder of the frames. So this work did restoration and colorization at the same time. This was absolutely amazing, and now comes something even better, today, we have a new piece of work that performs not only restoration and colorization, but super-resolution as well! What this means is that we can take an antique photo, which suffers from a lot of issues. Look, these old films exaggerate wrinkles a great deal, they even even darken the lips and do funny things with red colors. For instance, subsurface scattering is also missing, this is light penetrating our skin and bouncing inside before coming out again, and the lack of this effect is why the skin looks a little plasticky here. Luckily, we can simulate all these phenomena on our computers. I am a light transport researcher by trade, and this is from our earlier paper with the Activision Blizzard game development company, this is the same phenomenon, a simulation without subsurface scattering, and this one is with simulating this effect. Beautiful. You can find a link to this paper in the video description. So with all these problems with the antique photos, our question is, what did Lincoln really look like? Well, let’s try an earlier framework for restoration, colorization and super resolution…and. Well, unfortunately, most of our issues still remain. Lots of exaggerated wrinkles, plasticky look, lots of detail is missing. Can we do better? Well, hold on to your papers, and observe the output with the new technique. Wow. The restoration indeed took place properly, brought the wrinkles down to a much more realistic level, skin looks like skin because of subsurface scattering, and the super resolution part is responsible for a lot of new detail everywhere, but especially around the lips. Outstanding. It truly feels like this photo has been rephotographed with a modern camera. And with that, please meet Time-Travel Rephotography. And the curious thing is that all this sounds flat out impossible. Why is that? Since we don’t have old and new image pairs of Lincoln and many other historic figures, the question naturally arises in the mind of the curious Fellow Scholar how do we train a neural network to perform this? And the answer is that we need to use their siblings. Now this doesn’t mean that Lincoln had a long lost sibling that we don’t know about. What this means is that as the input image is fed through our neural network, we can generate a photorealistic image of someone, and this someone kind of resembles the target subject, and has all the details filled in. Then, in the next step, we can start morphing the sibling until is starts resembling the test subject. With this previously existing StyleGAN2 technique, morphing is now easy to do, but restoration is hard, so essentially, with this, we can skip the difficult restoration part, and just do the easier morphing instead. Trading a difficult problem for an easier one. Absolutely brilliant idea. And if you have been holding on to your papers so far, now, squeeze that paper, because it can do even more. Age progression! Look. If we have only a few a few target photos of Thomas Edison throughout his life, these will be our yardsticks, and the algorithm is able to generate his aging process between these yardstick images. And the best part is that these images have different lighting, pose, and none of this is an issue for the technique. It just doesn’t care and it still works beautifully. Wow. So we saw earlier that there are other methods that attempt to do this too, at least the colorization part. Yes, we have colorization and other techniques in abundance. So how does this compare to them? It appears to outpace all of them really convincingly. The numbers from the user study and the algorithmically generated scores also favor the new technique. This is a huge leap forward. Do you have some other applications in mind for this new technique? Let me know in the comments what you would do with this or how you would like to see it improved. Now, of course, not even this technique is perfect. Blurry and noisy regions can still appear here and there. And note that StyleGAN2, the basis for this algorithm came out just a little more than a year ago. And it is amazing that we are witnessing such incredible progress in so little time. My goodness. And just imagine what the next paper down the line will bring! What a  time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
476,"Finally, Instant Monsters!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. If we wish to create an adorable virtual monster  and animate it, we first have to engage in 3D   modeling. Then, if we wish to make it move,  and make this movement believable, we have   to specify where the bones and joints are located  within the model. This process is called rigging.   As you see, it is quite laborious, and requires  expertise in this domain to pull this off. And now, imagine this process with no 3D  modeling, and no rigging. This is, of course,   impossible…right? Well, now, you know what’s  coming, so hold on to your papers, because   here is a newer technique that indeed performs  the impossible. All we need to do is grab a   pencil and create a rough sketch of our character,  then, it will take a big breath, and inflate it   into a 3D model. This process was nearly 7 times  faster than the classical workflow, but what   matters even more, this new workflow requires  zero expertise in 3D modeling and rigging.   With this technique, absolutely  anybody can become an artist. So, we noted that these models can also be  animated. Is that so? Yes, that’s right,   we can indeed animate these models by using  these red control points. And even better,   we get to specify where these points go. That’s  a good thing because we can make sure that a   prescribed part can move around, opening up  the possibility of creating and animating   a wide range of characters. And I would say  all this can be done in a matter of minutes,   but even better, sometimes  even within a minute. Whoa. This new technique does a lot  of legwork that previous methods   were not able to pull off so well. For instance,  it takes a little information about which part   is in front or behind the model. Then, it  stitches all of these strokes together and   inflates our drawing into a 3D model, and it  does this better than previous methods, look.   Well, okay, the new one looks a bit better where  the body parts connect here, and that’s it?   Wait a second…a ha! Somebody  didn’t do their job correctly.   And we went from this work to this, in just  two years. This progress is absolute insanity. Now let’s have a look at a full workflow from  start to end. First, we draw the strokes,   note that we can specify that one arm and leg is  in front of the body, and the other is behind,   and, bam the 3D model is now done! Wow, that was  quick. And now, add the little red control points   for animation, and let the fun begin! Mister, your  paper has been officially accepted! Move the feet,   pin the hands, rock the body. Wait. Not only  that, but this paper was accepted to the   SIGGRAPH ASIA conference, which is equivalent  to winning an olympic gold medal in computer   graphics research if you will, so add a little  neck movement too. Oh yeah! Now we’re talking! With this technique, the possibilities really feel  endless. We can animate humanoids, monsters, other   adorable creatures, or can even make scientific  illustrations come to life without any modeling   and rigging expertise. Do you remember this  earlier video where we could paint on a piece of   3D geometry, and transfer its properties onto a 3D  model? This method can be combined with that too.   Yum! And in case you are wondering how quick this  combination is my goodness. Very, very quick. Now, this technique is also not perfect,  one of the limitations of this single-view   drawing workflow is that we have only limited  control over the proportions in depth. Texturing   occluded regions is also not that easy. The  authors proposed possible solutions to these   limitations in the paper, so make sure to have a  look in the video description, and it appears to   me that with a little polishing, this may be  ready to go for artistic projects right now.   If you have a closer look, you will also  see that this work also cites the flow paper   from Mihaly Csikszentmihalyi.  Extra style points for that. And with that said, when can we use this?   And that’s the best part right now! The authors  really put their papers where their mouth is,   or in other words, the source code for  this project is available, also, there is,   an online demo. Wo-hoo! The link is available  in the video description, make sure to read the   instructions before you start. So there you  go, instant 3D models with animation without   requiring 3D modeling and rigging expertise. What  do you think? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
477,Differentiable Material Synthesis Is Amazing!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. I am a light transport simulation researcher  by trade, and I am very happy today, because we   have an absolutely amazing light transport paper  we’re going to enjoy today. As many of you know,   we write these programs that you can run on your  computer to simulate millions and millions of   light rays, and calculate how they get absorbed or  scattered off of our objects in a virtual scene.   Initially, we start out with a really noisy image,  and as we add more rays, the image gets clearer   and clearer over time. We can also simulate  sophisticated material models in these programs.   A modern way of doing that is  through using these material nodes. With these, we can conjure up a ton of  different material models and change   their physical properties to our liking. As you  see, they are very expressive indeed, however,   the more nodes we use, the less clear it becomes  how they interact with each other. And as you see,   every time we change something, we have to wait  until a new image is rendered. That is very time   consuming, and more importantly, we have to have  some material editing expertise to use this. This concept is very powerful, for instance, I  think if you watch the Perceptilabs sponsorship   spot at the end of this video, you will be very  surprised to see that they also use node groups,   but with theirs, you don’t build material  models, you can build machine learning models. What would be really cool if we could just give  the machine a photo and it would figure out how   to set these nodes up so it looks exactly like  the material in the photo. So, is that possible,   or is that science fiction? Well, have a look our  paper, called Photorealistic Material Editing.   With this technique, we can easily create these  beautiful material models in a matter of seconds,   even if we don’t know a thing  about light transport simulations.   It does something that is similar to  what many call differentiable rendering. Here is the workflow, we give it a bunch  of images like these, which were created on   this particular test scene, and it guesses what  parameters to use to get these material models.   Now, of course, this doesn’t make any  sense whatsoever, because we have produced   these images ourselves, so we know exactly what  parameters to use to produce this. In other words,   this thing seems useless. And now comes the magic  part, because we don’t use these images. No-no! We load them into Photoshop, and edit them to our  liking, and just pretend that these images were   created with the light simulation program. This  means that we can create a lot of quickly, really   poorly executed edits. For instance, the stitched  specular highlight in the first example isn’t very   well done, and neither is the background of the  gold target image in the middle. However, the   key observation is that we built a mathematical  framework, which makes this pretending really   work! Look, in the next step, our method proceeds  to find a photorealistic material description   that, when rendered, resembles this target  image, and works well even in the presence   of these poorly executed edits. So these  materials are completely made up in Photoshop,   and it turns out, we can create photorealistic  materials through these node graphs that look   almost exactly the same. Quite remarkable.  The whole process executes in 20 seconds. If you are one of the more  curious Fellow Scholars out there,   this paper and its source code are  available in the video description. Now, this differentiable thing has a lot  of steam. For instance, there are more   works on differentiable rendering. In this  other work, we can take a photo of a scene,   and a learning-based method turns the  knobs until it finds a digital object   that matches its geometry and material  properties. This was a stunning piece of work,   from Wenzel Jakob and his group, of course, who  else. They are some of the best in the business. And we don’t even need to be in the area of light  transport simulations to enjoy the benefits of   differentiable formulations, for instance, this  is differentiable physics. So what is that? Imagine that we have this billiard game, where  we would like to hit the white ball with just   the right amount of force and from the right  direction, such that the blue ball ends up close   to the black spot. Well, this example shows that  this is unlikely to happen by chance, and we have   to engage in a fair amount of trial and error  to make this happen. What this differentiable   programming system does for us is that we can  specify an end state, which is the blue ball   on the black dot, and it is able to compute the  required forces and angles to make this happen.   Very close. So after you look here,   maybe you can now guess what’s next  for this differentiable technique…it   starts out with a piece of simulated ink  with a checkerboard pattern, and it exerts   just the appropriate forces so that it forms  exactly the Yin-Yang symbol shortly after. And now that we understand what  differentiable techniques are capable of,   we are ready to proceed to today’s paper. This is  a proper, fully differentiable material capture   technique for real photographs. All this needs  is one flash photograph of a real-world material.   We have those around us in abundance, and  similarly to our previous method, it sets up   the material nodes for it. That is a good thing,  because I don’t know about you, but I do not want   to touch this mess at all. Luckily, we don’t have  to, look! The left is the target photo, and the   right is the initial guess of the algorithm,  that is not bad, but also not very close.   And now, hold on to your papers and just look at  how it proceeds to refine this material until it   closely matches the target. And with that, we have  a digital representation of these materials. We   can now easily build a library of these materials  and assign them to the objects in our scene.   And then, we run the light simulation  program, and here we go. Beautiful. At this point, if we feel adventurous, we can  adjust small things in the material graphs to   create a digital material that is more in line  with our artistic vision. That is great, because   it is must easier to adjust an already existing  material model than creating one from scratch. So what are the key differences  between our work from last year,   and this new paper? Our work made a rough initial  guess and optimized the parameters afterwards,   it was also chock full of neural networks,  it also created materials from a sample,   but that sample was not a photograph, but  a photoshopped image. That is really cool,   however, this new method takes an almost  arbitrary photo, many of these we can take   ourselves or even get them from the internet,  therefore this new method is more general. It also supports 131 different  material node types, which is insanity.   Huge congratulations to the  authors, if I would be an artist,   I would want to work with this right  about now. What a time to be alive! So there you go, this was quite a ride, and I  hope you enjoyed it just half as much as I did.   And if you enjoyed it at least as much as I did,  and you feel a little stranded at home and are   thinking that this light transport thing is pretty  cool, and you would like to learn more about it,   I held a Master-level course on this topic  at the Technical University of Vienna.   Since I was always teaching it to a handful of  motivated students, I thought that the teachings   shouldn’t only be available for the privileged  few who can afford a college education, but   the teachings should be available for everyone.  Free education for everyone, that’s what I want.   So, the course is available free of charge  for everyone, no strings attached, so make   sure to click the link in the video description  to get started. We write a full light simulation   program from scratch there, and learn about  physics, the world around us, and more. Thanks for watching and for your generous  support, and I'll see you next time!"
478,This Magnetic Simulation Took Nearly A Month!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Have a look at these beautiful ferrofluid simulations from a previous paper. These are fluids that have magnetic properties and thus respond to an external magnetic field, and you are seeing correctly, they are able to even climb things. And the best part is that simulator was so accurate that we could run it side-by-side with real-life footage, and see that they run very similarly. Excellent. Now, running these simulations took a considerable amount of time. To address this, a followup paper appeared that showcased a surface-only formulation. What does that mean? Well, a key observation here was that for a class of ferrofluids, we don’t have to compute how the magnetic forces act on the entirety of the 3D fluid domain, we only have to compute them on the surface of the model. So what does this get us? Well, these amazing fluid labyrinths, and all of these ferrofluid simulations, but… faster! So, remember, the first work did something new, but took a very long time, and the second work improved it to make it faster and more practical. Please remember this for later in this video. And now let’s fast forward to today’s paper, this new work can also simulate ferrofluids, and not only that, but also supports a broader range of magnetic phenomena, including rigid and deformable magnetic bodies and two-way coupling too! Oh my! That is sensational, but first, what do these terms mean exactly? Let’s perform four experiments, and after you watch them, I promise that you will understand all about them. Let’s look at the rigid bodies first in experiment number one. Iron box versus magnet. We are starting out slow, and now, we are waiting for the attraction to kick in, and there we go. Wonderful. Experiment number two, deformable magnetic bodies. In other words, magnetic lotus versus a moving magnet. This one is absolutely beautiful, look at how the the petals here are modeled as thin elastic sheets that dance around in the presence of a moving magnet. And if you think this is dancing, stay tuned, there will be an example with even better dance moves in a moment. And experiment number three, two-way coupling. We noted this coupling thing earlier, so, what does that mean? What coupling means is that here, the water can have an effect on the magnet, and the two-way part means that in return, the magnet can also have an effect on the water as well. This is excellent, because we don’t have to think about the limitations of the simulation, we can just drop nearly anything into our simulation domain, be it a fluid, solid, magnetic or not, and we can expect that their interactions are going to be modeled properly. Outstanding. And, I promised some more dancing, so here goes, experiment number four, the dancing ferrofluid. I love how informative the compass is here, it is a simple object that tells us how an external magnetic field evolves over time. I love this elegant solution. Normally, we have to visualize the magnetic induction lines so we can better see why the tentacles of a magnetic octopus move, or why two ferrofluid droplets repel or attract each other. In this case, the authors opted for a much more concise and elegant solution, and I also liked that the compass is not just a 2D overlay, but a properly shaded 3D object with specular reflections as well. Excellent attention to detail. This is really my kind of paper. Now, these simulations were not run on any kind of supercomputer or a network of computers, this runs on the processor of your consumer machine at home. However, simulating even the simpler scenes takes hours. For more complex scenes, even days. And that’s not all, the ferrofluid with the Yin-Yang symbol took nearly a month to compute. So, is that a problem? No, no, of course not. Not in the slightest. Thanks to this paper, general magnetic simulations that were previous impossible are now possible, and don’t forget, research is a process. As you saw in the example at the start of this video with the surface-only ferrofluid formulation, it may become much faster just one more paper down the line. I wanted to show you the first two papers in this video to demonstrate how quickly that can happen. And two more papers down the line, oh my, then, the sky is the limit. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
479,5 Crazy Simulations That Were Previously Impossible!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. If we study the laws of physics and program them into a computer, we can create a beautiful simulation that showcases the process of baking, and if we so desire, when we are done, we can even tear a loaf of bread apart. And with this previous method, we can also smash oreos, candy crabs, pumpkins, and much, much more. This jelly fracture scene is my long-time favorite. And this new work asks a brazen question only a proper computer graphics researcher could ask can we write an even more extreme simulation? I don’t think so, but apparently, this paper promises a technique that supports more extreme compression and deformation, and when they say that, they really mean it. Let’s see what this can do through five super fun experiments. Experiment number one. Squishing. As you see, this paper aligns well with the favorite pastimes of a computer graphics researcher, which is, of course, destroying virtual objects in a spectacular fashion. First, we force these soft elastic virtual objects through a thin obstacle tube. Things get quite squishy here…ouch! And, when they come out on the other side, their geometries can also separate properly. And watch how beautifully they regain their original shapes afterwards. Experiment number two. The tendril test. We grab a squishy ball, and throw it at the wall, and here comes the cool part, this panel was made of glass, so we also get to view the whole interaction through it, and this way, we can see all the squishing happening. Look the tendrils are super detailed and every single one remains intact and intersection-free despite the intense compression. Outstanding. Experiment number three. The twisting test. We take a piece of mat, and keep twisting and twisting, and… still going. Note that the algorithm has to compute up to half a million contact events every time it advances the time a tiny bit, and still, no self intersections, no anomalies. This is crazy. Some of our more seasoned Fellow Scholars will immediately ask okay, great, but how real is all this? Is this just good enough to fool the untrained eye, or does it really simulate what would happen in reality? Well, hold on to your papers, because here comes my favorite part in these simulation papers and this is when we let reality be our judge, and try to reproduce real-world footage with a simulation. Experiment number four, the high-speed impact test. Here is the real footage of a foam practice ball fired at a plate. And now, at the point of impact, this part of the ball has stopped, but the other side is still flying with a high velocity. So what will be the result? A ton of compression. So what does the simulator say about this? My goodness. Just look at that. This is really accurate. Loving it. This sounds all great, but do we really need this technique? The answer shall be given by experiment number five, ghosts and chains. What could that mean? Here, you see Houdini’s Vellum, the industry standard simulator for cloth, soft-body and a number of other kinds of simulations. It is an absolutely amazing tool, but, wait a second. Look. Artificial ghost forces appear, even on a simple test case with 35 chain links. And I wonder if the new method can deal with these 35 chain links? The answer is a resounding yes. No ghost forces. And not only that, but it can deal with even longer chains, let’s try a 100 links. Oh, yeah! Now we’re talking! And now, only one question remains. How much do we have to wait for all this? All this new technique asks for is a few seconds per frame for the simpler scenes, and in the order of minutes per frame for the more crazy tests out there. Praise the papers! That is a fantastic deal. And, what is even more fantastic, all this is performed on your processor, so of course, if someone can implement it in a way that it runs on the graphics card, the next paper down the line will be much, much faster. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
480,3 New Things An AI Can Do With Your Photos!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Here you see people that don’t exist. How can that be? Well, they don’t exist because these images were created with a neural network-based learning method by the name StyleGAN2, which can not only create eye-poppingly detailed looking images, but it can also fuse these people together, or generate cars, churches, horses, and of course, cats. The even cooler thing is that many of these techniques allow us to exert artistic control over these images. So how does that happen? How do we control a neural network? It happens through exploring latent spaces. And what is that? A latent space is a made-up place where we are trying to organize data in a way that similar things are close to each other. What you see here is a 2D latent space for generating different fonts. It is hard to explain why these fonts are similar, but most of us would agree that they indeed share some common properties. The cool thing here is that we can explore this latent space with our cursor, and generate all kinds of new fonts. You can try this work in your browser, the link is available in the video description. And, luckily, we can build a latent space, not only for fonts, but, for nearly anything. I am a light transport researcher by trade, so in this earlier paper, we were interested in generating somewhat hundreds of variants of a material model to populate this scene. In this latent space, we can concoct all of these really cool digital material models. A link to this work is also available in the video description. So let’s recap one of the cool things we can do with latent spaces is generate new images that are somewhat similar. But there is a problem. As we go into nearly any direction, not just one thing, but many things about the image change. For instance, as we explore the space of fonts here, not just the width of the font changes, everything changes. Or if we explore materials here, not just the shininess or the colors of the material change, everything changes. This is great to explore if we can do it in real time. If I change this parameter, not just the car shape changes, the foreground changes, the background changes…again, everything changes! So, these are nice and intuitive controls, but not interpretable controls. Can we get that somehow? The answer is, yes, not everything must change, this previous technique is based on StyleGAN2 and is called StyleFlow, and it can take an input photo of a test subject, and edit a number of meaningful parameters. Age, expression, lighting, pose, you name it. For instance, it could also grew Elon musk a majestic beard. And that’s not all, because Elon Musk is not the only person who got a beard. Look, this is me here, after I got locked up for dropping my papers. And I spent so long in there, that I grew a beard. Or I mean, this neural network gave me one. And since the punishment for dropping your papers is not short…in fact, it is quite long…this happened. Ouch. I hereby promise to never drop my papers, ever again. You will also have to hold on to yours too, so stay alert. So, apparently interpretable controls already exist. And I wonder, how far can we push this concept? Beard or no beard is great, but what about cars, what about paintings? Well, this new technique found a way to navigate these latent spaces and introduces 3 amazing new examples of interpretable controls that I haven’t seen anywhere else yet. One, it can change the car geometry. We can change the sportiness of a car, and even ask the design to be more or less boxy. Note that there is some additional damage here, but we can counteract that by changing the foreground to our taste, for instance, add some grass in there. Two, it can repaint paintings. We can change the roughness of the brush strokes, simplify the style  or even rotate the model. This way, we can create or adjust a painting without having to even touch a paintbrush. Three, facial expressions. First, when I started reading this paper, I was a little suspicious. I have seen these controls before so I looked at it like this, but as I saw how well it did, I went more… like this. And this paper can do way more, for instance, it can add lipstick, change the shape of the mouth or the eyes, and do all this with very little collateral damage to the remainder of the image. Loving it. It can also find and blur the background similarly to those amazing portrait mode photos that newer smartphones can do. And, of course, it can also do the usual suspects. Adjusting the age, hairstyle, or growing a beard. So with that, there we go, now, with the power of neural network-based learning methods, we can create new car designs, can repaint paintings without ever touching a paintbrush, and give someone a shave. It truly feels like we are living in a science fiction world. What  a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
481,All Hail The Adaptive Staggered Grid!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to concoct some absolutely  insane fluid and smoke simulations. A common   property of these simulation programs is that  they subdivide the simulation domain into a grid,   and they compute important quantities like  velocity and pressure in these gridpoints. Normally, a regular grid looks something  like this, but this crazy new technique   throws away the idea of using this as a grid, and  uses this instead. This is called the Adaptive   Staggered-Tilted grid, an AST grid in  short. So what does that really mean? The tilted part means that cells can be rotated  by 45 degrees like this. And interestingly,   they typically appear only where needed, I’ll  show you in a moment. The adaptive part means   that the size of the grid cells is  not fixed, and can be all over the   place. And even better, this concept can  be easily generalized to 3D grids as well.   Now, when I first read this paper, two things  came to my mind, one, that is an insane idea,   I kinda like it, and two, it cannot possibly  work! It turns out, only one of these is true. And I was also wondering, why? Why do all this?  And the answer is because this way, we get   better fluid and smoke simulations. Oh yeah! Let’s  demonstrate it through four beautiful experiments! Experiment number one. Kármán vortex  streets. We noted that the tilted grid points   appear only where they are needed, and these are  places where there is great deal of vorticity.   Let’s test that. This phenomenon showcases  repeated vortex patterns and the algorithm is hard   at work here. How do we know? Well, of course, we  don’t know that…yet! So let’s look under the hood   together and see what is going on! Oh wow, look at  that! The algorithm knows where the vorticity is,   and as a result, these tilted cells are  flowing through the simulation beautifully. Experiment number two. Smoke plumes and  porous nets. This technique refines the   grids with these tilted cells in the areas  where there is a great deal of turbulence,   and, wait a second. What is this? The net is  also covered with tilted cells. Why is that?   The reason for this is that the tilted cells not  only cover turbulent regions, but other regions   of interest as well. In this case, it enables us  to capture this narrow flow around the obstacle.   Without this new AST grid, some of these  smoke plumes wouldn’t make it through the net. Experiment number three. The boat ride. Note that  the surface of the pool is completely covered with   the new tilted cells, making sure that the wake  of the boat is as detailed as it can possibly   be. But in the meantime, the algorithm is not  wasteful, look, the volume itself is free of them. And now, hold on to your papers  for experiment number four.   Thin water sheets. You can see the final  simulation here, and if we look under the hood,   my goodness, just look at how much work this  algorithm is doing. And what is even better,   it only does so where it is really needed it  doesn’t do any extra work in these regions. I am so far, very impressed with this technique.  We saw that it does a ton of work for us,   and increases the detail in our simulations, and  helps things flow through where they should really   flow through. Now, with that said, there is only  one question left. What does this cost us? How   much more expensive is this kind of AST grid  simulation than a regular grid? +100 percent   computation time? +50 percent? How much is it  worth to you? Please stop the video and leave a   comment with your guess. I’ll wait. Thank you! The  answer is none of those. It costs almost nothing,   and adds typically an additional 1% of computation  time. And in return for that almost nothing,   we get all of these beautiful fluid and  smoke simulations. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
482,NVIDIA’s AI Puts Video Calls On Steroids!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This paper is really something else. Scientists at NVIDIA just came up with an absolutely insane idea for video conferencing. Their idea is not to do what everyone else is doing, which is, transmitting our video to the person on the other end. No, of course not, that would be too easy! What they do in this work, is take only the first image from the video, and they throw away the entire video afterwards! But before that, it stores a tiny bit of information from it, which is, how our head is moving over time, and how our expressions change. That is an absolutely outrageous idea… and of course, we like those around here, so, does this work? Well, let’s have a look. This is the input video, note that this is not transmitted, only the first image and some additional information, and the rest of this video is discarded. And hold on to your papers, because this is the output of the algorithm compared to the input video. No, this is not some kind of misunderstanding, nobody has copy-pasted the results there. This is a near-perfect reconstruction of the input, except that the amount of information we need to transmit through the network is significantly less than with previous compression techniques. How much less? Well, you know what’s coming, so let’s try it out! Here is the output of the new technique, and here is the comparison against H.264, a powerful and commonly used video compression standard. Well, to our disappointment, the two seem close, the new technique appears better, especially around the glasses, but the rest is similar. And if you have been holding on to your papers so far, now, squeeze that paper, because this is not a reasonable comparison. And that is because the previous method was allowed to transmit 6 to 12 times more information. Look, as we further decrease the data allowance of the previous method, it still can transmit more than twice as much information, and at this point, there is no contest. This bitrate would be unusable for any kind of videoconferencing, while the new method uses less than half as much information, and still transmits a sharp and perfectly fine video. Overall, the authors report that their new method is ten times more efficient. That is unreal. This is an excellent video reconstruction technique, that much is clear. And if it only did that, it would be a great paper. But this is not a great paper, this is an absolutely amazing paper, so it does even more. Much, much more! For instance, it can also rotate our head and make a frontal video, can also fix potential framing issues by translating our head, and transferring all of our gestures to a new model. And, it is also evaluated well, so all of these new features are tested in isolation. Look at these two previous methods trying to frontalize the input video. One would think that it’s not even possible to perform properly given how much these techniques are struggling with the task…until we look at the new method. My goodness. There is some jumpiness in the neck movement in the output video here, and some warping issues here, but otherwise, very impressive results. Now if you have been holding on to your papers so far, now, squeeze that paper, because these previous methods are not some ancient papers that were published a long time ago. Not at all! Both of them were published within the same year as the new paper. How amazing is that. Wow. I really liked this page from the paper, which showcases both the images and the mathematical measurements against previous methods side by side. There are many ways to measure how close two videos are to each other, the up and down arrows tell us whether the given quality metric is subject to minimization or maximization, for instance, pixelwise errors are typically minimized, so lesser is better, but we are to maximize the the peak signal to noise ratio. And the cool thing is that none of this matters too much as soon as we insert the new technique, which really outpaces all of these. And we are still not done yet! So we said that the technique takes the first image, reads the evolution of expressions and the head pose from the input video, and then, it discards the entirety of the video, save for the first image. The cool thing about this was that we could pretend to rotate the head pose information, and the result is that the head appears rotated in the output image. That was great. But what if we take the source image from someone, and take this data, the driving keypoint sequence from someone else? Well, what we get is, motion transfer. Look! We only need one image of the target person, and we can transfer all of our gestures to them, in a way that is significantly better than most previous methods. Now, of course, not even this technique is perfect, it still struggles a great deal in the presence of occluder objects, but still, just the fact that this is possible feels like something straight out of a science fiction movie. What  a  time  to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
483,An AI That Makes Dog Photos - But How?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, we are going to explore a paper that improves on the incredible StyleGAN2. What is that? StyleGAN2 is a neural network-based learning algorithm that is not only capable of creating these eye-poppingly detailed images of human beings that don’t even exist, but, it also improved on its previous version in a number of different ways. For instance, with the original StyleGAN method, we could exert some artistic control over these images, however, look, you see how this part of the teeth and eyes are pinned to a particular location and the algorithm just refuses to let it go, sometimes to the detriment of its surroundings. The improved StyleGAN2 method addressed this problem, you can see the results here. Teeth and eyes are now allowed to float around freely, and perhaps this is the only place on the internet where we can say that and be happy about it. It could also mix two images together, and it could do it not only for human faces, but for cars, buildings, horses, and more. And get this, this paper was published in December 2019, and since then, it has been used in a number of absolutely incredible applications and followup works. Let’s look at three of them. One, for instance, the first question I usually hear when I talk about an amazing paper like this is “okay, great, but when do I get to use this”? And the answer is, right now, because it is implemented in Photoshop in a feature that is called Neural Filters. Two, artistic control over these images has improved so much that now, we can pin down a few intuitive parameters and change them with minimal changes to other parts of the image. For instance, it could grow Elon Musk a majestic beard…and, Elon Musk was not the only person who got an algorithmic beard, I hope you know what’s coming…yes, I got one too! Let me know in the comments whose beard you liked better! Three, a nice followup paper that could take a photo of Abraham Lincoln and other historic figures, and could restore their images as if we were time travelers and took these photos with a more modern camera. The best part here was that it leveraged the superb morphing capabilities of StyleGAN2 and took an image of their siblings, a person who has somewhat similar proportions to the target subject, and morph them into a modern image of this historic figure. This was brilliant, because restoring images is hard, but with StyleGAN2, morphing is now easy, so the authors decided to trade a difficult problem for an easier one. And the results speak for themselves. We cannot know for sure if this is what these historic figures really looked like, but for now, it makes one heck of a thought experiment. And now, let’s marvel together at these beautiful results with the new method, that goes by the name, StyleGAN2-ADA. While we look through these results, all of which were generated with the new method, here are three things that it does better. One, if often works just as well as StyleGAN2 but requires ten times fewer images for training. This means that now, it can create these beautiful images, and this can be done by training a set of neural networks from less than 10 thousand images at a time. Whoa. That is not much at all. Two, it creates better quality results. The baseline here is original StyleGAN2, the numbers are subject to minimization and are a measure of the quality of these images. As you see from the bolded numbers, the new method not only beats the baseline method substantially, but it does it across the board. That is a rare sight indeed. And three, we can train this method faster, it generates these images faster, and in the meantime, also consumes less memory, which is usually in short supply on our graphics cards. Now, we noted that the new version of the method is called StyleGAN2-ADA. What is ADA? This part means Adaptive Discriminator Augmentation. What does that mean exactly? This means that the new method endeavors to squeeze as much information out of these training datasets as it can. Data augmentation is not new, it has been done for many years now, and essentially this means that we rotate, colorize, or even corrupt these images during the training process. The key here is that with this, we are artificially increasing the number of training samples the neural network sees. The difference here is that they used a greater set of augmentations, and the adaptive part means that these augmentations are tailored more to the dataset at hand. And now comes the best part, hold on to your papers, and let’s look at the timeline here. StyleGAN2 appeared in December 2019, and StyleGAN2-ADA, this method came out just half a year later. Such immense progress, in just 6 months of time. The pace of progress in machine learning research is absolutely stunning these days. Imagine what we will be able to do with these techniques just a couple more years down the line. What a time to be alive! But this paper also teaches a very important lesson to us that I would like to show you. Have a look at this table that shows the energy expenditure for this project for transparency, but it also tells us the number of experiments that were required to finish such an amazing paper. And that is more than 3300 experiments. 255 of which were wasted due to technical problems. In the foreword of my PhD thesis, I wrote the following: “Research is the study of failure. More precisely, research is the study of obtaining new knowledge through failure. A bad researcher fails 100% of the time, while a good one fails only 99% of the time. Hence, what you see written here (and in most papers) is only 1% of the work that has been done. I would like to thank Felícia, my wife, for providing motivation, shielding me from distractions, and bringing sunshine to my life to endure through many of these failures.” This paper is a great testament to show how difficult the life of a researcher is. How many people give up their dreams, after being rejected once, or maybe two times? Ten times? Most people give up after 10 tries. And just imagine having a 1000 failed experiments and still not even being close to publishing a paper yet. And, with a little more effort, this amazing work came out of it. Failing doesn’t mean losing. Not in the slightest. Huge congratulations to the authors for their endurance, and for this amazing work, and I think this also shows that these Weights and Biases experiment tracking tools are invaluable, because it is next to impossible to remember what went wrong with each of them, and what should  be fixed. Thanks for watching and for your generous support, and I'll see you next time!"
484,DeepMind’s AI Watches YouTube and Learns To Play!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Between 2013 and 2015, DeepMind worked on  an incredible learning algorithm by the name   Deep Reinforcement Learning. This technique looked  at the pixels of the game, was given a controller   and played much like a human would… with the  exception that it learned to play some Atari games   on a superhuman level. I have tried to train it  a few years ago and would like to invite you for   a marvelous journey to see what happened. When  it starts learning to play an old game, Atari   breakout, at first, the algorithm loses all of  its lives without any signs of intelligent action. If we wait a bit, it becomes better at playing  the game, roughly matching the skill level of an   adept player. But here's the catch, if we wait for  longer, we get something absolutely spectacular.   Over time, it learns to play like a pro, and  finds out that the best way to win the game   is digging a tunnel through the  bricks and hit them from behind. This technique is combination of a neural network  that processes the visual data that we see on   the screen, and a reinforcement learner that  comes up with the gameplay-related decisions.   This is an amazing algorithm, a  true breakthrough in AI research. A key point in this work was that the  problem formulation here enabled us   to measure our progress easily: we  hit one brick, we get some points,   so do a lot of that. Lose a few lives, the game  ends, don’t do that! Easy enough. But there are   other, exploration-based games like Montezuma’s  revenge or Pitfall that it was not good at. And   man, these games are a nightmare for any AI,  because there is no score, or at the very least,   it’s hard to define how well we are doing. Because  there are no scores, it is hard to motivate the   AI to do anything at all other than just wander  around aimlessly. If no one tells us if we are   doing well or not, which way do we go? Explore  this place or go to the next one? How do we solve   all this? And with that, let’s discuss  the state of play in AIs playing difficult   exploration-based computer games. And I think  you will love to see how far we have some since. First, there is a previous line of work that  infused these agents with a very human-like   property… curiosity. That agent was able to do  much, much better at these games…and then got   addicted to the TV. But that’s a different story.  Note that this TV problem has been remedied since. And this new method attempts  to solve hard exploration games   by watching Youtube videos  of humans playing the game,   and learning from that, as you see, it just rips  through these levels in Montezuma’s revenge and   other games too. So, I wonder how does all this  magic happen? How did this agent learn to explore? Well, it has three things going  for it that really makes this work. One, the Skeptical Scholar would say, that all is  takes is just copy-pasting what it saw from the   human player! Also, imitation learning is not new,  which is a point that we will address in a moment,   so, why bother with this? Now, hold on  to your papers, and observe as it seems   noticeably less efficient than the human  teacher was. Until we realize that this   is not the human player, and this is  not the AI…but the other way around!   Look, it was so observant and took away so much  from the human demonstrations that in the end,   it became even more efficient than its  human teacher. Whoa! Absolutely amazing. And while we are here, I would like  to dissect this copy-paste argument.   You see, it has an understanding of the game,  and does not just copy the human demonstrator.   But even if it just copied what it saw, it would  not be so easy because the AI only sees images,   and it has to translate how the images change in  response to us pressing buttons on the controller.   We might also encounter the same  level, but at a different time,   and we have to understand how to vanquish  an opponent and how to perform that. Two, nobody hooked the agent  into the game information,   which is huge. This means that it doesn’t know  what buttons are pressed on the controller,   no internal numbers or the game state are given  to it, and most importantly, it is also not given   the score of the game. We discussed how difficult  this makes everything. Unfortunately, this means   that there is no easy way out it really has to  understand what it sees and mine out the relevant   information from each of these videos. And as you  see, it does that with flying colors. Loving it. And three, it can handle the domain gap. Previous  imitation learning methods did not deal with that   too well. So what does that mean? Let’s look at  this latent space together and find out. This   is what a latent space looks like if we just  embed the pixels that we see in the videos.   Don’t worry, I’ll tell you in a moment what that  is. Here, the clusters are nicely clumped up   away from each other, so that’s probably good,  right? Well, in this problem, not so much!   A latent space means a place where  similar kinds of data are meant to   be close to each other. These are snippets of the  demonstration videos that the clusters relate to.   Let’s test that together. Do you  think these images are similar? Yes?   Most of us humans would say that these are quite  similar, in fact, they are nearly the same. So,   is this a good latent space embedding? No,  not in the slightest. This data is similar,   therefore, these should be close to each other,  but this previous technique did not recognize that   because these images have slightly different  colors, aspect ratios, this has a text overlay,   but we all understand that despite all that, we  are looking at the same game through different   windows. So, does the new technique recognize  that? Oh yes, beautiful. Praise the papers!   Similar game states are now close to each  other, we can align them properly and therefore,   we can learn more easily from them. This is  one of the reasons why it can play so well. So there you go, these new AI agents can look  at how we perform complex exploration games,   and learn so well from us, that in the end, they  do even better than we do. And now, to get them to   write some amazing papers for us…or, you know, Two  Minute Papers episodes. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
485,OpenAI Outperforms Some Humans In Article Summarization!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. This paper will not have the visual fireworks  that you see in many of our videos. Oftentimes,   you get ice cream for the eyes, but today,  you’ll get an ice cream for the mind. And,   when I read this new paper,  I almost fell off the chair,   and I think this work teaches us important  lessons and I hope you will appreciate them too. So, with that, let’s talk about AIs and dealing  with text! This research field is improving at an   incredible pace. For instance, four years ago,  in 2017, scientists at OpenAI embarked on an AI   project where they wanted to show a neural network  a bunch of Amazon product reviews and wanted to   teach it to be able to generate new ones, or  continue a review when given one. Upon closer   inspection, they noticed that the neural network  has built up a knowledge of not only language,   but also learned that it needs to create a  state-of-the-art sentiment detector as well.   This means that the AI recognized that in order to  be able to continue a review, it needs to be able   to understand English, and efficiently detect  whether the review seems positive or negative. This new work is about text summarization, and it  really is something else. If you read reddit, the   popular online discussion website, and encounter  a longer post, you may also find a short summary,   a TLDR of the same post, written by a fellow  human. This is good for not only the other readers   who are in a hurry, but, it is less obvious  is that it is also good for something else. And now, hold on to your papers, because  these summaries also provide fertile   grounds for a learning algorithm to read a  piece of long text, and its short summary,   and learn how the two relate to each other.  This means that it can be used as training   data and can be fed to a learning algorithm.  Yum! And the point is that if we give enough   of these pairs to these learning algorithms,  they will learn to summarize other reddit posts. So, let’s see how well it performs. First,  this method learned on about a hundred thousand   well-curated reddit posts, and was also tested  on other posts that it hadn’t seen before. It   was asked to summarize this post from relationship  advice subreddit, and let’s see how well it did. If you feel like reading the text, you  can pause the video here, or if you feel   like embracing the TLDR spirit, just carry  on, and look at these two summarizations.   One of these is written by a human, and the other  one by this new summarization technique. Do you   know which is which? Please stop the video and let  me know in the comments below. Thank you! So this,   was written by a human and this by the new  AI. And while, of course, this is subjective,   I would say that the AI-written one feels at  the very least as good as the human summary,   and I can’t wait to have a look at the  more principled evaluation in the paper.   Let’s see…the higher we go here, the higher the  probability of a human favoring the AI-written   summary to a human-written one. And we have  smaller AI models on the left, bigger ones to   the right. This is the 50% reference line, below  it, people tend to favor the human’s version,   and if it can get above the 50% line, the  AI does a better job than human-written   TLDRs in the dataset. Here are two proposed  models, this one significantly underperforms,   this other one is a better match. However,  whoa! Look at that! The authors also proposed a   human feedback model that, even for the smallest  model, handily outperforms human-written TLDRs,   and as we grow the AI model, it gets even  better than that. Now that’s incredible,   and this is when I almost fell off  the chair when reading this paper. But! We’re not done yet, not even close. Don’t  forget, this AI was trained on reddit, and was   also tested on reddit. So our next question is,  of course, can it do anything else? How general   is the knowledge that it gained? What if we  give it a full news article from somewhere else,   outside of reddit? Let’s see how it performs.  Hmm…of course, this is also subjective, but I   would say both are quite good. The human-written  summary provides a little more information,   while the AI-written one captures the essence of  the article and does it very concisely. Great job. So, let’s see the same graph for summarizing  these articles outside reddit. I don’t expect   the AI to perform as well as with the reddit  posts as it is outside the comfort zone,   but…my goodness, this still performs nearly  as well as humans. That means that it indeed   derived general knowledge from a really narrow  training set, which is absolutely amazing. Now,   ironically, you see this Lead-3 technique  dominating both humans and the AI. What could that   be? Some unpublished, superintelligent technique?  Well, I will have to disappoint, this is not a   super sophisticated technique, but a dead simple  one. So simple that it is just taking the first   three sentences of the article, which humans seem  to prefer a great deal. But note, that this simple   Lead-3 technique only works for a narrow domain,  while the AI has learned the English language,   probably knows about sentiment, and a lot  of other things that can be used elsewhere. And now, the two most impressive  things from the paper, in my opinion:  One, this is not a neural network, but a  reinforcement learning algorithm that learns   from human feedback. A similar technique has been  used by DeepMind and other research labs to play   video games or control drones and it is really  cool to see them excel in text summarization too. Two, it learned from humans, but derived so much  knowledge from these scores, that over time,   it outperformed its own teacher. And the  teacher here is not humans in general,   but people who write TLDRs along their posts  on reddit. That truly feels like something   straight out of a science fiction  movie. What a time to be alive! Now, of course, not even this technique is  perfect, this human vs AI preference thing   is just one way of measuring the quality of the  summary, there are more sophisticated methods   that involve coverage, coherence, accuracy, and  more. In some of these measurements, the AI does   not perform as well. But just imagine what this  will be able to do two more papers down the line. Thanks for watching and for your generous  support, and I'll see you next time!"
486,This AI Learned To Stop Time!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we get to be a paper historian, and witness the amazing progress in machine learning research together, and learn what is new in the world of NERFs. But, first, what is a NERF? In March of 2020, a paper appeared describing an incredible technique by the name, Neural Radiance Fields, or NERF in short. This work enables us to take a bunch of input photos and their locations, learn it, and synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. And here, we are talking not only digital environments, but also, real scenes as well! Now that’s quite a value proposition, especially given that it also supported refractive and reflective surfaces as well, these are both quite a challenge. However, of course, NERF had its limitations. For instance, in many cases, it had trouble with scenes with variable lighting conditions and lots of occluders. And to my delight, only 5 months later, in August of 2020, a followup paper appeared by the name NERF in the Wild, or NERF-W in short. Its speciality was tourist attractions that a lot of people take photos of, and we then have a collection of photos taken during a different time of the day, and of course, with a lot of people around. And, lots of people, of course means, lots of occlusions. NERF-W improved the original algorithm to excel more in cases like this. A few months later, on 2020 November 25th, another followup paper appeared by the name Deformable Neural Radiance Fields. D-NERF. The goal here was to take a selfie video, and turn it into a portrait that we can rotate around freely. This is something that the authors call a nerfie. If we take the original NERF technique to perform this, we see that it does not do well at all with moving things and that's where this new deformable variant really shines. And today’s paper not only has some nice video results embedded in the front page, and it offers a new take on this problem and offers quote “Space-Time View Synthesis of Dynamic Scenes”. Whoa, that is amazing. But what does that mean? What does this paper really do? The space-time view synthesis means that we can record a video of someone doing something. Since we are recording movement in time, and there is also movement in space, or in other words, the camera is moving. Both time and space are moving. And what this can do is one, freeze one of these variables, in other words, pretend as if the camera didn’t move. Or, two, pretend as if time didn’t move. Or, three, generate new views of the scene while movement takes place. My favorite is that we can pretend to zoom in and even better, zoom out even if the recorded video looked like this, or, we can also make a really choppy family memory smoother and much more enjoyable. So how does this compare to previous methods? There are plenty of NERF variants around, is this really any good? Let’s find out together! This is the original NERF, we already know about this, and, we are not surprised in the slightest that it’s not so great on dynamic scenes with a lot of movement. However, what I am surprised by is that all of these previous techniques are from 2020, and all of them struggle with these cases. These comparisons are not against some ancient technology from 1985. No-no, all of them are from the same year. For instance, this previous work is called Consistent Video Depth Estimation, and it is from August 2020. We showcased it in this series, and marveled at all of these amazing augmented reality applications that it offered. The snowing example here was one of my favorites. And today’s paper appeared just three months later, in November 2020. And the authors still took the time and effort to compare against this work from just three months ago. That is fantastic. As you see, this previous method kind of works on this dog, but the lack of information in some regions is quite apparent. This is still maybe usable, but as soon as we transition into a more dynamic example, what do we get? Well, pandemonium. This is true for all previous methods. I cannot imagine that the new method from just a few months later could deal with this difficult case…and. Look at that. So much better! It is still not perfect, you see that the we have lost some detail, but witnessing this kind of progress in just a few months is truly a sight to behold. It really consistently outperforms all of these techniques from the same year. What a time to be alive! If you, like me, find yourself yearning for more quantitative comparisons, the numbers also show that the two variants of the new proposed technique indeed outpace the competition. And it can even do one more thing. Previous video stabilization techniques were good at taking a shaky input video and creating a smoother output, however, these results often came at the cost a great deal of cropping. Not this new work, look at how good it is at stabilization, and it does not have to crop all this data. Praise  the papers! Thanks for watching and for your generous support, and I'll see you next time!"
487,Soap Bubble Simulations Are Now Possible!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today I will try to show  you the incredible progress   in computer graphics research through the  lens of bubbles in computer simulations. Yes, bubbles indeed. Approximately a year ago,  we covered a technique which could be used   to add bubbles to an already existing fluid  simulation. This paper appeared in 2012 and   described a super simple method that helped us  compute where bubbles appear and disappear over   time. The best part of this was that this could  be added after the simulation has been finalized,   which is an insane value proposition. If we  find ourselves yearning for some bubbles,   we just add it afterwards, if we don’t like the  results, we can take them out with one click. Now, simulations are not only about sights,   what about sounds? In 2016, this paper did  something that previously seemed impossible:   it took this kind of simulation data,  and made sure that now, we can not only   add bubbles to a plain water simulation,  but also simulate how they would sound. On the geometry side, a followup paper appeared   just a year later that could simulate a handful  of bubbles colliding, sticking together. Then, three years later, in 2020, Christopher  Batty’s group also proposed a method   that was capable of simulating merging and  coalescing behavior on larger-scale simulations. So, what about today’s paper? Are we going  even larger with hundreds of thousands,   or maybe even millions of bubbles? No,  we are going to take just one bubble…   or at most a handful, and have a real close look  at a method that is capable of simulating these   beautiful evolving rainbow patterns. The  key to this work is that it is modeling   how the thickness of the surfaces changes  over time. That makes all the difference. Let’s look under the hood and observe how much  of an effect the evolving layer thickness has on   the outputs. The red color coding represents  thinner, and the blue shows us the thicker   regions. This shows us that some regions in these  bubbles are more than twice as thick as others.   And there are also more extreme cases, there is  a six-time difference between this and this part.   You can see how the difference in  thickness leads to waves of light   interfering with the bubble  and creating these beautiful   rainbow patterns. You can’t get this without  a proper simulator like this one. Loving it. This variation in thicknesses is responsible  for a selection of premium-quality effects   in a simulation beyond surface vortices,  interference patterns can also be simulated, and,   deformation-dependent rupturing of soap films.   This incredible technique can  simulate all of these phenomena. And now, our big question is, okay,   it simulates all these, but how well does it do  that? It is good enough to fool the human eye,   but how does it compare to the  strictest adversary of all… reality! I hope you know what is coming. Oh yeah!  Hold on to your papers, because now   we will let reality be our judge and  compare the simulated results to that.   That is one of the biggest challenges  in any kind of simulation research,   so, let’s see. This is a piece of real footage of  a curved soap film surface, where these rainbow   patterns get convected by an external force field.  Beautiful. And now, let’s see the simulation.   Wow, this has to be really close. Let’s  see them side by side and decide together.   Whoa. The match in the swirly region here is just  exceptional. Now, note that even if the algorithm   is a 100% correct, this experiment cannot be a  perfect match because not only the physics of   the soap film has to be simulated correctly, but  the forces that move the rainbow patterns as well.   We don’t have this information from the  real-world footage, so the authors had to try to   reproduce these forces, which is not part of the  algorithm, but a property of the environment. So,   I would say that this footage is as close as  one can possibly get. My goodness, well done! So, how much do we have to pay for  this in terms of computation time?   If you ask me, I would pay at the very least  double for this. And if you have been holding on   to your papers so far, now, squeeze that paper,  because now comes the best part, because in the   cheaper cases, only 4 to 7% extra  computation, which is outrageous.   There is this more complex case, with the large  deforming sphere. In this case, the new technique   indeed makes a huge difference. So, how much  extra computation do we have to pay for this?   Only 31%. 31% extra computation  for this? That is a fantastic deal,   you can sign me up right away. As you see, the  pace of progress in computer graphics research   is absolutely incredible, and these simulations  are just getting better and better by the day.   Imagine what we will be able to do just two more  papers down the line! What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
488,"Finally, Video Stabilization That Works!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to talk  about video stabilization.   A typical application of this is when we  record family memories, and other cool events,   and sometimes, the footage gets so shaky  that we barely know what is going on. In these cases, video stabilization  techniques can come to the rescue,   which means that in goes a shaky video,  and out comes a smooth video. Well,   that is easier said than done. Despite  many years of progress, there is a great   selection of previous methods that can do that,  however, they suffer from one of two issues. Issue number one is cropping. This  means that we get usable results,   but we have to pay a great price for it, which is,  cropping away a great deal of the video content.   Issue number two is when we get the  entirety of the video, no cropping,   however, the price to be paid for this is that we  get lots of issues that we call visual artifacts. Unfortunately, today, when we  stabilize, we have to choose our poison.   It’s either cropping, or artifacts. Which one  would you choose? That is difficult to decide,   of course, because none of these two tradeoffs are  great. So our question today is, can we do better?   Well, the Law of Papers says that of course, just  one or two more papers down the line, this will be   way better. So, let’s see…this is  what this new method is capable of.   Hold on to your papers, and notice that this will  indeed be a full-size video, so we already know   that probably there will be artifacts. But…wait  a second! No artifacts. Whoa. How can that be? What does this new method do that previous  techniques didn’t? These magical results are a   combination of several things: one, the new method  can estimate the motion of these objects better,   two, it removes blurred images from the videos  and three, collects data from neighboring video   frames more effectively. This leads to a greater  understanding of the video it is looking at. Now, of course, not even this technique is  perfect rapid camera motion may lead to warping,   and if you look carefully, you may find some  artifacts, usually around the sides of the screen. So far, we looked at previous methods, and the  new method. It seems better. That’s great. But   how do we measure which one is better? Do we  just look? An even harder question would be,   if the new method is indeed better,  okay, but by how much better is it? Let’s try to answer all of these questions. We  can evaluate these techniques against each other   in three different ways. One, we can look at the  footage ourselves. We have already done that,   and we had to tightly hold on to our  papers, it has done quite well in this test.   Test number two is a quantitative test. In other  words, we can mathematically define how much   distortion there is in an output video, how smooth  it is, and more, and compare the output videos   based on these metrics. In many cases, these  previous techniques are quite close to each other,   and now, let’s unveil the new method.  Whoa. It scored best or second best   on 6 out of 8 tests. This is truly remarkable,  especially given that some of these competitors   are from less than a year ago. That is nimble  progress in machine learning research. Loving it. And the third way to test which technique is  better, and by how much is by conducting a user   study. The authors have done that too! In this,  46 humans were called in, were shown the shaky   input video, the result of a previous method, and  the new method, and were asked three questions.   Which video preserves the most content, which has  fewer imperfections, and which is more stable. And the results were stunning despite  looking at many different competing techniques,   the participants found the new method to be  better at the very least 60% of the time,   on all three questions. In some cases, even  90% of the time or higher. Praise the papers! Now, there is only one question left. If it  is so much better than previous techniques,   how much longer does it take to run? With one exception, these previous methods take   from half a second to about 7.5 seconds per frame,  and this new one asks for 9.5 seconds per frame.   And in return, it creates these absolutely amazing  results. So, from this glorious day on, fewer,   or maybe no important memories will be lost  due to camera shaking. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
489,Do Neural Networks Think Like Our Brain? OpenAI Answers!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to cover many important questions in life. For instance, who is this? This is Halle Berry. Now, I show you this, who is it? It is, of course, also Halle Berry. And if I show you this piece of text, who does it refer to? Again, Halle Berry. So why are these questions interesting? Well, an earlier paper found out from brain readings that we indeed have person neurons in our brain. These are neurons specialized to recognize a particular human being. That is quite interesting. And what is even more interesting is that not that we have person neurons, but that these neurons are multimodal. What does that mean? This means that we understand the essence of what makes Halle Berry, regardless of whether it is a photo, a drawing, or anything else. I see, alright. Well then, our first question today is, do neural networks also have multimodal neurons? Not necessarily. The human brain is an inspiration for an artificial neural network that we can simulate on our computers, but do they work like the brain? Well, if we study their inner workings, our likely answer will be no, not in the slightest. But still, no one can stop us from a little experimentation, so let’s try this for a common neural network architecture. This neuron responds to human faces and says that this is indeed a human face. So far so good. Now, if we provide it with a drawing of a human face, it won’t recognize it to be a face. Well, so much for this multimodal idea, this one is surely not a brain in a jar. But wait, we don’t give up so easily around here. This is not the only neural network architecture that exists, let’s grab a different one. This one is called OpenAI’s CLIP, and it is remarkably good at generalizing concepts. Let’s see how it can deal with the same problem. Yes, this neuron responds to spiders, and Spiderman. That’s the easy part. Now, please hold on to your papers, because now comes the hard part. Drawings and comics of spiders and Spiderman. Yes, it responds to that too! Wonderful. Now comes the final boss, which is spider-related writings. And.. it responds to that too. Now, of course, this doesn’t mean that this neural network would be a brain in a jar, but it is a tiny bit closer to our thinking than previous architectures. And now comes the best part, this insight opens up the possibility for three amazing experiments. Experiment number one. Essence. So it appears to understand the essence of a concept or a person. That is absolutely amazing, so I wonder if we can turn this problem around and ask what it thinks about different concepts? It would be the equivalent to saying give me all things spiders and Spiderman. Let’s do that with Lady Gaga. It says this is the essence of Lady Gaga. We get the smug smile, very good. And it says that the essence of Jesus Christ is this, and it also includes the crown of thorns. So far, flying colors. Now, we will see some images of feelings, some of you might find some of them disturbing. I think the vast majority of you humans will be just fine looking at them, but I wanted to let you know just in case. So, what is the essence of someone being shocked? Well, this. I can attest to that, this is basically me when reading this paper. My eyes were popping out just like this. Sleepiness. Yes, that is a coffee person before coffee alright. The happiness, crying and seriousness neurons also embody these feelings really well. Experiment number two, adversarial attacks. We know that OpenAI’s clip responds to photos and drawings of the same thing, so let’s try some nasty attacks involving combining the two. When we give it these images, it can classify them with ease. This is an Apple, this is a laptop, a mug, and so on. Nothing too crazy going on here. However, now, let’s prepare a nasty adversarial attack. Previously, sophisticated techniques were developed to fool a neural network by adding some nearly imperceptible noise to an image. Let’s have a look at how this works. First, we present a previous neural network with an image of a bus, and it will successfully tell us that yes, this is indeed a bus. Of course it does. Now, we show it not an image of a bus, but a bus plus some carefully crafted noise that is barely perceptible, that forces the neural network to misclassify it as an ostrich. I will stress that this is not any kind of noise, but the kind of noise that exploits biases in the neural network, which is, by no means trivial to craft. So now, I hope you are expecting a sophisticated adversarial attack against the wonderful CLIP neural network. Yes, that will do. Or will it? Let’s see together! Yes, indeed, I don’t know if you knew, but this is not an apple, this is a pizza. And so is this one. The neural network fell for these ones, but it was able to resist this sophisticated attack in the case of the coffee mug and the phone. Perhaps the pizza labels had too small a footprint in an image, so let’s try an even more sophisticated version of this attack. Now, you may think that this is a chihuahua, but that is completely wrong, because this, is a pizza indeed. Not a chihuahua in sight anywhere in this image. No-no! So what did we learn here? Well, interestingly, this CLIP neural network is more general than previous techniques, however, its superpowers come at a price. And that price is that it can be exploited easily with simple systematic attacks. That is a great lesson indeed. Experiment number three, understanding feelings. Now this will be one heck of an experiment. We will try to answer the age-old question, which is, how would you describe feelings to a machine? Well, it’s hard to explain such a concept, but all humans understand what being bored means. However, luckily, these neural networks have neurons, and they can use those to explain to us what they think about different concepts. An interesting idea here is that feelings could sort of emerge as a combination of other, more elementary neurons that are already understood. If this sounds a little nebulous, let’s go with that example, what does the machine think it means that someone is bored? Well, it says that bored, is relaxed + grumpy. This isn’t quite the way I think about it, but not bad at all, little machine. I like the way you think. Let’s try one more. How do we explain to a machine what a surprise means? Well, it says surprise is celebration + shock. Nice. What about madness? Let’s see. Evil + serious + a hint of mental illness. And when talking about combinations, there are two more examples that I really liked. If we are looking for text that embodies evil, we get something like this. And now, give me an evil building. Oh yes, I think that works really well, but there are internet forums where we have black-belt experts specializing in this very topic, so, if you are one of them, please let me know in the comments what you think. The paper also contains a ton more information, for instance, there is an experiment with the Stroop effect. This explores whether the neural network reacts to the meaning of the text, or the color of the text? I will only tease this because I would really like you to read the paper, which is available below in the video description. So there we go, neural networks are by no means brains in a jar, they are very much computer programs, however, CLIP has some similarities, and we also found that there is a cost to that. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
490,Can You Put All This In a Photo?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Virtual Reality, VR in short, is maturing at a rapid pace and its promise is truly incredible. If one day it comes to fruition, doctors could be trained to perform surgery in a virtual environment, we could train pilots with better flight simulators, teach astronauts to deal with zero-gravity simulations, you name it. And you will see that with today’s paper, we are able to visit nearly any place from afar. Now, to be able to do anything in a virtual world, we have to put on a head-mounted display that can tell the orientation of our head, and often, hands at all times. So, what can we do with this? Oh boy, a great deal. For instance, we can type on a virtual keyboard, or implement all kinds of virtual user interfaces that we can interact with. We can also organize imaginary boxes, and of course, we can’t leave out the Two Minute Papers favorite, going into a physics simulation and playing with it with our own hands. In this previous work, hand-hand interactions did not work too well, which was addressed one more paper down the line, which absolutely nailed the solution. This followup work would look at our hands in challenging hand-hand interactions, and could deal with deformations, lots of self-contact and self-occlusion. Take a look at this footage. And, look, interestingly, they also recorded the real hand model with gloves on. We might think, what a curious design decision! What could that be for? Well, what you see here is not a pair of gloves, what you see here is the reconstruction of the hand model by this followup paper. Absolute witchcraft. Now, as you see, we can manipulate objects, or even wash our hands in virtual reality. This is all great when we play in a computer game, because the entire world around us was previously modeled, so we can look and go anywhere, anytime. But what about operating in the real world? What if we wish to look at a historic landmark from afar? Well, in this case, someone needs to capture a 360-degree photo of it. A regular photo will not cut it, because we can’t turn our head around and look behind things. And this, is what today’s paper will be about. This new paper is called Omniphotos, and it helps us produce this 360 view synthesis, and when we put on that head-mounted display, we can really get a good feel of a remote place, a group photo, or an important family festivity. So, clearly, the value proposition is excellent, but we have two questions. One, what do we have to do for it? Flailing. Yes. We need to be flailing. You see, we need to attach a consumer 360-camera to a selfie stick, and start flailing for about 10 seconds. Like this. This is a crazy idea, because now, we created a ton of raw data, roughly what you see here. So this, is a deluge of information, and the algorithm needs to crystallize all this mess into a proper 360 photograph. What is even more difficult here is that this flailing will almost never create a perfect circle trajectory, so the algorithm first has to estimate the exact camera positions and view directions. And hold on to your papers, because the entirety of this work is handcrafted, no machine learning is in sight, and the result is quite general technique, or in other words, it works on a wide variety of real-world scenes, you see a good selection of those here. Excellent. Our second question is, this is, of course, not the first method published in this area, so how does it relate to previous techniques? Is it really better? Well, let’s see for ourselves! Previous methods either suffered from not allowing too much motion, or, the ones that give us more freedom to move around, did it by introducing quite a bit of warping into the outputs. And now, let’s see if the new method improves upon that. Oh yeah, a great deal! Look, we have the advantages of both methods, we can move around freely, and additionally there is much less warping than here. Now, of course, not even this new technique is perfect, if you look behind the building, you see that the warping hasn’t been completely eliminated, but it is a big step ahead of the previous paper. While we look at some more side by side comparisons. One more bonus question: what about memory consumption? Well, it eats over a gigabyte of memory. That is typically not too much of a problem for desktop computers, but we might need a little optimization if we wish to do these computations on a mobile device. And now comes the best part. You can browse these Omniphotos online through the link in the video description, and even the source code, and a Windows-based demo is available that works with and without a VR headset. Try it out and let me know in the comments how it went! So, with that, we can create these beautiful Omniphotos cheaply and efficiently, and navigate the real world as if it were a computer game. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
491,This AI Makes Beautiful Videos From Your Images!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. In June 2020, OpenAI published an incredible  AI-based technique by the name Image-GPT. The problem here was simple to understand, but  nearly impossible to actually do: so here goes,   we give it an incomplete image, and we ask  the AI to fill in the missing pixels. That is,   of course, an immensely difficult task, because  these images may depict any part of the world   around us. It would have to know a great deal  about our world to be able to continue the images,   so how well did it do? Let’s have a look! This is undoubtedly a cat. But look! See that  white part that is just starting? The interesting   part has been sneakily cut out of the image. What  could that be? A piece of paper? Something else?   Now, let’s leave the dirty work to the machine and  ask it to finish it! Oh yeah, that makes sense. Now, even better, let’s have a look at this  water droplet example too. We humans, know that   since we see the remnants of ripples over there  too, there must be a splash, but the question   is does the AI know that? Oh yes, yes it  does! Amazing! And the true image for reference. But wait a second. If Image GPT could understand  that this is a splash, and finish the image like   this, then, here is an absolutely insane idea. If  a machine can understand that this is a splash,   could it, maybe, not only finish the  photo, but make a video out of it? Yes,   that is indeed an absolutely insane idea, we  like those around here. So, what do you think,   is this a reasonable question,  or is this still science fiction? Well, let’s have a look at what this new  learning-based method does when looking at   such an image. It would do something  very similar to what we would do,   look at the image, estimate the direction of  the motion, recognize that these ripples should   probably travel outwards, and based on the fact  that we’ve seen many splashes in our lives, if   we had the artistic skill, we could surely fill in  something similar. So, can the machine do it too? And now, hold on to your papers, because  this technique does exactly that.   Whoa! Please meet Eulerian Motion Synthesis.  And it not only works amazingly well,   but look at the output video. It even  loops perfectly. Yum yum yum, I love it! And it works mostly on fluids and smoke.  I like that! I like that a lot, because   fluids and smoke have difficult, but predictable  motion. That is an excellent combination for us,   especially given that you see plenty  of those simulations on this channel,   so if you are a long-time Fellow Scholar,  you already have a keen eye for them. Here are a few example images, paired  with the synthesized motion fields,   these define the trajectory of  each pixel, or in other words,   regions that the AI thinks should be animated  and how it thinks should be animated. Now, it gets better, I have found three things   that I did not expect to work, but was  pleasantly surprised that they did.  One, reflections, kind of work. Two, fire. Kind of works.  And now, if you have been holding on to your  papers so far, now, squeeze that paper, because   here comes the best one, three, my beard works  too. Yes, you heard it right. Now, first things   first, this is not any kind of beard, this is an  algorithmic beard that was made by an AI, and now,   it is animated as if it were a piece of  fluid using a different AI. Of course,   this is not supposed to be a correct result,  just a happy accident, but in any case, this   sounds like something straight out of a science  fiction movie. I also like how this has a nice   Obi-Wan Kenobi quality to it. Loving it. Thank you  very much to my friend Oliver Wang and the authors   for being so kind and generating these results  only for us. That is a huge honor, thank you. This previous work is from 2019 and creates  high-quality motion, but, has a limited   understanding of the scene itself. And of course,  let’s see how the new method fares in these cases.   Oh yeah, this is a huge leap forward.   And what I like even better here is that new  research techniques often provide different   tradeoffs than previous methods, but are rarely  strictly better than them. In other words,   competing techniques usually do some things better  and some things worse than their predecessors…but   not this. Look, this is so much better across  the board. That is such a rare sight. Amazing. Now, of course, not even this technique is  perfect. For example, this part of the image   should have been animated, but remains stationary.  Also, even though it did well with reflections,   refraction is a tougher nut to crack. Finally,  thin geometry also still remains a challenge. But this was one paper that made the impossible  possible, and just think about what we will   be able to do, two more papers down the  line. My goodness. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
492,9 Years of Progress In Cloth Simulation!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This day is the perfect day to simulate the kinematics of yarn and cloth on our computers. As you just saw, this is not the usual intro that we use in every episode, so, what could that be? Well, this is a simulation specifically made for us, using a technique from today’s paper. And it has a super high stitching density, which makes it all the better, by the end of this video, you will know exactly what that means and why that matters. But first, for context I would like to show you what researchers were able to do in 2012 and we will see together how far we have come since. This previous work was about creating these highly detailed cloth geometries for digital characters. Here you see one of its coolest results where it shows how the simulated forces pull the entire piece of garment together. We start out by dreaming up a piece of cloth geometry, and this simulator gradually transforms it into a real-world version of that by subjecting it to real physical forces. This is a step that we call yarn-level relaxation. So, this paper was published in 2012, and now, nearly 9 years have passed, so I wonder how far have we come some since? Well, we can still simulate knitted and woven materials through similar programs that we call direct yarn-level simulations. Here’s one. I think we can all agree that these are absolutely beautiful, so what’s the catch? The catch is that this is not free, there is a price we have to pay for these results. Look. Whoa, these really take forever. We are talking several hours, or for this one, almost even an entire day to simulate just one piece of garment. And it gets worse, look, this one takes more than two full days to compute. Imagine how long we would have to wait for complex scenes in a feature-length movie with several characters. Now, of course, this problem is very challenging, and to solve it, we have to perform a direct yarn-level simulation. This means that every single strand of yarn is treated as an elastic rod, and we have to compute how they react to external forces, bending deformations, and more. That takes a great deal of computation. So, our question today is, can we do this in a more reasonable amount of time? Well, the first Law Of Papers says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. Let’s see if the law holds up here! This new paper promises to retain many important characteristics of the full simulation, but takes much less time to compute. That is amazing. Things stretch, bend, and even curl up similarly, but the simulation time is cut down a great deal. How much less? Well, this one is five times faster, that’s great. This one, twenty times, oh my! But it gets better! Now, hold on to your papers, and look! This one is almost 60 times faster than the full yarn-level simulation. My goodness! However, of course, it’s not perfect, in return, pulling effects on individual yarns is neglected, so we lose the look of this amazing holey geometry. I’d love to get that back. Now, these examples were using materials with a relatively small stitching density. Now, the previous, full yarn-level simulation method scales with the number of yarn segments we add to the garment. So, what does all this mean? This means that higher stitching density gives us more yarn strands, and the more yarn strands there are, the longer it takes to simulate them. In these cases, you can see the knitting patterns, so there aren’t that many yarns, and even with that, it still took multiple days to compute a simulation with one piece of garment. So, I hope you know what’s coming. It can also simulate super high stitching densities efficiently. What does that mean? It means that is can also simulate materials like the satin example here. This is not bad by any means, but similar simulations can be done with much simpler simulators, so our question is, why does this matter? Well, let’s look at the backside here, and marvel at this beautiful scene showcasing the second best curl of the day. Loving it. And now I hope you are wondering what the best curl of the day is. Yes, here goes, this is the one that was showcased in our intro, which is Two Minute Papers curling up. You can only see the footage here. Beautiful. Huge congratulations and a big thank you to Georg Sperl, the first author of this paper, who simulated this Two Minute Papers scene only for us, and with a super high stitching density. That is quite an honor. Thank you so much! As I promised, we now understand exactly why the stitching density makes it all the better. And if you have been watching this series for a while, I am sure you have seen computer graphics researchers destroy armadillos in the most spectacular manner. Make sure to leave a comment if this is the case. That is super fun, and I thought there must be a version of that for cloth simulations. And of course there is. Now, please please meet the Yarnmadillo. The naming game is very strong here. And just one more thing. Georg Sperl, the first author of this work was a student of mine in 2014 at the Technical University of Vienna where he finished a practical project in light simulations programs, and, he did excellent work there. Of course he did. And I will note that I was not part of this project in any way, I am just super happy to see him come so far since then, he is now nearing the completion of his PhD, and this cloth simulation paper of his was accepted to the SIGGRAPH conference. That is as good  as it gets well done Georg! Thanks  for watching and for your generous support, and I'll see you next time!"
493,Is Simulating Wet Papers Possible?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Yes, you see it correctly, this is a paper on paper. The paper-paper if you will. And today, you will witness some amazing works in the domain of computer graphics and physics simulations. There is so much progress in this area. For instance, we can simulate honey coiling, baking and melting, bouncy jelly and many related phenomena. And none of these techniques use any machine learning, these are all good old-fashined handcrafted algorithms. And using these, we can even simulate stretching and compression, to the point that muscle movement simulations are possible. When attaching the muscles to bones, as we move the character, the muscles move and contract accurately. What’s more, this work can even perform muscle growth simulations. So, are we done here? Did these ingenious computer graphics researchers max out physics simulations, where there is nothing else to do? Oh no, of course not! Look, this footage is from an earlier graphics paper that simulates viscosity and melting fluids, and what I would like you to look at here is not what it does, but what it doesn’t do. It starts melting these Armadillos beautifully, however, what there is something that it doesn’t do, which is, mixing. The materials start separate, and remain separate. Can we improve upon that somehow? Well, this new paper promises that and so much more that it truly makes my head spin. For instance, it can simulate hyperelastic, elastoplastic, viscous, fracturing and multiphase coupling behaviors, and most importantly, all of these can be simulated within the same framework. Not one paper for each behavior, one paper that can do all of these. That is absolutely insane. What does all that mean? Well, I say, let’s see them all right now through 5 super fun experiments. Experiment number one. Wet papers. As you see, this technique handles the ball of water. Okay, we’ve seen that before. And what else? Well, it handles the paper too, okay, that’s getting better, but, hold on to your papers, and look, it also handles the water’s interaction with the paper. Now we’re talking! And careful with holding on to that paper, because if you do it correctly, this might happen. As you see, the arguments contained within this paper really hold water. Experiment number two, fracturing. As you know, most computer graphics papers on physics simulation contain creative solutions to destroying Armadillos in the most spectacular fashion. This work, is, of course, no different. Yum. Experiment number three. Dissolution. Here, we take a glass of water, add some starch powder, it starts piling up, and then, slowly starts to dissolve. And note that the water itself also becomes stickier during the process. Number four. Dipping. We first take a piece of biscuit, and dip it into the water. Note that the coupling works correctly here, in other words, the water now moves, but what is even better is that the biscuit started absorbing some of that water. And now, when we rip it apart, oh yes. Excellent! And as a light transport researcher by trade, I love watching the shape of the biscuits distorted here due to the refraction of the water. This is a beautiful demonstration of that phenomenon. And, number five. The dog! What kind of dog you ask? Well, this virtual dog gets a big splash of water, starts shaking it off, and manages to get rid of most of it. But only most of it. And it can do all of these, using one algorithm. Not one per each of these beautiful phenomena, one technique can perform all of these. That is absolutely amazing. But it does not stop there, it can also simulate snow, and it not only does it well, but it does that swiftly. How swiftly? It simulated this a bit faster than one frame per second. The starch powder experiment was about one minute per frame, and the slowest example was the dog shaking off the ball of water. The main reason for this is that it required near a quarter million particles of water and for hair, and when the algorithm computes these interactions between them, it can only advance the time in very small increments. It has to do this a hundred thousand times for each second of footage that you see here. Based on how much computation there is to do, that is really, really fast. And, don’t forget that the First Law Of Papers says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. And even now, the generality of this system is truly something to behold. Congratulations to the authors on this amazing paper. What a time to be alive! So, if you wish to read a beautifully written paper today that does not dissolve in your hands, I highly  recommend this one. Thanks for watching and for your generous support, and I'll see you next time!"
494,5 Fiber-Like Tools That Can Now Be 3D-Printed!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. I would like to show you some results from the area of 3D printing, a topic which is, I think, a little overlooked, and show you that works this field are improving at an incredible pace. Now, a common theme among research papers in this area is that they typically allow us to design objects and materials by thinking about how they should look. Let’s see if this is really true by applying the Second Law of Papers, which says whatever you are thinking about, there is already a Two Minute Papers episode on that. Let’s see if it applies here! For instance, just prescribing a shape for 3D printing is old-old news. Here is a previous technique that is able to print auxetic materials. These are materials that we can start stretching, and if we do, instead of thinning, they get fatter. We can also 3D print filigree patterns with ease. These are detailed, thin patterns typically found in jewelry, fabrics and ornaments, and as you may imagine, crafting such motifs on objects would be incredibly laborious to do by hand. We can also prescribe an image, and 3D print an object that will cast a caustic pattern that shows exactly that image. And printing textured 3D objects in a number of different ways is also possible. This is called hydrographic printing and is one of the most flamboyant ways of doing that. So what happens here? Well, we place a film in water, use a chemical activator spray on it, and shove the object in the water, and…oh yes, there we go! Note that these were all showcased in previous episodes of this series. So, in 3D printing, we typically design things by how they should look. Of course, how else would we be designing? Well, the authors of this crazy paper don’t care about looks at all. Well, what else would they care about if not the looks? Get this, they care about how these objects deform. Yes, with this work, we can design deformations, and the algorithm will find out what the orientation of the fibers should be to create a prescribed effect. Okay, but what does this really mean? This means that we can now 3D print really cool, fiber-like microstructures deform well from one direction. In other words, they can be smashed easily and flatten a great deal during that process. I bet there was a ton of fun to be had at the lab on this day. However, research is not only fun and joy, look, if we turn this object around, ouch. This side is very rigid, and resists deformations well, so there was probably a lot of injuries in the lab that day too. So, clearly, this is really cool. But of course, our question is, what is all this good for? Is this just an interesting experiment, or is this thing really useful? Well, let’s see what this paper has to offer in 5 amazing examples. Example number one. Pliers. The jaws and the hand grips are supposed to be very rigid, checkmark, however, there needs to be a joint between them to allow us to operate it. This joint needs to be deformable, and not any kind of deformable, but exactly the right kind of deformable to make sure it opens and closes properly. Loving this one. 3D printing pliers from fiber-like structures. How cool is that? Example number two. Structured plates. This shows that not all sides have to have the same properties. We can also print a material which has rigid and flexible parts on the same side, a few inches apart, thus introducing interesting directional bending characteristics. For instance, this one shows a strong collapsing behavior, and can grip our finger at the same time. Example number three. Bendy plates. We can even design structures where one side absorbs deformations, while the other one transfers it forward, bending the whole structure. Example number four. Seat-like structures. The seat surface is designed to deform a little more to create a comfy sensation, but the rest of the seat has to be rigid to not collapse and last a long time. And finally, example number five. Knee-like structures. These freely collapse in this direction to allow movement. However, they resist forces from any other direction. And these are really just some rudimentary examples of what this method can do, but the structures showcased here could be used in soft robotics, soft mechanisms, prosthetics, and even more areas. The main challenge of this work is creating an algorithm that can deal with these breaking patterns, which make for an object that is nearly impossible to manufacture. However, this method can not only eliminate these, but it can design structures that can be manufactured on low-end 3D printers and it also uses inexpensive materials to accomplish that. And hold on to your papers, because this work showcases a handcrafted technique to perform all this. Not a learning algorithm in sight. And there are two more things that I really liked in this paper. One is that these proposed structures collapse way better than this previous method. And, not only the source code of this project is available, but it is available for you to try on one of the best websites on the entirety of the internet. Shadertoy. So good! So, I hope you now agree that the field of 3D printing research is improving at an incredible pace, and I also hope that you had some fun learning about it. What a time  to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
495,Burning Down Virtual Trees... In Real Time!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to burn some virtual trees down. This is a fantastic computer graphics paper from four years ago. I ask you to hold on to your papers immediately, and do not get surprised if it spontaneously lights on fire. Yes, this work is about simulating wood combustion, and it is one of my favorite kinds of papers that takes an extremely narrow task, and absolutely nails it. Everything we can possibly ask for from such a simulation is there. Each leaf has its own individual mass and area, they burn individually, transfer heat to their surroundings, and finally, branches bend, and, look can eventually even break in this process. If we look under the hood, we see that these trees are defined as a system of connected particles embedded within a physics simulator. These particles have their own properties, for instance, you see the temperature changes here at different regions of the tree as the fire gradually consumes it. Now if you have been holding on to your papers, now, squeeze that paper and look. What do you think is this fire movement pre-programmed? It doesn’t seem like it. This seems more like some real-time mouse movement, which is great news indeed, and, yes, that means that this simulation, and all the interactions we can do with it runs in real time. Here is a list of the many quantities it can simulate, oh my goodness! There is so much yummy physics here I don’t even know where to start. Let’s pick the water content here and see how changing it would look. This is a tree with a lower water content, it catches fire rather easily, and now, let’s pour some rain on it. Then, afterwards, look, it becomes much more difficult to light on fire and emits huge plumes of dense, dense smoke. Beautiful. And, we can even play with these parameters in real time. We can also have a ton of fun by choosing non-physical parameters for the breaking coefficient, which, of course, can lead to the tree suddenly falling apart in a non-physical way. The cool thing here is that we can either set these parameters to physically plausible values and get a really realistic simulation, or, we can choose to bend reality in directions that are in line with our artistic vision. How cool is that? I could play with this all day. So, as an experienced Scholar, you ask, okay, this looks great, but how good are these simulations, really? Are they just good enough to fool the untrained eye, or are they indeed close to reality? I hope you know what’s coming. Because what is coming is my favorite part in all simulation research, and that is when we let reality be our judge and compare the simulation to that. This is a piece of real footage of a piece of burning wood, and this is the simulation. Well, we see that the resolution of the fire simulation was a little limited, it was four years ago after all, however, it runs very similarly to the real life footage. Bravo! And all this was done in 2017. What a time to be alive! But we are not even close to be done yet, this paper teaches us one more important lesson. After publishing such an incredible work, it was accepted to SIGGRAPH ASIA 2017. That is one of the most prestigious conferences in this research field, getting a paper accepted here is equivalent to winning the olympic gold medal of computer graphics research. So with that, we would expect that the authors now revel in eternal glory. Right? Well, let’s see. What? Is this serious? The original video was seen by less than a thousand people online. How can that be? And the paper was referred to only ten times by other works in these four years. Now, you see, that is not so bad in computer graphics at all, it is an order, maybe even orders of magnitude smaller field than machine learning, but I think this is an excellent demonstration of why I started this series. And it is because I get so excited by these incredible human achievements, and I feel that they deserve a little more love than they are given, and of course, these are so amazing, everybody has to know about them. Happy to have you Fellow Scholars watching this and celebrating these papers with me for more than 500 episodes now. Thank you so much, it is a true honor to have such an amazing and receptive audience. Thanks for watching and for your generous support, and I'll see you next time!"
496,AI Makes Near-Perfect DeepFakes in 40 Seconds!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Imagine that you are a film critic and you are recording a video review of a movie, but unfortunately you are not the best kind of movie critic, and you record it before watching the movie. But here is the problem you don’t really know if it’s going to be any good. So you record this. So far so good. Nothing too crazy going on here. However, you go in, watch the movie, and it turns out to be amazing. So, what do we do if we don’t have time to re-record the video? Well, we grab this AI, type in the new text, and it will give us this. Whoa! What just happened? What kind of black magic is this? Well, let’s look behind the person, on the blackboard you see some delicious partial derivatives, and I am starting to think that this person is not a movie critic. And of course, of course he isn’t, because this is Yoshua Bengio, legendary machine learning researcher. And this was an introduction video where he says this. And, what happened is that it has been repurposed by this new DeepFake generator AI, where we can type in anything we wish, and out comes a near-perfect result. It synthesizes both the video and audio content for us. But we are not quite done yet. Something is missing. If the movie gets an A+, the gestures of the subject also have reflect that this is a favorable review. So, what do we do? Maybe add a smile there. Is that possible? Oh yes, there we go! Amazing. Let’s have a closer look at one more example where we see how easily we can drop in new text with this editor. Now, this is not the first method performing this task previous techniques typically required hours and hours of video from a target subject. So how much training data does this require to perform all this? Well, let’s have a look together! Look, this is not the same footage copy-pasted three times, this is a synthesized video output if we have 10 minutes of video data from the test subject. This looks nearly as good, has fewer sharp details, but, in return, this only requires 2.5 minutes. And here comes the best part. If you look here, you may be able to see the difference. And if you have been holding on to your papers so far, now, squeeze that paper because synthesizing this only required 30 seconds of video footage of the target subject. My goodness. But we are not nearly done yet, it can do more! For instance, it can tone up or down the intensity of gestures to match the tone of what is being said. Look. So how does this wizardry happen? Well, this new technique improves two things really well, one is that it can search for phonemes and other units better. Here is an example, we crossed out the word “spider” and wish to use the word “fox” instead, and it tries to assemble this word from previous occurrences of individual sounds. For instance, the “ox” part is available when the test subject utters the word “box”. And two, it can then stitch them together better than previous methods. And surely this means that since it needs less data, the synthesis must take a great deal longer, right? No, not at all, the synthesis part only takes 40 seconds. And even if it couldn’t this so quickly, the performance control aspect where we can tone the gestures up or down, or add a smile would still be an amazing selling point in and of itself. But no, it does all of these things quickly, and with high quality at the same time. Wow. I now invite you to look at the results carefully, and give them a hard time. Did you find anything out of ordinary? Did you find this believable? Let me know in the comments below. The authors of the paper also conducted a user study with a 110 participants, who were asked to look at 25 videos and say which one they felt was real. The results showed that the new method outperforms previous techniques, even if they have access to 12 times more training data. Which is absolutely absolutely amazing, but what is even better, the longer the video clips were, the better this method fared. What a time to be alive! Now, of course, beyond the many amazing use-cases of DeepFakes in reviving deceased actors, creating beautiful visual art, redubbing movies, and more, we have to be vigilant about the fact that they can also be used for nefarious purposes. The goal of this video is to let you, and the public know that these DeepFakes can now be created quickly and inexpensively, and they don’t require a trained scientist anymore. If this can be done, it is of utmost importance that we all know about it! And beyond that, whenever they invite me, I inform key political and military decision makers about the existence and details of these techniques to make sure that they also know about these and using that knowledge, they can make better decisions for us. You can see me doing that here. Note that these talks and consultations all happen free of charge, and if they keep inviting me, I’ll keep showing up to help with this in the future as a service  to the public. Thanks for watching and for your generous support, and I'll see you next time!"
497,This AI Made Me Look Like Obi-Wan Kenobi!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today’s paper is about creating synthetic human faces, and not only that, but it can also make me look like Obi-Wan Kenobi. You will see the rest of this footage in a few minutes. Now, of course, this is not the first paper to generate artificial human faces. For instance, in December of 2019 a technique by the name StyleGAN2 was published. This is a neural network-based learning algorithm that is capable of synthesizing these eye-poppingly detailed images of human beings that don’t even exist. This work answered some questions, and as any good paper, raised many more good ones. For instance, generating images of virtual humans is fine, but what if the results are not exactly what we are looking for? Can we have some artistic control over the outputs? How do we even tell the AI what we are looking for? Well, we are in luck because StyleGAN2 offers somewhat rudimentary control over the outputs where we can give it input images of two people, and fuse them together. Now, that is absolutely amazing, but I wonder if we can ask for a little more? Can we get even more granular control over these images? What if we could just type in what we are looking for, and somehow the AI would understand and execute our wishes? Is that possible, or, is that science fiction? Well, hold on to your papers, and let’s see. This new technique works as follows. We type what aspect of the input image we wish to change, and what the change should be. Wow, really cool! And we can even play with these sliders to adjust the magnitude of the changes as well. This means that we can give someone a new hairstyle, add or remove makeup, or give them some wrinkles for good measure. Now, the original StyleGAN2 method worked on not only humans, but on a multitude of different classes too. And the new technique also inherits this property, look. We can even design new car shapes, make them a little sportier, or make our adorable cat even cuter. For some definition of cuter, of course. We can even make their hair longer  or change their colors, and the results are of super high quality. Absolutely stunning. While we are enjoying some more results here, make sure to have a look at the paper in the video description, and if you do, you will find that we really just scratched the surface here, for instance, it can even add clouds to the background of an image, or redesign the architecture of buildings, and much, much more. There are also comparisons against previous methods in there, showcasing the improvements of the new method. And now, let’s experiment a little on me. Look, this is me here, after I got locked up for dropping my papers. And I spent so long in there, that I grew a beard. Or I mean, a previous learning-based AI called StyleFlow gave me one. And since the dropping your papers is a serious crime, the sentence is long…quite long. Ouch. I hereby promise to never drop my papers, ever again. So now, let’s try to move forward with this image, and give it to this new algorithm for some additional work. This is the original. And by original I mean image with the added algorithmic beard from a previous AI. And this is the embedded version of the image. This image looks a little different. Why is that? It is because StyleGAN2 runs an embedding operation on the photo before starting its work. This is its own internal understanding of my image if you will. This is great information, and is something that we can only experience if we have hands-on experience with the algorithm. And now, let’s use this new technique to apply some more magic to this image. This is where the goodness happens. And, oh my, it does not disappoint! You see, it can gradually transform me into Obi-Wan Kenobi. An elegant algorithm, for a more civilized age. But that’s not all. It can also create a ginger Károly, hippie Károly, Károly who found a shiny new paper on fluid simulations, and Károly who read said paper outside for quite a while, and was perhaps disappointed. And now, hold on to your papers and please welcome Dr. Karolina Zsolnai-Fehér. And one more, I apologize in advance…rockstar Károly, with a mohawk. How cool is that? I would like to send a huge thank you to the authors for taking time out of their workday to create these images only for us, you can really only see these here on Two Minute Papers. And as you see, the pace of progress in machine learning research is absolutely stunning, and with this, the limit of our artistic workflow is going to be not our mechanical skills, but only our imagination. What  a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
498,Can An AI Perform A Cartwheel?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we will see how this AI-based technique can help our virtual characters not only learn new movements, but they can even perform them with style. Now, here you see a piece of reference motion this is what we would like our virtual character to learn. The task is to then, enter a physics simulation where try to find the correct joint angles and movements to perform that. Of course, this is already a challenge because even a small difference in joint positions can make a great deal of difference in the output. Then, the second, more difficult task is to do this with style. No two people perform cartwheels exactly the same way, so would it be possible to have our virtual characters imbued with style, so that they, much like people, would have their own kinds of movement? Is that possible somehow? Well, let’s have a look at the simulated characters. Nice, so this chap surely learned to at the very least reproduce the reference motion, but let’s stop the footage here and there and look for differences. Oh yes, this is indeed different. This virtual character indeed has its own style, but at the same time, it is still faithful to the original reference motion. This is a magnificent solution to a very difficult task. And the authors made it look deceptively easy, but you will see in a moment that this is really challenging. So how does all this magic happen? How do we imbue these virtual characters with style? Well, let’s define style as a creative deviation from the reference motion, so it can be different, but not too different, or else, this happens. So, what are we seeing here? Here, with green, you see the algorithm’s estimation of the center of mass for this character. And our goal would be to reproduce that as faithfully as possible. That would be the copying machine solution. But, here comes the key for style. And that key is using spacetime bounds. This means that the center of mass of the character can deviate from the original, but only as long as it remains strictly within these boundaries. And that is where the style emerges! If we wish to add a little style to the equation, we can set relatively loose spacetime bounds around it, leaving room for the AI to explore. If we wish to strictly reproduce the reference motion, we can set the bounds to be really tight, instead. This is a great technique to learn running, jumping, rolling behaviors, it can even perform a stylish cartwheel and backflips. Oh yeah. Loving it. These spacetime bounds also help us retarget the motion to different virtual body types. And furthermore, it also helps us salvage really bad quality reference motions and make something useful out of them. So, are we done here? Is that all? No, not in the slightest! Now, hold on to your papers because here comes the best part. With these novel spacetime bounds, we can specify additional stylistic choices to the character moves. For instance, we can encourage the character to use more energy for a more intense dancing sequence, or, we can make it sleepier by asking it to decrease its energy use. And I wonder if we can put bounds on the energy use, can we do more, for instance, do the same with, for instance, body volume use. Oh yeah! This really opens up new kinds of motions that I haven’t seen virtual characters perform yet. For instance, this chap was encouraged to use its entire body volume for a walk, and thus, looks like someone who is clearly looking for trouble. And this poor thing just finished their paper for a conference deadline and is barely alive. We can even mix multiple motions together. For instance, what could be a combination of a regular running sequence, and a bent walk? Well, this. And if we have a standard running sequence, and a happy walk, we can fuse them into a happy running sequence. How cool is that? So, with this technique, we can finally not only teach virtual characters to perform nearly any kind of reference motion, but we can even ask them to do these with style. What an incredible idea. Loving it! Now, before we go. I would like to show you a short message that we got that melted my heart. This I got from Nathan, who has been inspired by these incredible works and he decided to turn his life around, and go back to study more. I love my job, and reading messages like this is one of the absolute best parts of it. Congratulations Nathan, thank you so much, and good luck! If you feel that you have a similar story with this video series, make sure to let us know in  the comments! Thanks for watching and for your generous support, and I'll see you next time!"
499,AI “Artist” Creates Near-Perfect Toonifications!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to generate human faces, and even better, we will keep them intact. You will see what that means in a moment. This new neural network-based technique can dream up completely new images, and more. However, this is not the first technique to do this, but this…this does them better. Let’s look at 3 amazing features that it offers, and then discuss, how, and why it is better than its predecessors. Hold on to your papers for the first example, which is my favorite, image toonification. Would you like to see what the AI thinks you would look like if you were a Disney character? Well, here you go. And these are not some rudimentary, first-paper-in-the-works kind of results. These are proper toonifications, you could ask for money for some of these, and they are done completely automatically by a learning algorithm. Woah. At the end of the video, you will also witness as I myself get toonified. And what is even cooler is that we can not only produce these still images, but even compute intermediate images between two input photos, and get meaningful results. I’ll stop the process here and there to show you how good these are. I am blown away. Two, it can also perform the usual suspects. For instance, it can make us older or younger, or put a smile on our face too. However, three it works not only on human faces, but cars, animals, and buildings too. So the results are all great, but how does all this wizardry happen? Well, we take an image, embed it into a latent space, and in this space, we can easily apply modifications. Okay…but what is this latent space thing? A latent space is a made-up place where we are trying to organize data in a way that similar things are close to each other. What you see here is a 2D latent space for generating different fonts. It is hard to explain why these fonts are similar, but most of us would agree that they indeed share some common properties. The cool thing here is that we can explore this latent space with our cursor, and generate all kinds of new fonts. You can try this work in your browser, the link is available in the video description. And, luckily, we can build a latent space, not only for fonts, but, for nearly anything. I am a light transport researcher by trade, so in this earlier paper, we were interested in generating hundreds of variants of a material model to populate this scene. In this latent space, we can concoct all of these really cool digital material models. A link to this work is also available in the video description. Now, for the face generation algorithms, this embedding step is typically imperfect, which means that we might lose some information during the process. In the better cases, things may look a little different, and that’s not even the worst case scenario. I’ll show you that in a moment. For the milder case, here is an earlier example from a paper by the name StyleFlow, where the authors embedded me into a latent space and it indeed came out a little different. But not so bad. A later work, StyleClip, was able to make me look like Obi-Wan Kenobi, which is excellent. However, the embedding step was more imperfect. The bearded image was embedded like this. You are probably saying that this looks different, but even this is not so bad. If you want to see a much worse example, look at this. My goodness. Now this is quite different. Now that we saw what it could do, it is time to ask the big question. How much better is it than previous works? Do we have an A-B test for that? And the answer is yes, of course! Let’s embed this gentleman and see how he comes out on the other end. Well, without the improvements of this paper, once again, quite different. The beard is nearly gone. And when we toonify the image, let’s see…yup, that beard is gone for good. So, can this paper get that beard back? Let’s see…oh yes! If we refine the embedding with the new method, we get that glorious beard back. That is one heck of a toon image, congratulations! Loving it. And now, it’s my turn. One of the results was amazing, I really like this one. How about this one? Well, not bad. And I wonder if it can deal with sunglasses? Well, kind of, but not in the way you might think. What do you think? Let me know in the comments below! Note that you can only see these results here on Two Minute Papers, and a big thank you to the authors for taking time off their busy day and doing these experiments for us. And here are a few more tests, let’s see how it fares with these. The inputs are a diverse set of images from different classes, and the initial embeddings are, well, a bit of a disappointment. But that is kind of the point, because this new technique does not let it stop there and iteratively improves them, yes, getting better, and by the end, my goodness. Very close to the input. Don’t forget that the goal here is not to implement a copying machine, the key difference is that we can’t do too much with the input image, but after the embedding step, we can do all this toonification and other kinds magic with it, and the results are only relevant as long as the two images are close. And they are really close. Bravo! So good! So, I hope that now you agree that the pace of progress in machine learning and synthetic image generation is absolutely incredible. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
500,Beautiful Glitter Simulation…Faster Than Real Time!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This incredible paper promises procedural and physically-based rendering of glittering surfaces. Whoa. Okay, I am sold… provided that it does as well as advertised. We shall see about that! Oh, goodness, the results look lovely. And now, while we marvel at these, let’s discuss what these terms really mean. One, physically-based means that it is built on a foundation that is based in physics. That is not surprising, as the name says so. The surprise usually comes when people use the term without careful consideration. You see, light transport researchers take this term very seriously, if you claim that your model is physically-based, you better bring your A-game. We will inspect that. Two, the procedural part means that we ourselves can algorithmically generate many of these materials models ourselves. For instance, this earlier paper was able procedurally generate the geometry of climbing plants and simulate their growth. Or, procedural generation can come to the rescue when we are creating a virtual environment and we need a hundreds of different flowers, thousands of blades of grass, and more. This is an actual example of procedural geometry that was created with the wonderful Terragen program. Three, rendering means a computer program that we run on our machine at home and it creates a beautiful series of images, like these ones. In the case of photorealistic rendering, we typically need to wait from minutes to hours for every single image. So, how fast is this technique? Well, hold on to your papers, because we don’t need to wait from minutes to hours for every image. Instead, we need only 2.5 milliseconds per frame. Absolute witchcraft. The fact that we can do this in real time blows my mind, and, yes! This means that we can test the procedural part in an interactive demo too! Here we can play with a set of parameters, look, the roughness of the surface is not set in stone, we can specify it and see as the material changes in real time. We can also change the density of microfacets, these are tiny imperfections in the surface that really make these materials come alive. And if we make the microfacet density much larger, the material becomes more diffuse. And if we make them really small…ohoho! Loving this. So, as much as I love this, I would also love to know how accurate this is. For reference, here is a result from a previous technique that is really accurate, however, this is one of those methods that takes from minutes to hours for just one image. And here is the other end of the spectrum, this is a different technique that is lower in quality, however, in return, it can produce these in real time. So which one is the new one closer to? The accurate, slow one, or less accurate, quick one What?! Its quality is as good as the accurate one, and it also runs in real time. The best of both worlds. Wow! Now, of course, not even this technique is perfect, the fact that this particular example worked so well is great, but it doesn’t always come out so well. And, I know, I know, you’re asking, can we try this new method. And the answer is a resounding yes, you can try it right now in multiple places! There is a web-demo, and it was even implemented in Shadertoy. So, now we know what it means to render procedural and physically-based rendering of glittering surfaces in real time. Absolutely incredible. What a time to be alive! And if you enjoyed this episode, we may have two more incredible glint rendering papers coming up in the near future. This, and this. Let me know in the comments below if you would like to see them. Write something like, “yes please!”. Thanks for watching and for your generous support, and I'll see you next time!"
501,Meet Your Virtual AI Stuntman!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to look at a paper from three years ago, and not any kind of paper, but my kind of paper, which is in the intersection of machine learning, computer graphics, and physics simulations. This work zooms in on reproducing reference motions, but, with a twist, and adds lots of amazing additional features. So what does all this mean? You see, we are given this virtual character, a reference motion that we wish to teach it, and here, additionally, we are given a task that needs to be done. So, when the reference motion is specified, we place our AI into a physics simulation where it tries to reproduce these motions. That is a good thing because if it would try to learn to run by itself alone, it would look something like this. And if we ask it to mimic the reference motion, oh yes…much better. Now that we have built up confidence in this technique, let’s think bigger, and perform a backflip. Uh-oh. Well, that didn’t quite work. Why is that? We just established that we can give it a reference motion and it can learn it by itself. Well, this chap failed to learn a backflip because it explored many motions during training, most of which resulted in failure. So it didn’t find a good solution and settled for a mediocre solution instead. A proposed technique by the name Reference State Initialization, RSI in short remedies this issue by letting the agent explore better during the training phase. Got it, so we add this RSI, and now, all is well, right? Let’s see. Ouch. Not so much! It appears to fall on the ground and tries to continue the motion from there. A+ for effort, little AI, but unfortunately that’s not what we are looking for. So what is the issue here? The issue is that the agent has hit the ground, and after that, it still tries to score some additional points by continuing to mimic the reference motion. Again, A+ for effort, but this should not give the agent additional scores. This method we just described is called early termination. Let’s try it! Now, we add the early termination and RSI together, and let’s see if this will do the trick! …And…yes! Finally, with these two additions, it can now perform that sweet sweet backflip, rolls, and much, much more with flying colors. So now, the agent has the basics down, and can even perform explosive, dynamic motions as well. So, it is time. Now hold on to your papers as now comes the coolest part we can perform different kinds of retargeting as well. What is that? Well, one kind is retargeting the environment. This means that we can teach the AI a landing motion in an idealized case, and then, ask it to perform the same, but now, off of a tall ledge. Or, we can teach it to run, and then drop it into computer game level and see if it performs well. And it really does. Amazing! This part is very important because in any reasonable industry use, these characters have to perform in a variety of environments that are different from the training environment. Two is retargeting not the environment, but the body type. We can have different types of characters learn the same motions. This is pretty nice for the Atlas robot, which has a drastically different weight distribution, and you can also see that the technique is robust against perturbations. Yes, this means one of the favorite pastimes of a computer graphics researcher, which is throwing boxes at virtual characters and seeing how well it can take it. Might as well make sure of the fact that in a simulated world, we make up all the rules! This one is doing really well, … oh. Note that the Atlas robot is indeed different than the previous model, and these motions can be retargeted to it, however, this is also a humanoid. Can we ask for non-humanoids as well perhaps? Oh yes! This technique supports retargeting to T-Rexes, dragons, lions, you name it. It can even get used to the gravity of different virtual planets that we dream up. Bravo! So the value proposition of this paper is just completely out of this world. Reference State Initialization, Early Termination, retargeting to different body types, environments, oh my! To have digital applications, like computer games use this would already be amazing, and just imagine what we could do if we could deploy these to real-world robots. And don’t forget, these research works just keep on improving every year. The First Law Of Papers says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. Now, fortunately, we can do that right now! Why is that? It is because this paper is from 2018, which means that followup papers already exist. What’s more, we even discussed one that teaches these agents to not only reproduce these reference motions, but to do those with style. And style there meant that the agent is allowed to make creative deviations from the reference motion, thus, developing its own way of doing it. An amazing improvement. And I wonder what researchers will come up with in the near future? If you have some ideas, let me know in the comments below. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
502,Beautiful Fluid Simulations...In Just 40 Seconds!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Through the power of computer graphics research works, today, it is possible to simulate honey coiling, water flow with debris, or even get a neural network to look at these simulations and learn how to continue them! Now, if we look under the hood, we see that not all, but many of these simulations contain particles. And our task is to simulate the pressure, velocity, and other physical quantities for these particles and create a surface where we can watch the evolution of their movement. Once again, the simulations are typically based on particles. But not this new technique. Look. It takes a coarse simulation, well, this one is not too exciting. So why are we looking at this? Well, look! Whoa! The new method can add these crispy, high-frequency details to it. And the result is an absolutely beautiful simulation. And it does not use millions and millions of particles to get this done. In fact, it does not use any particles at all! Instead, it uses wave curves. These are curve-shaped wave-packets that can enrich a coarse wave simulation and improve it a great deal to create a really detailed, crisp output. And it gets even better, because these wave curves can be applied as a simple post processing step. What this means is that the workflow what you saw here really works like that. When we have a coarse simulation that is already done, and we are not happy with it, with many other existing methods, it is time to simulate the whole thing again from scratch, but not here. With this one, we can just add all this detail to an already existing simulation. Wow. Loving it. Note that the surface of the fluid is made opaque so that we can get a better view of the waves. Of course, the final simulations that we get for production use are transparent, like the one you see here. Now, another interesting detail is that that the execution time is linear with respect to the curve points. So what does that mean? Well, let’s have a look together. In the first scenario, we get a low-quality underlying simulation, and we add a 100 thousand wave curves. This takes approximately 10 seconds and looks like this. This already greatly enhanced the quality of the results, but we can decide to add more. So, first case, a 100k wave curves, in 10-ish seconds. Now comes the linear part if we decide that we are yearning for a little more, we can run 200k wave curves, and the execution time will be 20-ish seconds. It looks like this. Better, we’re getting there! And for a 400k wave curves, 40-ish seconds, and for 800k curves, yes, you guessed it right, 80-ish seconds. Double the number of curves, double the execution time. This is what the linear scaling part means. Now, of course, not even this technique is perfect. The post-processing nature of the method means that it can enrich the underlying simulation a great deal, but, it cannot add changes that are too intrusive to it. It can only add small waves relative to the size of the fluid domain. But even with these, the value proposition of this paper is just out of this world. So, from now on, if we have a relatively poor quality fluid simulation that we abandoned years ago, we don’t need to despair. What we need is to harness the power of wave curves. Thanks for watching and for your generous support, and I'll see you next time!"
503,Can We Teach Physics To A DeepMind's AI?,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. If you have been watching this series for  a while, you know very well that I love   learning algorithms and fluid simulations.  But do you know what I like even better?   Learning algorithms applied to fluid simulations,  so I couldn’t be happier with today’s paper. We can create wondrous fluid simulations like  the ones you see here by studying the laws of   fluid motion from physics, and writing a  computer program that contains these laws.   However, I just mentioned learning algorithms.   How do these even come to the picture? If we  can write a program that simulates the laws,   why would we need learning-based algorithms?  This doesn’t seem to make any sense. You see, in this task, the neural networks  can be also applied to solve something that   we already know how to solve. However, if we  use a neural network to perform this task,   we have to train it, which is a long and  arduous process. I hope to have convinced you   that this is a bad, bad idea. Why would anyone  bother to do that? Does this make any sense? Well, it does make a lot of sense! And the  reason for that is that this training step   only has to be done once, and afterwards,  querying the neural network, that is, predicting   what happens next in the simulation runs almost  immediately. This takes way less time than   calculating all the forces and pressures in the  simulation while retaining high quality results. This earlier work from last year  absolutely nailed this problem. Look,   this is a scene with the  boxes it has been trained on.   And now, let’s ask it to try to simulate the  evolution of significantly different shapes.   Wow. It not only does well with  these previously unseen shapes,   but it also handles their  interactions really well. But there was more! We could also train it  on a tiny domain with only a few particles,   and then, it was able to learn general  concepts that we can reuse to simulate   a much bigger domain, and also,  with more particles. Fantastic! This was a simple, general model that  truly is a force to be reckoned with.   Now, this is a great leap in neural network-based  physics simulations, but of course, not everything   was perfect there. For instance, over longer  timeframes, solids became incorrectly deformed. And now, a newer iteration of a similar system  just came out from DeepMind’s research lab that   promises to extend these neural networks  for an incredible set of use cases:   aerodynamics, structural mechanics,   cloth simulations and more. I am very  excited to see how far they have come since! So let’s see how well it does, first, with  rollouts, then, with generalization experiments.   Here is the first rollout experiment, so what  does that mean, and what are we seeing here?   On the left, you see a verified handcrafted  algorithm performing the simulation,   we will accept this as the true data, and  on the right, the AI is trying to continue   the initial simulation. But there is one problem.  And that problem is that the AI was only trained   on short simulations with 400 time steps,  that’s only a few seconds! And unfortunately,   this test will be a hundred times longer. So, it  only learned on short simulations, can it manage   to run a longer one and remain stable? Well, that  will be tough…but, so far so good…still running.   My goodness, this is really something. Still  running and it’s very close to the ground truth! Okay, that is fantastic, but that was just a  piece of cloth. What about interaction with other   objects? Well, let’s see. I’ll stop the process  here and there so we can inspect the differences.   Again, flying colors. Loving it. And, apparently, the same can be said for  simulations in structural mechanics, and   incompressible fluid dynamics. Now, that is one  more important lesson here to be able to solve   such a wide variety of simulation problems, we  need a bunch of different hancrafted algorithms   that took many-many years to develop. But this  one neural network can learn and perform them all,   and it can do it 10 to a 100 times quicker. And now comes the second half, generalization  experiments. This means a simulation scenario   with shapes that the algorithm has never seen  before. And let’s see if it obtained general   knowledge of the underlying laws of physics  to be able to pull this off. Oh my! Look at   that! Even the tiny piece that is hanging off  of the flag is simulated nearly perfectly. In this one, they gave it different wind speeds  and directions that it hadn’t seen before, and not   only that, but we are varying these parameters  in time, and it doesn’t even break a sweat. And hold on to your papers, because  here comes my favorite it can even   learn on a small-scale simulation with  a simple rectangular flag, and now,   we throw at it a much more detailed, cylindrical  flag with tassels. Surely this will be way beyond   what any learning algorithm can do today.  And…okay, come on…I am truly out of words. Look. So now, this is official. We can ask an AI to  perform something that we already know how to do,   and it will not only be able to reproduce  similar simulations, but we can even ask things   that were previously quite unreasonably outside  of what it had seen, and it handles all these   with flying colors. And it does this much better  than previous techniques were able to. And it can   learn from multiple different algorithms at  the same time. Wow. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
504,Intel's Video Game Looks Like Reality!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This paper is called Enhancing photorealism enhancement. Hmm! Let’s try to unpack what that exactly means. This means that we take video footage from a game, for instance, GTA 5, which is an action game where the city we can play in was modeled after real places in California. Now, as we are living the advent of neural network-based learning algorithms, we have a ton of training data at our disposal on the internet. For instance, the cityscapes dataset contains images and videos taken in 50 real cities, and it also contains annotations that describe which object is which. And the authors of this paper looked at this, and had an absolutely insane idea. And the idea is let’s learn on the cityscapes dataset what cars, cities and architecture looks like, then take a piece of video footage from the game, and translate it into a real movie. So basically something that is impossible. That is an insane idea, and when I read this paper, I thought that cannot possibly work in any case, but especially not given that the game takes place in California, and the Cityscapes dataset contains mostly footage of German cities. How would a learning algorithm pull that off? There is no way this will work. Now, there are previous techniques that attempted this, here you see a few of them. And…well, the realism is just not there, and there was an even bigger issue. And that is the lack of temporal coherence. This is the flickering that you see where the AI processes these images independently and does not do that consistently. This quickly breaks the immersion and is typically a deal-breaker. And now, hold on to your papers…and let’s have a look together at the new technique. Whoa! This is nothing like the previous ones! It renders the exact same place, the exact same cars, and the badges are still correct and still refer to real-world brands. And that’s not even the best part, look! The carpaint materials are significantly more realistic, something that is really difficult to capture in a real-time rendering engine. Lots of realistic looking specular highlights off of something that feels like the real geometry of the car. Wow. Now, as you see, most of the generated photorealistic images are dimmer, and less saturated than the video game graphics. Why is that? This is because computer game engines often create a more stylized world where the saturation, haze, and bloom effects are often more pronounced. Let’s try to fight this bias where many people consider the more saturated images to be better, and focus our attention to the realism in these image pairs. While we are there, for reference, we can have a look at what the output would be if we didn’t do any of the photorealistic magic, but instead, we just tried to breathe more life into the video game footage by trying to transfer the color schemes from these real-world videos in the training set. So, only color transfer. Let’s see. Yes, that helps…until we compare the results with the photorealistic images synthesized by this new AI. Look. The trees don’t look nearly as realistic as the new method, and after we see the real roads, it’s hard to settle for the synthetic ones from the game. However, no one said that Cityscapes is the only dataset we can use for this method. In fact, if we still find ourselves yearning for that saturated look, we can try to plug in a more stylized dataset, and get…this! This is fantastic, because these images don’t have many of the limitations of computer graphics rendering systems. Why is that? Because, look at the grass here. In the game, it looks like a 2D texture to save resources and be able to render an image quicker. However, the new system can put more real-looking grass in there, which is a fully 3D object where every single blade of grass is considered. The most mind-blowing thing here is that this AI finally has enough generalization capabilities to learn about cities in Germany, and still be able to make convincing photorealistic images for California. The algorithm never saw California, and yet, it can recreate it from video game footage better than I ever imagined would be possible. That is mind blowing. Unreal. And if you have been holding on to your papers so far, now, squeeze that paper. Because here, we have one of those rare cases where we squeeze our papers for not a feature, but for a limitation…of sorts. You see, there are limits to this technique too. For instance since the AI was trained on the beautiful lush hills of Germany and Austria, it hasn’t really seen the dry hills of LA. So, what does it do with them? Look, it redrew the hills the only way it saw hills exist, which is, with trees. Now, we can think of this as a limitation, but also as an opportunity. Just imagine the amazing artistic effects we could achieve by playing this trick to our advantage. Also, we won’t need to create an 80% photorealistic game like this one and push it up to a 100% with the AI. We could draw not 80%, but the bare minimum, maybe only 20% for the video game, a coarse draft, if you will, and let the AI do the heavy lifting! Imagine how much modeling time we could save for artists as well. I love this. What a time to be alive! Now, all of this only makes sense for real-world use if it can run quickly. So can it? How long do we have to wait to get such a photorealistic video? Do we wait from minutes to hours? No! The whole thing runs interactively, which means that it is already usable, we can plug this into the game as a post-processing step. And remember the First Law Of Papers, which says that two more papers down the line, and it will be even better. What improvements do you expect to happen soon? And what would you use this for? Let me know in the comments below! Thanks for watching and for your generous support, and I'll see you next time!"
505,Can An AI Heal This Image?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to play with a cellular automaton and fuse it together with a neural network. It will be quite an experience! But of course, first, what are these things anyway? You can imagine a cellular automaton as a small game where we have a bunch of cells, and a set of simple rules that describe when a cell should be full, and when it should be empty. What you see here is a popular example called Game of Life, which simulates a tiny world where each cell represents a little life form. So why is this so interesting? Well, this cellular automaton shows us that even a small set of simple rules can give rise to remarkably complex life forms, such as gliders, spaceships, and even John von Neumann’s universal constructor, or in other words, self-replicating machines. Now, this gets more interesting, a later paper fused this cellular automaton with a neural network. It was tasked to grow, and even better, maintain a prescribed shape. Remember these two words, grow and maintain shape. And the question was, if it can recover from undesirable states, can it perhaps..regenerate when damaged? Well, here, you will see all kinds of damage…and then, this happens. Nice! The best part is that this thing wasn’t even trained to be able to perform this kind of regeneration! The objective for training was that it should be able to perform its task of growing and maintaining shape, and it turns out, some sort of regeneration is included in that. This sounds very promising, and I wonder if we can we apply this concept to something where healing is instrumental? Are there such applications in computer science? If so, what could those be? Oh yes, yes there are! For instance, think about texture synthesis. This is a topic that is subject to a great deal of research in computer graphics, and those folks have this down to a science. So, what are we doing here? Texture synthesis typically means that we need lots of concrete or gravel road, skin, marble, create unique stripes for zebras, for instance, for a computer game or the post-production of a movie, and we really don’t want to draw miles and miles of these textures by hand. Instead, we give it to an algorithm to continue this small sample, where the output should be a bigger version of this pattern, with the same characteristics. So how do we know if we have a good technique at hand? Well, first, it must not be repetitive, checkmark, and it has to be free of seams. This part means that we should not be able to see any lines or artifacts that would quickly give the trick away. Now, get this, this new paper attempts to do the same with neural cellular automatons. What an insane idea! We like those around here, so let’s give it a try! How? Well, first, by trying to expand this simple checkerboard pattern. The algorithm is starting out from random noise, and as it evolves…well, this is a disaster. We are looking for squares, but we have quadrilaterals. They are also misaligned. And they are also inconsistent. But, luckily, we are not done yet. And now, hold on to your papers, and observe how the grid cells communicate with each other to improve the result. First, the misalignment is taken care of, then, the quadrilaterals become squares, and then, the consistency of their placement is improved. And the end result is, look! In this other example, we can not only see these beautiful bubbles grow out of nowhere. But the density of the bubbles remains roughly the same over the process. Look! As two of them get too close to each other, they coalesce, or pop. Damaged bubbles can also regrow. Very cool! Okay, it can do proper texture synthesis, but so can a ton of other handcrafted computer graphics algorithms, so why is this interesting? Why bother with this? Well, first, you may think that the result of this technique is the same as other techniques, but it isn’t. The output is not necessarily just an image, but can be an animation too! Excellent. Here, it was also able to animate the checkerboard pattern, and, even better, it can not only reproduce the weave pattern, but the animation part extends to this too. And now comes the even more interesting part. Let’s ask why does it output an animation and not an image? The answer lies within these weaving patterns. We just need to carefully observe them. Let’s see. Yes, again, we start out from noise, where some woven patterns emerge, but then, it almost looks like a person who started weaving them until it resembles the initial sample. Yes, that is the key! The neural network learned to create not an image, not an animation, but no less than a computer program to accomplish this kind of texture synthesis! How cool is that? So, armed with all that knowledge, do you remember the regenerating iguana project? Let’s try to destroy these textures too and see if it can use these computer programs to recover and get us a seamless texture. First, we delete parts of the texture, then, it fills in the gap with noise, and now, let’s run that program! Wow! Resilient, self-healing texture synthesis. How cool is that? And in every case, it starts out from a solution that is completely wrong, improves it to be just kind of wrong, and after further improvement, there you go. Fantastic! What a time to be alive! And, note that this is a paper in the wonderful Distill journal, which not only means that it is excellent, but also interactive, so you can run many of these experiments yourself right in your web browser. The link is available  in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
506,This is Grammar For Robots. What? Why?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to generate robots with grammars. Wait a second. Grammars? Of all things, what do grammars have to do with robots? Do we need to teach them grammar to speak correctly? No no, of course not! To answer these questions, let’s invoke the Second Law Of Papers, which says that whatever you are thinking about, there is already Two Minute Papers episode on that. Even on grammars. Let’s see if it applies here! In this earlier work, we talked about generating buildings with grammars. So how does that work? Grammars are a set of rules that tell us how to build up a structure, such as a sentence properly from small elements, like nouns, adjectives and so on. My friend, Martin Ilcik loves to build buildings from grammars. For instance, a shape grammar for buildings can describe rules like a wall can contain several windows, below a window goes a window sill, one wall may have at most two doors attached, and so on. A later paper also used a similar concept to generate tangle patterns. So this grammar thing has some power in assembling things after all! So, can we apply this knowledge to build robots! First, the robots in this new paper are built up as a collection of these joint types, links and wheels, which can come in all kinds of sizes and weights. Now, our question is, how do we assemble them in a way so that they can traverse a given terrain effectively? Well, time for some experiments! Look at this robot. It has a lot of character, I must say, and can deal with this terrain pretty well. Now look at this poor thing. Someone in the lab at MIT had a super fun day with this one I am sure. Now, these can sort of do the job, but now, let’s see the power of grammars and search algorithms in creating more optimized robots for a variety of terrains! First, a flat terrain. Let’s see…yes, now we’re talking! This one is traversing at great speed… and this one works too. I like how it was able to find vastly different robot structures that both perform well here. Now, let’s look at a little harder level, with gapped terrains. Look, oh wow, loving this. The algorithm recognized that a more rigid body is required to efficiently step through the gaps. And now, I wonder what happens if we add some ridges to the levels, so it cannot only step through the gaps, but has to climb? Let’s see…and we get those long long limbs that can indeed climb through the ridges. Excellent! Now, add a staircase, and see who can climb these well! The algorithm says, well, someone with long arms and a somewhat elastic body. Let’s challenge the algorithm some more! Let’s add, for instance, a frozen lake. Who can climb a flat surface that is really slippery? Does the algorithm know? Look, it says, someone who can utilize a low-friction surface by dragging itself through it, or someone with many legs. Loving this. Now, this is way too much fun, so let’s do two more. What about a walled terrain example? What kind of robot would work there? One with a more elastic body, carefully designed to be able curve sharply, enabling rapid direction changes. But it cannot be too long, or else it would bang its head into the wall. This is indeed a carefully crafted specimen for this particular level. Now, of course, real-world situations often involve multiple kinds of terrains, not just one. And of course, the authors of this paper know that very well, and also asked the algorithm to design a specimen that can traverse walled and ridged terrains really well. Make sure to have a look at the paper, which even shows graphs for robot archetypes that work on different terrains. It turns out, one can even make claims about the optimality, which is a strong statement. I did not expect that at all. So, apparently, grammars are amazing at generating many kinds of complex structures, including robots. And note that this paper also has a followup work from the same group where they took it a step further, and made figure skating and breakdancing robots. What a time to be alive! The link is also available in the video description for  that one. Thanks for watching and for your generous support, and I'll see you next time!"
507,Google’s New AI Puts Video Calls On Steroids!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. With the increased popularity of online meetings, telepresence applications are on the rise where we can talk to each other from afar. Today, let’s see how these powerful, new neural network-based learning methods can be applied to them. It turns out, they can help us do everyone’s favorite, which is, showing up to a meeting, and changing our background to pretend we are somewhere else. Now that is a deceptively difficult problem. Here, the background has been changed, that is the easier problem, but, look! The lighting of the new environment hasn’t been applied to the subject. And now, hold on to your papers, and check this out. This is the result of the new technique after it recreates the image as if she was really there. I particularly like the fact that the result includes high-quality specular highlights too, or in other words, the environment reflecting off of our skin. However, of course, this is not the first method attempting this. So let’s see how it performs compared to the competition! These techniques are from one and two years ago, and…they don’t perform so well. Not only did they lose a lot of detail all across the image, but, the specular highlights are gone. As a result, the image feels more like a video game character than a real person. Luckily, the authors also have access to the reference information to make our job of comparing the results easier. Roughly speaking, the more the outputs look like this, the better. So now, hold on to your papers, and let’s see how the new method performed. Oh yes! Now we’re talking! Now, of course, not even this is perfect. Clearly, the specularity of clothing was determined incorrectly, and the matting around the thinner parts of the hair could be better, which is notoriously difficult to get right. But, this is such a huge step forward in just one paper. And we are not nearly done there are two more things that I found to be remarkable about this work. One, is that the whole method was trained on still images, yet, it still works on video too! And we don’t have any apparent temporal coherence issues, or in other words, no flickering arises from the fact that it processes the video as not a video, but a series of separate images. Very cool. Two, if we are in a meeting with someone and we like their background, we can simply borrow it. Look. This technique can take their image, get the background out, estimate its lighting, and give the whole package to us too. I think this will be a game changer. People may start to become more selective with these backgrounds, not just because of how the background looks, but, because how it makes them look. Remember, lighting off of a well chosen background makes a great deal of a difference in our appearance in the real world, and now, with this method in virtual worlds too. And this will likely happen not decades from now, but in the near future. So this new method is clearly capable of some serious magic. But how? What is going on under the hood to achieve this? This method performs two important steps to accomplish this, step number one is matting. This means separating the foreground from the background, and then, if done well, we can now easily cut out the background and also have the subject on a separate layer and proceed to step number two. Which is, relighting. In this step, the goal is to estimate the illumination of the new scene and recolor the subject as if she were really there. This new technique performs both, but most of the contributions lie in this step. To be able to accomplish this, we have to be able to estimate the material properties of the subject. The technique has to know one, where the diffuse parts are, these are the parts that don’t change too much as the lighting changes, and two, where the specular parts are, in other words, shiny regions that reflect back the environment more clearly. Putting it all together, we get really high-quality relighting for ourselves, and given that this was developed by Google, I expect that this will supercharge our meetings quite soon. And just imagine what we will have two more papers down the line. My goodness, what a time  to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
508,"Glitter Simulation, Now Faster Than Ever!","Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. I am a light transport researcher by trade, and due to popular request, today I am delighted to show you these beautiful results from a research paper. This is a new light simulation technique that can create an image like this. This looks almost exactly like reality. It can also make an image like this. And, have a look at this this is a virtual object that glitters. Oh my goodness. Absolutely beautiful. Right? Well, believe it or not, this third result is wrong. Now, you see, it is not so bad, but there is a flaw in it somewhere. By the end of this video, you will know exactly where and what is wrong. So, light transport eh? How do these techniques work anyway? We can create such an image by simulating the path of millions and millions of light rays. And initially, this image will look noisy, and as we add more and more rays, this image will slowly clean up over time. If we don’t have a well-optimized program, this can take from hours to days to compute for difficult scenes. For instance, this difficult scene took us several weeks to compute. Okay, so what makes a scene difficult? Typically, caustics and specular light transport. What does that mean? Look! Here we have a caustic pattern that takes many-many millions, if not billions of light rays to compute properly. This can get tricky because these are light paths that we are very unlikely to hit with randomly generated light rays. So, how do we solve this problem? Well, one way of doing it is not trusting random light rays, but systematically finding these caustic light paths and computing them. This fantastic paper does exactly that, so let’s look at one of those classic closeups that are the hallmark of any modern light transport paper. Let’s see. Yes. On this scene, you see beautiful caustic patterns under these glossy metallic objects. Let’s see what a simple, random algorithm can do with this with an allowance of two minutes of rendering time. Well, do you see any caustics here? Do you see these bright points? These are the first signs of the algorithm finding small point samples of the caustic pattern, but that’s about it. It would take at the very least several days for this algorithm to compute the entirety of it. This is what the fully rendered reference image looks like. This is the one that takes forever to compute. Quite different, right? So let’s allocate 2 minutes of our time for the new method and see how well it does. Which one will it be closer to? Can it beat the naive algorithm? Now, hold on to your papers, and let’s see together. What? On this part, it looks almost exactly the same as the reference. This is insanity! A converged caustic region in two minutes! Whoa. The green closeup is also nearly completely done. Now, not everything is sunshine and rainbows, look, the blue closeup is still a bit behind, but it still beats the naive algorithm handily. That is quite something. And yes, it can also render these beautiful underwater caustics as well in as little as 5 minutes. 5 minutes! And I would not be surprised if many people would think this is an actual photograph from the real world. Loving it. Now, what about the glittery origami scene from the start of the video? This one. Was that footage really wrong? Yes it was! Why? Well, look here! These glittery patterns are unstable. The effect especially pronounced around here. This arises from the fact that the technique does not take into consideration the curvature of this object correctly when computing the image. Let’s look at the corrected version, and, oh my goodness. No unnecessary flickering anywhere to be seen, just the beautiful glitter slowly changing as we rotate the object around. I could stare at this all day. Now, note that these kinds of glints are much more practical than most people would think. For instance, it also has a really pronounced effect when rendering a vinyl record and many other materials as well. So, from now on, we can render photorealistic images of difficult scenes with caustics and glitter, not in a matter of days, but in a matter of minutes. What a time to be alive! And when watching all these beautiful results, you are thinking that this light transport thing is pretty cool, and you would like to learn more about it, I held a Master-level course on this topic at the Technical University of Vienna. Since I was always teaching it to a handful of motivated students, I thought that the teachings shouldn’t only be available for the privileged few who can afford a college education, but the teachings should be available for everyone. Free education for everyone, that’s what I want. So, the course is available free of charge for everyone, no strings attached, so make sure to click the link in the video description to get started. We write a full light simulation program from scratch there, and learn about physics, the world around us, and more. Thanks for watching and for your generous support, and I'll see you next time!"
509,Burning Down an Entire Virtual Forest!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. In a previous episode not so long ago, we burned down a virtual tree. This was possible through an amazing simulation paper from 4 years ago, where each leaf has its own individual mass and area, they burn individually, transfer heat to their surroundings, and finally, branches bend, and, look can eventually even break in this process. How quickly did this run? Of course, in real time. Well, that is quite a paper, so if this was so good, how does anyone improve that? Burn down another virtual tree? No, no, that would be too easy. You know what, instead, let’s set on fire an entire virtual forest. Oh yeah! Here you see a simulation of a devastating fire from a lightning strike in Yosemite national park. The simulations this time around are typically sped up a great deal to be able to give us a better view of how it spreads, so if you see some flickering, that is the reason for that. But wait, is that really that much harder? Why not just put a bunch of trees next to each other and start the simulation? Would that work? The answer is a resounding no. Let’s have a look why, and with that, hold on to your papers because here comes the best part: it also simulates not only the fire, but cloud dynamics as well. Here you see how the wildfire creates lots of hot, and dark smoke closer to the ground, and, wait for it…yes! There we go! Higher up, the condensation of water creates this lighter, cloudy region. Yes, this is key to the simulation, not just because of the aesthetic effects, but this wildfire can indeed create a cloud type that goes by the name flammagenitus. So, is that good or bad news? Well, both! Let’s start with the good news: it often produces rainfall, which helps putting out the fire. Well, that is wonderful news, so then, what is so bad about it? Well, flammagenitus clouds may also trigger a thunderstorm, and thus, create another huge fire. That’s bad news number one. And bad news number two: it also occludes the fire, thereby making it harder to locate and extinguish it. So, got it, add cloud dynamics to the tree fire simulator, and we are done, right? No, not even close. In a forest fire simulation, not just clouds, everything matters. For instance, first we need to take into consideration the wind intensity and direction. This can mean the difference between a manageable or a devastating forest fire. Second, it takes into consideration the density and moisture intensity of different tree types for instance, you see that the darker trees here are burning down really slowly. Why is that? This is because these trees are denser birches and oak trees. Third, the distribution of the trees also matter. Of course, the more the area is covered by trees, the more degrees of freedom there are for the fire to spread. And, fourth. Fire can not only spread horizontally from tree to tree, but vertically too. Look! When a small tree catches fire, this can happen. So, as we established from the previous paper, one tree catching fire can be simulated in real time. What about an entire forest? Let’s take the simulation with the most number of trees, my goodness, they simulated 120k trees there. And the computation time for one simulation step was…95. So, 95 what? 95 milliseconds. Wow! So this thing runs interactively, which means that all of these phenomena can be simulated in close to real time. With that, we can now model how a fire would spread in real forests around the world, test different kinds of fire barriers and their advantages, and we can even simulate how to effectively put out the fire. And don’t forget, we went from simulating one burning tree to a hundred and twenty thousand in just one more paper down the line. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
510,Simulating The Olympics… On Mars!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, it is possible to teach virtual characters to perform highly dynamic motions, like a cartwheel or backflips. And not only that, but we can teach an AI to perform this differently from other characters, to do it with style if you will. But! Today, we are not looking to be stylish. Today, we are looking to be efficient! In this paper, researchers placed an AI in a physics simulation and asked it to control a virtual character and gave it one task: to jump as high as it can. And when I heard this idea, I was elated, and immediately wondered did it come up with popular techniques that exist in the real world? Well, let’s see…yes! Woo-hoo! That is indeed a Fosbury flop. This allows the athlete to jump backward over the bar, thus lowering their center of gravity. Even today, this is the prevalent technique in high jump competitions. With this technique, the takeoff takes place relatively late. The only problem is that the AI didn’t clear the bar so far…so, can it? Well, this is a learning-based algorithm, so, with a little more practice, it should improve yes, great work! If we lower the bar just a tiny bit for this virtual athlete, we can also observe it performing the western roll. With this technique, we take off a little earlier and we don’t jump backward, but sideways. If it had nothing else, this would already be a great paper, but we are not nearly done yet. The best is yet to come! This is a simulation. A virtual world if you will, and here, we make all the rules. The limit is only our imagination. The authors know that very well and you will see that they indeed have a very vivid imagination. For instance, we can also simulate a jump with a weak take-off leg and see that with this condition, the little AI can only clear a bar that is approximately one foot lower than its previous record. What about another virtual athlete with an inflexible spine? It can jump approximately two feet lower. Here is the difference compared to the original. I am enjoying this a great deal, and it’s only getting better. Next, what happens if we are injured and have a cast on the take-off knee? What results can we expect? Something like this. We can jump a little more than two feet lower. What about organizing the olympics on Mars? What would that look like? What would the world record be with the weaker gravity there? Well, hold on to your papers, and look… yes, we could jump three feet higher than on earth, and then…ouch, well missed the foam matting, but otherwise, very impressive. And if we are already there, why limit the simulation to high jumps? Why not try something else? Again, in a simulation, we can do anything! Previously, the task was to jump over the bar, but we can also recreate the simulation to include instead, jumping through obstacles. To get all of these magical results, the authors propose a step they call “Bayesian Diversity Search”. This helps systematically creating a rich selection of novel strategies, and it does this efficiently. The authors also went the extra mile and included a comparison to motion capture footage performed by a real athlete. But note that the AI’s version uses a similar technique and is able to clear a significantly higher bar without ever seeing a high jump move. The method was trained on motion capture footage to get used to humanlike movements, like walking, running, and kicks, but it has never seen any high jump techniques before. Wow. So, if this can invent high jumping techniques that took decades for humans to invent, I wonder what else it could invent? What do you think? Let me know in the comments below! What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
511,"One Simulation Paper, Tons of Progress!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Through the power of computer  graphics research, today,   we can write wondrous programs which can simulate  all kinds of interactions in virtual worlds.   These works are some of my favorites, and  looking at the results, one would think that   these algorithms are so advanced, there is  hardly anything new to invent in this area.   But as amazing as these previous techniques  are, they don’t come without limitations. Alright, well, what do those limitations look  like? Let’s have a look at this example. The   water is coming out of the nozzle, and it behaves  unnaturally. But that’s only the smaller problem,   there is an even bigger one. What is that  problem? Let’s slow this down, and look carefully.   Oh! Where did the water go? Yes, this is  the classical numerical dissipation problem   in fluid simulations, where due to an averaging  step, particles disappear into thin air.   And now, hold on to your papers and let’s see if  this new method can properly address this problem.   And…oh yes! Fantastic! So it dissipates less. Great! What else does this  do? Let’s have a look through this experiment   where a wheel gets submerged into sand, that’s  good, but…the simulation is a little mellow.   You see, the sand particles  are flowing down like a fluid,   and the wheel does not really roll up the  particles in the air. And the new one.   So, apparently, it not only  helps with numerical dissipation,   but also with particle separation  too. More value. I like it. If this technique can really solve these two  phenomena, we don’t even need sandy tires and   water sprinklers to make it shine, there are  so many scenarios where it performs better   than previous techniques. For instance, when  simulating this non-frictional elastic plate   with previous methods, some of the particles get  glued to it. And, did you catch the other issue?   Yes, the rest of the particles also  refuse to slide off of each other.   And now, let’s see the new method. Oh my! It  can simulate these phenomena correctly too! And it does not stop there. It also simulates  strand-strand interactions better than previous   methods. In these cases, sometimes the  collision of short strands with boundaries   was also simulated incorrectly. Look at how all  this geometry intersected through the brush.   And the new method? Yes, of course,  it addresses these issues too. So, if it can simulate the movement and  intersection of short strands better…does   that mean that it can also perform higher-quality  hair simulations? Oh yes, yes it does! Excellent! So, as you see, the pace of progress  in computer graphics research   is absolutely stunning. Things  that were previously impossible   can become possible in a matter of just  one paper. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
512,NVIDIA’s Minecraft AI: Feels Like Magic!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. We just hit a million subscribers! I can  hardly believe that so many of you Fellow   Scholars are enjoying the Papers! Thank you so  much for all the love! In a previous episode,   we explored an absolutely insane idea. The idea  was to unleash a learning algorithm on a dataset   that contains images and videos of cities,  then take a piece of video footage from a game,   and translate it into a real movie. It  is an absolute miracle that this works,   and it not only works, but it works reliably and  interactively. And it also works much better than   its predecessors. Now, we discussed that the  input video game footage is pretty detailed. And I was wondering, what if we don’t create  the entire game in such detail. What about,   creating just the bare minimum,  a draft of the game if you will   and let the algorithm do the heavy lifting.  Let’s call this world to world translation!   So, is world to world translation  possible, or is this science fiction? Fortunately, scientists at NVIDIA and Cornell  University thought of that problem and came   up with a remarkable solution. But, the first  question is what form should this draft take?   And they say, it should be a Minecraft world, or  in other words, a landscape assembled from little   blocks. Yes, that is simple enough indeed. So this  goes in. And now, let’s see what comes out. Oh my! It created water, it understands the concept of  an island, and it created a beautiful landscape,   also, with vegetation. Insanity. It even seems to have some concept   of reflections, although they will need  some extra work to get perfectly right. But, what about artistic control? Do we get  this one solution, or can we give more detailed   instructions to the technique? Yes we can! Look at  that. Since the training data contains desert and   snowy landscapes too, is also supports them as  outputs. Whoa, this is getting wild. I like it. And it even supports interpolation, which  means that we can create one landscape   and ask the AI to create a  blend between different styles.   We just look at the output animations, and pick  the one that we like best. Absolutely amazing. What I also really liked is that  it also supports rendering fog.   But this is not some trivial fog technique,  no-no, look how beautifully it occludes the trees.   If we look under the hood, oh my! I am a  light transport researcher by trade, and boy,   am I happy to see the authors having done  their homework. Look, we are not playing   games here, the technique contains bona-fide  volumetric light transmission calculations. Now, this is not the first technique to perform  this kind of world to world translation.   What about the competition? As you see, there  are many prior techniques here, but there is   one key issue that almost all of them share.  So, what is that? Oh yes, much like with the   other video game papers, the issue is the lack of  temporal coherence, which means that the previous   techniques don’t remember they it did a few images  earlier, and may create a drastically different   series of images. And the result is this kind of  flickering that is often a deal-breaker regardless   of how good the technique is otherwise. Look,  the new method does this significantly better. This could help level generation for computer  games, creating all kinds simulations, and   if it improves some more, these could maybe even  become backdrops to be used in animated movies.   Now, of course, this is still not perfect,  some of the outputs are still blocky. But, with this method, creating virtual worlds  has never been easier. I cannot believe that we   can have a learning-based algorithm where the  input is one draft world, and it transforms it   to a much more detailed and beautiful one. Yes,  it has its limitations, but just imagine what   we will be able to do two more papers down the  line. Especially given that the quality of the   results can be algorithmically measured, which is  a godsend for comparing this to future methods.   And for now, huge congratulations to NVIDIA  and Cornell University for this amazing paper. Thanks for watching and for your generous  support, and I'll see you next time!"
513,This AI Helps Testing The Games Of The Future!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Have you ever got stuck in a video game? Or found a glitch that would prevent you from finishing it? As many of you know, most well-known computer games undergo a ton of playtesting, an important step that is supposed to unveil these issues. So how is it possible that all these bugs and glitches still make it the final product? Why did the creators not find these issues? Well, you see, playtesting is often done by humans. That sounds like a good thing, and it often is. But, here comes the problem whenever we change something in the game, our changes may also have unintended consequences somewhere else away from where we applied them. New oversights may appear elsewhere, for instance, moving a platform may make the level more playable, however, also, this might happen. The player may now be able to enter a part of the level that shouldn’t be accessible, or, be more likely to encounter a collision bug and get stuck. Unfortunately, all this means that it’s not enough to just test what we have changed, but we have to retest the whole level, or maybe the whole game itself. For every single change, no matter how small. That not only takes a ton of time and effort, but is often flat out impractical. So what is the solution? Apparently, a proper solution would require asking tons of curious humans to test the game. But wait a second. We already have curious learning-based algorithms. Can we use them for playtesting? That sounds amazing! Well, yes, until we try it. You see, here is an automated agent, but a naive one trying to explore the level. Unfortunately, it seems to have missed half the map! Well, that’s not the rigorous testing we are looking for, is it? Let’s see what this new AI offers. Can it do any better? Oh my, now we’re talking! The new technique was able to explore not less than 50%, but a whopping 95% of the map. Excellent. But we are experienced Fellow Scholars over here, so of course, we have some questions. So, apparently this one has great coverage, so it can cruise around, great, but our question is, can these AI agents really find game-breaking issues? Well, look, it just found a bug where it could climb to the top of the platform without having to use the elevator. It can also build a graph that describes which parts of the level are accessible and through what path. Look! This visualization tells us about the earlier issue where one could climb the wall through an unintentional issue, and, after the level designer supposedly fixed it by adjusting the steepness of the wall, let’s see the new path. Yes, now it could only get up there by using the elevator. That is the intended way to traverse the level. Excellent! And it gets better, it can even tell us the trajectories that enabled it to leave the map so we know exactly what issues we need to fix without having to look through hours and hours of video footage. And, whenever we applied the fixes, we can easily unleash another bunch of these AIs to search every nook and cranny, and try these crazy strategies, even ones that don’t make any sense, but appear to work well. So, how long does this take? Well, the new method can explore half the map in approximately an hour or two, can explore 90% of the map in about 28 hours, and if we give it a couple more days, it goes up to about 95%. That is quite a bit, so we don’t get immediate feedback as soon as we change something, since this method is geared towards curiosity and not efficiency. Note that this is just the first crack at the problem, and I would not be surprised if just one more paper down the line, this would take about an hour, and two more papers down the line, it might even be done in a matter of minutes. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
514,This Magical AI Makes Your Photos Move!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Look at this video of a moving escalator.  Nothing too crazy going on, only the escalator   is moving. And I am wondering, would it be  possible to not record a video for this,   just an image, and have one of these amazing  new learning-based algorithms animate it? Well, that is easier said than done. Look, this  is what was possible with a research work from   2 years ago, but the results are…well, what you  see here. So how about a method from one year ago?   This is the result. A great deal of improvement  the water is not animated in this region,   and is generally all around the place, and we  still have a lot of artifacts around the fence.   And now, hold on to your papers, and  let’s see this new method…and…whoa!   Look at that! What an improvement! Apparently, we can now give this one a still   image, and for the things that  should move, it makes them move.   It is still not perfect by any means, but  this is so much progress in just two years. And there’s more! Get this, for the things  that shouldn’t move, it even imagines   how they should move. It works  on this building really well.   But it also imagines how my tie would move around.  Or my beard, which is not mine, by the way,   but was made by a different AI, or the windows.  Thank you very much for the authors of the paper   for generating these results only for us! And  this can lead to really cool artistic effects,   for instance, this moving brickwall, or  animating the stairs here. Loving it. So, how does this work exactly? Does  it know what regions to animate?   No it doesn’t, and it shouldn’t. We can  specify that ourselves by using a brush   to highlight the region that we wish to see  animated, and we also have a little more   artistic control over the results by prescribing  a direction in which things should go. And it appears to work on a really wide  variety of images, which is only one   of its most appealing features. Here  are some of my favorite results,   I particularly love the one with the  apple rotation here. Very impressive. Now, let’s compare it to the previous method from  just one year ago, and let’s see what the numbers   say. Well, they say that the previous  one performs better on fluid elements   than the new one. My experience is that it  indeed works better on specialized cases,   like this fire texture, but on many water  images, they perform roughly equivalently.   Both are doing quite well. So,  is the new one really better? Well, here comes the interesting part. When  presented with a diverse set of images,   look. There is no contest here the previous  one creates no results, incorrect results,   or if it does something, the new technique  almost always comes out way better. Not only that, but let’s see what the  execution time looks like for the new method.   How much do we have to wait  for these results? The one from   last year took 20 seconds per image and  required a big honking graphics card,   while the new one only needs your smartphone,  and runs in… what? Just one second. Loving it. So, what images would you try with this  one? Let me know in the comments. Well,   in fact, you don’t need to just think about  what you would try, because you can try this   yourself. It has a mobile app, the link  is available in the video description,   make sure to let me know in the comments  below if you had some success with it! Here comes the even more interesting part.  The previous method was using a learning-based   algorithm, while this one is a bona-fide,  almost completely handcrafted technique.   Partly because training neural networks  requires a great deal of training data,   and there are very few, if any training examples  for moving buildings and these other surreal   phenomena. Ingenious. Huge congratulations  to the authors for pulling this off! Now, of course, not even this technique is  perfect, there are still cases where it does   not create appealing results, however,  since it only takes a second to compute,   we can easily retry with a different pixel mask  or direction and see if it does better. And just   imagine what we will be able to do two more  papers down the line. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
515,A Simulation That Looks Like Reality!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. I am stunned by this new graphics paper that promises to simulate three-way coupling and enables beautiful surface tension simulations like this, and this, and more. Yes, none of this is real footage, these are all simulated on a computer. I have seen quite a few simulations, and I am still baffled by this. How is this even possible? Also, three-way coupling, eh? That is quite peculiar, to the point that the term doesn’t even sound real. Let’s find out why together. So, what does that mean exactly? Well, first, let’s have a look at one way coupling. As the box moves here, it has an effect on the smoke plume around it. This example also showcases one-way coupling, where the falling plate stirs up the smoke around it. And now, on to two-way coupling. In this case, similarly to the previous ones, the boxes are allowed to move the smoke, but the added two-way coupling part means that now, the smoke is also allowed to blow away the boxes. What’s more, the vortices here on the right were even able to suspend the red box in the air for a few seconds. An excellent demonstration of a beautiful phenomenon. So, coupling means interaction between different kinds of objects. And two-way coupling seems like the real deal. Here, it is also required to compute how this fiery smoke trail propels the rocket upward. But wait, we just mentioned that the new method performs three-way coupling. Two-way was solid-fluid interactions, and it seemed absolutely amazing, so what is the third element then? And why do we even need that? Well, depending on what object is in contact with the liquid, gravity, buoyancy, and surface tension forces need additional considerations. To be able to do this, now look carefully! Yes, there is the third element, it simulates this thin liquid membrane too, which is in interaction with the solid and the fluid at the same time. And with that, please meet three-way coupling! So, what can it do? It can simulate this paperclip floating on water. That is quite remarkable because the density of the paperclip is 8 times as much as the water itself, and yet, it still sits on top of the water. But how is that possible? Especially given that gravity wants to constantly pull down a solid object. Well, it has two formidable opponents, two forces that try to counteract it, one is buoyancy, which is an upward force, and two, the capillary force, which is a consequence of the formation of a thin membrane. If these two friends are as strong as gravity, the object will float. But, this balance is very delicate, for instance, in the case of milk and cherries, this happens. And, during that time, the simulator creates a beautiful bent liquid surface that is truly a sight to behold. Once again, all of this footage is simulated on a computer. The fact that this new work can simulate these three physical systems and their interactions is a true miracle. Absolutely incredible. Now, if you have been holding on to your papers so far, squeeze that paper, because we will now do my favorite thing in any simulation paper, and that is when we let reality be our judge, and compare the simulated results to real life footage. This is a photograph. And now comes the simulation. Whoa. I have to say, if no one told me which is which, I might not be able to tell. And I am delighted to no end by this fact, so much so that I had to ask the authors to double-check if this really is a simulation and they managed to reproduce the illumination of these scenes so perfectly. Yes they did! Fantastic attention to detail. Very impressive. So, how long do we have to wait for all this? For a 2 dimensional scene, it pretty much runs interactively, that is great news. And, we are firmly in the seconds per frame region for the 3D scenes, but look, the Boat and Leaves scene runs in less than two seconds per time step. That is absolutely amazing. Not real time, because one frame contains several time steps, but why would it be real time? That this is the kind of paper that makes something previously impossible possible, and it even does that swiftly. I would wager, we are just one, or at most, two more papers away from getting this in real time. This is unbelievable progress in just one paper. And all handcrafted, no learning algorithms anywhere to be seen. Huge congratulations to the authors. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
516,Neural Materials Are Amazing!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This image is the result of a light simulation program, created by research scientists. It looks absolutely beautiful, but the light simulation algorithm is only part of the recipe here. To create something like this, we also need a good artist who can produce high-quality geometry, lighting, and, of course, good, life-like material models. For instance, without the materials part, we would see something like this. Not very exciting, right? Previously, we introduced a technique that learns our preferences and helps filling these scenes with materials. This work can also generate variants of the same materials as well. In a later technique, we could even take a sample image, completely destroy it in photoshop, and our neural networks would find a photorealistic material that matches these crazy ideas. Links to both of these works are available in the video description. And, to improve these digital materials, this new paper introduces something that the authors call a multi-resolution neural material representation. What is that? Well, it is something that is able to put amazingly complex material models in our light transport programs, and not only that, but…oh my. Look at that! We can even zoom in so far that we see the snagged threads. That is the magic of the multi-resolution part of the technique. The neural part means that the technique looks at lots of measured material reflectance data, this is what describes a real-world material, and compresses this description down into a representation that is manageable. Okay…why? Well, look. Here is a reference material. You see, these are absolutely beautiful, no doubt, but are often prohibitively expensive to store directly. This new method introduces these neural materials to approximate the real world materials, but in a way that is super cheap to compute and store. So, our first question is, how do these neural materials compare to these real, reference materials? What do you think? How much worse a quality do we have to expect to be able to use these in our rendering systems? Well, you tell me, because you are already looking at the new technique right now. I quickly switched from the reference to the result with new method already. How cool is that? Look. This was the expensive reference material, and this is fast neural material counterpart for it. So, how hard is this to pull off? Well, let’s look at some more results side by side. Here is the reference. And here are two techniques from one and two years ago that try to approximate it. And you see that if we zoom in real close, these fine details are gone. Do we have to live with that? Or, maybe, can the new method do better? Hold on to your papers, and let’s see. Wow! While it is not a 100% perfect, there is absolutely no contest compared to the previous methods. It outperforms them handily in every single case of these complex materials I came across. And when I say complex materials, I really mean it. Look at how beautifully it captures not only the texture of this piece of embroidery, but, when we move the light source around, oh wow! Look at the area here around the vertical black stripe and how its specular reflections change with the lighting. And note that none of these are real images, all of them come from a computer program. This is truly something else. Loving it. So, if it really works so well, where is the catch? Does it work only on cloth-like materials? No-no, not in the slightest! It also works really well on rocks, insulation foam, even turtle shells and a variety of other materials. The paper contains a ton more examples than we can showcase here, so make sure to have a look in the video description. I guess this means that it requires a huge and expensive neural network to pull off, right? Well, let’s have a look. Whoa, now that’s something. It does not require a deep and heavy-duty neural network, just 4 layers are enough. And this, by today’s standard, is a lightweight network that can take these expensive reference materials and compress them down in a matter of milliseconds. And they almost look the same. Materials into our computer simulations straight from reality? Yes please! So, from now on, we will get cheaper and better material models for animation movies, computer games, and visualization applications! Sign me up right now! What a time to  be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
517,NVIDIA’s Face Generator AI: This Is The Next Level!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, we will see how a small change to an already existing learning-based technique can result in a huge difference in its results. This is StyleGAN2, is a technique that appeared in December of 2019. It is a neural network-based learning algorithm that is capable of synthesizing these eye-poppingly detailed images of human beings that don’t even exist. This is all synthetic. It also supports a cool feature where we can give it a photo, then, it embeds this image into a latent space, and in this space, we can easily apply modifications to it. Okay…but what is this latent space thing? A latent space is a made-up place where we are trying to organize data in a way that similar things are close to each other. In our earlier work, we were looking to generate hundreds of variants of a material model to populate this scene. In this latent space, we can concoct all of these really cool digital material models. A link to this work is available in the video description. StyleGAN uses walks in a similar latent space to create these human faces and animate them. So, let’s see that. When we take a walk in the internal latent space of this technique, we can generate animations. Let’s see how StyleGAN2 does this. It is a true miracle that a computer can create images like this. However, wait a second. Look closely…Did you notice it? Something is not right here. Don’t despair if not, it is hard to pin down what the exact problem is, but it is easy to see that there is some sort of flickering going on. So, what is the issue? Well, the issue is that there are landmarks, for instance, the beard, which don’t really, or just barely move, and essentially, the face is being generated under it with these constraints. The authors refer to this problem as texture sticking. The AI suffers from a sticky beard if you will. Imagine saying that 20 years ago to someone, you would end up in a madhouse. Now, this new paper from scientists at NVIDIA promises a tiny but important architectural change. And we will see if this issue, which seems like quite a limitation, can be solved with it, or not. And now, hold on to your papers, and let’s see the new method. Holy Mother of Papers. Do you see what I see here? The sticky beards are a thing of the past, and facial landmarks are allowed to fly about freely. And not only that, but the results are much smoother and more consistent, to the point that it can not only generate photorealistic images of virtual humans. Come on, that is so 2020. This generates photorealistic videos of virtual humans! So, I wonder, did the new technique also inherit the generality of StyleGAN2? Let’s see. We know that it works on real humans, and now, paintings and art pieces, yes, excellent, and of course, cats, and other animals as well. The small change that creates these beautiful results is what we call an equivariant filter design, essentially this ensures that finer details move together in the inner thinking of the neural network. This is an excellent lesson on how a small and carefully designed architectural change can have a huge effect on the results. If we look under the hood, we see that the inner representation of the new method is completely different from its predecessor. You see, the features are indeed allowed to fly about, and the new method even seems to have invented a coordinate system of sorts to be able to move these things around. What an incredible idea. These learning algorithms are getting better and better with every published paper. Now, good news! It is only marginally more expensive to train and run than StyleGAN2, and less good news is that training these huge neural networks still requires a great deal of computation. The silver lining is that if it has been trained once, it can be run inexpensively for as long as we wish. So, images of virtual humans might soon become a thing of the past, because from now on, we can generate photorealistic videos of them. Absolutely amazing. Thanks for watching and for your generous support, and I'll see you next time!"
518,New AI Research Work Fixes Your Choppy Videos!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to take a bad choppy video, and make a beautiful, smooth and creamy footage out of it. With today's camera and graphics technology, we can create videos with 60 frames per second. Those are really smooth, I also make each of these videos using 60 frames per second, however, it almost always happens that I encounter the paper videos that have only from 24 to 30 frames per second. In this case, I put them in my video editor that has a 60 fps timeline, where half or even more of these frames will not provide any new information. That’s neither smooth nor creamy. And it gets worse. Look! As we try to slow down the videos for some nice slow-motion action, this ratio becomes even worse, creating an extremely choppy output video because we have huge gaps between these frames. So, does this mean that there is nothing we can do and have to put up with this choppy footage? No, not at all! Look at this technique from 2019 that we covered in an earlier video. The results truly speak for themselves. In goes a choppy video, and out comes a smooth, and creamy result. So good! But wait, it is not 2019, it is 2021, and we always say that two more papers down the line, and it will be improved significantly. From this example, it seems that we are done here, we don’t need any new papers. Is that so? Well, let’s see what we have only one more paper down the line! Now, look. It promises that it can deal with 10 to 1, or even 20 to 1 ratios, which means that for every single image in the video, it creates 10 or 20 new ones, and supposedly we shouldn’t notice that. Well, those are big words, so I will believe it when I see it. Let’s have a look together! Holy mother of papers! This can really pull this off, and it seems nearly perfect. Wow. It also knocked it out of the park with this one. And all this improvement in just one more paper down the line. The pace of progress in machine learning research is absolutely amazing. But, we are experienced Fellow Scholars over here, so we will immediately ask, is this really better than the previous 2019 paper? Let’s compare them! Can we have side by side comparisons? Of course we can! You know how much I love fluid simulations. Well, these are not simulations, but a real piece of fluid, and in this one, there is no contest. The new one understands the flow so much better, while the previous method sometimes even seems to propagate the waves backwards in time. A big checkmark for the new one. In this case, the previous method assumes linear motion when it shouldn’t, thereby introducing a ton of artifacts. The new one isn’t perfect either, but it performs significantly better. Do not worry for a second, we will talk about linear motion some more in a moment. So how does all this wizardry happen? One of the key contributions of the paper is that it can find out when to use the easy way and the hard way. What are those? The easy way is using already existing information in the video and computing inbetween states for a movement. That is all well and good if we have simple, linear motion in our video. But, look, the easy way fails here. Why is that? It fails because we have a difficult situation where reflections off of this object rapidly change, and it reflects something. We have to know what that something is. So, look, this is not even close to the true image, which means that here, we can’t just reuse the information in the video, this requires introducing new information. Yes, that is the hard way! And this excels when new information has to be synthesized. Let’s see how well it does! My goodness, look at that, it matches the true reference image almost perfectly. And also, look, the face of the human did not require synthesizing a great deal of new information, it did not change over time, so we can easily refer to the previous frame for it, hence, the easy way did better here. Did you notice? That is fantastic, because the two are complementary. Both techniques work well, but they work well elsewhere. They need each other! So, yes, you guessed right, to tie it all together, there is also an attention-based averaging step that helps us decide when to use the easy, and the hard ways. Now, this is a good paper, so it tells us how these individual techniques contribute to the final image. Using only the easy way can give us about 26 decibels, that would not beat the previous methods in this area. However, look! By adding the hard way, we get a premium quality result that is already super competitive, and, if we add the step that helps us decide when to use the easy and hard ways, we get an extra decibel. I will happily take it, thank you very much! And, if we put it all together, oh yes, we get a technique that really outpaces the competition. Excellent. So, in the near future, perhaps we will be able to record a choppy video of a family festivity, and have a chance at making this choppy video enjoyable, or maybe even create slow-motion videos with a regular camera. No slow-motion camera is required. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
519,Simulating Bursting Soap Bubbles!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to simulate these absolutely beautiful thin film structures. You see, computer graphics researchers have been writing physics simulation programs for decades now, and the pace of progress in this research area is absolutely stunning. Here are three examples of where we are at the moment. One, this work was able to create a breathtaking honey coiling simulation. I find it absolutely amazing that through the power of computer graphics research, all this was possible four years ago. And the realistic simulation works just kept coming in. This work appeared just one year ago and could simulate not only a piece of viscous fluid, but, also deal with glugging and coalescing bubbles. And three, this particular one is blazing fast. So much so that it can simulate this dam break scene in about 5 frames per second, not seconds per frame, while it can run this water drop scene with about about 7 frames per second. Remember, this simulates quantities like the velocity, pressure, and more for several million particles this quickly. Very impressive. So, are we done here? Is there anything else left to be done in fluid simulation research? Well, hold on to your papers, and check this out. This new paper can simulate thin-film phenomena. What does that mean? Four things. First, here is a beautiful oscillating soap bubble. Yes, its color varies as a function of the evolving film thickness. But that’s not all. Let’s poke it, and then…did you see that? It can even simulate it bursting into tiny, sparkly droplets. Phew. One more time. Loving it. Second, it can simulate one of my favorites, the Rayleigh-Taylor instability. The upper half of the thin film has a larger density, while the lower half carries a larger volume. Essentially, this is the phenomenon when two fluids of different densities meet. And what is the result? Turbulence. First, the interface between the two is well defined, but over time, it slowly disintegrates into this beautiful swirly pattern. Oh yeah…oh yeah! Look! And it just keeps on going and going. Third, ah yes, the catenoid experiment. What is that? This is a surface tension-driven deformation experiment, where the film is trying to shrink as we move the two rims away from each other, forming this catenoid surface. Of course, we won’t stop there, what happens when we keep moving them away? What do you do think? Please stop the video and let me know in the comments below. I’ll wait. A little. Thank you! Now then, the membrane keeps shrinking, until…yes, it finally collapses into a small droplet. The authors also went the extra mile and did the most difficult thing for any physics simulation paper…comparing the results to reality. So, is this just good enough to fool the untrained human eye, or is this the real deal? Well, look at this, this is an actual photograph of the catenoid experiment. And this is the simulation. Dear Fellow Scholars, that is a clean simulation right there. And, fourth, a thin film within a square subjected to a gravitational pull that is changing over time. And the result is more swirly patterns. So how quickly can we perform all this? Disregard the FPS, this is the inverse of the time step size, and is mainly information for fellow researchers. For now, gaze upon the time per frame column, and, my goodness. This is blazing fast too! It takes less than a second per frame for the catenoid experiment, this is one of the cheaper ones. And all this on a laptop! Wow! Now, the most expensive experiment in this paper was the Rayleigh-Taylor instability, this took about 13 seconds per frame. This is not bad at all, we can get a proper simulation of this quality within an hour or so. However, note that the authors used a big honking machine to compute this scene. And remember, this paper is not about optimization, but it is about making the impossible possible. And it is doing all that, swiftly. Huge congratulations to the authors! What  a  time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
520,Virtual Bones Make Everything Better!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see that virtual bones make everything better. This new paper is about setting up bones and joints for our virtual characters to be able to compute deformations. Deformations are at the heart of computer animation, look, all of these sequences require a carefully designed technique that can move these joints around and simulate its effect on the entirety of the body. Things move around, stretch, and bulge. But, there is a problem. What’s the problem? Well, even with state of the art deformation techniques, sometimes, this happens. Did you catch it? There is the problem, look! The hip region unfortunately bulges inwards. Is this specific to this technique? No-no, not in the slightest pretty much all of the previous techniques showcase that to some effect. This is perhaps our most intense case of this inward bulging. So, let’s have a taste of the new method. How does it deal with a case like this? Perfectly, that’s how. Loving it. Now, hold on to your papers, because it works by creating something that the authors refer to as virtual bones. Let’s look under the hood and locate them. There they are! These red dots showcase these virtual bones. We can set them up as a parameter and the algorithm distributes them automatically. Here, we have a 100 of them, but we can go to 200, or, if we so desire, we can request even 500 of them. So, what difference does this make? With a 100 virtual bones. Let’s see…yes. Here you see that the cooler colors like blue showcase the regions that are deforming accurately, and the warmer colors, for instance, red showcases the problematic regions where the technique did not perform well. The red part means that these deformations can be off by about 2 centimeters, or about three quarters of an inch. I would say that is great news, because even even with only a 100 virtual bones, we get an acceptable animation. However, the technique is still somewhat inaccurate around the knee and the hips. However, if you are one of our really precise Fellow Scholars, and feel that even that tiny mismatch is too much, we can raise the number of virtual bones to 500, and, let’s see…there we go! Still some imperfections around the knees, but, the rest is accurate to a small fraction of an inch. Excellent. The hips and knees seem to be a common theme, look, they show up in this example too. And, as in the previous case, even the 100 virtual bone animation is acceptable, and most of the problems can be remedied by adding 500 of them. Still some issues around the elbows. So far, we have looked at the new solution, and marked the good and bad regions with heatmaps. So now, how about looking at the reference footage and the new technique side by side. Why? Because we’ll find out whether it is just good at fooling the human eye, or does it really match up? Let’s have a look together. This is linear blend skinning, a state of the art method. For now, we can accept this as a reference. Note that setting this up is expensive both in terms of computation, and it also requires a skilled artist to place these helper joints correctly. This looks great. So how does the new method with the virtual bones look under the hood? These correspond to those, so, why do all this? Because the new method can be computed much, much cheaper. So let’s see what the results look like! Mmmm! Yeah! Very close to the reference results. Absolutely amazing. Now, let’s run a torture test that would make any computer graphics researcher blush. Oh my. There are so many characters here animated at the same time. So how long do we have to wait for these accurate simulations? Minutes to hours? Let me know your guess in the comments below. I’ll wait. Thank you! Now, hold on to your papers because all this takes about 5 milliseconds per frame. 5 milliseconds! This seems well over a hundred characters rocking out, and the new technique doesn’t even break a sweat. So, I hope that with this, computer animations are going to become a lot more realistic in the near future. What a time to be alive! Also, make sure to have a look at the paper in the video description. I loved the beautiful mathematics, and the clarity in there. It clearly states the contributions in a bulleted list, which is a more and more common occurrence, that’s good, but look! It even provides an image of these contributions right there, making it even clearer to the reader. Generally, details like this show that the authors went out of their way and spent a great deal of extra time writing a crystal-clear paper. It takes much, much more time than many may imagine, so, I would like to send big thank you to the authors for that. Way  to go! Thanks for watching and for your generous support, and I'll see you next time!"
521,DeepMind’s AI Plays Catch…And So Much More!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how an AI can win a complex game that it has never seen before. Zero prior training on that game. Yes, really! Now, before that, for context, have a look at this related work from 2019, where scientists at OpenAI built a super fun hide and seek game for their AI agents to play. And, boy, did they do some crazy stuff. Now, these agents learn from previous experiences, and to the surprise of no one, for the first few million rounds, we start out with…pandemonium. Everyone just running around aimlessly. Then, over time, the hiders learned to lock out the seekers by blocking the doors off with these boxes and started winning consistently. I think the coolest part about this is that the map was deliberately designed by the OpenAI scientists in a way that the hiders can only succeed through collaboration. But then, something happened. Did you notice this pointy, doorstop-shaped object? Are you thinking what I am thinking? Well, probably, and not only that, but later, the AI also discovered that it can be pushed near a wall and be used as a ramp, and, tadaa! Got’em! Then, it was up to the hiders again to invent something new. So, did they do that? Can this crazy strategy be defeated? Well, check this out. These resourceful little critters learned that since there is a little time at the start of the game when the seekers are frozen, apparently, during this time, they cannot see them, so why not just sneak out and steal the ramp, and lock it away from them. Absolutely incredible. Look at those happy eyes as they are carrying that ramp. But today is not 2019, it is 2021, so I wonder what scientists at the other amazing AI lab, DeepMind have been up to. Can this paper be topped? Well, believe it or not, they have managed to create something that is perhaps even crazier than this. This new paper proposes that these AI agents look at the screen, just like a human would, and engage in open-ended learning where the tasks are always changing. What does this mean? Well, it means that these agents are not preparing for an exam. They are preparing for life! And hence, hopefully they learn more general concepts, and, as a result, maybe excel at a variety of different tasks. Even better, these scientists at DeepMind claim that their AI agents not only excel at a variety of tasks, but they excel at new ones they have never seen before! Those are big words, so, let’s see the results! The red agent here is the hider, and the blue is the seeker. They both understand their roles, the red agent is running, and the blue is seeking. Look, its viewing direction is shown with this lightsaber-looking line pointing at the red agent. No wonder it is running away! And, look, it manages to get some distance from the seeker, and finds a new, previously unexplored part of the map and hides there. Excellent. And you would think that the Star Wars references end here? No! Not even close. Look, in a more advanced variant of the game, this green seeker lost the two other hiders, and what does he do. Ah yes, of course, grabs his lightsaber, and takes the high ground. Then, it spots the red agent and starts chasing it. All all this without ever having played this game before. That is excellent. In this cooperative game, the agents are asked to get as close to the purple pyramid as they can. Of course, to achieve that, they need to build a ramp. Which they successfully realize. Excellent. But it gets better! Now note that we did not say that the task is to build a ramp. The task is to get as close to the purple pyramid as we can. Does that mean that? …Yes, yes it does. Great job bending the rules, little AI! In this game, the agent is asked to stop the purple ball from touching the red floor. At first, it tries its best to block the rolling of the ball with its body, then, look! It realizes that it is much better to just push it against the wall. And it gets even better, look, it learned that best is to just chuck the ball behind this slab. It is completely right, this needs no further energy expenditure, and the ball never touches the red floor again. Great! And finally, in this King of the Hill game, the goal is to take the white floor and get the other agent out of there. As they are playing this game for the first time, they have no idea where the white floor is. As soon as the blue agent finds it, it stays there…so far so good. But, this is not a cooperative game, we have an opponent here. Look! Boom! A quite potent opponent indeed who can take the blue agent out, and, it understands that it has to camp in there and defend the region. Again. Awesome! So, the goal here is not to be an expert in one game, but to be a journeyman in many games. And these agents are working really well at a variety of games without ever having played them. So, in summary, OpenAI’s agent expert in a narrower domain. DeepMind’s agent journeyman in a broader domain. Two different kinds of intelligence. Both doing amazing things. Loving it. What a time to be alive! Scientists at DeepMind have knocked it out of the park with this one. They have also published AlphaFold this year, a huge breakthrough that makes an AI predict protein structures. Now, I saw some of you asking why we didn’t cover it. Is it not an important work? Well, quite the opposite! I am spellbound by it and I think that paper is a great gift to humanity, however. I try my best to to educate myself on this topic, however, I don’t feel that I am qualified to speak about it. Not yet anyway. So, I think it is best to let the experts who know more about this take the stage! This is, of course, bad for views, but no matter, we are not maximizing views here. We are maximizing meaning. Thanks for watching and for your generous support, and I'll see you next time!"
522,This Magical AI Cuts People Out Of Your Videos!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see magical things that  open up when we are able to automatically find   the foreground and the background of a video. Let’s see why that matters! This new technique leans on a previous method  to find the boy, and the dog. Let’s call this   level 1 segmentation. So far so good, but  this is not the state of the art. Yet.  Now, comes level 2 it also found the shadow  of the boy, and, the shadow of the dog.   Now we’re talking! But it doesn’t stop there!   It gets even better. Level three, this  is where things get out of hand look,   the dog is occluding the boy’s shadow,  and it is able to deal with that too. So, if we can identify all of the effects that  are attached to the boy and the dog, what can   we do with all this information? Well, for  instance, we can even remove them from the video.   Nothing to see here. Now, a common problem  is that still, the silhouette of the subject   still remains in the final footage,  so let’s take a close look together!   I don’t see anything at all. Wow. Do  you? Let me know in the comments below! Just to showcase how good this removal is, here  is a good technique from just one year ago.   Do you see it? This requires the  shadows to be found manually,   so we have to work with that. And still, in the  outputs, you can see the silhouette we mentioned.   And, how much better is the new method?  Well, it finds the shadows automatically.   That is already mind blowing, and the outputs  are…yes, much cleaner. Not perfect, there is still   some silhouette action, but if I were not actively  looking for it, I might not have noticed it. It can also remove people  from this trampoline scene,   and not only the bodies, but it also removes  their effect on the trampolines as well. Wow. And as this method can perform  all this reliably, it opens up   the possibility for new, magical effects. For instance, we can duplicate this test subject,   and even fade it in and out. Note that it  has found its shadows as well. Excellent! So, it can deal with finding not only  the shape of the boy and the dog,   and it knows that it’s not enough to just find  their silhouettes, but it also has to find   additional effects they have on the footage.  For instance, their shadows. That is wonderful,   and what is even more wonderful is that this  was only one of the simpler things it could do. Shadows are not the only  potential correlated effects,   look. A previous method was able to find the  swan here, but that’s not enough to remove it,   because it has additional effects on the scene.  What are those? Well, look, it has reflections,   and it creates ripples too. This is so much  more difficult than just finding shadows.   And now, let’s see the new method..and! Whoa.  It knows about the reflections and ripples,   finds both of them, and gives us this  beautifully clean result. Nothing to see here. Also, look at this elephant. Removing  just the silhouette of the elephant   is not enough, it also has to  find all the dust around it,   and it gets worse, the dust is changing  rapidly over time. And believe it or not…wow,   it can find the dust too, and remove the  elephant. Again, nothing to see here. And if you think that this dust was the new  algorithm at its best, then have a look at this   drifting car. Previous method. Yes, that is the  car, but you know what I want. I want the smoke   gone too. That’s probably impossible, right?  Well, let’s have a look. Wow. I can’t believe   it. It grabbed and removed the car and the smoke  together…and, once again, nothing to see here. So, what are those more magical things  that this opens up? Watch carefully…it   can make the colors pop here. And, remember, it  can find the reflections of the flamingo, so, it   keeps not only the flamingo, but the reflection of  the flamingo in color as well. Absolutely amazing. And, if we can find the background of a video,  we can even change the background. This works   even in the presence of a moving  camera, which is a challenging problem.   Now, of course, not even this technique is  perfect look here. The reflections are copied   off of the previous scene,  and it shows on the new one. So what do you think? What would  you use this technique for?   Let me know in the comments, or if you wish  to discuss similar topics with other Fellow   Scholars in a warm and welcoming environment,  make sure to join our Discord channel. Also,   I would like to send a big thank you to the mods  and everyone who helps running this community.   The link to the server is available in  the video description. You’re invited. Thanks for watching and for your generous  support, and I'll see you next time!"
523,This AI Learned Boxing…With Serious Knockout Power!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see an AI learn boxing and even mimic gorillas during this process. Now, in an earlier work, we saw a few examples of AI agents playing two-player sports, for instance, this is the “You Shall Not Pass” game, where the red agent is trying to hold back the blue character and not let it cross the line. Here you see two regular AIs duking it out, sometimes the red wins, sometimes the blue is able to get through. Nothing too crazy here. Until…this happens. Look. What is happening? It seems that this agent started to do nothing…and still won. Not only that, but it suddenly started winning almost all the games. How is this even possible? Well, what the agent did is perhaps the AI equivalent of hypnotizing the opponent, if you will. The more rigorous term for this is that it induces off-distribution activations in its opponent. This adversarial agent is really doing nothing, but that’s not enough it is doing nothing in a way that reprograms its opponent to make mistakes and behave close to a completely randomly acting agent! Now, this new paper showcases AI agents that can learn boxing. The AI is asked to control these joint-actuated characters which are embedded in a physics simulation. Well, that is quite a challenge look, for quite a while after 130 million steps of training, it cannot even hold it together. And, yes…these folks collapse. But this is not the good kind of hypnotic adversarial collapsing. I am afraid, this is just passing out without any particular benefits. That was quite a bit of training, and all this for nearly nothing. Right? Well, maybe…let’s see what they did after 200 million training steps. Look! They can not only hold it together, but they have a little footwork going on, and can circle each other and try to take the middle of the ring. Improvements. Good. But this is not dancing practice, this is boxing. I would really like to see some boxing today and it doesn’t seem to happen. Until we wait for a little longer…which is 250 million training steps. Now, is this boxing? Not quite, this is more like two drunkards trying to duke it out, where neither of them knows how to throw a real punch…but! Their gloves are starting to touch the opponent, and they start getting rewards for it. What does that mean for an intelligent agent? Well, it means that over time, it will learn to do that a little better. And hold on to your papers and see what they do after 420 million steps. Oh wow! Look at that! I am seeing some punches, and not only that, but I also see some body and head movement to evade the punches, very cool. And if we keep going for longer, whoa! These guys can fight! They now learned to perform feints, jabs, and have some proper knockout power too. And if you have been holding on to your papers, now, squeeze that paper, because all they looked at before starting the training was 90 seconds of motion capture data. This is a general framework that also works for fencing as well. Look! The agents learned to lunge, deflect, evade attacks, and more. Absolutely amazing. What a time to be alive! So, this was approximately a billion training steps, right. So how long did that take to compute? It took approximately a week. And, you know what’s coming. Of course, we invoke the First Law Of Papers, which says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. And two more papers down the line, I bet this will be possible in a matter of hours. This is the part with the gorillas. It is also interesting that even though there were plenty of reasons to, the researchers didn’t quit after a 130 million steps. They just kept on going, and eventually, succeeded. Especially in the presence of not so trivial training curves where the blocking of the other player can worsen the performance, and it’s often not as easy to tell where we are. That is a great life lesson right there. Thanks for watching and for your generous support, and I'll see you next time!"
524,This AI Helps Making A Music Video!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to make some crazy synthetic music videos. In machine learning research, view synthesis papers are on the rise these days. These techniques are also referred to as NERF variants, which is a learning-based algorithm tries that to reproduce real-world scenes from only a few views. It is very challenging, look! In go a bunch of photos of a scene, and the method has to be able to synthesize new photorealistic images between these photos. But this is not the only paper in this area, researchers are very aware of the potential here, and thus, a great number of NERF variants are appearing every month. For instance, here is a recent one that extends the original technique to handle shiny and reflective objects better. So, what else is there to do here? Well, look here. This new one demands not a bunch of photos from just one camera, but from 16 different cameras. That’s a big ask. But, in return, the method now has tons of information about the geometry and the movement of these test subjects, so, is it intelligent enough to make something useful out of it? Now, believe it or not, this, in return, can not only help us look around in the scene, but even edit it in three new ways. For instance, one, we can change the scale of these subjects, add and remove them from the scene, and even copy-paste them. Excellent for creating music videos. Well, talking about music videos. Do you know what is even more excellent for those? Retiming movements…that is also possible. This can, for instance, improve an okay dancing performance into an excellent one. And three, because now we are in charge of the final footage, if the original footage is shaky, well, we can choose to eliminate that camera shake. Game changer. Still, it’s not quite the hardware requirement where you just whip out your smartphone and start nerfing and editing, but for what it can do, it really does not ask for a lot. Look, if we wish to, we can even remove some of those cameras and still expect reasonable results. We lose roughly a decibel of signal per camera. Here is what that looks like. Not too shabby! And all this progress just one more paper down the line. And I like the idea behind this paper a great deal because typically what we are looking for in a followup paper is trying to achieve similar results while asking for less data from the user. This paper goes into the exact other direction, and asks what amazing things could be done if we had more data instead. Loving it. And with that, not only neural view synthesis, but neural scene editing is also possible. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
525,This AI Creates Virtual Fingers!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how we can use our hands, but not our fingers to mingle with objects in virtual worlds. The promise of virtual reality, VR is indeed truly incredible. If one day it comes to fruition, doctors could be trained to perform surgery in a virtual environment, expose astronauts to virtual zero-gravity simulations, work together with telepresence applications, you name it. The dream is getting closer and closer, but something is still missing! For instance, this previous work uses a learning-based algorithm to teach a head-mounted camera to tell the orientation of our hands at all times. One more paper down the line, this technique appeared that can deal with examples with challenging hand-hand interactions, deformations, lots of self-contact and self-occlusion. This was absolutely amazing, because these are not gloves. No-no. This is the reconstruction of the hand by the algorithm. Absolutely amazing. However, it is slow, and mingling with other objects is still, quite limited. So, what is missing? What is left to be done here? So, let’s have a look at today’s paper and find out together. This is its output…yes, mingling that looks very natural. But, what is so interesting here? The interesting part is that it has realistic finger movements. Well, that means, that it just reads the data from sensors on the fingers, right? Now, hold on to your papers, and we’ll find out once we look at the input…oh my! Is this really true? No sensors on the fingers anywhere! What kind of black magic is this? And with that, we can now make the most important observation in the paper: it reads information from only the wrist and the objects in the hand. Look, the sensors are on these gloves, but none are on the fingers. Once again: the sensors have no idea what we are doing with our fingers, it only reads the movement of our wrist and the object, and all the finger movement is synthesized by it automatically. Whoa! And, with this, we can not only have a virtual version of our hand, but we can also manipulate virtual objects with very few sensor readings. The rest is up to the AI to synthesize. This means that we can have a drink with a friend online, use a virtual hammer to, depending on our mood, fix or destroy virtual objects. This is very challenging because the finger movements have to follow the geometry of the object. Look, here, the same hand is holding different objects, and the AI knows how to synthesize the appropriate finger movements for both of them. This is especially apparent when we change the scale of the object. You see, the small one requires small and precise finger movements to turn around, these are motions that need to be completely re-synthesized for the bigger objects. So cool. And now comes the key so, does this only work on objects that it has been trained on? No, not at all! For instance, the method has not seen this kind of teapot before, and still, it knows how to use its handle, and now to hold it from the bottom too, even if both of these parts look different. Be careful though, who knows, maybe virtual teapots can get hot too! What’s more, it also handles the independent movement of the left and right hands. Now, how fast is all this? Can we have coffee together in virtual reality? Yes, absolutely! All this runs in close to real time! There is a tiny bit of delay though, but, a result like this is already amazing, and this is typically the kind of thing that can be fixed one more paper down the line. However, not even this technique is perfect. It still might miss small features on an object. For instance, a very thin handle might confuse it. Or, if it has an inaccurate reading of the hand pose and distances, this might happen. But for now, having a virtual coffee together…yes please, sign me up! Thanks for watching and for your generous support, and I'll see you next time!"
526,Watch Tesla’s Self-Driving Car Learn In a Simulation!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how Tesla uses no less than a simulated game world to train their self-driving cars. And more. In their AI day presentation video, they really put up a clinic of recent AI research results and how they apply them to develop self-driving cars. And of course, there is plenty of coverage of the event, but, as always, we are going to look at it from different angle. We’re doing it Papers style. Why? Because after nearly every Two Minute Papers episode where we showcase an amazing paper, I get a question saying something like “okay, but when do I get to see or use this in the real world?”. And rightfully so, that is a good question. And in this presentation, you will see that these papers that you see here get transferred into real-world products so fast, it really makes my head spin. Let’s see this effect demonstrated by looking through their system. Now, first, their cars have many cameras, no depth information, just the pixels from these cameras, and one of their goals is to create this vector space view that you see here. That is almost like a map, or a video game version of the real roads and objects around us. That is a very difficult problem. Why is that? Because the car has many cameras. Is that a problem? Yes… kind of. I’ll explain in a moment. You see, there is a bottom layer that processes the raw sensor data from the cameras mounted on the vehicle. So here, in go the raw pixels, and out comes more useful, high-level information that can be used to determine whether this clump is pixels is a car or a traffic light. Then, in the upper layers, this data can be used for more specific tasks, for instance, trying to estimate where the lanes and curbs are. So, what papers are used to accomplish this? Looking through the architecture diagrams, we see, transformer neural networks, BiFPNs, and Regnet. All papers from the last few years. For instance, RegNet is a neural network variant that is great at extracting spatio-temporal information from the raw sensor data. And that is a paper from 2020. From just one year ago. Already actively used in training self-driving cars. That is unreal. Now, we mentioned that having many cameras is a bit of a problem. Why is that? Isn’t that supposed to be a good thing? Well, look! Each of the cameras only sees parts of the truck. So how do we know where exactly it is, and how long it is? We need to know all of this information to be able to accurately put the truck into the vector space view. What we need for this is a technique that can fuse information from many cameras together intelligently. Note that this is devilishly difficult due to each of the cameras having a different calibration, location, view directions, and other properties. So who is to tell that a point here corresponds to which point in a different camera view? And this is accomplished through, yes…a transformer neural network. A paper from 2017. So, does this multi-camera technique work? Does this improve anything? Well, let’s see! Oh yes, the yellow predictions here are from the previous single-camera network, and as you see, unfortunately, things flicker in and out of existence. Why is that? It is because a passing car is leaving the view of one of the cameras, and as it enters the view of the next one, they don’t have this correspondence technique that would say where it is exactly. And, look! The blue objects show the prediction of the multi-camera network that can do that, and things aren’t perfect, but they are significantly better the single-camera network. That is great, however, we are still not taking into consideration time. Why is that important? Let’s have a look at two examples. One, if we are only looking at still images and not take into consideration how they change over time, how do we know if this car is stationary? Is it about to park somewhere? Or, is it speeding? Also, two, this car is now occluded but we saw it second ago, so we should know what it is up to. That sounds great. And what else can we do if our self-driving system has a concept of time? Much like humans do, we can make predictions. These predictions can take place both in terms of mapping what is likely to come, an intersection, a roundabout, and so on. But, perhaps even more importantly, we can also make predictions about vehicle behavior. Let’s see how that works. The green lines show how far away the next vehicle is, and how fast it is going. This green line tells us the real, true information about it. Do you see the green? No? That’s right, it is barely visible, because it is occluded by a blue line, which is the prediction of the new video network. That means that its predictions are barely off from the real velocities and distances, which is absolutely amazing. And, as you see with orange, the old network that was based on single images is off by quite a bit. So now, a single car can make a rough map of its environment wherever it drives, and they can also stitch the readings of multiple cars together into an even more accurate map. Putting this all together, these cars have a proper understanding of their environment and this makes navigation much easier. Look at those crisp, temporally stable labelings. It has very little flickering. Still, not perfect by any means, but this is remarkable progress in so little time. And we are at the point where predicting the behaviors of other vehicles and pedestrians can also lead to better decision making. But, we are still not done yet. Not even close. Look! The sad truth of driving is that unexpected things happen. For instance, this truck makes it very difficult for us to see, and the self-driving system does not have a lot of training data to deal with that. So, what is a possible solution to that? There are two solutions. One is fetching more training data. One car can submit an unexpected event and request that the entire Tesla fleet sends over if they have encountered something similar. Since there are so many of these cars on the streets, tens of thousands of similar examples can be fetched from them, and added to the training data to improve the entire fleet. That is mind blowing. One car encounters a difficult situation, and then, every car can learn from it. How cool is that? That sounds great. So what is the second solution? Not fetching more training data, but creating more training data. What, just make stuff up? Yes, that’s exactly right. And if you think that is ridiculous, and are asking how could that possibly work? Well, hold on to your papers, because it does work… you are looking at it right now! Yes, this is a photorealistic simulation that teaches self-driving cars to handle difficult corner cases better. In the real world, we can learn from things that already happened, but in a simulation, we can make anything happen. This concept really works, and is one of my favorite examples is OpenAI’s robot hand that we have showcased earlier in this series. This also learns the rotation techniques in a simulation, and it does it so well, that the software can be uploaded to a real robot hand, and it will work in real situations too. And now, the same concept for self-driving cars. Loving it. With these simulations, we can even teach these cars about cases that would otherwise be impossible or unsafe to test. For instance, in this system, the car can safely learn what it should do if it sees people and dogs running on the highway. A capable artist can also create miles and miles of these virtual locations within a day of work. This simulation technique is truly a treasure trove of data, because it can also be procedurally generated, and the moment the self-driving system makes an incorrect decision, a Tesla employee can immediately create an endless set of similar situations to teach it. Now, I don’t know if you remember, we talked about a fantastic paper a couple months ago that looked at real-world videos, then, took video footage from a game, and improved it to look like the real world. Convert video games to reality if you will. This had an interesting limitation. For instance since the AI was trained on the beautiful lush hills of Germany and Austria, it hasn’t really seen the dry hills of LA. So, what does it do with them? Look, it redrew the hills the only way it saw hills exist, which is, covered with trees. So, what does this have to do with Tesla’s self-driving cars? Well, if you have been holding on to your papers so far, now, squeeze that paper, because they went the other way around! Yes, that’s right! They take the video footage of a real, unexpected event where the self-driving system failed, use their automatic labeler used for the vector space view, and what do they make out of it? A video game version! Holy mother of papers. And, in this video game, it is suddenly much easier to teach the algorithm safely. You can also make it easier, harder, replace a car with a dog, or a pack of dogs, and make many similar examples so that the AI can learn from these “what if” situations as much as possible. So, there you go. Full tech transfer into a real AI system in just a year or two. So, yes, the papers you see here are for real. As real as it gets. And yes, the robot is not real, just a silly joke. For now. And two more things that make all this even more mind-blowing. One, remember, they don’t showcase the latest and greatest that they have. Just imagine that everything that you heard today is old news compared to the tech they have now. And two, we have only looked at just one side of what is going on, for instance, we haven’t even talked about their amazing Dojo chip. And if all this comes to fruition, we will be able to travel cheaper, more relaxed, and also, perhaps most importantly, safer. I can’t wait. I really cannot wait. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
527,OpenAI Codex: An AI That Writes Video Games!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see if an AI can become a good software engineer. Spoiler alert, the answer is: yes, kind of. Let me explain. Just one year ago, scientists at OpenAI published a technique by the name GPT-3, and it is an AI that was unleashed to read the internet with the sole task of finishing your sentences. So, what happened then? Well, now we know that of course, it learned whatever it needed to learn to perform the sentence completion properly. And to do this, it would need to learn English by itself, and that’s exactly what it did! It also learned about a lot of topics to be able to discuss them well. For instance, we gave it a try, and I was somewhat surprised when I saw that it was able to continue a Two Minute Papers script, even though it seems to have turned into a history lesson. It also learned how to generate properly formatted plots from a tiny prompt written in plain English. Not just one kind many kinds! And remember, this happened just about a year ago, and this AI was pretty good at many things. But soon after, a newer work was published by the name Image-GPT. What did this do? Well, this was a GPT-variant that could not finish your sentences, but your images. Yes, really. The problem statement is simple: we give it an incomplete image, and we ask the AI to fill in the missing pixels. Have a look at this water droplet example. We humans, know that since we see the remnants of some ripples over there too, there must be a splash, but does the AI know? Oh yes, yes it does! Amazing! And this is the true image for reference. So, what did they come out with now? Well, the previous GPT-3 was pretty good at many things, and this new work, OpenAI Codex is a GPT language model that was fine-tuned to be excellent at one thing. And that is, writing computer programs, or, finishing your code. Sounds good! Let’s give it a try. First, please write a program that says hello world five times. It can do that. And, we can also ask it to create a graphical user interface for it. No coding skills required. That’s not bad by any means, but this is OpenAI we are talking about, so I am sure it can do even better. Let’s try something a tiny bit more challenging. For instance, writing a simple space game. First, we get an image of a spaceship that we like, instruct the algorithm to resize and crop it. And here comes one of my favorites: start animating it. Look, it immediately wrote the appropriate code where it will travel with a prescribed speed, and yes, it should get flipped as soon as it hits the wall. Looks good. Will it work? Let’s see. It does. And all this from a written English description. Outstanding. Of course, this is still not quite the physics simulation that you all see and love around here, but I’ll take it. But this is still not a game, so please, add a moving asteroid, check for collisions, and infuse the game with a scoring system. There we go. So, how long did all this take? And now, hold on to your papers, because this game was written in approximately 9 minutes. No coding knowledge is required. Wow. What a time to be alive! Now, in this 9-ish minutes, most of the time was not spent by the AI thinking, but the human typing. So, still, the human is the bottleneck. But, today, with all the amazing voice recognition systems that we have, we don’t even need to type these instructions. Just say what you want and it will be able to do it! So, what else can it do? For instance, it can also deal with similar requests to what software engineers are asked in interviews, and I have to say, the results indicate that this AI would get hired to some places. But that’s not all, it can also nail a first-grade math test. An AI. Food for thought. Now, this OpenAI Codex work has been out there for a few days now, and I decided to not cover it immediately, but wait a little and see where the users take it. This is, of course, not great for views, but no matter, we are not maximizing views, we are maximizing meaning. In return, now, there are some examples out there in the wild. Let’s look at three of them. One, it can be asked to explain a piece of code, even if it is written in assembly. Two, it can create a pong game in 30 seconds. Remember, this used to be a blockbuster Atari game, and now, an AI can write it in half a minute. And yes, again, most of the half minute is taken by waiting for the human for instructions. Wow. It can also create a plugin for Blender, an amazing free 3D modeler program. These things used to take several hours of work at the very least. And with that, I feel that what I said for GPT-3 rings even more true today. I am replacing GPT-3 with Codex, and quoting: “The main point is that working with Codex is a really peculiar process where we know that a vast body of knowledge lies within, but it only emerges if we can bring it out with properly written prompts. It almost feels like a new kind of programming that is open to everyone, even people without any programming or technical knowledge. If a computer is a bicycle for the mind, then Codex is a fighter jet.” And all this progress, in just one year. I cannot wait to see where you Fellow Scholars will take it, and what OpenAI has in mind for just one more paper down the line. And until then, software coding might soon be a thing anyone can do. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
528,Can An AI Design A Good Game Level?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Testing modern computer games by using an AI is getting more and more popular these days. This earlier work showcased how we can use an automated agent test the integrity of the game by finding spots where we can get stuck. And when we fixed the problem, we could easily ask the agent to check whether the fix really worked. In this case, it did! And this new work also uses learning algorithms to test our levels. Now this chap has been trained on a fixed level, mastered it, and let’s see if it has managed to obtain general knowledge from it. How? Well, by testing how it performs on a different level. It is very confident, good…but..uh-oh! As you see, it is confidently incorrect. So, is it possible to train an agent to be able to beat these levels more reliably? Well, how about creating a more elaborate curriculum for them to learn on. Yes, let’s do that…but, with a twist! In this work, the authors chose not to feed the AI a fixed set of levels…no-no! They created another AI that builds the levels for the player AI. So, both the builder and the player are learning algorithms, who are tasked to succeed together in getting the agent to the finish line. They have to collaborate to succeed. Building the level means choosing the appropriate distance, height, angle and size for these blocks. Let’s see them playing together on an easy level. Okay, so far so good, but let’s not let them build a little cartel where only easy levels are being generated so they get a higher score. I want to see a challenge! To do that, let’s force the builder AI to use a larger average distance between the blocks, thereby creating levels of a prescribed difficulty. And with that, let’s ramp up the difficulty a little. Things get a little more interesting here, because… whoa! Do you see what I see here? Look! It even found a shortcut to the end of the level. And, let’s see the harder levels together. While many of these chaps failed, some of them are still able to succeed. Very cool! Let’s compare the performance of the new technique with the previous, fixed track agent. This is the chap that learned by mastering only a fixed track. And this one learned in the wilderness. Neither of them have seen these levels before. So, who is going to be scrappier? Of course, the wilderness guy described in the new technique. Excellent. So, all this sounds great, but I hear you asking the key question here: what do we use these for? Well, one, the player AI can test the levels that we are building for our game and give us feedback on whether it is possible to finish, is it too hard or too easy, and more. This can be a godsend when updating some levels, because the agent will almost immediately tell us whether it has gotten easier or harder, or if we have broken the level. No human testing is required. Now, hold on to your papers, because the thing runs so quickly that we can even refine a level in real time. Loving it. Or two, the builder can also be given to a human player who might enjoy a level being built in real time in front of them. And here comes the best part. The whole concept generalizes well for other kinds of games too. Look, the builder can build race tracks, and the player can try to drive through them. So, do these great results also generalize to the racing game? Let’s see what the numbers say. The agent that trained on a fixed track can succeed on an easy level about 75% of the time, while the newly proposed agent can do it nearly with a 100% chance. A bit of an improvement, okay. Now, look at this. The fixed track agent can only beat a hard level about 2 times out of 10, while the new agent can do it about six times out of ten. That is quite a bit of an improvement. Now, note that in a research paper, choosing a proper baseline to compare to is always a crucial question. I would like to note that the baseline here is not the state of the art, and with that, it is a little easier to make the new solution pop. No matter, the solutions are still good, but I think this is worth a note. So, from now on, whenever we create a new level in a computer game, we can have hundreds of competent AI players testing it in real time. So good! What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
529,This AI Makes Digital Copies of Humans!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to look at a  paper with two twists. You know what,   I’ll give you twist number one right away. This  human isn’t here. This human isn’t here either.   And, neither is this human here.   Now, you are probably asking Károly,  what are you even talking about? Now hold on to your papers,  because I am talking about this.   Look. This is the geometry of this virtual  human inside a virtual world. Whoa. Yes,   all of these people are in a synthetic video, and  in a virtual environment that can be changed with   a simple click. And more importantly, as we change  the lighting or the environment, it also simulates   the effect of that environment on the character,  making it look like they are really there. So that sounds good, but how do we take  a human and make a digital copy of them? Well, first, we place them in a capture  system that contains hundreds of led lights   and an elaborate sensor for capturing  depth information. Why do we need these?   Well, all this this gives the system plenty  of data on how the skin, hair and the clothes   reflect light. And, at this point, we know  everything we need to know and can now proceed   and place our virtual copy in a computer  game or even a telepresence meeting. Now, this is already amazing, but two things  really stick out here. One you will see   when you look at this previous competing work.  This had a really smooth output geometry,   which means that only few high-frequency  details were retained. This other work was   better at retaining the details, but, look, tons  of artifacts appear when the model is moving. And, what does the new one look like?  Is it any better? Let’s have a look.   Oh my! We get tons of fine details, and  the movements have improved significantly.   Not perfect by any means, look here,  but still, an amazing leap forward. Two, the other remarkable thing here  is that the results are so realistic   that objects the in virtual scene can cast a  shadow on our model. What a time to be alive! And now, yes, you remember that I promised  two twists. Where is twist number two? Well,   it has been here all along for the entirety of  this video. Have you noticed? Look, all this   is from 2019. From two years ago. Two years is  a long time in machine learning and computer   graphics research, and I cannot wait to see how it  will be improved two more papers down the line. If   you are excited too, make sure to subscribe and  hit the bell icon to not miss it when it appears. Thanks for watching and for your generous  support, and I'll see you next time!"
530,The Tale Of The Unscrewable Bolt!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. I have to say we haven’t had a simulation paper in  a while so today’s episode is going to be my way   of medicating myself. You are more  than welcome to watch the process. And this paper is about performing collision  detection. You see, when we write a simple   space game, detecting whether a collision has  happened or not is a mostly trivial endeavor.   However, now, instead, let’s look  at the kind of simulation complexity   that you are expecting from a Two Minute Papers  video…first, let’s try to screw this bolt in   using an industry standard simulation  system, and it is…stuck. Hm…why? Because here, we would need to simulate in  detail, not only whether two things collide,   they collide all the time, but, we need to check  for and simulate friction too! Let’s see what this   new simulation method does with the same scene.  And…oh yes! This one isn’t screwing with us and   does the job perfectly. Excellent. However, this  was not nearly the most complex thing it can do.   Let’s try some crazy geometry, with  crazy movements and tons of friction. There we go. This one will do. Welcome  to the expanding lock box experiment. So,   what is this? Look, as we turn the key,  the locking pins retract, and the bottom   is now allowed to fall. This scene contains  tens to hundreds of thousands of contacts,   and yet, it still works perfectly. Beautiful.  I love this one because with this simulation,   we can test intricate mechanisms for robotics and  more before committing to manufacturing anything.   And, unlike with previous methods, we don’t need  to worry whether the simulation is correct or not,   and we can be sure that if we 3D print this,  it will behave exactly this way. So good! Also, here come some of my favorite  experiments from the paper.   For instance, it can also simulate a  piston attached to a rotating disk, or,   smooth motion on one wheel, leading to  intermittent motion on the other one.   And, if you feel the urge to build a  virtual bike, do not worry for a second,   because your chain and sprocket mechanisms  will work exactly as you expect to. Loving it. Now, interestingly, look here. The time step size  used with the new technique is a hundred times   bigger, which is great, we can advance the time  in bigger pieces when computing the simulation.   That is good news indeed. However, every time we  do so, we still have to compute a great deal more.   The resulting computation time is still at least  a hundred times slower than previous methods.   However, those methods don’t count, at least  not on these scenes, because they have produced   incorrect results. Look at it some other way -  this is the fastest simulator that actually works. Still, it is not that slow. The one  with the intermittent motion takes   less than a second per time step, which  likely means a few seconds per frame,   while the bolt screwing scene is likely in the  minutes per frame domain. Very impressive! And,   if you are a seasoned Fellow Scholar, you  know what’s coming, this is where we invoke   the First Law Of Papers, which says that research  is a process. Do not look at where we are,   look at where we will be two more papers down the  line. And two more papers down the line, I am sure   even the more complex simulations will be done  in a matter of seconds. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
531,NVIDIA’s New Technique: Beautiful Models For Less!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how crazy good NVIDIA’s  new system is at simplifying virtual objects. These objects are used to create photorealistic  footage for feature-length movies, virtual worlds,   and more. But, here comes the problem.  Sometimes these geometries are so detailed   they are prohibitively expensive to store and  render efficiently. Here are some examples from   one of our papers that were quite challenging to  iterate on and render. This took several minutes   to render and always ate all the memory in my  computer. So, what can we do if we would still   like to get crisp, high-quality geometry, but  cheaper and quicker? I’ll show you in a moment. This is part of a super complex scene. Get this,  it is so complex that it takes nearly a hundred   gigabytes of storage space to render just one  image of this and is typically for benchmarking   rendering algorithms. This is the Nurburgring  of light transport algorithms if you will. Well, hold on to your papers because  I said that I’ll show you in a moment   what we can do to get all this  at a more affordable cost, but,   in fact, you are looking at the  results of the new method right now. Yes, parts of this image  are the original geometry,   and other parts have already been simplified. So, which is which? Do you see the difference?  Please stop the video and let me know in the   comments below. I’ll wait. Thank you. So, let’s see together…yes,   this is the original geometry that requires over  5 billion triangles. And, this is the simplified   one, which…what? Can this really be? This uses  less than 1 percent of the number of triangles   compared to this. In fact, it’s less  than half a percent. That is insanity.   This really means that about every 200  triangles are replaced with just one triangle,   and it still looks mostly the same. That  sounds flat out impossible to me. Wow. So, how does this witchcraft  even work? Well, now, you see,   this is the power of differentiable rendering.  The problem formulation is as follows. We tell the   algorithm that here are the results that you need  to get, find the geometry and material properties   that will result in this. It runs all this by  means of optimization, which means that it will   have a really crude initial guess that doesn’t  even seem to resemble the target geometry.   But then, over time, it starts refining it,  and it gets closer and closer to the reference.   This process is truly a sight to behold.  Look how beautifully it is approximating   the target geometry. This looks very close,  and is much cheaper to store and render. I loved this example too. Previously, this differentiable rendering concept  has been used to be able to take a photograph,   and find a photorealistic material model  that we can put into our simulation program   that matches it. This work did very well with  materials, but, it did not capture the geometry. This other work did something similar  to this new paper, which means   that it jointly found the geometry and  material properties. But, as you see   high-frequency details were not as good as with  this one. You see here these details are gone.   And, now, just two years and one paper later, we  can get a piece of geometry that is so detailed   that it needs billions of triangles, and  it can be simplified two hundred to one. Now if even that is not enough, admittedly, it is  still a little rudimentary, but it even works for   animated characters. I wonder where we will  be two more papers down the line from here?   Wow. Scientists at NVIDIA knocked  it out of the park with this one.   Huge congratulations to the  team! What a time to be alive! So there you go, this was quite a ride, and I hope  you enjoyed it at least half as much as I did. And   if you enjoyed it at least as much as I did,  and are thinking that this light transport thing   is pretty cool, and you would like to learn more  about it, I held a Master-level course on this   topic at the Technical University of Vienna. Since  I was always teaching it to a handful of motivated   students, I thought that the teachings shouldn’t  only be available for the privileged few who can   afford a college education, no-no. The teachings  should be available for everyone. Free education   for everyone, that’s what I want. So, the  course is available free of charge for everyone,   no strings attached, so make sure to click the  link in the video description to get started.   We write a full light simulation  program from scratch there,   and learn about physics, the  world around us, and more. Thanks for watching and for your generous  support, and I'll see you next time!"
532,This AI Stuntman Just Keeps Getting Better!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how an AI can learn crazy stunts…from just one video clip. And if even that’s not enough, it can even do more. This agent is embedded in a physics simulation, and first, it looks at a piece of reference motion, like this one. And then, after looking, it can reproduce it. That is already pretty cool, but it doesn’t stop there. I think you know what’s coming…yes! Not only learning, but improving the original motion. Look, it can refine this motion a bit…and then, a bit more…and then, a bit more. And this just keeps on going, until…wait a second. Hold on to your papers…because this looks impossible! Are you trying to tell me that it’s improved the move so much, that it can jump through this? Yes, yes it does. Here is the first reproduction of the jump motion, and the improved version side by side. Whoa. The difference speaks for itself. Absolutely amazing. We can also give it this reference clip to teach it to jump from one box to another. This isn’t quite difficult. And now comes one of my favorites from the paper! And that is testing how much it can improve upon this technique. Let’s give it a try! It also learned how to perform a shorter jump, a longer jump…and now, oh yes, the final boss. Wow, it could even pull off this super long jump. It seems that this super bot can do absolutely anything! Well…almost. And, it can not only learn these amazing moves, but it can also weave them together so well, that we can build a cool little playground, and it gets through it with ease… well, most of it anyway. So at this point, I was wondering how general the knowledge is that it learns from these example clips? A good sign of an intelligent actor is that things can change a little and it can adapt to that. Now, it clearly can deal with a changing environment, that is fantastic, but do you know what else it can deal with? And now, if you have been holding on to your papers, squeeze that paper, because it can also deal with changing body proportions. Yes, really. We can put it in a different body, and it will still work. This chap is cursed with this crazy configuration, and can still pull off a cartwheel. If you haven’t been exercising lately, what’s your excuse now? We can also ask it to perform the same task with more or less energy, or to even apply just a tiny bit of force for a punch, or to go full Mike Tyson on the opponent. So how is all this wizardry possible? Well, one of the key contributions of this work is that the authors devised a method to search this space of motions efficiently. Since it does it in a continuous reinforcement learning environment, this is super challenging. At the risk of simplifying the solution, their method solves this by running both an exploration phase to find new ways of pulling off a move, and, with blue you see that when it found something that seems to work, it also keeps refining it. Similar endeavors are also referred to as the exploration-exploitation problem, and the authors proposed a really cool new way of handling it. Now, there are plenty more contributions in the paper, so make sure to have a look at it in the video description. Especially given that this is a fantastic paper, and a presentation is second to none. I am sure that the authors could have worked half as much on this project and this paper would still have been accepted, but they still decided to put in that extra mile. And I am honored to be able to celebrate their amazing work together with you Fellow Scholars. And, for now, an AI agent can look at a single clip of a motion, and can not only perform it, but it can make it better, pull it off in different environments, and it can even be put in a different body and still do it well. What  a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
533,This AI Learned Some Crazy Fighting Moves!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see if a virtual AI character can learn, or perhaps even invent these amazing signature moves. And this is a paper that was written by Sebastian Starke and his colleagues. He is a recurring scientist on this series, for instance, earlier he wrote this magnificent paper about Dribbling AI characters. Look. The key challenge here was that we were given only 3 hours of unstructured motion capture data. That is next to nothing, and from this next to nothing, it not only learned these motions really well, but it could weave them together, even when a specific movement combination was not present in this training data. But, as these motions are created by human animators, and may show at least three problems. One, the training data may contain poses that don’t quite adhere to the physics of a real human character. Two, it is possible that the upper body does something that makes sense, the lower body also does something that makes sense, but the whole thing, put together, does not make too much sense anymore. Or, three, we may have these foot sliding artifacts that you see here. These are more common than you might first think, here is an example of it from a previous work, and, look, nearly all of the previous methods struggle with it. Now, this new work uses 20 hours of unstructured training data. Now, remember, the previous one only used 3, so we rightfully expect that by using more information, it can also learn more. But the previous work was already amazing, so, what more can we really expect this new one to do? Well, it can not only learn these motions, weave together these motions like previous works, but, hold on to your papers, because it can now also come up with novel moves as well! Wow. This includes new attacking sequences, and combining already existing attacks with novel footwork patterns. And it does all this spectacularly well. For instance, if we show it how to have its guard up, and how to throw a punch, what will it learn? Get this, it will keep its guard up while throwing that punch. And it not only does that in a realistic, fluid movement pattern, but it also found out about something that has strategic value. Same with evading an attack with some head movement and counterattacking. Loving it. But how easy is it to use this? Do we need to be an AI scientist to be able to invoke these amazing motions? Well, if you have been holding on to your papers so far, now, squeeze that paper, and look here. Wow. You don’t need to be an AI scientist to play with this! Not at all! All you need is a controller to invoke these beautiful motions. And all this runs in real time. My goodness! For instance, you can crouch down and evade a potential attack by controlling the right stick, and launch a punch in the meantime. Remember, not only both halves have to make sense separately, the body motion has to make sense as a whole. And it really does, look at that. And here comes the best part, you can even assemble your own signature attacks, for instance, perform that surprise spinning backfist, an amazing spin kick, and, yes! You can even go full Karate Kid with that crane kick. And as a cherry on top, the characters can also react to the strikes, clinch, or even try a takedown. So, with that, there we go, we are again, one step closer to having access to super realistic motion techniques for virtual characters, and all we need for this is a controller. And remember, all this already runs in real time! Another amazing SIGGRAPH paper from Sebastian Starke. And get this, he is currently a 4th year PhD student and already made profound contributions to the industry. Huge congratulations on this amazing achievement. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
534,3D Modeling This Toaster Just Became Easier!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to build the best virtual toaster that you have ever seen. And it’s going to be so easy that it hardly seems possible. All this technique asks for is our input geometry to be a collection of parametric shapes. I’ll tell you in a moment what that is, but for now, let’s see that toaster. Hmm, this looks fine, but what if we feel that it is not quite tall enough? Well, with this technique, look! It can change the height of it, which is fantastic, but something else also happens! Look, it also understands the body’s relation to other objects that are connected to it. We can also change the location of the handle, the slits can be adjusted symmetrically, and when we move the toasts, it understands that it moves together with the handles. This is super useful, for instance, have a look at this train example, where if we change the wheels, it also understands that not only the wheels, but the wheel wells also have to change as well. This concept also works really well on this curtain. And all this means that we can not only dream up and execute these changes ourselves without having to ask a trained artist, but we can also do it super quickly and efficiently. Loving it. Just to demonstrate how profound and non-trivial this understanding of interrelations is, here is an example of a complex object. Without this technique, if we grab one thing, exactly that one thing moves, which is represented by one of these sliders changing here. However, if we would grab this contraption in the real world, not only one thing would move, nearly every part would move at the same time. So, does this new method know that? Oh wow, it does, look at that! And, at the same time, not one, but many sliders are dancing around beautifully. Now, I mentioned that the requirement was that the input object has to be a parametric shape, this is something that can be generated from intuitive parameters, for instance, we can generate a circle if we say what the radius of the circle should be. The radius would be the parameter here, and the resulting circle is hence a parametric shape. In many domains, this is standard procedure, for instance, many computer aided design systems work with parametric objects. But we are not done yet, not even close! It also understands how the brush sizes that we use relates to our thinking. Don’t believe it? Let’s have a look together! Right after we click, it detects that we have a small brush size, and therefore infers that we probably wish to do something with the handle, and, there we go! That is really cool. And now, let’s increase the brush size and click nearby, and, bam! There we go! Now it knows that we wish to interact with the drawer. Same with the doors. And, hold on to your papers, because to demonstrate the utility of their technique, the authors also made a scene just for us. Look! Nice this is no less than a Two Minute Papers branded chronometer! And here, we can change the proportions, the dial, the hands, whatever we wish, and it is so easy and showcases the utility of the new method so well. Now, I know what you’re thinking, let’s see it ticking…and…will it be two minutes? Well, close enough. Certainly, much closer to two minutes than the length of these videos, that is for sure. Thank you so much for the authors for taking time off their busy day just to make this. So, this truly is a wonderful tool because even a novice artist without 3D modeling expertise can apply meaningful changes to complex pieces of geometry. No trained artist is required! What a time to be alive! Now, this didn’t quite fit anywhere in this video, but I really wanted to show you this heartwarming message from Mark Chen, research scientist at OpenAI. This really showcases one of the best parts of my job, and that is when the authors of the paper come in and enjoy the results with you Fellow Scholars. Loving it. Thank you so much again! Also make sure to check out Yannic’s channel for cool in-depth videos on machine learning works. The link is available  in  the video description. Thanks for watching and for your generous support, and I'll see you next time!"
535,This AI Learned Physics...But How Good Is It?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to engage in the favorite pastime of the computer graphics researcher, which is…well, this. And this. And believe it or not, all of this is simulated through a learning-based technique. Earlier, we marveled at a paper that showed that an AI can indeed learn to perform a fluid simulation. And just one more paper down the line, the simulations it was able to perform extended to other fields, like structural mechanics, incompressible fluid dynamics, and more. And even better, it could even simulate shapes and geometries that it had never seen before. So today, the question is not whether an AI can learn physics. The question is how well can an AI learn physics? Let’s try to answer that question by having a look at our first experiment. Here is a traditional, handcrafted technique, and the new, neural network-based physics simulator. Both are doing fine so nothing to see he…whoa! What happened? Well, dear Fellow Scholars, this is when a simulation blows up. But the new one is still running, even when some traditional simulators blow up. That is excellent. But, we don’t have to bend over backwards to find other situations where the new technique is better than the previous ones. You see the reference simulation here, and it is all well and good that the new method does not blow up, but how accurate is it on this challenging scene? Let’s have a look. The reference shows a large amount of bending, where the head is roughly in line with the knees. Let’s memorize that. Head in line with the knees. Got it. Let’s see how the previous methods were able to deal with this challenging simulation. When simulating a system of smaller size…well, none of these are too promising. When we crank up the simulation domain size, the physical modal derivative, PMD in short does pretty well. So, what about the new method? Both bend quite well. Not quite perfect, remember, the head would have to go down to be almost in line with the knees. But, amazing progress nonetheless. This was a really challenging scene, and, in other cases, the new method is able to match the reference simulator perfectly. So far this sounds pretty good, but PMD seems to be a contender, and that, dear Fellow Scholars, is a paper from 2005. From 16 years ago! So why showcase this new work? Well, we have forgotten about one important thing. And here comes the key. The new simulation technique runs from 30 to almost 60 times faster than previous methods. Whoa. How is that even possible? Well, this is a neural network-based technique. And training a neural network typically takes a long time, but, we only need to do this once, and when we are done, querying this neural network typically can be done very quickly. Does this mean…yes! Yes it does. All this runs in real time for these dinosaur, bunny and armadillo scenes, all of which are built from about ten thousand triangles! And, we can play with them by using our mouse on our home computer. The cactus and hairball scenes require simulating not tens, but hundreds of thousands of triangles, so these took a bit longer as they are running between one and a half and two and a half frames per second. So, this is not only more accurate than previous techniques, not only more resilient than the previous techniques, but is also 30 to 60 timers faster at the same time. Wow. And just think about the fact that just a year ago, an AI could only perform low-resolution fluid simulations, then, a few months ago, more kinds of simulations. And then, today, just one more paper down the line, simulations of this complexity. And just imagine what we will be able to do just two more papers down the line! What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
536,Watch This Virtual Dinosaur Fall Into A Cactus!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to do this and this and this on a budget. Today, through the power of computer graphics research works, we can simulate all these amazing elastic interactions. If we are very patient that is, because they take forever to compute. But, if we wish to run these simulations quicker, what we can is increasing something that we call the time step size. Usually, this means that the simulation takes less time, but is also less accurate. Let’s see this phenomenon through a previous method from just a year ago. Here, we set the time step size relatively small, and drop an elastic barbarian ship onto these rods. And this is a challenging scene because the ship is made out of half a million tiny elements, and we have to simulate their interactions with the scene. How does this perform? Uh-oh. This isn’t good. Did you see the issues? Issue number one is that the simulation is unstable. Look. Things remain in motion when they shouldn’t. And two, this is also troubling. Penetrations. Now, let’s increase the time step size. What do we expect to happen? Well, now, we advance the simulation in bigger chunks, so we should expect to miss even more interactions in between these bigger steps. And…whoa! Sure enough. Even more instability, even more penetration. So, what is the solution? Well, let’s have a look at this new method and see if it can deal with this difficult scene. Now, hold on to your papers, and…wow, I am loving this. Issue number one, things coming to rest is solved, and, issue number two, no penetrations. That is amazing. Now, what is so interesting here? Well, what you see here should not be possible at all, because this new technique computes a reduced simulation instead. This is a simulation on a budget. And not only that, but let’s increase the time step size a little. This means that we can advance the time when in bigger chunks when computing the simulation, at the cost of potentially missing important interactions between these steps. Expect a bad simulation now, like with the previous one, and…wow, this is amazing. It still looks fine. But we don’t know that for sure, because we haven’t seen the reference simulation yet. So, you know what’s coming. Oh yes! Let’s compare it to the reference simulation that takes forever to compute. This looks great. And, let’s see… this looks great too. They don’t look the same, but if I were asked which one reference is, and which is the cheaper, reduced simulation, I am not sure if I would be able to tell. Are you able to tell? Well, be careful with your answer, because I have swapped the two. In reality, this is the reference, and this is the reduced simulation. Were you able to tell? Let me know in the comments below. And that is exactly the point! All this means that we got away with only computing the simulation in bigger steps. So, why is that good? Well, of course, we get through it quicker! Okay, but how much quicker? 110 times quicker. What? The two are close to equivalent, but this is more than a 100 times quicker? Sign me up, right away! Note that this is still not real time, but we are firmly in the seconds per frame domain, so we don’t need an all nighter for such a simulation, just a coffee break. Now note that this particular scene is really suited for the new technique, other scenes aren’t typically a hundred times faster, but, worst case scenario is when we throw around a bunch of furballs, but even that is at least ten to fifteen times faster. What does that mean? Well, an all-nighter simulation can be done maybe not during a coffee break, but during a quick little nap. Yes, we can rest like this tiny dinosaur for a while, and by the time we wake up, the simulation is done, and we can count on it being close to the real deal. So good! Just make sure to keep the friction high while resting here, or otherwise…this happens. So, from now on, we get better simulations, up to a hundred times faster. What  a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
537,Is Simulating Tiny Cloth Wrinkles Possible?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to create beautiful virtual clothes and marvel at how quickly we can simulate them. And the kicker is that these simulations are able to create details that are so tiny, they are smaller than a millimeter. When I first saw this title of this paper, I had two thoughts. First I asked, submillimeter level? Is this really necessary? Well, here is a dress rendered at the level of millimeters. And here is what it would look like with this new simulation technique. Hm, so many crisp details suddenly appeared. Okay, you got me, I am now converted. So let’s proceed to the second issue here I also said I will believe this kind of quality in a simulation when I see it. So, how does this work? Well, we can give it a piece of coarse input geometry, and this new technique synthesizes and simulates additional wrinkles on it. For instance, here, we can add these vertical shirring patterns to it. And not only that, but we can still manage collision detection, so it can interact with other objects and suddenly, the whole piece looks beautifully lifelike. And as you see here, the direction of these patterns can be chosen by us, and, it gets better, because it can handle other kinds of patterns too. I am a light transport researcher by trade, and I am very pleased by how beautifully these specular highlights are playing with the light. So good. Now, let’s see it in practice. On the left, you see the coarse geometry input, and on the right, you see the magical new clothes this new method can create from them. And, yes, this is it! Finally! I always wondered why virtual characters with simulated clothes always looked a little flat for many years now. These are the details that were missing. Just look at the difference this makes! Loving it. But, how does this stack up agains previous methods? Well, here is a previous technique from last year. And here is the new one. Okay. How do we know which one is really better? Well, we do it in terms of mathematics, and, when looking at the relative errors from a reference simulation, the new one is more accurate. Great. But, hold on to your papers, because here comes the best part! Whoa! It is not only more accurate, but it is blistering fast. Easily eight times faster than this previous method. So, where does this put us in terms of total execution time? At approximately one second per frame, often even less. What? One second per image for a crisp, submillimeter level cloth simulation that is also quite accurate? Wow. The pace of progress in computer graphics research is absolutely amazing. And just imagine what we will be able to do just two more papers down the line. This might run in real time easily. Now, drawbacks. Well, not really a drawback, but there are cases where the submillimeter simulation results materialize in a bit more crisp creases…but not much more. I really had to go to hunt for differences in this one. If you have spotted stark differences between the two here, please let me know in the comments. And here comes one more amazing thing: this paper was written by Huamin Wang and…that’s it. A single author paper that has been accepted to the SIGGRAPH conference, which is perhaps the most prestigious conference in computer graphics. That is a rare sight indeed. You see, sometimes even a single researcher can  make all the difference. Huge congratulations! Thanks for watching and for your generous support, and I'll see you next time!"
538,NVIDIA’s Stretchy Simulation: Super Quick!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to have a look at beautiful  simulations from a quick paper. But wait,   how can a research paper be quick?  Well, it is quick for two key reasons. Reason number one. Look at this complex soft body  simulation. This is not a jumpsuit, this showcases   the geometry of the outer tissue of this elephant,  and is made of 80 thousand elements. And now,   hold on to your papers, away with the geometry…and  feast your eyes upon this beautiful simulation!   My goodness, tons of stretching,  moving and deformation. Wow! So, how long do we have to wait for a  result like this? All-nighters, right? Well,   about that quick part I just mentioned…it runs  very, very quickly. 8 milliseconds per frame.   Yes, that means that it runs easily in  real time on a modern graphics card. And this work has some other aspect that is  also quick, which we will discuss in a moment.   But first, let’s see some of the additional  advantages it has compared to previous methods. For instance, if you think this was a stretchy  simulation, no-no! This is a stretchy simulation.   Look, this is a dragon. Well, it doesn’t  look like a dragon, does it. Why is that?   Well, it has been compressed  and scrambled into a tiny plane,   but if we let go of the forces. Ah, there it  is. It was able to regain its original shape.   And the algorithm can withstand even this sort  of torture test, which is absolutely amazing. One more key advantage is the lack of  volume dissipation. Yes. Believe it or not,   many previous simulation methods struggle  with things disappearing over time.   Don’t believe it? Let me show you this experiment   with gooey dragons and bowls. When using a  traditional technique, whoa, this guy is GONE. So, let’s see what a previous method would do  in this case. We start out with this block,   and after a fair bit of stretching..wait a  second. Are you trying to tell me that this   has the same amount of volume as this? No sir.  This is volume dissipation at its finest. So,   can the new method be so quick, and still retain  the entirety of the volume? Yes sir! Loving it. Let’s see another example  of volume preservation…okay,   I am loving this…these transformations are not  reasonable, this is indeed a very challenging   test. Can it withstand all this? Keep your eyes  on the volume of the cubes, which change a little,   it’s not perfect, but considering the crazy things  we are doing to it, this is very respectable. And   in the end, when we let go, we get most of it  back. I’ll freeze the appropriate frame for you. So, second quick aspect. What is it? Yes,  it is quick to run, but that’s not all.   It is also quick to implement. For reference, if  you wish to implement one of our earlier papers   on material synthesis, this is the  number of variables you have to remember,   and this is the pseudocode for the algorithm  itself. What is that? Well, this shows what   steps we need to take to implement our technique  in a computer program. I don’t consider this to   be too complex, but now, compare it to  this simulation algorithm’s pseudocode.   Whoa, much simpler. I would wager  that if everything goes well,   a competent computer graphics research  scientist could implement this   in a day. And that is a rare sight for a modern  simulation algorithm, and that is excellent. I think you are going to hear from this  technique a great deal more. Who wrote it?   Of course, Miles Macklin and Matthias Müller,   two excellent research scientists at NVIDIA.  Congratulations, and with this kind of progress,   just imagine what we will be able to do two more  papers down the line! What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
539,Watch This Statue Grow Out Of Nothing!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to learn how to take any shape in the real world, make a digital copy of it, and place it into our virtual world. These can be done through something that we call differentiable rendering. What is that? Well, simple, we take a photograph, and find a photorealistic material model that we can put into our light simulation program that matches it. What this means is that now, we can essentially put this real material into a virtual world. This work did very well with materials, but, it did not capture the geometry. This other work is from Wenzel Jakob’s group and it jointly found the geometry and material properties. Seeing these images gradually morph into the right solution is an absolutely beautiful sight. But, as you see high-frequency details were not as good. You see here these details are gone. So, where does this put us? If we need only the materials, we can do really well, but then, no geometry. If we wish to get the geometry and the materials, we can use this, but we lose a lot of detail. There seems to be no way out. Is there a solution for this? Well, Wenzel Jakob’s amazing light transport simulation group is now back with guns blazing! Let’s see what they were up to. Let’s try to reproduce a 3D geometry from a bunch of triangles. Let’s see….and. Ouch. Not good, right? So what is the problem here? The problem is that we have tons of self-intersections, leading to a piece of tangled mesh geometry. This is incorrect. Not what we are looking for. Now, let’s try to improve this apply a step that we call regularization, this guides the potential solutions towards smoother results. Sounds good, hoping that this will do the trick. Let’s have a look together. So, what do you think? Better, but there are some details that are lost, and my main issue is that the whole geometry is in fluctuation. Is that a problem? Yes it is. Why? Because this jumpy behavior means that it has many competing solutions that it can’t choose from. Essentially, the algorithm says, maybe this, or not…perhaps this instead, no, not this. How about this? And it just keeps going on forever. It doesn’t really know what makes a good reconstruction. Now, hold on to your papers, and let’s see how the new method does with these examples. Oh my! This converges somewhere, which means that at the end of the process, it settles on something. Finally, this one knows what good is. Fantastic. But now, these geometries were not that difficult. Let’s give it a real challenge! Here comes the dragon. Can it deal with that? Just look at how beautifully it grows out of this block. Yes, but, look the details are not quite there. So, are we done? That’s it? No! Not even close! But, do not despair for a second! This new paper also proposes an isotropic remeshing step, which does…this! On the count of one, two, three. Boom, we are done. Whoa! So good. And, get this, the solution is not restricted to only geometry, it also works on textures. Now, make no mistake, this is not super useful in its current state, but it may pave a way to even more sophisticated methods two more papers down the line. So, importing real geometry into our virtual worlds. Yes please! Thanks for  watching and for your generous support, and I'll see you next time!"
540,Google's Enhance AI - Super Resolution Is Here!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to grow people out of… noise of all things. So, I hear you asking, what is going on here? Well, what this work performs is something that we call super resolution. What is that? Simple. The enhance thing. Have a look at this technique from last year. In goes a course image or video, and this AI-based method is tasked with…this! Yes. This is not science fiction. This is super resolution, which means that the AI synthesized crisp details onto the image. Now, fast forward a year later, and let’s see what this new paper from scientists are Google Brain is capable of. First, a hallmark of a good technique is when we can give it a really coarse input and it can still do something with it. In this case, this image will be 64 by 64 pixels, which…is almost nothing I’m afraid, and, let’s see how it fares. This will not be easy. And, well, the initial results are…not good. But don’t put too much of a stake in the initial results, because this work iteratively refines this noise, which means that you should hold on to your papers, and…oh yes, it means that it improves over time…it’s getting there….whoa, still going. And, wow. I can hardly believe what has happened here. In each case, in goes a really coarse input image, where we get so little information. Look, the eye color is often given by only a couple pixels, and we get a really crisp, and believable output. What’s more, it can even deal with glasses too. Now, of course, this is not the first paper on super resolution. What’s more, it is not even the hundredth paper performing super resolution. So, comparing to previous works is vital here. We will compare this to previous methods in two different ways. One, of course, we are going to look. Previous, regression-based methods perform reasonably well, however, if we take a closer look, we see that the images are a little blurry. High-frequency details are missing. And now, let’s see if the new method can do any better. Well, this looks great, but we are Fellow Scholars here, we know that we can only evaluate this result in the presence of the true image. Now let’s see. Nice. We would have to zoom in real close to find out that the two images are not the same. Fantastic. Now, while we are looking at these very convincing high-resolution outputs. Please note that we are only really scratching the surface here. The heart and soul of a good super resolution paper is proper evaluation and user studies, and the paper contains a ton more details on that. For instance, this part of the study shows how likely people were to confuse the synthesized images with the real ones. Previous methods, especially PULSE, which is an amazing technique reached about 33%. Which means that most of the time, people found out the trick, but, whoa, look here. The new method is almost at the 50% mark. This is the very first time that I see a super resolution technique where people can barely tell that these images are synthetic. We are getting one step closer to this technique getting deployed in real-world products. It could improve the quality of your Zoom meetings, video games, online images, and much, much more. Now, note that not even this one is perfect…look, as we increase the resolution of the output of the image, the users are more likely to find out that these are synthetic images. But still, for now, this is an amazing leap forward in just one paper. I can hardly believe that we can take this image, and make it into this image using a learning-based method. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
541,"New AI: Photos Go In, Reality Comes Out!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going take a  collection of photos like these,   and magically, create a video where  we can fly through these photos. How is this even possible? Especially that  the input is only a handful of photos.   Well, we give it to a learning algorithm  and ask it to synthesize a photorealistic   video where we fly through the scene  as we please. That sounds impossible.   Especially that some information is given  about the scene, but this is really not much.   And everything in between these photos has  to be synthesized here. Let’s see how well   this new method can perform that,  but don’t expect too much…and….wow. It took a handful of photos and filled in the  rest so well that we got a smooth and creamy video   out of it. So, the images certainly look good in  isolation. Now let’s compare it to the real world   images that we already have, but have hidden from  the algorithm, and hold on to your papers…wow,   this is breathtaking. It is not a great  deal different, is it? Does this mean…yes!   It means that it guesses what reality  should look like almost perfectly. Now note that this mainly applies for looking  at these images in isolation. As soon as we   weave them together into a video and start flying  through the scene, we will see some flickering,   artifacts, but that is to be expected. The AI  has to create so much information from so little,   and the tiny inaccuracies that appear in each  image are different, and when played abruptly   after each other, this introduces these artifacts.  So, which regions should we look at to find these   flaws? Well, usually regions where we have  very little information in our set of photos,   and a lot of variation when we move our head.  For instance, visibility around thin structures   is still a challenge. But of course, you know the  joke how do you spot a Two Minute Papers viewer?   They are always looking behind thin  fences. Shiny surfaces are a challenge   too as they reflect their environment and  change a lot as we move our head around. So how does it compare to previous methods?  Well, it creates images that are sharper and   more true to the real images. Look! What you see  here is a very rare sight. Usually, when we see   a new technique like this emerge, it almost always  does better on some datasets, and worse on others.   The comparisons are almost always a wash. But  here? Not at all. Not in the slightest. Look,   here you see four previous techniques, four  scenes, and three different ways of measuring   the quality of the output images. And almost  none of it matters, because the new technique   reliably outperforms all of them, everywhere.  Except here, in this one case, depending on how   we measure how good a solution is. And even  then, it’s quite close. Absolutely amazing. Make sure to also have a look at the paper in  the video description to see that it can also   perform filmic tone mapping, change the  exposure of the output images, and more. So, how did they pull this off? What hardware  do we need to train such a neural network?   Do we need the server warehouses of  Google or OpenAI to make this happen?   No, not at all! And here comes the best part…if  you have been holding on to your papers so far,   now, squeeze that paper, because all it takes is  a consumer graphics card and 12 to 24 hours of   training. And after that, we can use the neural  network for as long as we wish. So, recreating   reality from a handful of photos with a neural  network that some people today can train at home   themselves? The pace of progress in AI research  is absolutely amazing. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
542,Virtual Reality Fluid Drawing Is Here!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to control the fate of liquids in virtual worlds. This footage is from one of my earlier papers where I attempted fluid control. This means that we are not only simulating the movement of a piece of fluid, but we wish to coerce it to flow into a prescribed shape. This was super challenging and I haven’t really seen a satisfactory solution that I think, artist could use in the industry yet. And now that we have these modern, neural-network based algorithms, we are now able to solve problems that we never even dreamed of solving just a few years ago. For instance, they can already perform this kind of style transfer for smoke simulations. Which is, incredible. So, are you thinking what I am thinking? Can one of those maybe tackle fluid control too? Well, that’s a tough call… just to showcase how difficult this problem is. If we wish to have any control over our fluid simulations, if we are a trained artist, we can sculpt the fluid directly ourselves. Of course, this requires a great deal of expertise, and often, hours of work. Can we do better? Well, yes, kind of. We can use a particle system built into most modern 3D modeling programs, with which we can try to guide these particles to a given direction. This took about 20 minutes, and it still requires some artistic expertise. So, that’s it then? No, hold on to your papers, and check this out! The preparation for this work takes place in virtual reality, where we can make these sketches in 3D, and look at that! The liquid magically takes the shape of our sketch. So, how long did this take? Well, not one hour, and not even 20 minutes. It took one minute. One minute. Now we’re talking! And even better, we can embed this into a simulation, and it will behave like a real piece of fluid should. So, what is all this good for? Well, my experience has been in computer graphics is that if we put a powerful tool like this into the hands of capable artist, they are going to create things that we never even thought of creating. With this, they can make a heart from wine. Or create a milky skirt. Several variants even, if we wish. Or a liquid butterfly. I am loving these solutions, and don’t forget, all of these can be then embedded into a virtual world and simulated as a real liquid. Now, we talked about three solutions, and how much they take, but we didn’t see what they looked like. Clearly, it is hard to compare these mathematically, so this is going to be, of course, subjective. So, this took an hour. It looks very smooth, and is perhaps the most beautiful of the three solutions. That is great. However, as a drawback, it does not look like a real-world water splash. The particle system took 20 minutes, it creates a more lifelike version of our letter, but the physics is still missing. This still looks like a trail of particles, not a physics system. And, let’s see the new method. Yes, this took only a minute, and it finally looks like a real splash. Now, make no mistake, all three of these solutions can be excellent, depending on our artistic vision. So, how does all this magic happen? What is the architecture of this neural network? Well, this behavior emerges not from one, but from the battle of two neural networks. A generator neural network creates new splashes, and the discriminator finds out whether these splashes are real or fake. Over time, they challenge each other, and they teach each other to do better. The technique also goes the extra mile beyond just sketching. Look, for instance, your brush strokes can also describe velocities. With this, we can not only control the shape, but even the behavior of the fluid too. So, there we go. Finally, a learning-based technique gives us a proper solution for fluid control. And here comes the best part is it not only quicker than previous solutions, it can also be used by anyone. No artistic expertise is required! What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
543,Can A Virtual Sponge Sink?,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to not only simulate fluids,   but even better, we're going to absorb  them with sponge-like porous materials too.   This paper does not have the usual super high  resolution results that you see with many other   simulation papers. However, I really wanted  to show you what it can do through four lovely   experiments. And, at the end of the episode,  you will see how slowly or quickly it runs! Experiment number one. Spongy dumbbells. One side  is made of a spongy material, and if it starts   absorbing the water, let’s see if it sinks. Well,  it slowly starts to descend as it gets heavier,   and then, it gets so heavy that eventually, it  sinks the other half too. That is a good start. Now, experiment number two. Absorption.  Here, the green liquid is hard to absorb,   therefore we expect it to pass through.  While the red liquid is easier to absorb,   and should get stuck in this perforated material.  Let’s see if that is indeed what happens…the green   is coming through, and where is all the red  goo? Well, most of it is getting absorbed here.   But wait, the paper promises to ensure that  mass and momentum still gets transferred   through the interactions properly. So, if  the fluid volumes are simulated correctly,   we expect a lot more green down there than  red. Did we get it? Oh yes, checkmark! And if you think this was a  great absorption simulation,   wait until you see this. Experiment  number three. Absorption on steroids.  This is a liquid mixture, and these are porous  materials. Each of which will absorb exactly one   component of the liquid and let through  the rest. Let’s see…the first obstacle   absorbs the blue component quickly, the second  retains all the green component, and the red   flows through. And, look at how gracefully  the red fluid fills the last sponge. Lovely. Onwards to experiment number four. Artistic  control. Since we are building our own little   virtual world, we make all the rules.  So let’s see the first absorption case.   Nothing too crazy here. But, if this is  not in line with our artistic vision,   do not despair, because this is our world, so  we can play with these physical parameters.   For instance, let’s increase the absorption  rate, so that the fluid enters the solid faster.   Or, we can increase the permeability, this is the  ease of passage of the fluid through the material.   Does it transfer quicker into the sponge?   Yes it does! And finally, we can speed up  both how quickly the fluid enters the solid,   how quickly it travels within, and we have  also increased the amount of absorption.   Let’s see if we end up with a smaller remaining  volume. Indeed we do! The beauty of building these   virtual worlds is that we can have these  simulations under our artistic control. So, how long do we wait for the sponges? Well,  hold on to your papers, because we not only   don’t have to sit down and watch an episode of  Spongebob to get these done, but… get this. The   crazy three-sponge experiment used about quarter  of a million particles for the fluid, and another   quarter-ish million particles for the solids,  and runs at approximately a second per frame.   Yes, it runs interactively, and all  this on a consumer graphics card. And, yes, so that’s why the resolution of the  simulations is not that high. The authors could   have posted a much higher resolution simulation  and kept the execution time in the minutes per   frame domain, but no. They wanted to show us  what this can do interactively. And in this case,   it is important that you apply the First Law Of  Papers, which says that research is a process.   Do not look at where we are, look at where we will  be two more papers down the line. So, there we go,   I really wanted to show you this paper, because  unfortunately, if we don’t talk about it, almost   no one will see it. This is why Two Minute Papers  exists. And, if you wish to discuss this paper,   make sure to drop by on our Discord server.  The link is available in the video description. Thanks for watching and for your generous  support, and I'll see you next time!"
544,Man VS Machine: Who Plays Table Tennis Better?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see if a robot can learn to play table tennis. Spoiler alert, the answer is yes, quite well in fact. That is surprising, but what is even more surprising is how quickly it learned to do that. Recently, we have seen a growing number of techniques where robots learn in a computer simulation, and then, get deployed into the real world. Yes, that sounds like science fiction. So, does this work in practice? Well, I’ll give you two recent examples, and you can decide for yourself. What you see here is example number one, where OpenAI’s robot hand that learned to dexterously rotate this Rubik cube to a given target state. How did it do it? Yes, you guessed it right, it learned in a simulation. However, no simulation is as detailed as the real world, so they used a technique called automatic domain randomization in which they create a large number of random environments, each of which are a little different, and the AI is meant to learn how to solve many different variants of the same problem. And the result? Did it learn general knowledge from that? Yes, what’s more, this became not only a dexterous robot hand that can execute these rotations, but, we can make up creative ways to torment this little machine, and it still stood its ground. Okay, so this works, but is this concept good enough for commercial applications? You bet. Example number two, Tesla uses no less than a simulated game world to train their self-driving cars. For instance, when we are in this synthetic video game, it is suddenly much easier to teach the algorithm safely. You can also make any scenario easier, harder, replace a car with a dog, or a pack of dogs, and make many similar examples so that the AI can learn from these “what if” situations as much as possible. Now, that’s all great, but today, we are going to see whether this concept can be generalized to playing table tennis. And I have to be honest…I am very enthused, but a little skeptical too. This task requires finesse, rapid movement, and predicting what is about to happen in the near future. It really is the whole package, isn’t it. Now, let’s enter the training simulation and see how this goes. First, we hit the ball over to its side, specify a desired return position, and ask it to practice returning the ball around this desired position. Then, after a quick retraining step against the ball throwing machine, we observe the first amazing thing. You see, the first cool thing here is that it practices against side spin and top spin balls. What are those? These are techniques where the players hit the ball in ways to make their trajectory much more difficult to predict. Okay, enough of this, now, hold on to your papers, and let’s see how the final version of the AI fares against a player. And…whoa. It really made the transition into the real world. Look at that, this seems like it could go on forever. Let’s watch for a few seconds. Yep, still going. Still going. But, we are not done yet. Not even close! We said at the start of the video that this training is quick. How quick? Well, if you have been holding on to your papers, now, squeeze that paper, because all the robot took was 1.5 hours of training. And wait, there are two more mind-blowing numbers here. It can return 98% of the balls. And most of them are within 25 centimeters, or about 10 inches of the desired spot. And again, great news, this is one more technique that does not require Google or OpenAI-level resources to make something really amazing. Loving it. And you know, this is the way to make an excellent excuse to play table tennis during work hours. They really made it work. Huge congratulations to the team. Now, of course, not even this technique is perfect. We noted that it can handle side spin and top spin balls, but can’t deal with backspin balls yet, because, get this, quoting ”it causes too much acceleration in a robot joint”. Yes a robot with joint pain. What a time to be alive! Now, one more thing. As of the making of this video, this was seen by a grand total of… 54 people. Again, there is a real possibility that if we don’t talk about this amazing work, no one will. And this is why I started Two Minute Papers. Thank you very much for coming on this journey with me. Please subscribe if you wish to see more  of these. Thanks for watching and for your generous support, and I'll see you next time!"
545,Simulating A Virtual World…For A Thousand Years!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to simulate thousands of years of vegetation in a virtual world. And I am telling you, this paper is unbelievable. Now, normally, if we are building a virtual world, we don’t really think about simulating a physics and biology-based ecosystem. Let’s be honest, it’s more like “Yeah, just throw in some trees and we are done here”. But, in reality, the kinds of vegetation we have in a virtual world should be at the very least a function of precipitation and temperature. And here at this point, we know that this paper means business. You see, if there is no rain and it’s super cold, we get a tundra. With no rain and high temperature we get a desert. If we keep the temperature high, and add a ton of precipitation, we get a tropical rainforest, and this technique promises to be able to simulate these, and everything inbetween. See these beautiful little worlds here? These are not illustrations. No-no! These are already the result of the simulation program. Nice! Now, let’s run a tiny simulation with 400 years and a few hundred plants. Step number one. The first few decades are dominated by these shrubs, blocking away the sunlight from everyone else, but, over time, step two, watch these resilient pine trees slowly overtake them and deny them their precious sunlight. And, what happens as a result? Look! Their downfall brings forth a complete ecosystem change. And then, step three, spruce trees start to appear. This changes the game. Why? Well, these are more shade-tolerant, and let’s see…yes, they take over the ecosystem from the pine trees. A beautifully done story, in such a little simulation. I absolutely love it. Of course, any self-respecting virtual world will also contain other objects, not just the vegetation, and, the simulator says, no problem. Just chuck ‘em in there, and we’ll just react accordingly and grow around it. Now, let’s see a mid-sized simulation. Look at that. Imagine the previous story with the pine trees, but, with not a few hundred, but, a hundred thousand plants. This can simulate that too. And now comes the final boss. A large simulation. Half a million plants, more than a thousand years. Yes, really. Let’s see the story here in images first, and then, you will see the full simulation. Yes, over the first hundred years, fast-growing shrubs dominate and start growing everywhere, and after a few hundred more years, the slower-growing trees catch up, and start overshadowing the shrubs at lower elevation levels. Then, more kinds of trees appear at lower elevations, and slowly over 1400 years, a beautiful, mixed-age forest emerges. I shiver just thinking about the fact that through the power of computer graphics research works, we can simulate all this on a computer. What a time to be alive! And, while we look at the full simulation, please note that there is more happening here, make sure to have a look at the paper in the video description if you wish to know more details. Now, about the paper. And here comes an additional interesting part. As of the making of this video, this paper has been referred to 20 times. Yes, but this paper is from 2019, so it had years to soak up some citations, which didn’t really happen. Now note that one, citations are not everything, and two, 20 citations for a paper in the field of computer graphics is not bad at all. But every time I see an amazing paper, I really wish that more people would hear about it, and always I find that almost nobody knows about them. And once again, this is why I started to make Two Minute Papers. Thank you so much for coming on this amazing journey with me. Thanks for watching and  for your generous support, and I'll see you next time!"
546,This AI Makes Celebrities Old…For a Price!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to take a bunch of celebrities,   and imagine what they looked as tiny little  babies. And then, we will also make them old. And   at the end of this video, I’ll also step  up to the plate, and become a baby myself. So, what is this black magic here? Well, what  you see here is a bunch of synthetic humans,   created by a learning-based technique called  StyleGAN3, which appeared this year, in June   2021. It is a neural network-based learning  algorithm that is capable of synthesizing these   eye-poppingly detailed images of human beings  that don’t even exist, and even animate them. Now, how does it do all this black magic? Well,  it takes walks in a latent space. What is that?   A latent space is a made-up place  where we are trying to organize data   in a way that similar things are close to each  other. In our earlier work, we were looking to   generate hundreds of variants of a material model  to populate this scene. In this latent space,   we can concoct all of these really  cool digital material models.   A link to this work is available  in the video description.   StyleGAN uses walks in a similar latent space  to create these human faces and animate them. And now, hold on to your papers, because a  latent space can represent not only materials   or the head movement and smiles for people, but  even better, age too. You remember these amazing   transformations from the intro. So, how does it  do that? Well, similarly to the font and material   examples, we can embed the source image into a  latent space, and take a path therein. It looks   like this. Please remember this embedding step,  because we are going to refer to it in a moment. And now comes the twist, the latent space for  this new method is built such that when we   take these walks, it disentangles age from other  attributes. This means that only the age changes,   and nothing else changes. This is very  challenging to pull off, because normally,   when we change our location in the latent space,   not just one thing changes, everything  changes. This was the case with the materials. But not with this method, which can  take photos of well-known celebrities,   and make them look younger or older. I  kind of want to do this myself too. So,   you know what? Now, it’s my turn. This is what  baby Károly might look like after reading baby   papers. And this is old man Károly, complaining  that papers were way better back in his day. And this is supposedly baby Károly from a talk   a NATO conference. Look, apparently  they let anybody in these days! Now, this is all well and good, but there  is a price to be paid for all this. So   what is the price? Let’s find out together  what that is. Here is the reference image of   me. And here is how the transformations came  out. Did you find the issue? Well, the issue   is that I don’t really look like this. Not only  because this beard was synthesized onto my face   by an earlier AI, but really, I can’t really find  my exact image in this. Take another look. This   is what the input image looked like. Can you  find it in the outputs somewhere? Not really. Same with the conference image, this  is the actual original image of me,   and this is the output of the AI.  So, I can’t find myself. Why is that? Now, you remember I mentioned earlier that we  embed the source image into the latent space.   And this step, is, unfortunately, imperfect.  We start out from not exactly the same image,   but only something similar to it. This is the  price to be paid for these amazing results,   and with that, please remember to  invoke the First Law of Papers.   Which says, do not look at where we are, look at  where we will be two more papers down the line. Now, even better news! As of the writing  of this episode, you can try it yourself!   Now, be warned that our club  of Fellow Scholars is growing   rapidly and you all are always so curious that  we usually go over and crash these websites   upon the publishing of these episodes.  If that happens, please be patient! Otherwise, if you tried it, please let  me know in the comments how it went   or just tweet at me. I’d love to see some  more baby scholars. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
547,"Finally, This Table Cloth Pull is Now Possible!","Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today is a glorious day, because we are going to witness the first table cloth pull simulation I have ever seen. Well, I hoped that it would go a little more glorious than this. Maybe if we pull a bit quicker. Yes, there we go, and…we’re good. Loving it. Now, if you are a seasoned Fellow Scholar, you might remember from about 50 videos ago that we covered the predecessor of this paper, called Incremental Potential Contact, IPC in short. So, what could it do? It could perform seriously impressive squishing experiments. And it also passed the tendril test, where we threw a squishy ball at a glass wall, and watched this process from the other side. A beautiful, and rare sight indeed. Unless you have cats and glass tables at home, of course. Outstanding. So, I hear you asking, Károly, are you trying to say that this new paper tops all that? Yes, that is exactly what I am saying. The table cloth pulling is one thing, but it can do so much more. You can immediately start holding on to your papers, and let’s go. This new variant of IPC is capable of simulating super thin materials, and all this in a penetration-free manner. Now, why is that so interesting or difficult? Well, that is quite a challenge, remember this earlier paper with the barbarian ship. Tons of penetration artifacts. And that is not even a thin object. Not nearly as thin as this stack would be. Let’s see what a previous simulation method would do with this if these are 10 millimeters each. That looks reasonable, now let’s cut the thickness of the sheets in half. Yes, some bumpy artifacts appear, and at 1 millimeter, my goodness. It’s only getting worse. And, when we plug in the same thin sheets into the new simulator. All of them look good, and what’s more, they can be simulated together with other elastic objects without any issues. And this was a low-stress simulation. If we use the previous technique for a higher-stress simulation. This starts out well, until…uh-oh. The thickness of the cloth is seriously decreasing over time. That is not realistic. But, if we plug the same scene into the new technique, now that is realistic. So, what is all this good for? Well, if we wish to simulate a bowl of noodles, tons of thick objects, let’s see if we can hope for an intersection-free simulation. Let’s look under the hood…and, there we go! All of the noodles are separated. But wait a second, I promised you thin objects, these are not thin. Yes, now these, are thin. Still, no intersections. That is absolutely incredible. Other practical applications include simulating hair. Braids in particular. Granular materials against a thin sheet work too. And, if you have been holding on to your papers so far, now, squeeze that paper, because the authors promise that we can even simulate this in-hand shuffling technique in a virtual world. Well, I will believe it when I see it. Let’s see…my goodness. Look at that. Love the attention to detail where the authors color coded the left and right stack so we can better see how they mix, and if they intersect. Spoiler alert: they don’t. What a time to be alive! It can also simulate this piece of cloth with a ton of detail, and not only that, with large timesteps, which means that we can advance the time after each simulation step in bigger packets, thereby speeding up the execution time of the method. I also love how we get a better view of the geometry changes as the other side of the cloth has a different color. Once again, great attention to detail. We are still in the minutes per frame region, and note that this runs on your processor, therefore, if someone can implement this on the graphics card in a smart way, it could become close to real time in at most a couple papers more down the line. And, this is a research paper that the authors give away to all of us, free of charge. How cool is that. Thank you. Thank you so much for creating these miracles and just giving them away for free. What a noble endeavor research is! Thanks for watching and for your generous support, and I'll see you next time!"
548,From Mesh To Yarn... In Real Time!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This is not some hot new Two Minute Papers merchandise this is what a new cloth simulation paper is capable of doing today. So good! And today, we are also going to see how to put back the holes into holey geometry, and do it quickly! But wait a second, that is impossible. We know that yarn-based cloth simulations take forever to compute. Here is an example, we showcased this previous work approximately 60 episodes ago. And curiously, get this, some of the simulations here were nearly 60 times faster than the yarn-based reference simulation. That is amazing. However, we noted that even though this technique was a great leap forward, but of course, it wasn’t perfect, there was a price to be paid for this amazing speed, which was, look, that pulling effects on individual yarns were neglected. That amazing holey geometry is lost. I noted that I’d love to get that back. You will see in a moment that maybe, my wish comes true today. Hmm! So, quick rundown. We either go for a mesh-based cloth simulation. These are really fast, but, no holey geometry. Or, we choose the yarn-level simulations, these are the real deal, however, they take forever. Unfortunately, still, there seems to be no way out here. Now, let’s have a look at this new technique. It promises to try to marry these two concepts, or in other words, use a fast mesh simulator, and add the yarn-level cloth details on top of it. Now of course, that is easier said than done, so let’s see how this new method can deal with this challenging problem. So, this is the fast mesh simulation to start with, and now, hold on to your papers, and let’s add those yarns. Oh my, that is beautiful. I absolutely love how at different points, you can even see through these garments. Beautiful. So, how does it do all this magic? Well, look at this naive method. This neglects the proper tension between the yarns, and, look at how beautifully they tighten as we add the new method on top of it. This technique can do it properly. And not only that, but it also simulates how the garment interacts with a virtual body in a simulated world. Once again, note how the previous, naive method neglects the tightening of the yarns. So, I am super excited, let’s see how long we have to wait for this. Are we talking fast mesh simulation timings, or slow yarn simulation timings? My goodness. Look at that! The mesh part runs on your processor, while the yarn part of the simulation is implemented on the graphics card. And, whoa, the whole thing runs in the order of milliseconds, easily in real time. Even if we have a super detailed garment with tens of millions of vertices in the simulation, the timings are in the order of tens of milliseconds at worst. That’s also super fast. Wow. So, yes, part of this runs on our graphics card and in real time. Here, we have nearly a million vertices, not a problem. Here, nearly two million vertices, two million points if you will, it runs like a dream. And it only starts to slow down when we go all the way to a super detailed piece of garment with 42 million vertices. And here, there is so much detail that finally, not the simulation technique is the bottleneck, but the light simulation algorithm is. Look, there are so many high-frequency details that we would need a higher-resolution video, or more advanced antialiasing techniques to resolve all this detail. All this means that the simulation technique did its job really well. So, finally, yarn-level simulations in real time. What a time to be alive! Huge congratulations to the entire team, and the first author, Georg Sperl, who is still a PhD student, making important contributions like this. And, get this Georg’s short presentation was seen by… oh my…61 people. Views are not everything. Not even close! But, once again, if we don’t talk about this work here, I am worried that almost no one will. And this is why Two Minute Papers exists. Subscribe if you wish to see more of these miracle papers, we have some great ones coming up. Thanks  for watching and for your generous support, and I'll see you next time!"
549,NVIDIA’s New AI: Journey Into Virtual Reality!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how NVIDIA’s crazy new AI is able to understand basically any human movement. So, how is this even possible? And even more importantly, what is pose estimation? Well, simple, a video of people goes in, and the posture they are taking comes out. Now, you see here that previous techniques can already do this quite well, what’s more, if we allow an AI to read the wifi-signals bouncing around in a room, it can perform pose estimation, even through walls. Kind of. Now, you may be wondering what is pose estimation good for? By the end of this video, you will see that this can help us move around in virtual worlds, the metaverse, if you will. But let’s not rush there yet, because still, there are several key challenges here. One, we need to be super accurate to even put a dent into this problem. Why? Well, because if we have a video, and we are off by just by a tiny bit from frame to frame, this kind of flickering may happen. That’s a challenge. What else is challenging here? Two, foot sliding. Yes, you heard it right. Yes, previous methods suffer from this phenomenon, you can see it in action here. And also, here too. So, why does this happen? It happens because the technique has no knowledge of the physics of real human movements. So, scientists at NVIDIA, the University of Toronto and the Vector Institute fired up a collaboration, and when I first heard about their concept, I thought you are doing what? But, check this out. First, they perform a regular pose estimation. Of course, this is no good, as it has the dreaded temporal inconsistency, or in other words, flickering. And in other cases, often foot sliding too. Now, hold on to your papers, because here comes the magic! Now, they transfer the motion to a video game character, and embed that character in a physics simulation. In this virtual world, the motion can be corrected to make sure they are physically correct. Now, remember, foot sliding happens because of the lack of knowledge in physics, so, perhaps this idea is not that crazy after all! Let’s have a look! Now this will be quite a challenge. Explosive sprinting motions. And…whoa. This is amazing. This, dear Fellow Scholars, is superb pose estimation and tracking. How about this? A good tennis serve includes lots of dynamic motion, and just look at how beautifully it reconstructs this move. Apparently, physics works. Now, the output also needn’t be stickmen, we can retarget these to proper textured virtual characters built from a triangle mesh. And, that’s just one step away from us being able to appear in a metaverse. No head mounted displays are required, no expensive studio, no motion capture equipment is required. So, what IS required? Actually, nothing! Just a raw input video of us. That is insanity. And all this produces physically correct motion. So, that crazy idea about taking people and transforming them into video game characters is not so crazy after all. So now, we are one step closer to be able to work, and even have some coffee together in a virtual world. What a  time  to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
550,"Yes, These Are Virtual Dumplings!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today, we are going to absolutely destroy  this virtual bunny. And then, inflate this cat   until it becomes… a bit of a chonker. And, I hope  you like dumplings, because we’ll also be able to   start a cooking show in a virtual world. For  instance, we can now squash and wrap and pinch   and squeeze. And this is a computer  graphics paper, so let’s smash these   things for good measure. There we go. Add  some more. And the meal is now ready. Enjoy! This is all possible through this new paper  that is capable of creating physics simulations   with drastic geometry changes. And when I say  drastic, I really mean it. And looking through   the results, this paper feels like it can do  absolutely anything. It promises a technique   called Injective deformation processing. What  does all this mean? It means great news. Finally,   when we do these crazy experiments, things  don’t turn inside out, and the geometry   does not overlap. Wanna see what these  overlaps looks like? Well, here it is. And, luckily, we don’t have to worry about  this phenomenon with this new technique.   Not only when inflating, but, it also works  correctly when we start deflating things. Now,   talking about overlaps, let’s see this animation  sequence simulated with a previous method.   Oh no! The belly is not moving,  and, my goodness. Look at that.   It gets even worse. What is even  worse than a belly that does not move?   Of course, intersection artifacts. Now, what you  will see is not a re-simulation of this experiment   from scratch with this with the new method,  no-no. Even better. We give this flawed   simulation to the new technique, and…yes, it  can even repair it! Wow. An absolute miracle.   No more self-intersections, and finally, the  belly is moving around in a realistic manner. And, while we find out whether this armadillo  simulated with the new method is dabbing,   or if it is just shy, let’s talk about how long  we have to wait for a simulation like this.   All nighters? No, not at all! The font inflation  examples take roughly half a second per frame,   that is unbelievable, and it goes up to 12 minutes  per frame, which is required for the larger   deformation experiments. Repairing an  already existing, flawed simulation   also takes a similar amount of time. So, what about this armadillo? Yes,   that is definitely a shy armadillo. So, from  now on, we can apply drastic geometry changes   in our virtual worlds, and I am sure that two more  papers down the line, all this will be possible in   real time. Real-time dumplings in a virtual world?  Yes please, sign me up! What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
551,Ubisoft’s New AI Predicts the Future of Virtual Characters!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to take an AI and use it to synthesize these beautiful, crisp movement types. And you will see in a moment that this can do much, much more. So how does this process work? There are similar previous techniques that took a big soup of motion capture data and outbid each other what they could learn from it. And they did it really well for instance, one these AIs was able to not only learn these movements, but even improve them, and even better, adapt them to different kinds of terrains. This other work used a small training set of general movements to reinvent popular high jump technique, the Fosbury flop by itself. This allows the athlete to jump backward over the bar, thus lowering their center of gravity. And, it could also do it on Mars. How cool is that? But, this new paper takes a different vantage point. Instead of asking for more and training videos, it seeks to settle with less. But first, let’s see what it does. Yes, we see that this new AI can match reference movements well, but that’s not all of it. Not even close. The hallmark of a good AI is not being restricted to just a few movements, but being able to synthesize to a great variety of different motions. So, can it do that? Wow, that is a ton of different kinds of motions, and the AI always seems to match the reference motions really well across the board. Bravo! We’ll talk more about what that means in a moment. And here comes the best part, it does not only generalize to a variety of motions, but to a variety of body types as well. And we can bring these body types to different virtual environments too. This really seems like the whole package. And now, if you have been holding on to your papers, now, squeeze that paper, because we can control them, in real time! My goodness. So, how does all this this work? Let’s see…yes, here, the green is the target movement we would like to achieve, and the yellow is the AI’s result. Now, here, the trails represent the past. So, how close are they? Well, of course, we don’t know exactly yet, so let’s line them up, and…now we’re talking! They are the almost the same. But wait, does this even make sense? Aren’t we just inventing a copying machine here? What is so interesting about being able to copy an already existing movement? But, no-no-no, we are not copying here. Not even close. What this new work does instead is that we give it the initial pose, and ask it to predict the future. In particular, we ask what is about to happen to this model. And the result is…a messy trail. So, what does this mess mean? Well, actually, the mess is great news, this means that the true physics results and the AI predictions line up so well that they almost completely cover each other. But wait, this is not the first technique to attempt this…what about previous methods? Can they also do this? Well, these are all doing pretty good. Maybe this new work is not that big of an improvement…wait a second…oh boy. One contestant is down. And now, two have failed, and I love how they still keep dancing while down. A+ for effort, little AIs. But the third one, is still in the game…careful…ouch! Yup, the new method is absolutely amazing. No question about it. And of course, do not be fooled by these mannequins, these can be mapped to real characters in real video games too. So, this amazing new method is able to create higher quality animations, let us grab a controller and play with them, and also requires a shorter training time. Not only that, but the new method predicts more, and hence, relies much less on the motion dataset we feed it, and therefore, it is also less sensitive to its flaws. I love this. A solid step towards democratizing the creation of superb computer animations. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
552,Google’s New AI: This is Where Selfies Go Hyper!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, we are not going to create selfies, we are going to create NERFies instead. What are those? Well, NERFies are selfies from the future. The goal here is to take a selfie video, and turn it into a portrait that we can rotate around in complete freedom. This technique appeared in November 2020, a little more than a year ago, and as you see, it easily outperformed its predecessors. It shows a lot of strength in these examples and seems nearly invincible, however, there is a problem. What is the problem? Well, it still did not do all that well on moving things. Don’t believe it? Let’s try it out! This is the input video. Aha! There we go, as soon as there is a little movement in time, we see that it looks like a researcher who is about have a powerful paper deadline experience. I wonder what the NERFIE technique will do with this. Uh-oh. That’s not optimal. And now, let’s see if this new method called HyperNERF is able to salvage the situation. Oh my! One moment perfectly frozen in time, and we can move also around with the camera. Kind of like the bullet time effect from the Matrix. Sensational. What is also sensational is that of course, you Seasoned Fellow Scholars immediately ask okay, but which moment will get frozen in time? The one with the mouth closed, or open? And, HyperNERF says, well, you tell me. Yes, we can even choose which moment we should freeze in time by exploring this thing that they call the hyperspace. Hence the name, HyperNERF. The process looks like this. So, if this can handle animations, well then, let’s give it some real tough animations. This is one of my favorite examples, where we can even make a video of coffee being made. Yes, that is indeed the true paper deadline experience. And the key to creating these NERFies correctly is getting this depth map right. Here, the colors describe how far things are from the camera. That is challenging in and of itself, and now imagine that the camera is moving all the time too. And not only that, but the subject of the scene is also moving all the time too. This is very challenging, and this technique does it just right. The previous NERFIE technique from just a year ago had a great deal of trouble with this chocolate melting scene. Look. Tons of artifacts and deformations as we move the camera. Now, hold on to your papers and let’s see if the new method can do even this, now that would be something…and! Wow. Outstanding. I also loved how the authors went the extra mile with the presentation of their website…look! All the authors are there, animated with this very method. Putting their money where their papers are. Loving it. So, with that, there you go. These are NERFIES, selfies from the future. And finally, they really work such amazing improvements in approximately a year. The pace of progress in AI research is nothing short of amazing. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
553,Microsoft’s AI Understands Humans…But It Had Never Seen One!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. None of these faces are real. And today we are going to find out whether these synthetic humans can, in a way, pass for real humans, but not in the sense that you might think. Now, through the power of computer graphics algorithms, we are able to create virtual worlds, and of course, within those virtual worlds, virtual humans too. So, here is a wacky idea. If we have all this virtual data, why not use these instead of real photos to train new neural networks? Hmm…wait a second. Maybe this idea is not so wacky after all. Especially because we can generate as many of these virtual humans as we wish, and all this data is perfectly annotated. The location and shape of the eyebrows is known, even when it they are occluded, and we know the depth and geometry of every single hair strand of the beard. If done well, there will be no issues about the identity of the subjects, or the distribution of the data. Also, we are not limited by our wardrobe or the environments we have access to. In this virtual world, we can do anything we wish. So good! But of course, here is the ultimate question that decides the fate of this project. And that question is: does this work? Now, we can give all this data to train a neural network, and the crazy thing about this is that this neural network never saw a real human. So here’s the ultimate test. Videos of real humans. Now, hold on to your papers and and, let’s see if the little AI can label the image and find the important landmarks. Wow. I can hardly believe my eyes it not only does it for a still image, but for even a video, and it is so accurate from frame to frame that no flickering artifacts emerge. That is outstanding. And get this, the measurements say that it can stand up to other state of the art detector neural networks that were trained on real human faces. And…oh my! Are you seeing what I am seeing? Can this really be? If we use the same neural network to learn on real human faces, it actually won’t to better at all, in fact, it will do worse than the same AI with the virtual data. That is insanity. The advantages of this practically infinitely flexible synthetic dataset shows really well here. The paper also discusses in detail that that this only holds if we really use the synthetic dataset well and include different rotations and lighting environments for the same photos. Something that is not always so easy in real environments. Now this test was called face parsing, and now comes landmark detection. This also works remarkably well, but wait a second. Once again, are you seeing what I am seeing? The landmarks can rotate all they want, and it will know where they should be, even if they are occluded by headphones, the hair, or when they are not visible at all. Now, of course, not even this technique is perfect. Tracking the eyes correctly requires additional considerations and real data, but only for that, which is relatively easy to produce. Also, the simulator at this point can only generate the head and the neck regions, and nothing else. But, you know the drill, a couple papers down the line, and I am sure that this will be able to generate full human bodies. Thanks for watching and for  your generous support, and I'll see you next time!"
554,New AI Makes You Play Table Tennis…In a Virtual World!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how easily we can transfer our motion onto a virtual character. And even play virtual table tennis as if we were a character in a computer game! Hmm! This research work proposes to not use the industry standard motion capture sensors to do this. Instead, they promise a neural network that can perform full-body motion capture from 6 Inertial Measurement Units. These are essentially gyroscopes that report accelerations and orientations. And this is the key. This presents us with two key advantages. Hold on to your papers for advantage number one, which is no cameras are needed. Yes, that’s right. Why is that great news? Well, a camera is a vision-based system, and if people are far away from the camera, it might have some trouble making out what they are doing, of course, because it can barely see them. But now with the inertial measurement units and this neural network. They can also be further away, maybe a room or two away, and no problem. So good! And, if it doesn’t even need to see us, we can hide behind different objects, and look! It can still reconstruct where we are. Loving it. Or, we can get two people to play table tennis. We can only see the back of this player, and the occlusion situation is getting even worse as they turn and jump around a great deal. Now you made me curious let’s look at the reconstruction together. Wow, that is an amazing reconstruction. In the end, if the contestants agree that it was a good match, they can hug it out, or…wait a second, maybe not so much. In any case, the system still works. And, advantage number two, ah, of course. Since it thinks in terms of orientation and acceleration, it doesn’t even need to see you. So…oh yes! It also works in the dark. Note that so do some infrared camera-based motion capture systems, but here, no cameras are needed. And, let’s see that reconstruction. There is a little jitter in the movement, but otherwise, very cool. And, if you have been holding on to your papers so far, now, squeeze that paper, because I am going to tell you how quickly this runs. And that is 90 frames per second. Easily in real time. Now, of course, not even this technique is perfect. I am sure you noticed that there is a bit of a delay in the movements, and often, some jitter too. And, as always, do not think of this paper as the final destination. Think of this as an amazing leap forward, and always, always apply the First Law of Papers. What is that? Well, just imagine how much this could improve just two more papers down the line. I am sure there will be no delays and no jitter. So, from now on, we are one step closer to be able to play virtual table tennis, or even work out with our friends in a virtual world. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!"
555,This New AI Creates Lava From Water!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. This new learning-based method is capable of generating new videos, creating lava from water, enhancing your dance videos, adding new players to a football game, and more. This one will feel like it can do absolutely anything! For instance, have a look at these videos. Believe it or not, some of these were generated by an AI from this new paper. Let’s try to find out together which ones are real, and which are generated. Now before we start, many of the results that you will see here will be flawed, but I promise that there will be some gems in there too. This one doesn’t make too much sense. One down. Two, the bus appears to be changing length from time to time. Three, the bus pops out of existence. Four…wait…this actually looks pretty good…until we find out that someone is about to get rear-ended. This leaves us with two examples. Are they real, or are they fakes? Now, please stop the video and let me know in the comments. So, this one is real footage. Okay, what about this guy? Well, get this…this one is actually one of the generated ones. Just this one was real. And this is what this new technique is capable of. We give it a sample video, and it reimagines variations of it. Of course, the results are hit or miss, for instance, here, people and cars just pop in and out of existence, but if we try it on the billiard balls. Wow, now this is something. Look at how well it preserved the specular highlights, and also, the shadows move with the balls in the synthetic variations too. Once again, the results are generally quite hit or miss, but if we look through a few results, we often find some gems out there. But this is nothing compared to what is to come. Let’s call this first feature video synthesis, and we have 4 more really cool applications with somewhat flawed, but sometimes amazing results. Now, check this out, here comes number two, video analogies. This makes it possible for us to mix two videos that depict things that follow a similar logic. For instance, here, only four of these sixteen videos are real, the rest are all generated. Now, here comes feature number three time retargeting. For instance, we can lengthen or shorten videos. Well, that sounds simple enough, no need for an AI for that…but here is the key! This does not mean just adding new frames to the end of the video. Look! They are different. Yes, the entirety of the video is getting redesigned here. We can use this to lengthen these videos without really adding new content to it. Absolutely amazing. We can also use it to shorten a video. Now, of course, once again, this doesn’t mean that it just chops off the end of the video. No-no! It means something much more challenging. Look! It makes sure that all of your killer moves make it into the video, but, in a shorter amount of time. It makes the video tighter if you will. Amazing feature number 4. Sketch to video. Here, we can take an input video, add a crude drawing, and the video will follow this drawing. The result is, of course, not perfect, but, in return, this can handle many number to number transitions. And now, feature number five. Video inpainting on steroids. Previous techniques can help us delete part of an image, or even a video, and generate data in these holes that make sense given their surroundings. But this, this one does something way better! Look, we can cut out different parts of a video, and mark the missing region with a color, and then, what happens? Oh yes! The blue regions will contain a player from the blue team, the white region a player from the white team, or, if we wish to get someone out of the way, we can do that too. Just mark them green. Here, the white regions will be inpainted with clouds, and the blues with birds. Loving it. But wait a second, we noted that there are already good techniques out there for image inpainting. There are also good techniques for video time retargeting. So, what is so interesting here? Well, two things. One, here all of these things are being done with just one technique. Normally, we would need a collection of different methods to perform all of these. But here, just one AI. Two, now hold on to your papers, and look at this! Whoa. Previous techniques take forever to generate these results. 144p means an extremely crude, pixelated image, and even then, they take from hours…to, my goodness. Even days. So, what about this new one? It can generate much higher resolution images, and, in a matter of minutes. That is an incredible leap in just one paper. Now, clearly, most of these results aren’t perfect. I would argue that they are not even close to perfect. But some of them already show a lot of promise. And, as always, dear Fellow Scholars, do not forget to apply the First Law of Papers. Which says, do not look at where we are. Look at where we will be two more papers down the line. And two more papers down the line, I am sure that not two, but five, or maybe even six out of these six videos will look significantly more real. Thanks for watching and for your generous support, and I'll see you next time!"
556,Stanford Invented The Ultimate Bouncy Simulator!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to design a crazy baseball  bat, a hockey stick that’s not working well,   springs that won’t stop bouncing, and  letters that can kind of stop bouncing. And, yes, you see it correctly, this  is a paper from 2017. Why? Well,   there are some good works that are timeless.  This is one of them. You’ll see why in a moment. When I read this paper, I saw that it is  from professor Doug James’s group at Stanford   University, and at this point, I knew that crazy  things are to be expected. Our seasoned Fellow   Scholars know that these folks do some absolutely  amazing things that shouldn’t even be possible. For instance, one of their earlier papers takes an  animation and the physics data for these bubbles,   and, impossible as it might appear,  synthesizes the sound of these bubbles. So,   video goes in, sound comes out.  And hold on to your papers because that’s  nothing compared to their other work,   which takes not the video, but the sound  that we recorded. So, the sound goes in,   that’s easy enough, and what does it do? Creates  a video animation that matches these sounds.   That’s a crazy paper that works exceptionally  well. We love those around here. And, to top it off, both of these were handcrafted  techniques, no machine learning is applied.   So, what about today’s paper? Today’s paper  is about designing things that don’t work,   or things that work too well. What  does that mean? Well, let me explain. Here is a map that shows the physical  bounciness parameter for a not too exciting   old simulation. Everything, and every part  does the same. And here is the new method.   Red means bouncy, blue means stiff. And now, on to the simulation. The old one  with the fixed parameters. Not bad, but it not   too exciting either. And, here is the new one.  Now that is a simulation has some personality. So, with this, we can reimagine as if parts  of things were made of rubber, and other parts   of wood or steel. Look. Here, the red color on the  knob of the baseball bat means that this will be   made bouncy, while the end will be made stiff. What does this do? Well, let’s see together.   This is the reference simulation where  every part has the same bounciness. And now,   let’s see that bouncy knob. There we go!  With this, we can unleash our artistic vision   and get a simulation that works properly,  given these crazy material parameters. Now, let’s design a crazy hockey stick. This  part of the hockey stick is very bouncy,   this part too, however, this part will be  the sweet spot, at least for this experiment.   Let’s hit that puck and see how they behave.   Yes, from the red, bouncy regions, indeed,  the puck rebounds a great deal. And now,   let’s see the sweet spot. Yes, it rebounds  much less. Let’s see all of them side by side. Now, creating such an algorithm  is quite a challenge. Look,   it has to work on smooth geometry, built  from hundreds of thousands of triangles.   And one of the key challenges is that the  duration of the contacts can be… what?! Are   you seeing what I am seeing? The duration of some  of these contacts is measured in the order of tens   of microseconds. And, it still works well, and  it’s still accurate. That is absolutely amazing. Now, of course, even though we can have  detailed geometry made of crazy new materials,   this is not only a great tool for artists.  This could also help with contact analysis,   and other cool engineering applications where we  manufacture things that hit each other. Glorious. So this is an amazing, timeless work from 2017,  and I am worried that if we don’t talk about it   here on Two Minute Papers, almost no one will  talk about it. And these works are so good,   people have to know! Thank you very much for  watching this, and let’s spread the word together! Thanks for watching and for your generous  support, and I'll see you next time!"
557,"Photos Go In, Reality Comes Out…And Fast!","Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going take a collection of photos like these, and magically, create a video where we can fly through these photos. And it gets better, we will be able to do it quickly, and, get this, no AI is required. So, how is this even possible? Especially that the input is only a handful of photos. Well, typically, we give it to a learning algorithm and ask it to synthesize a photorealistic video where we fly through the scene as we please. Of course, that sounds impossible. Especially that some information is given about the scene, but this is really not much. And as you see, this is not impossible at all through the power of learning-based techniques, this previous AI is already capable of pulling off this amazing trick. And today, I am going to show you that through this incredible new paper, that something like this can even be done at home, on our own machines. Now, the previously showcased technique, and its predecessors are building on gathering training data and training a neural network to pull this off. Here you see the training process of one of them compared to the reference results. Well, it looks like we need to be really patient as this process is quite lengthy, and for the majority of the time, we don’t get any usable results for nearly a day into this process. Now, hold on to your papers, because here comes the twist. What is the twist? Well, these are not reference results. No-no. These are the results from the new technique. Yes, you heard it right. It doesn’t require a neural network, and thus trains so quickly, that it almost immediately looks like the final result, while the original technique is still unable to produce anything usable. That is absolutely insane. Okay, so it’s quick. Real quick. But how good are the results? Well, the previous technique was able to produce this after approximately 1.5 days of training. And, what about the new technique? All it needs is 8.8. 8.8 what? Days? Hours? No-no. 8.8 minutes. And the result looks like this. Not only as good, but even a bit better than what the previous method could do in 1.5 days. Whoa. So, I mentioned that the results are typically even a bit better, which is quite remarkable. Let’s take a closer look. This is the previous technique after more than a day, this is the new method after 18 minutes. Now, it says that the new technique is 0.3 decibels better, that does not sound like much, does it? Well, note that the decibel scale is not linear, it is logarithmic. What does this mean? It means this! Look. A small numerical difference in the numbers can mean mean a big difference in quality. And, it is really close to the real results. And all this after 20 minutes of processing? Bravo! And, it does not stop there. The technique is also quite robust. It works well on forward-facing scenes, 360 degree rotations, and we can even use it to disassemble a scene into its foreground and background elements. Note that the previous NERF technique we compared to was published just about a year and a half ago. Such incredible improvement in so little time. And here comes the kicker: all this is possible today with a handcrafted technique, no AI is required. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
558,New AI: Next Level Video Editing!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to spice up this family video with a new, stylized environment. Then, we make a video of a boat trip much more memorable, up the authenticity of a parkour video, decorate a swan, enhance our clothing a little, and conveniently forget about a speeding motorcycle. Only for scientific purposes, of course. So, these are all very challenging tasks to perform, and of course, none of these should be possible. And this is a new AI-based solution that can pull of all of these! But… how? Well, in goes an input video, and this AI decomposes it into a foreground and background, but in a way that it understands that this is just a 2D video that represents a 3D world. Clearly, humans understand this, but does Adobe’s new AI do it too? And I wonder how much it understands about that? Well, let’s give it a try together. Put those flowers on that dress, and…what? The flowers look like they are really there as they wrinkle as the dress wrinkles, and it catches shadows just as the dress catches shadows too. That is absolutely incredible. It supports these high-frequency movements so well that we can even stylize our kite sailing videos with it, where there is tons of tiny water droplets flying about. No problems at all. We can also draw on this dog, and remember, we mentioned that it understands the difference between foreground and background, and…look, the scribble correctly travels behind objects. Aha, and this is also the reason why we can easily remove a speeding motorbike from this security footage. Just cut out the foreground layer. Nothing to see here. But I wonder, can we go a little more extreme here? And it turns out, these are really nothing compared to what this new AI can pull off. Look. We can not only decorate this swan, but, here is the key…and…oh yes! The swan is fine, yes, but the reflection of the swan is also computed correctly. Now this feels like black magic, and we are not even done yet, now hold on to your papers, because here come my two favorite examples! Example number one. Biking in wonderland. I love this. Now, you see that not even this technique is perfect, if you look behind the spokes of the wheel, you see a fair amount of warping. I still think this is a miracle that it can be pulled off at all. Example number two, a picturesque trip. Here, not only the background has been changed, but even the water has changed as chunks of ice have also been added. And with that, I wonder how easy it is to do this? I’ll tell you in a moment, after we look at this. Yes. There is also one issue here with the warping of the water in the wake of the boat, but this is an excellent point for us to invoke the First Law of Papers, which says, do not think of where we are, look at where we will be two more papers down the line. So, how much work do we have to put in to pull all this off? A day of work maybe? Nope, not even close! If you have been holding on to your papers so far, now, squeeze that paper, and look at this. We can unwrap the background of the footage, and stylize it as we please. All this takes is just editing one image. This goes in, this comes out. Wow. Absolutely anyone can do this, so this is a huge step in democratizing video stylization and bringing it to everyone. What  a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!"
559,NVIDIA’s New AI Draws Images With The Speed of Thought!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Welcome to episode 600! And today you will see  your own rough drawings come to life as beautiful   photorealistic images. And it turns out, you can  try it too. I’ll tell you about it in a moment. This technique is called GauGAN2, and yes, this  is really happening. In goes a rough drawing,   and out comes an image of this quality. That  is incredible. But here, there is something   that is even more incredible. What is it? Well,  drawing is an iterative process. But, once we are   committed to an idea, we need to refine it over  and over, which takes quite a bit of time, and   let’s be honest here, sometimes, things come out  differently than we may have imagined. But this,   this is different. Here, you can change things  as quickly as you can think of the change. You   can even request a bunch of variations on  the same theme, and get them right away. But that’s not all, not even close. Get this, with  this, you can draw, even without drawing. Yes,   really. How is that even possible?  Well, if we don’t feel like drawing,   we can just type what we wish to see,  and…my goodness, it not only generates   these images according to the written description,  but this description can get pretty elaborate. For instance, we can get ocean waves, that’s  great, but now, let’s add some rocks…and   a beach too. And there we go! We can also use an image as a starting point,  then, just delete the undesirable parts,   and have it inpainted by the algorithm. Now,  okay this is nothing new, computer graphics   researchers were able to do this for more  than 10 years now. But hold on to your papers,   because they couldn’t do this. We can fill  in these gaps with a written description.   Couldn’t witness the northern lights  in person? No worries, here you go. And, wait a second…did you see that? There  are two cool really cool things to see here.   Thing number one, it even redraws  the reflections on the water,   even if we haven’t highlighted that part for  inpainting. We don’t need to say anything,   it will update the whole environment to reflect  the new changes by itself. That is amazing.   Now, I am a light transport researcher by  trade, and this makes me very, very happy. Thing number two, I don’t know if you caught this,   but this is so fast, it doesn’t  even wait for your full request,   it updates after every single keystroke. Look.  Drawing is an inherently iterative process,   and iterating with this is an absolute  breeze. Not will be a breeze. It is a breeze. Now, after nearly every Two Minute Papers  episode where we showcase an amazing paper,   I get a question saying something like  “okay, but when do I get to see or use this   in the real world?”. And rightfully so, that  is a good question. The previous GauGAN paper   was published in 2019, and here we are, just  a bit more than 2 years later, and it has been   transferred into a real product. Not only that,  but the resolution has improved a great deal,   about 4 times of what it was before, plus  the new version also supports more materials. And we are at the point where this  is finally not just a cool tech demo,   but a tool that is useful for real  artists. What a time to be alive! Now, I noted that earlier, I did not say that  iterating this will be a breeze. But it is a   breeze. Why? Well, great news, because you  can try it right now in two different ways. One, it is now part of as  desktop application called   NVIDIA Canvas. With this, you can even export the  layers to Photoshop and continue your work there.   This will require a relatively  recent NVIDIA graphics card. And two, there is a web app too that you  can try right now! The link is available   in the video description, and if  you try it, please scroll down,   and make sure to read the instructions and  watch the tutorial video to not get lost. And remember, all this tech transfer from  paper to product took place in a matter   of two years. Bravo NVIDIA! The pace of  progress in AI research is absolutely amazing. Thanks for watching and for your generous  support, and I'll see you next time!"
560,Opening The First AI Hair Salon!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to open an AI-powered hair salon. How cool is that. But, how? Well, we recently looked at this related technique from NVIDIA that is able to make our rough drawings come to life as beautiful photorealistic images. In goes a rough drawing, and out comes an image of this quality. And it processes changes to the drawings with the speed of thought. You can even request a bunch of variations on the same theme, and get them right away. So here is a crazy idea how about doing this with hair styles? Just draw them, and the AI puts it onto the model and makes it look photorealistic. Well, that sounds like science fiction, so I will believe it when I see it. And, this new method claims to be able to do exactly that, so, let’s see the pros first. This will go in, and this will come out. How is that even possible? Well, all we need to do is once again, produce a crude drawing that is just enough to convey our intentions. We can even control the color and the length of the hair locks, or, even request a braid, and all of these work pretty well. Another great aspect of this technique is that it works very rapidly, so we can easily iterate over these hair styles, and if we don’t feel like our original idea will be the one, we can refine it for as long as we please. The AI too works with the speed of thought. One of the best aspects of this work. You can see an example here. This also does not look right…yet, but this quick iteration also means that we can also lengthen those braids in just a moment. And, yes, better! So, how does it perform compared to previous techniques. Well, let’s have a look together! This is the input hairstyle, and a matte telling the AI where we seek to place the hair, and, let’s see the previous methods. The first two techniques are very rough, this followup work is significantly better than the previous ones, but it is still well behind the true photograph for reference. This is much more realistic. So, hold on to your papers, and let’s see the new technique. Now you’re probably thinking Károly…what’s going on? Why is the video not changing? Is this an error? Well, I have to tell you something. It’s not an error. What you are looking at now is not the reference photo. No-no. This is the result of the new technique. The true reference photo is this one. That is a wonderful result. The new method understands not only the geometry of the hair better, but also how the hair should play with the illumination of the scene too. I am a light transport researcher by trade, and this makes me very, very happy. So much so, that in some ways, this technique looks more realistic than the true image. That is super cool. What a time to be alive! Now, not even this technique is perfect. Even though the new method understands geometry better than its predecessors, we have to be careful with our edits, because geometry problems still emerge. Look here. Also, the other issue is that the resolution of the generated hair should match the resolution of the underlying image. I feel that these is usually a bit more pixelated. Now note that these kinds of issues are what typically get improved from one paper to the next, so, you know the deal, a couple more papers down the line, and I am sure, these edits will become nearly indistinguishable from reality. As you see, there are still issues, the results are often not even close to perfect, but, the pace of progress in AI research is nothing short of amazing. With this, we can kind of open an AI-powered hair salon, and some cases will work, but soon, it might be that nearly all of our drawing will result in photorealistic results that are indistinguishable from reality. Thanks for watching and for your generous support, and I'll see you next time!"
561,"Wow, A Simulation That Matches Reality!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today, we are going to look at an incredible new  paper, where we are going to simulate flows around   thin shells, rods, the wind blowing at leaves,  airflow through a city, and get this, we will   produce spiral vortices around an aircraft,  and even perform some wind tunnel simulations. Now, many of our previous  videos are about real time   fluid simulation methods that  are often used in computer games,   and others that run for a long time and are  typically used in feature-length movies. But not this, this work is not for that. This new  paper can simulate difficult coupling effects,   which means that it can deal with the movement  of air currents around this thin shell. We can also take a hair brush without  bristles, and have an interesting simulation.   Or even better, add tiny tiny  bristles to its geometry,   and see how much more of a turbulent flow this  creates. I already love this paper. So good. Now, onwards to more serious applications.  So why simulate thin shells? What is   all this useful for? Well, of course, simulating  wind turbines. That is an excellent application,   we are getting there. Hmm…talking about thin  shells…what else? Of course, aircraft! Now hold   on to your papers and marvel at these beautiful  spiral vortices that this simulator can create.   So, is this all for show, or is  this what would really happen   in reality? We will take a  closer look at that in a moment. With that, yes, the authors claim that this  is much more accurate than previous methods in   its class. Well then, let’s give it a devilishly  difficult test, a real aerodynamic simulation in   a wind tunnel. In these cases, getting really  accurate results is critical. For instance,   here we would like to see that if we were to add  a spoiler to this car, how much of an aerodynamic   advantage we would get in return. Here are  the results from the real wind tunnel test,   and now, hold on to your papers and let’s see  how the new method compares. Wow. Goodness! It   is not perfect by any means, but seems accurate  enough that we can see the wake flow of the car   clearly enough so that we can make a decision  on that spoiler. You know what, let’s keep it. So we compared the simulation against  reality. Now, let’s compare a simulation   against another simulation. So, against a previous  method. Yes, the new one is significantly more   accurate than its predecessor. Why? Because the  previous one introduces significant boundary layer   separations at the top of the car. The new one  says that this is what will happen in reality, so   how do we know? Of course, we check. Yes,  that is indeed right. Absolutely amazing.   And note that this work appeared just about  a year ago. So much improvement in so little   time. The pace of progress in computer  graphics research is out of this world! Okay, so it is accurate. Really accurate. But, we  already have accurate algorithms, so how fast is   it? Well, the proper aerodynamic simulations take  from days to weeks to compute, but this airplane   simulation took only minutes. How many minutes?  60 minutes to be exact. Wow. And within these 60   minutes, the same spiral vortices show up as  the ones in real wind tunnel tests. So good! But really, how can this be so fast? How is this  even possible? The key to doing all this faster   is that the new proposed method is massively  parallel. This means that it can slice up   one big computing task into many small ones  that can be taken care of independently. And,   as a result, it takes advantage of our graphics  cards and can squeeze every drop of performance   out of it. That is very challenging  for an algorithm of this complexity. So, where does this put us? Of course, the final  simulation should still be done with the old tools   just to make sure. However, this new method will  be able to help engineers quickly iterate on early   ideas, and only commit to week-long simulations  when absolutely necessary. So, we can go from   idea to conclusions in a matter of an hour, not  in a matter of a week. This will be an amazing   time saver so the engineers can try more designs.  Testing more ideas leads to more knowledge. That   leads to better vehicles, better wind turbines.  Loving it. And all this improvement in just   a year. If I didn’t see this with my own eyes, I  could not believe this. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
562,Adobe's New Simulation: Bunnies Everywhere!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today is going to be all about simulating  virtual bunnies. What kinds of bunnies?   Bunnies in an hourglass with granular  material, bunnies in a mixing drum,   bunnies that disappear, we’ll  try to fix this one too. So, what was all this footage? Well, this is  a followup work to the amazing Monolith paper.   What is Monolith? It is a technique that helps  fixing commonly occurring two-way coupling issues   in physics simulations. Okay, that sounds great,   but what does two-way coupling mean? It means that  here, the boxes are allowed to move the smoke,   and the added two-way coupling part means that  now, the smoke is also allowed to blow away   the boxes. This previous work makes  can simulate these phenomena properly. It also makes sure that when thrown at the  wall, things stick correctly, and a ton of   other goodies too. So, this previous method shows  a lot of strength. Now, I hear you asking, Károly,   can this get even better? And the answer is yes,  yes it can! That’s exactly why we are here today. This new paper improves this  technique to work better in cases   where we have a lot of friction. For instance,  it can simulate how some of these tiny bunnies   get squeezed through the hourglass, and get  showered by this sand-like granular material.   It can also simulate how some of them remain stuck  up there because of the frictional contact. Good. Now, have a look at this, with this one, with  an earlier technique, we start with one bunny,   and we end up with….wait a minute. That  volume is not one bunny amount of volume.   This is what we call the volume dissipation  problem. I wonder if we can get our bunny back   with the new technique. What do you think? Well,  let’s see…one bunny goes in, friction happens,   and…yes! One bunny amount of volume comes out of  the simulation. Then, we a bunch of them into a   mixing drum in the next experiment where their  tormenting shall continue. This is also a very   challenging scene because we have over 70 thousand  particles, rubbing against each other. And,   just look at that. The new technique is so robust  that there are no issues whatsoever. Loving it! So, what is all this simulation math good for?  Well, for instance, it helps us set up a scene   where we get a great deal of artistic freedom. For  instance, we can put this glass container with the   water between two walls, and, look carefully! Yes,  we apply a little leftward force to this wall.   And, since this technique can  simulate what is going to happen,   we can create an imaginary world where only  our creativity is the limit. For instance,   we can make a world in which there is just a  tiny bit of friction to slow down the fall. Or, we can create a world  with a ton more friction,   and now, we have so much friction  going on that the weight of the liquid   cannot overcome anymore, and thus, the cube  is quickly brought to rest between the walls. Luckily, since our bunny still exists, we can  proceed onto the next experiment, where we will   drop it onto a pile of granular material. This  previous method did not do too well in this case,   as the bunny sinks down. And if you think  that cannot possibly get any worse, well,   I have some news for you. It can. How? Look!  With a different previous technique, it doesn’t   even get to sink in, because it crashes when it  would need to perform these calculations. Now,   let’s see if the new method  can save the day, and…yes!   Great! It indeed can help our little  bunny remain on top of things. So, let’s pop the Scholarly question -  how much do we wait for such a simulation? The hourglass experiment takes  about 5 minutes per frame,   while the rotating drum experiment takes about  half of that, 2.5 minutes. So, it takes a while.   Why? Of course, because many of these scenes  contain tens of thousands of particles,   and almost all of them are in constant frictional  interaction with almost all the others at the   same time, and the algorithm mustn’t miss  any of these interactions. All of them   have to be simulated. And the fact that through  the power of computer graphics research we can   simulate all of these in reasonable amount of time  is an absolute miracle. What a time to be alive! And as always, you know what’s coming. Yes, please  do not forget to invoke the First Law of Papers,   which says says that research is a  process. Do not look at where we are,   look at where we will be two more papers  down the line. Granular materials,   and frictional contact in a matter of seconds  perhaps? Well, sign me up for this one! Thanks for watching and for your generous  support, and I'll see you next time!"
563,Next Level Paint Simulations Are Coming!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. As you see, this is not our usual intro.  Why is that? Because today we are going   simulate the process of real painting  on a computer, and we’ll find out that   all previous techniques mix paint incorrectly,  and finally, and create this beautiful image. Now, there are plenty of previous  techniques that help us paint   digitally, so, why do we  need a new paper for this? Well, believe it or not, these previous methods  think differently about the shape that the   paint has to take, the more sophisticated  ones even simulate the diffusion of paint,   which is fantastic. They all do these things a  little differently, but they agree on one thing.   And, here comes the problem. The one thing  they agree on is that blue plus yellow equals   a creamy color. But, wait a second… let’s  actually try this. Many of you know already what   is coming…of course. In real life, blue plus  yellow is not a creamy color, it is green.   Does the new method know this?  Yes it does! But only this one. And we can try the similar experiments over and  over and over again. The results are the same. So, how is this even possible? Does no one  know that blue plus yellow equals green?   Well, of course they know, but a  proper simulation of pigments mixing   is very challenging. For instance, it requires  keeping track of pigment concentrations, and   we also have to simulate subsurface  scattering, which is the absorption   and scattering of light within a volume of  these pigments. In some critical applications,   just this part can take several hours to days  to compute for a challenging case. And now,   hold on to your papers, because this new technique  can do all this correctly, and in real time. I   loved this visualization as it is really dense  in information, and super easy to read at the   same time. That is quite a challenge, and in my  opinion, just this one figure could win an award   by itself. As you see, most of the time, it runs  easily with 60, or higher frames per second,   and even in the craziest cases, it can compute all  this about 30 times per second. That is insanity. So, what do we get for all this effort? A  more realistic digital painting experience.   For instance, with the new method, color  mixing now feels absolutely amazing,   and if we feel like applying a ton of paint  and let it stain the paper, we get something   much more lifelike too. Artists who try this  will appreciate these a great deal I am sure. Especially that the authors also made this paint  color mixing technique available for everyone,   free of charge. As we noted, computing this kind  of paint mixing simulation is not easy, however,   using the final technique is, on the other  hand, extremely easy. As easy as it gets. If   you feel like coding up a simulation and include  this method in it, this is all you need to do.   Very few paper implementations are this simple to  use. It conceals all the mathematical difficulties   away from you. Extra point for elegance,  and, huge congratulations to the authors! Now, after nearly every Two Minute  Papers episode where we showcase   such an amazing paper, I get a question saying  something like “okay, but when do I get to see   or use this in the real world?”. And rightfully  so, that is a good question. For instance,   this previous GauGAN paper was  published in 2019, and here we are,   just a bit more than 2 years later, and it  has been transferred into a real product. Some machine learning papers also made it to  Tesla’s self driving cars in one or two years,   so tech transfer from research to real products  is real. But, is it real with this technique?   Yes it is! So much so that it is already available   in an app name Rebelle 5, which offers a  next level digital painting experience.   It even simulates different kinds  of papers and how they absorb paint.   A paper simulation you say? Yes, here, at Two  Minute Papers, we appreciate that a great deal. If you use this to paint something, please  make sure to leave a comment or tweet at me.   I would love to see your scholarly  paintings! What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
564,Can A Goldfish Drive a Car? Yes! But How?,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to try to teach a  goldfish to drive a car…of sorts. Now,   you may be asking, are we  talking about a virtual fish,   like one of the virtual characters here?  No-no. This is a real fish! Yes, really.   I am not kidding. This is an experiment where  researchers took a goldfish, and put it into   this contraption that they call an FOV, a  Fish Operated Vehicle. Very good. I love it. This car is built in a way such that it goes  in the direction where the fish is swimming. Now, let’s start the experiment by specifying  a target and asking the fish to get there,   and give them a treat if they  do. After a few days of training,   we get something like this.  Wow, it really went there. But, wait a second. We are experienced  Fellow Scholars here, so we immediately say   that this could happen by chance. Was this  a cherry picked experiment? How do we know   that this is real proficiency, not just chance?  How do we know if real learning is taking place? Well, we can test that, so let’s  randomize the starting point,   and see if it can still get there. The answer  is yes… yes, it can! Absolutely amazing.   This is just one example from the many  experiments that were done in the paper. So, learning is happening. So much so,  that over time over time, our little friend   learned so much that when it made a mistake,  it could even correct it. Perhaps this means   that in a followup paper, maybe they can  learn to even deal with obstacles in the way. Now, note that this was still just a couple of  videos, there are many many more experiments   reported in the paper. Now, at this point,  we are still not perfectly sure that learning   is really taking place here, so let’s run a  multifish experiment and assess the results. Let’s see…yes! As we let them train  for longer, all 6 of our participants   show remarkable improvement in finding the  targets. The average amount of time taken is   also decreasing rapidly over time. These two seem  to be extremely good drivers, perhaps they should   be doing this for a living. And if we sum up  the performance of every fish in the experiment,   we see that they were not too proficient in  the first sessions, but after the training,   wow, that is a huge improvement. Yes,  learning indeed seems to be happening here. So much so that…yes, as a result, I kid you not,  they can kind of navigate in the real world too. Now note that the details of  the study were approved by the   university and an animal care committee and  were conducted in accordance with government   regulations to make sure that nobody  gets hurt or mistreated in the process. So, this was a lot of fun, but what is the  insight here? The key insight is maybe navigation   capabilities are universal across species. We  don’t know for sure, but if it is true, that is   an amazing insight. And who knows, a couple papers  down the line, if the self-driving car projects   don’t come to fruition, maybe we will have fish  operated Teslas instead. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
565,OpenAI GLIDE AI: Astounding Power!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to play with a magical AI  where we just sit in our armchair, say the words,   and it draws an image for us. Almost anything  we can imagine. Almost. And before you ask,   yes, this includes drawing corgis too. In the last few years, OpenAI set out to  train an AI named GPT-3 that could finish   your sentences. Then, they made Image-GPT,  this could even finish your images.   Yes, not kidding. It could identify that  the cat here likely holds a piece of paper   and finish the picture accordingly, and even  understood that if we have a droplet here   and we see just a portion of the ripples,  then this means a splash must be filled in. And it gets better, then, they invented an  AI they call Dall-E. This one is insanity:   we just tell the AI what  image we would like to see,   and it will draw it. Look. It can  create a custom storefront for us,   understands the concept of low-polygon rendering,  isometric views, clay objects, and more. And that’s not all. It could even invent  clocks with new shapes when asked. The crazy thing here is that it understands  geometry, shapes, even materials. For instance,   look at this white clock here on the blue table.  And it did not only put it on the table, but it   also made sure to generate appropriate glossy  reflections that matches the color of the clock. And get this, Dall-E was published  just about a year ago, and OpenAI   already has a followup paper that they call  GLIDE. And believe it or not, this can do more,   and it can do it better. Well, I will  believe it when I see it, so, let’s go! Now, hold on to your papers, and, let’s start with  a hedgehog using a calculator. Wow, that looks   incredible. It’s not just a hedgehog  plus a calculator. It really is using   the calculator. Now, paint a fox in the style  of the starry night painting. I love the style,   and even the framing of the picture is quite good,  there is some space left to make sure that we see   that starry night. Great decision making. Now, a  corgi with a red bowtie and a purple party hat.   Excellent. And a pixel art corgi with a pizza. These are really good, but they are nothing  compared to what is to come. Because it can   also perform conditional inpainting with text.  Yes, I am not kidding. Have a look at this little   girl hugging a dog. But, there is a problem  with this. Do you know what the problem is?   Of course, the problem is that this is not a  corgi. Now it is. That is another great result. And if we wish that some zebras were added here,  that’s possible too, and we can also add a vase   here. Hmm…look at that. It even understood  that this is a glass table and added its own   reflection. Now, I am a light transport researcher  by trade, and this makes me very, very happy.   However, it is also true that it seems to have  changed the material properties of the table,   it is now much more diffuse than it was before.  Perhaps this is the AI’s understanding of a new   object blocking reflections. It’s not perfect  by any means, but it is a solid step forward. We can also give this gentleman a white  hat. And as I look through these results,   I find it absolutely amazing how  well the hat blends into the scene. That is very challenging. Why? Well, in light  transport research, we need to simulate the path   of millions and millions of light rays  to make sure that indirect illumination   appears in a scene, for instance, look here.  This is from one of our previous paper that   showcases how fluids of different colors paint  their diffuse surroundings to their own color.   I find it absolutely beautiful. Let’s  switch the fluid to a different one, and,   you see the difference. The link to this work  is available in the description below. And,   you see, simulating these effects  is very costly, and very difficult.   But this is how proper light transport  simulations need to be done. And this GLIDE AI   can put in new objects into a scene and make  them blend in so well, this to me, seems also a   proper understanding of light transport. I can  hardly believe what is going on here. Bravo. But wait, how do we know if this is really better  than Dall-E? Are we supposed to just believe it?   No, not at all! Fortunately, comparing the results  against Dall-E is very easy, look. We just add   the same prompts, and see that there is no  contest. The new GLIDE technique creates sharper,   higher-resolution images with more details, and  it even follows our instructions better. The   paper also showcases a user study where human  evaluators also favored the new technique. Now, of course, we are not done here, not  even this technique is perfect. Look. We can   request a cat with 8 legs, and…wait a minute. It  tried some multiplication trick, but we are not   falling for it. A+ for effort, little AI, but, of  course, this is clearly one of the failure cases. And once once again, this is an AI where  a vast body of knowledge lies within,   but it only emerges if we can bring it out with  properly written prompts. It almost feels like a   new kind of programming that is open to everyone,  even people without any programming or technical   knowledge. If a computer is a bicycle for the  mind, then OpenAI’s Glide is a fighter jet.   Absolutely incredible. Soon, this might  democratize creating illustrations, paintings,   and maybe even help inventing new things. And here comes the best part you can try  it too. The notebook for it is available in   the video description. Make sure to leave  your experiment results in the comments,   or just tweet them at me. I’d love to see what you  ingenious Fellow Scholars bring out of this AI. Thanks for watching and for your generous  support, and I'll see you next time!"
566,Adobe's New Method: Stunning Creatures... Even Cheaper!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to create a virtual world  with breathtakingly detailed pieces of geometry.   These are some results from a new technique,   and boy, are they amazing. I wonder how expensive  this will get. We’ll see about that in a moment. Now, here , we wish to create a virtual world, and  we want this world to be convincing, therefore we   will need tons of high-resolution geometry, like  the ones you see from our previous episodes. But all this detail in the geometry means tons of  data that has to be stored somewhere. Traditional   techniques allow us to crank up the detail in the  geometry, but there is a price to be paid for it.   And the price is that we need to throw more memory  and computation at the problem. The more we do,   the higher the resolution of the geometry  that we get. However, here comes the problem.  Ouch. Look. At higher-resolution levels, yes,  the price to be paid is getting a little steep.   Hundreds of megabytes of memory is quite steep  for just one object, but wait, it gets worse!   Oh, come on. Gigabytes? That is a little too  much. Imagine a scene with hundreds of these   objects laying around, no graphics  card has enough memory to do that. This new technique helps us add small bumps and  ridges to these objects much more efficiently   than previous techniques. This opens up the  possibility for creating breathtakingly detailed   digital objects, where even the tiniest  imperfections on our armor can be seen. Okay,   that is wonderful. But let’s compare to how  previous techniques can deal with this problem   and see if this new one is any better. Here is a traditional technique at its best,  when using 2 gigabytes of memory. That is quite   expensive. And here is the new technique. Hmm!  Look at that this looks even better. Surely   this needs even more memory. How many gigabytes  does this need? Or, if not gigabytes, how many   hundreds of megabytes? What do you think? Please  let me know in the comments section. I’ll wait.   Thank you! I am super excited to see your guesses.  Now, hold on to your papers, because actually,   it’s not gigabytes. Not even hundreds  of megabytes. It is 34 megabytes.   That is a pittance for a piece of geometry  of this quality. This is insanity. But wait, it gets better! We can even  dynamically change the displacements   on our models. Here is the workflow we plug  this geometry into a light simulation program,   and we can change the geometry, the lighting  material properties, or even clone our object   many-many times, and it still runs interactively.  What about the noise in these images?   This is a light simulation technique called  path tracing, and the noise that you see   here slowly clears up over time as we simulate  the path of many more millions of light rays. If we let it run for long enough, we end  up with a nearly perfect piece of geometry. I wonder how the traditional  technique is able to perform   if it is also given the same memory  allowance? Well, let’s see…wow,   there is no contest here. Even for other cases, it  is not uncommon that the new technique can create   an equivalent, or better geometry quality  but use 50 times less memory to do that. So from now on, we might get  more detailed virtual worlds   for cheaper. Sign me up right  now! What a time to be alive! And, one more thing. When checking out the  talk for the paper, I saw this. Whoa…20 views.   20 people have seen this talk. Now, I always  say that views are, of course, not everything,   but I am worried that if we don’t talk  about it here on Two Minute Papers,   almost no one will talk about it. And these  works are so good, people have to know!   And if you agree, please spread the word on  these papers and show them to people you know! Thanks for watching and for your generous  support, and I'll see you next time!"
567,"NVIDIA’s New AI: Superb Details, Super Fast!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to look at NVIDIA’s spectacular  new AI that can generate beautiful images for us.   But, this is not image generation of any kind,   no-no! This is different.  Let’s see how it is different. For instance, we can write a description,  and the appropriate image comes out. Snowy   mountains, pink cloudy sky. Checkmark. Okay, we can give it art direction.   Make no mistake, this is fantastic, but this  has been done before, nothing new here. Yet.  Or, we can create a segmentation map,  this tells the AI what things are.   The sea is down there, mountains,  and sky up there. Looking great, but   this has been done before too. For  instance, NVIDIA’s previous GauGAN paper   could do this too. Nothing new here. Yet. Or, we can tell the AI where things are   by sketching. This one also  works, but this has been done too. So, is there nothing new in this paper? Well, of  course there is! And now, hold on to your papers,   and watch as we fuse all of  these descriptions together. We tell it where things are and what  things are. But I wonder if we can make   this mountain snowy and a pink cloudy sky  on top of all things….yes we can! Oh wow,   I love it. The sea could have a little more  detail, but the rest of the image is spectacular. So, with this new technique, we  can tell an AI where things are,   what things are, and on top of it,  we can also give it art direction.   And here is the key, all of these  can be done, in any combination! Now, did you see it? Curiously, an image  popped up at the start of the video when we   unchecked all the boxes. Why is that? Is that  a bug? I’ll tell you in a bit what that is. So far, these were great examples, but let’s try  to push this to its limits and see what it can do. For instance, how quickly can we iterate with  it? How quick is it to correct mistakes or   improve the work? Oh boy. Super quick!  When giving art direction to the AI,   we can update the text, and the output  refreshes almost as quickly as we can type. The sketch to image feature is also  a great tool by itself. Of course,   there is not only one way to satisfy these labels,  there are many pictures that this could describe,   so, how do we control what we get? Well,  it can even generate variants for us.   With this new work, we can even draw a  piece of rock within a within the sea,   and the rock will indeed appear. Not only  that, but it understands that the waves   have to go around it too. An understanding  of physics. That is insanity. My goodness. Or, better, if we know in advance that we are  looking for tall trees and autumn leaves, we can   start with the art direction. And then, when  we add our labels, they will be satisfied,   we can have our river, but the trees and  the leaves will always be there. Finally,   we can sketch on top of this to have  additional control over the hills and clouds. And, get this we can even edit real images. So, how does this black magic work? Well, we  have four neural networks, four experts if you   will. And the new technique describes how to fuse  their expertise together into one amazing package. And the result is a technique that outperforms  previous techniques in most of the tested cases.   So, are these some ancient methods from many  years ago that are outperformed, or are they   cutting edge? And here comes the best part. If  you have been holding on to your papers so far,   now, squeeze that paper, because these  techniques are not some ancient methods.   Not at all. Both of these methods are from the  same year as this technique. The same year.   Such improvement in less than a year. That  is outstanding. What a time to be alive! Now, I made a promise to you early in the  video. And the promise is explaining this. Yes,   the technique generates, even when  not given a lot of instruction. Yes,   this was the early example when we unchecked all  the boxes. What quality images can we expect then?   Well, here are some uncurated examples, this  means that the authors did not cherrypick here,   just dumped a bunch of results here. And…oh my  goodness, these are really good! The details are   really there, the resolution of the image could  be improved, but we saw with the previous GauGAN   paper that this can improve a great deal in just a  couple years. Or, with this paper, in less than a   year. I would like to send a huge congratulations  to the scientists at NVIDIA bravo! And, you Fellow Scholars just saw how much of an  improvement we can get just one more paper down   the line, so imagine what the next paper could  bring. If you are curious, make sure to subscribe   and hit the bell icon to not miss it, when the  followup paper appears on Two Minute Papers. Thanks for watching and for your generous  support, and I'll see you next time!"
568,"NVIDIA’s New AI: Wow, Instant Neural Graphics!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to do this, and this, and this. One of these applications is called a NERF.   What is that? NERFs mean that we have  a collection of photos like these,   and magically, create a video where we can fly  through these photos. Yes, typically, scientists   now use some sort of learning-based AI method to  fill in all this information between these photos.   Something that sounded like science fiction  just a couple years ago, and now, here we are. Now, these are mostly learning-based  methods, therefore, these techniques need   some training time. Wanna see how their  results evolve over time? I surely do, so,   let’s have a look together! This NERF paper was  published about a year and a half or two years   ago. We typically have to wait for at least a  few hours for something to happen. Then came   the Plenoxels paper with something that looks  like black magic. Yes, that’s right, this trains   in a matter of minutes. And it was published two  months ago. Such improvement, in just two years. But, here is NVIDIA’s new paper from  about a month ago. And I hear you asking,   Károly, are you telling me that a two month  old paper of this caliber is going to be   outperformed by a one month old paper?  Yes, that is exactly what I am saying. Now, hold on to your papers, and look here,  with the new method, the training takes…what?   Less time than I have to utter this  sentence, because it is already done. So   first, we wait from hours to days, then,  2 years later, it trains in minutes,   and a month later, just a month later, it trains  in a couple seconds. Basically, nearly instantly. And, if we let it run for a bit longer, but still  less than two minutes, it will not only outperform   a naive technique, but will provide better  quality results than a previous method,   while training for about ten times quicker.  That is absolutely incredible. I would say   that this is swift progress  in machine learning research,   but that word will not cut it  here. This is truly something else. But, if that wasn’t enough, NERFing  is not the only thing this one can do. It can also approximate a gigapixel image. What  is that? That is an image with tons of data in it,   and the AI is asked to create a cheaper  neural representation of this image. And,   we can just keep zooming in, and zooming  in, and we still find new details there. Now if you have been holding on to your papers  so far, now, squeeze that paper, because what   you see here is not the result, but the whole  training process itself. Really? Yes, really.   Did you see it? Well, did you blink? Because  if you did, you almost certainly missed it.   This was also trained from scratch, right in front  of our eyes. But it’s so quick, that if you take   just a moment to hold on to your papers a bit more  tightly, and you already missed it. Once again,   a couple papers before, this took several  hours at the very least. That is outstanding. And if we were done here, I would be very happy.  But, we are not done yet, not even close! It can   still do two more amazing things. One, this is a  neural signed distance field it has produced. That   is a mapping from 3D coordinates in a virtual  world to distance to a surface. Essentially,   it learns the geometry of the object better  because it knows what parts are inside, and   outside. And it is blazing fast, surprisingly,  even for objects with detailed geometry. And, my favorite, it can also do neural radiance  caching. What is that? At the risk of simplifying   the problem, essentially, it is learning to  perform a light transport simulation. It took   me several years of research to be able to produce  such a light simulation, so let’s see how long it   takes for the AI to do. Well…let’s see…holy mother  of papers! NVIDIA, what are you doing! I give up. As you see, the pace of progress in  AI and computer graphics research   is absolutely incredible, and even  better, it is accelerating over time.   Things that were wishful thinking 10  years ago become not only possible,   but are now easy over the span of just a couple  of papers. I am stunned. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
569,These Smoke Simulations Have A Catch!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. After reading a physics textbook on the laws of  fluid motion, with a little effort, we can make   a virtual world come alive by writing a computer  program that contains these laws, resulting in   beautiful fluid simulations like the one you  see here. The amount of detail we can simulate   with these programs is increasing every year,  not only due to the fact that computer hardware   improves over time, but also, the pace of progress  in computer graphics research is truly remarkable. And, this new paper promises   detailed spiral-spectral fluid and  smoke simulations. What does that mean?   It means that the simulations can be run in  a torus, spheres, cylinders, you name it. But wait, is that really new? When using  traditional simulation techniques, we can just   enclose the smoke in all kinds of domain  shapes where the simulation takes place.   People have done this for decades now. Here  is an example of that. So, what is new here?   Well, let’s have a look at some results  and hopefully, find out together. Let’s start with the details first.  This is the new technique. Hmmm.   I like this one. This is a detailed simulation,  sure, I’ll give it that. But we can already create   detailed simulations with traditional techniques,  so, once again, what is new here? You know what,   let’s compare it to a traditional smoke simulation  technique and give it the same amount of time to   run, and see what that looks like. Wow. That is a  huge difference. And, yes, believe it or not, the   two simulations run in the same amount of time.  So, it creates detailed simulations. Checkmark. And, it has not only the details, but it has  other virtues too. Now, let’s bring up the heat   some more. This is a comparison to not an older,  classical technique, but a spherical-spectral   technique from 2019. Let’s see how the new method  fares against it. Well, they both look good,   so maybe the new method is not so much  better after a…wait a second. Ouch!   The previous one blew up. And the new one,   yes, this still keeps going. Such improvement in  just about two years. So, it is not only fast,   but it is robust too. That is super important for  real-world use. Details and robustness. Checkmark. Now, let’s continue with the shape of the  simulation domain. Yes, we can enclose the   simulation within this domain where the spherical  domain itself can be imagined as an impenetrable   wall, but it doesn’t have to be that way.  Look. We can even open it up! Very good. Okay, so it is fast, it is robust, supports  crazy simulation domain shapes, and even better,   it looks detailed, but are these the right  details? Is this just pleasing for the eye,   or is this really how the smoke should  behave? Well, the authors tested that too,   and now, hold on to your papers, and…look! I could  add the labels here, but does it really matter?   The two look almost exactly the same. Almost  pixel perfect. By the way, here you go. So, wow, the list of positives just keep on  growing. But we are experienced Fellow Scholars   here, so let’s continue interrogating this method.  Does it work for different viscosities? At the   risk of simplifying what is going on, the  viscosity of a puff of smoke relates to how   nimble the simulation is. And, it can handle  a variety of these physical parameters too. Okay, next, can it interact with other objects  too? Some techniques look great in a simple,   empty simulation domain, but break down when  placed into a real scene where a lot of other   objects are moving around. Well, not this new  one. That is a beautiful simulation, I love it. So, I am getting more and more convinced with each  test. So, where is the catch? What is the price   to be paid for all this? Let’s have a look.  For the quality of simulations that we get,   it runs in a few seconds per frame. And  it doesn’t even need your graphics card,   it runs on your processor. And  even that, this is blazing fast.   And, implementing this on the graphics-card could  very well put this into the real-time domain,   and boy, getting these beautiful smoke puffs  in real time would be an amazing treat. So, once again, what is the price to be paid  for this? Well, have a look. Aha! There it is.   That is a steep price. Look. This needs  tons of memory. Tens of gigabytes.   No wonder this was run on the processor, this  is because modern consumer graphics cards   don’t have nearly as much memory on board, so  what do we do? Well, don’t despair not even   for a second. We still have good news. And the  good news is that there are earlier research works   that explore compressing these datasets down,  and it turns out, their size can be decreased   dramatically. A perfect direction  for the next paper down the line!   And, what do you think? Let me know in the  comments below what you would use this for. And, just one or two more papers down the line,  and maybe, we will get these beautiful simulations   in our virtual worlds in real time. I can’t wait.  I really cannot wait. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
570,Is OpenAI’s AI As Smart As A University Student?,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to have a little taste of how  smart an AI can be these days. And it turns out,   these new AIs are not only smart enough to solve  some grade-school math problems, but get this,   a new development can perhaps even take a crack at  university-level problems. Is this even possible,   or is this science fiction? Well, the  answer is yes, it is possible…kind of.   So, why kind of? Let me try to explain. This is OpenAI’s work from Oct 2021. The goal  is to have their AI understand these questions,   understand the mathematics, and reason about a  possible solution for grade-school problems. Hmm,   alright. So, this means that the GPT-3 AI might  be suitable for the substrate of the solution.   What is that? GPT-3 is a technique that can  understand text, try to finish your sentences,   even build websites, and more. So, can  it even deal with these test questions? Let’s see together. Hold on to your  papers, because in goes a grade-school   level question. A little math brain  teaser if you will. And out comes,   my goodness. Is that right? Here, out comes  not only the correct solution to the question,   but even the thought process that led to this  solution. Imagine someone claiming that they   had developed an AI this capable ten years ago.  This person would have been locked into an asylum.   And now, it is all there, right in  front of our eyes. Absolutely amazing. Okay, but, how amazing? Well, it can’t  get everything right all the time. Not   even close. If we do everything right,  we can expect it to be correct about 35%   of the time. Not perfect, not even  close, but it is an amazing step forward. So what is the key here? Well, yes, you guessed  it right. The usual suspects. A big neural network   and lots of training data, the key  numbers are 175 billion model parameters,   and it needs to read a few thousand  problems and their solutions   as training samples. That is a big rocket,  and lots of rocket fuel if you will. But, this is nothing compared to what is to come.  Now, believe it or not, here is a followup paper   from just a few months later, January 2022  that claims to do something even better.   This is not from OpenAI, but it piggybacks on  OpenAI technology as you will see in a moment.   And this work promises that it can solve  university-level problems. And when I saw   this reading the paper, I thought…really? Now,  grade school materials, okay, that is a great   leap forward, but solving university-level  math exams? Now we’re talking! That’s where   the gloves come off. I am really curious to see  what this can do! Let’s have a look together. Some of these brain teasers smell very  much like MIT to me. Surprisingly short   and elegant questions, that often seem much  easier than they are. However, all of these   require a solid understanding of fundamentals,  and sometimes even a hint of creativity.   Let’s see. Yes! That is indeed right. These are  MIT introductory course questions. I love it. So,   can it answer them? Now, if you have been holding  on to your papers, now, squeeze that paper, and   let’s see the results together…my goodness. These  are all correct. Flying colors! Perfect accuracy,   at least on these questions. This is swift  progress in just a few months. Absolutely amazing. So, how is this black magic done? Yes,  I know that’s what you’re waiting for,   let’s pop the hood, and look inside together.   Um-hm. Alright! Two key differences  from OpenAI’s GPT3-based solution. Difference number one. It gets additional  guidance. For instance, it is told what topic are   we talking about, what code library to reach out  for, and what is the definition of mathematical   concepts, for instance, what is a singular value  decomposition. I would argue that this is not   too bad, students typically get taught these  things before the exam too. In my opinion, the   key is that this additional guidance is done in an  automated manner. The more automated, the better. Difference number two. The  substrate here is not GPT-3,   at least, not directly, but Codex. What is  that? Codex is OpenAI’s GPT language model   that was fine-tuned to be excellent at one  thing. And that is, writing computer programs,   or, finishing your code. And as we’ve seen in  a previous episode, it really is excellent. For   instance, it can not only be asked to explain a  piece of code, even if it is written in assembly.   Or, create a pong game in 30 seconds. But,  we can also give it plain text descriptions   about a space game, and it will write it.  Codex is super powerful. And now, it can be   used to solve previously unseen university-level  math problems. Now that is really something. And, it can even generate a bunch of  new questions, and these are bona fide,   real questions. Not just exchanging the  numbers, the new questions often require   completely different insights to solve  these problems. A little creativity I see!   Well done little AI! So, how good are these? Well,  according to human evaluators, they are almost   as good as the ones written by other humans. And  thus, these can even be used to provide more and   more training data for such an AI. More fuel for  that rocket. And, good kind of fuel. Excellent. And, it doesn’t end there, in the meantime,  as of February 2022, scientists at OpenAI   are already working on a followup paper that  solves no less than high-school mathematical   olympiad problems. These problems require a solid  understanding of fundamentals, proper reasoning,   and often even that is not enough. Many of these  tasks put up a seemingly impenetrable wall,   and climbing the wall typically requires a real  creative spark. Yes, this means that these can   get quite tough. And their new method is doing  really well at these. Once again, not perfect,   not even close, but it can solve about 30 to 40%  of these tasks, a that is a remarkable hit rate. Now we see that all of these works are  amazing, and they have their own tradeoffs.   They are good and bad at different things and  have different requirements. And most of all,   they all have their own limitations. Thus, none  of these works should be thought of as an AI that   just automatically does human-level math. However,  what we now see is that there is swift progress   in this area, and amazing new papers are popping  up not every year, pretty much every month.   And, this is an excellent place to apply The  First Law Of Papers, which says that research is   a process. Do not look at where we are, look at  where we will be two more papers down the line. So, what would you use this for? Please let me  know in the comments below, I’d love to hear   your ideas. And also, if you are excited by this  kind of incredible progress in AI research, make   sure to subscribe and hit the bell icon to not  miss it when we cover these amazing new papers. Thanks for watching and for your generous  support, and I'll see you next time!"
571,Is Simulating A Jelly Sandwich Possible?,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to torment a virtual  armadillo, become happy, or sad depending on   which way we are bending, create a ton of jelly  sandwiches, design a crazy bridge, and more. So, what is going on here? Well, this new paper  helps us enhance our physics simulation programs   by improving how we evaluate derivatives.  Derivatives describe how things change.   Now, of course, evaluating derivatives is not new.  It’s not even old. It is ancient. Here you see   an ancient technique for this, and it works well  most of the time…but whoa! Well, this simulation   blew up in our face, so, yes, it may work well  most of the time, but unfortunately, not here. Now, if you are wondering what should  be happening in this scene. Here is the   reference simulation that showcases  what should have happened. Yes,   this is the famous pastime of the computer  graphics researcher, tormenting virtual   objects basically all day long. So, I hope  you are enjoying this reference simulation.   Because this is a great reference simulation.  It is as reference as a simulation can get.   Now, I hope you know what’s coming. Hold on to  your papers, because this is not the reference   simulation. What you see here is the new technique  described in this paper. Absolutely amazing. Actually, let’s compare the two. This is the  reference simulation, for real this time,   and this is the new, complex  step finite difference method.   The two are so close that they are  essentially the same. I love it. So good. Now, if the comparison made you hungry, of course,  we can proceed to the jelly sandwiches. Here is   the same scene simulated with a bunch of previous  techniques, and…my goodness! All of them look   different! So, which of these jelly sandwiches  is better? Well, the new technique is better,   because this is the only one that preserves volume  properly. This is the one that gets us the most   jelly. With each of the other methods, the jelly  either reacts incorrectly, or at the end of the   simulation, there is less jelly than the amount  we started with. Now, you are probably asking,   is this really possible? Yes, yes it is! What’s  more, this is not only possible, but this is a   widespread problem in physics simulation. Our  seasoned Fellow Scholars had seen this problem   in many previous episodes, for instance, here is  one with the tragedy of the disappearing bunnies. And, preserving the volume of the simulated  materials is not only useful for jelly sandwiches.   It is also useful for doing extreme yoga. Look -  here are a bunch of previous techniques trying to   simulate this, and, what do we see? Extreme  bending, extreme bending… and more extreme   bending. Good I guess? Well, not quite! This yoga  shouldn’t be nearly as extreme as we see here. The   new technique reveals that this kind of bending  shouldn’t happen given these material properties. And wait, here comes one of my favorites.  The new technique can also deal with this   crazy example. Look, a nice little virtual  hyperelastic material, where the bending energy   changes depending on the bending orientation,  revealing a secret. Or, two secrets, as you see,   it does not like bending to the right so much,  but bending towards the left, now we’re talking! And, it can also help us perform  inverse design problems. For instance,   here we have a hyperelastic bridge built from  over 20 thousand parts. And here, we can design   what vibration frequencies should be  present when the wind blows at our bridge.   And here comes the coolest part we can choose  this in advance, and then, the new technique   quickly finds the suitable geometry that  will showcase the prescribed vibration types. It pretty much converges after 4 to 6  Newton iterations. What does that mean?   Yes, it means that the new technique  comes up with an initial guess,   and it needs to refine it only 4 to 6 times  until it comes up with an excellent solution. So, better hyperelastic simulations,  quickly and conveniently? Yes please,   sign me up right now! And, what would you use  this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
572,This New AI Can Find Your Dog In A Video!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to perform pose estimation, with an amazing twist. You’ll love it! But wait, what is pose estimation? Well, simple, a video of people goes in, and the posture they are taking comes out. Now, you see here that previous techniques can already do this quite well, even for videos. So, by today, the game has evolved. Just pose estimation is not that new, we need pose estimation plus something else. We need a little extra, if you will. So, what can that little extra be? Let’s look at three examples. For instance, one, NVIDIA has an highly advanced pose estimation technique that can refine its estimations by putting these humans into a virtual physics simulation. Without that, this kind of foot sliding often happens, but after the physics simulation, not anymore. As a result, it can understand even this explosive sprinting motion. This dynamic tennis serve. You name it. All of them are very close. So, what is this good for? Well, many things, but here is my favorite if we can track the motion well, we can put it onto a virtual character, so we ourselves can move around in a beautiful, imagined world. So, that was one. Pose estimation + something extra, where the something extra is a physics simulation. Nice. Now, two, if we allow an AI to read the wifi-signals bouncing around in a room, it can perform pose estimation, even through walls. Kind of. Once again, pose estimation with something extra. And three, this is pose estimation with inertial sensors. This works when playing a friendly game of table tennis with a friend…or…wait. Or maybe, a not so friendly game of table tennis. And this works really well even in the dark. So, all of these are pose estimation plus something extra. And now, let’s have a look at this new paper, which performs pose estimation, plus…well, pose estimation as it seems. Okay, I don’t see anything nothing new here, really. Why is this work on Two Minute Papers? Well, now hold on to your papers, and check this out. Yes! Oh yes! So what is this? Well, here is the twist. We can give a piece of video to this AI, write a piece of text as you see up here, and it will not only find what we are looking for in the video, mark it, and then, even track it over time. Now that is really cool. We just say what we want to be tracked over the video, and it will do it automatically. It can find the dog and the capybara. These are rather rudimentary descriptions, but it is by no means limited to that. Look, we can also say a man wearing a white shirt and blue shorts riding a surfboard. And, yes! It can find it. And, also we can add a description of the surfboard, and it can tell which is which. And I like the tracking too. This scene has tons of high-frequency changes, lots of occlusion, and it is still doing really well. Loving it. So, I am thinking that this helps us take full advantage of the text descriptions. Look. We can ask it to mark the parrot and the cockatoo, and it knows which is which. So I can imagine more advanced applications where we need to find the appropriate kind of animal or object among many others, and we don’t even need to know what to look for. Just say what you want, and it will find it! I also liked how this is done with a transformer neural network that can jointly process the text and video in one elegant solution. That is really cool. Now, of course, every single one of you Fellow Scholars can see that this is not perfect. Not even close. Depending on the input, temporal coherence issues can arise, these are the jumpy artifacts from frame to frame. But, still, this is swift progress in machine learning research. We could only do this in 2018 and were very happy about it, and just a couple papers down the line, and now, we just say what we want and the AI will do it. And just imagine what we will have a couple more papers down the line. I cannot wait. So, what would you use this for? Please let me know in the comments below! And wait, the source code, an interactive demo, and a notebook are available in the video description. So, you know what to do. Yes, let the experiments begin! Thanks for watching and for your generous support, and I'll see you next time!"
573,NVIDIA's Magical AI Speaks Using Your Voice!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, through the power of AI research, we are going to see how easily we can ask virtual characters to not only say what we say, but we can even become an art director and ask them to add some emotion  to it. So, how does this happen? Well, in goes what we say, for instance, me uttering Dear Fellow Scholars, or anything else. And, here is the key, we can also specify the emotional state of the character. And this AI does the rest. That is absolutely amazing. But, it gets even more amazing. Now, hold on to your papers, and, look. Yes, that’s right, this was possible back in 2017, approximately 400 Two Minute Papers episodes ago. And whenever I showcase results like this, I always get the question from you Fellow Scholars, asking that “yes, this all looks great, but when do I get to use this?”. And the answer is, right now. Why? Because NVIDIA has released Audio2Face, a collection of AI techniques that we can use to perform this quickly and easily. Look, we can record our voice live, and have a virtual character say what we are saying. But, it does not stop there, it also has 3 amazing features. One, we can even perform a face swap, not only between humanoid characters, but, my goodness. Even from a humanoid to, for instance, a Rhino! Now that’s something. I love it. But wait, there is more. There is this. And this too. Two, we can still specify emotions, like anger, sadness and excitement, and the virtual character will perform that for us. We only need to provide our voice, no more acting skills are required. In my opinion, this will be a godsend in any kind of digital media, computer games, or even when meeting our friends in a virtual space. Three, the usability of this technique is out of this world. For instance, it does not eat a great deal of resources, so we can run multiple instances of it at the same time. This is a wonderful usability feature, one of many that really makes or breaks when it comes to a new technique being used in the industry or not. An aspect not to be underestimated. And here is another usability feature: it works well with Unreal Engine's MetaHuman, this is a piece of software that can create virtual humans. And with that, we can not only create these virtual humans, but become the voice actors for them without having to hire a bunch of animators. How cool is that. Now I believe this is an earlier version of MetaHuman, here is the newer one. Wow, way better. Just imagine how cool it would be to voice these characters automatically. Now, the important lesson is that this was possible in a paper in 2017, and now, in a few years it has vastly improved, so much so that it is now out there deployed in a real product that we can use right now. That is a powerful democratizing force for computer animation. So, yes, the papers that you see here are real. As real as it gets, and this tech transfer can often occur in just a few years time. In some other cases, even quicker. What a time to be alive! So, what would you use this for? I’d love to know what you think. Let me know in the comments below! Thanks for watching and for your generous support, and I'll see you next time!"
574,DeepMind's New AI: As Smart As An Engineer... Kind Of!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. You will not believe the things you will see in this episode, I promise. Earlier we saw that OpenAI’s GPT3 and Codex AI techniques can be used to solve grade school level math brain teasers, then, they were improved to be able to solve university-level math questions. Note that this technique required additional help to do that. And, a followup work could even take a crack at roughly a third of mathematical olympiad level math problems. And now, let’s see what the excellent scientists at DeepMind have been up to in the meantime. Check this out this is AlphaCode. What is that? Well, in college, computer scientists learn how to teach the computers to program themselves. Now, DeepMind decided to instead, teach them how to program themselves. Wow. Now, here you see an absolute miracle in the making. Look. Here is the description of the problem, and here is the solution. Well, I hear you saying, Károly, there is no solution here. And you are indeed right, just give it a second. Yes, that’s right! Now, hold on to your papers, and marvel at how this AI is coding up the solution right on front of our eyes! But that’s nothing, we can also ask what the neural network is looking at. Check this out! It is peeping at different, important parts of the problem statement, and proceeds to write the solution taking these into consideration. You see that it also looks at different parts of the previous code that it had written to make sure that the new additions are consistent with those. Wow. That is absolutely amazing. It almost feels like watching a human solve this problem. Well, it solved this problem correctly. Unbelievable. So, how good is it? Well, it can solve solve about 34% of the problems in this dataset. What does that mean? Is that good or bad? Now, if you have been holding on to your papers so far, now, squeeze that paper, because it means that it roughly matches the expertise level of the average human competitor. Let’s stop there for a moment and think about that. An AI that understands of an English description mixed in with mathematical notation, and codes up a solution as well as the average human competitor, at least, on the tasks given in this dataset. So, what is this wizardry, and what is the key? Well, let’s pop the hood, and have a look together! Oh yes! One of the keys here is that it generates a ton of candidate programs and is able to filter them down to just a few promising solutions. And, it can do this quickly and accurately. This is huge. Why? Because this means that the AI is able to have a quick look at a computer program and tell with a pretty high accuracy whether this will solve the given task or not. It has an intuition of sorts, if you will. Now, interestingly, it also uses 41 billion parameters. 41 billion is tiny compared to OpenAI’s GPT-3, which has 175 billion. This means that currently, AlphaCode uses a more compact neural network, and it is possible that the number of parameters can be increased here to further improve the results. If we look at DeepMind’s track record of improving on these ideas, I have no doubt that however amazing these results seem now, we have really seen nothing yet. And wait, there is more this is where I completely lost my papers: in the case that you see here, it even learned to invent algorithms. A simple one, mind you, this is DFS, a search algorithm that is taught in first-year undergrad computer science courses, but that does not matter. What matters is that this is an AI that can finally, invent new things. Wow. A Google engineer, who is also a world-class competitive programmer was also asked to have a look at these solutions, and he was quite happy with the results. He said to one of the solutions that “it looks like it could easily be written by a human, very impressive!” Now, clearly, it is not perfect, and thus, some criticisms were also voiced. For instance, sometimes it forgets about variables that remain unused. Even that is very humanlike, I must say. But, do not think of this paper as the end of something. This is but a stepping stone towards something much greater. And I will be honest I can’t even imagine what we will have just a couple more papers down the line. What a time to be alive! So, what would you use this for? What do you think will happen a couple papers down the line? I’d love to know let me know in the comments below. And, if you are also excited to hear about potential followup papers to this, make sure to subscribe and hit the bell icon. You definitely not want to not miss it when it comes! Thanks for watching and for  your generous support, and I'll see you  next time!"
575,This Blind Robot Can Walk...But How?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how a robot doggy can help us explore dangerous areas without putting humans at risk. Well, not this one, mind you, because this is not performing too well. Why is that? Well, this is a proprioceptive robot. This means that the robot is essentially blind. Yes. Really. It only senses its own internal state and that’s it. For instance, it knows about its orientation and twist of the base unit, a little joint information, like positions and velocities, and that’s about it. It is still remarkable that it can navigate at all. However, this often means that the robot has to feel out the terrain before being able to navigate on it. Why would we do that? Well, if we have difficult seeing conditions, for instance, dust, fog, smoke, a robot that does not rely on seeing is suddenly super useful. However, when we have good seeing conditions, as you see, we have to be able to do better than this. So, can we? Well, have a look at this new technique. Well, now we’re talking! This new robot is exteroceptive, which means that it has cameras, it can see, but it also has proprioceptive sensors too, these are the ones that tell the orientation and similar information. But, this new one fuses together proprioception and exteroception. What does that mean? The best of both worlds! Here you see how it sees the world. Thus, it does not need to feel out the terrain before the navigation, but, and here is the key. Now, hold on to your papers, because it can even navigate reasonably well even if its sensors get covered. Look. That is absolutely amazing. And, with the covers removed, we can give it another try, let’s see the difference…and, oh yeah! Back in the game baby! So far, great news! But, what are the limits of this machine? Can it climb stairs? Well, let’s have a look. Yup. Not a problem. Not only that, but it can even go on a long hike in Switzerland and reach the summit without any issues. And in general, it can navigate a variety of really difficult terrains. So, if it can do all that, now, let’s be really tough on it, and put it to the real test. This is the testing footage before the grand challenge. Flying colors. So, let’s see how it did in the grand challenge itself. Oh my. You see, here, it has to navigate an underground environment completely autonomously. This really is the ultimate test. No help is allowed, it has to figure out everything by itself. This test has uneven terrain with lots of rubble, difficult seeing conditions, dust, tunnels, caves, you name it. An absolute nightmare scenario. And, this is incredible. During this challenge, it explored more than a mile, and how many times has it fallen? Well, what do you think? Zero. Yes. Zero times. Wow. Absolutely amazing. While we look at how well it can navigate these super slippery and squishy terrains, let’s ask the key question: what is all this good for? Well, chiefly, for exploring under explored and dangerous areas. This means tons of useful applications, for instance, a variant of this this could help save humans stuck under rubble, or perhaps even explore other planets without putting humans at risk. And more. And, what do you think? What would you use this for? Let me know in the comments below! Thanks for watching and for your generous support, and I'll see you next time!"
576,DeepMind AlphaFold: A Gift To Humanity!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Oh my goodness. This work is history in the making. Today we are going to have a look at AlphaFold, perhaps one of the most important papers of the last few years. And you will see that nothing that came before even comes close to it, and that it truly is a gift to humanity. So, what is AlphaFold? AlphaFold is an AI that is capable of solving protein structure prediction, which we will refer to as protein folding. Okay…but what is a protein and why does it need folding? A protein is a string of amino acids, these are the building blocks of life. This is what goes in, which in reality, has a 3D structure. And that is protein folding. Letters go in, a 3D object comes out. This is hard. How hard exactly? Well, let’s compare it to DeepMind’s amazing previous projects, and we’ll see that none of these projects even come close in difficulty. For instance, DeepMind’s previous AI learned to play chess. Now, why does this matter, as we already have Deep Blue, which is a chess computer that can play at the very least as well as Kasparov did, and it was built in 1995. So, why is chess interesting? The space of possible moves is huge. And Deep Blue in 1995 was not an AI in the strictest sense, but a handcrafted technique. This means that it can play chess and that’s it. One algorithm, one game. If you want a different game, you write a different algorithm. And, yes, that is the key difference. DeepMind’s Chess AI is a general learning algorithm that can learn many games, for instance, Japanese chess, Shogi too. One algorithm, many games. And, yes, chess is hard. But these days, the AI can manage. Then, Go is the next level. This is not just hard, it is really hard. The space of possible moves is significantly bigger, and we can’t just evaluate all the long-term effects of our moves, it is even more hopeless than chess, and that’s often why people say that this game requires some sort of intuition to play. But DeepMind’s AI solved that too and beat the world champion Go player 4 to 1 in a huge media event. The AI can still manage. Now, get this, if Chess is hard, and Go is very hard, then protein folding is sinfully difficult. Once again, a string of text encoding the amino acids go in, a 3D structure comes out. Why is this hard? Why not just try every possible 3D structure and see what sticks? Well, not quite. The search space for this problem is still stupendously large, perhaps not as big as playing a continuous strategy game like Starcraft 2, but the search here is much less forgiving. Also, we don’t have access to a perfect scoring function, so it is very difficult to define what exactly should be learned. In a strategy game, a win is a win, but for proteins, nature doesn’t really tell us what it is up to when creating these structures. Thus, DeepMind did very well in Chess and Go, and Starcraft too, and challenging as they are, they are not even close to being as challenging as protein folding. Not even close. To demonstrate that, look, this is CASP. I've heard DeepMind CEO Demis Hassabis call it the Olympics of protein folding. If you look at how teams of scientists prepare for this event, you will probably agree that yes, this is indeed the Olympics of protein folding. At about a score of 90, we can think of protein folding as a mostly solved problem. No need to worry about definitions though, look, we are not even close to 90. And it gets even worse, look. This GDT score means the global distance test, this is a measure of similarity between the predicted and the real protein structure. And, wait a second. What? The results are not only not too good, but they appear to get worse over time. Is that true? What is going on here? Well, there is an explanation. The competition gets a little harder over time, so even flat results mean that there is a little improvement over time. And now, hold on to your papers, and let’s look at the results from DeepMind’s AI-based solution, AlphaFold. Wow, now we’re talking. Look at that! The competition gets harder, and it is not only flat, but can that be? It is even better than the previous methods. But, we are not done here, no-no, not even close. If you have been holding on to your papers, now, squeeze that paper, because what you see here is old news. Because only two years later Alphafold 2 appeared. And just look at that. It came in guns blazing. So much so that the result is…I can’t believe it! It is around the 90 mark. My goodness, that is history in the making. Yes, this is the place on the internet where we get unreasonably excited by a large blue bar. Welcome to Two Minute Papers! But what does this really mean? Well, in absolute terms, AlphaFold 2 is considered to be about three times better than previous solutions. And all that in just two years. That is a miracle right in front of our eyes. Now, let’s pop the hood, and see what is inside this AI, and...hmm! Look at all these elements in the system that make this happen. So, where do we start? Which of these is the most important? What is the key? Well, everything…and nothing. I will explain this in a moment. That does not sound very enlightening, so, what is going on? DeepMind ran a detailed ablation study on what mattered and the result is the following: everything mattered. Look. With few exceptions, every part adds its own little piece to the final result, but, none of these techniques are a silver bullet. But to understand a bit more about what is going on here, let’s look at three things. One, AlphaFold 2 is an end to end network that can perform iterative refinement. What do these mean? What this means is that everything needed to solve the task is learned by the network, and that it starts out from a rough initial guess, and then, it gradually improves it. You see this process here, and it truly is a sight to behold. Two, it uses an attention-based model. What does that mean? Well, look! This is a convolutional neural network. This is wired in a way that information flows to neighboring neurons. This is great for image recognition, because usually, the required information is located nearby. For instance, let’s imagine that we wish to train a neural network that can recognize a dog. What do we need to look at? Floppy ears, black snout, fur, okay, we’re good, we can conclude that we have a dog here. Now, have you noticed? Yes, all of this information is  located nearby. Therefore a convolutional neural network is expected to do really well at that. However, check this out. This is a transformer, which is an attention-based model. Here, information does not flow between neighbors. No sir! Here, information flows everywhere! This has spontaneous connections that are great for almost anything if we can use them well. For instance, when reading a book, if we are at page 100, we might need to recall some information from page 1. Transformers are excellent for tasks like that. They are still quite new, just a few years old, and are already making breakthroughs. So, why use them for protein folding? Well, things that are 200 amino acids apart in the text description can still be next to each other in the 3-D space. Yes, now we know that for that, we need attention networks, for instance, a transformer. These are seeing a great deal of use these days, for instance, Tesla also uses them for training their self-driving cars. Yes, so these things mattered. But so many other things did too. Now, I mentioned that the key is everything…and nothing. What does that mean? Well, look here. Apart from a couple examples, there is no silver bullet here. Every single one of these improvements bump the score a little bit. But all of them are needed for the breakthrough. Now, one of the important elements is also adding physics knowledge. How do you do that? Typically the answer is that you don’t. When we design a handcrafted technique, we write the knowledge into an algorithm by hand. For instance, in chess, there are a bunch of well-known openings for the algorithm to consider. For protein folding, we can tell the algorithm that if you see this structure, it typically bends this way. Or we can also show it common protein templates, kind of like openings for chess. We can add all this valuable expertise to a handcrafted technique. Now, we noted that scientists at DeepMind decided to use an end to end learning system. I would like to unpack that for a moment, because this design decision is not trivial at all. In fact, in a moment, I bet you will think it’s flat out counterintuitive. Let me explain. If we are a physics simulation researcher and we have a physics simulation problem, we take our physics knowledge, and write a computer program to make use of that knowledge. For instance, here, you see this being used to great effect, so much so that what you see here is not reality, but a physics simulation. All handcrafted. Clearly, using this concept, we can see that human ingenuity goes very far, and we can write super powerful programs. Or, we can do end to end learning, where, surprisingly, we don’t write our knowledge into the algorithm at all. We give it training data instead, and let the AI build up its own knowledge base from that. And AlphaFold is an end to end learning project, so almost everything is learned. Almost. And, one of the great challenges of this project was to infuse the AI with physics knowledge without impacting the learning. That is super hard. So, training huh? How long does this take? Well, get this, DeepMind can train this incredible folding AI in as little as 2 weeks. Why is two weeks little? Well, after this step is done, the AI can be given a new input and will be able to create this 3D structure in about a minute. And, we can then reuse this trained neural network for as long as we wish. Whew, so… this is a lot of trouble to fold these proteins. So what is all this good for? The list of applications is very impressive, I’ll give you just a small subset of them that I really liked: it helps with us better understand the human body, or create better medicine against malaria and many other diseases, develop more healthy food, or develop enzymes to break down plastic waste, and more. And that’s just the start. Well, you are probably asking, Károly, you keep saying that this is a gift to humanity. So…why is it a gift to humanity? Well, here comes the best part. A little after publishing the paper, DeepMind made these 3D structure predictions available for free for everyone. For instance, they have made their human protein predictions public. Beyond that, they already have made their predictions public for yeast, important pathogens, crop species, and more. And I have already seen followup works on how to use this for developing new drugs. What a time to be alive! Now note that this is but one step in a thousand-step journey. But one important step nonetheless. And, I would like to send huge congratulations to DeepMind. Something like this costs a ton to develop, and note that it is not easy, or maybe not even possible to immediately make a product out of this and monetize it. This truly is a gift to humanity, and a project like this can only emerge from proper long term thinking that focuses on what matters in the long term. Not just thinking about what is right now. Bravo. Now, of course, not even AlphaFold 2 is perfect. For instance, it's not always very confident about its own solutions. It also performs poorly in antibody interactions. Both of these are subject to intense scrutiny and followup papers are already appearing in these directions. Now, one last thing. Why does this video exist? I got a lot of questions from you asking why I made no video on AlphaFold. Well, protein folding is a highly multidisciplinary problem, which, beyond machine learning, requires tons of knowledge in biology, physics, engineering. And my answer was that I don’t feel qualified to speak about this project, so I better not. However, something has changed. What has changed? Well, now I had the help of someone who is very qualified. As qualified as it gets, because it is the one and only John Jumper, the first author of the paper, who kindly agreed to review the contents of this video to make sure that I did not mess up too badly. Thus, I would like to send a big thank you to John, his team, and DeepMind, for creating AlphaFold, and helping this video to come into existence. It came late, so we missed out on a ton of views, but that doesn't matter. What matters is that you get an easy to understand and accurate description of AlphaFold. Thank you so much for your patience! Thanks for watching and for your generous support, and I'll see you next time!"
577,NVIDIA's New AI: Enhance!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how NVIDIA’s new  AI can do 3 seemingly impossible things   with just one elegant technique. For reference, here is a previous method that  can take a collection of photos like these,   and magically, create a video where we can fly  through these photos. This is what we call a   NERF-based technique, and these are truly amazing.  Essentially, photos go in, and reality comes out. So, I know you are thinking, Károly, this looks  like science fiction. Can even this be topped?   And the answer is yes. Yes it can! Now, you see, this new technique can also   look at a small collection of photos, and, be  it people or cats, and it learns to create a   continuous video of them. These look  fantastic. And remember, most of the   information that you see here is synthetic.  Which means, it is created by the AI. So good! But, wait, hold on to your papers,   because there is a twist! It is often the  case for some techniques that they think   in terms of photos. While other techniques  think in terms of volumes. And get this,   this is a hybrid technique that thinks in terms of  both! Okay…so what does that mean? It means this!   Yes, it also learned to now only generate these  photos, but also, the 3D geometry of these models   at the same time. And this quality for the results  is truly something else. Look at how previous   techniques struggle with the same task. Wow. They  are completely different than the input model.   And you might think that of course they are not  so good. They are probably very old methods. Well,   not quite! Look, these are not some ancient  techniques, for instance, GIRAFFE is from   the end of 2020 to the end of 2021 depending  on which variant they used. And, let’s see   what the new method does on the same data…wow. My  goodness. Now that is something. Such improvement   in so little time. The pace of progress in  AI research is nothing short of amazing. And not only that, but everything it produces is  multi-view consistent. This means that we don’t   see a significant amount of flickering as we  rotate these models. There is a tiny bit on the   fur of the cats, but other than that, very little.  That is a super important usability feature. But wait, it does even more. Two, it can also  perform one of our favorites, super resolution.   What is super resolution? Simple, a coarse,  pixelated image goes in, and what comes out?   Of course, a beautiful, detailed image. And here comes number three. It projects these  images into a latent space. What does that mean?   A latent space is a made-up place  where we are trying to organize data   in a way that similar things are close  to each other. In our earlier work,   we were looking to generate hundreds of variants  of a material model to populate this scene. In   this latent space, we can concoct all of  these really cool digital material models.   A link to this work is available  in the video description. Now, let's see. Yes. When we take a walk in  the internal latent space of this technique,   we can pick a starting point, a human face,  and generate these animations as this face   morphs into other possible human faces. In short,  this can generate a variety of different people.   Very cool. I love it! Now, of course,  not even this technique is perfect, I see   some flickering around the teeth, but otherwise,   this will be a fantastic tool for creating virtual  people. And remember, not only photos of virtual   people, we get the 3D geometry for their  heads too. With this, we are one step closer   to democratizing the creation of virtual humans  in our virtual worlds. What a time to be alive! And, if you have been holding on to your papers  so far, now, squeeze that paper! Because get this,   you can do all of this in real time. And all  of these applications can be done with just one   elegant AI technique. Once again, scientists at  NVIDIA knocked it out of the park with this one.   Bravo. So, what about you? If all this can be  done in real time, what would you use this for?   I’d love to know please let  me know in the comments below. Thanks for watching and for your generous  support, and I'll see you next time!"
578,This AI Creates Beautiful Light Simulations!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to have a look at this insane   light transport simulation paper and  get our minds blown in three chapters. Chapter 1. Radiosity. Radiosity is an  old old light transport algorithm that   can simulate the flow of light within a scene.  And it can generate a scene of this quality.   Well, not the best, clearly, but this  technique is from decades ago. Back in the day,   this was a simple and intuitive attempt  at creating a light simulation program,   and quite frankly, the best we could do. It  goes something like this: slice up the scene   into tiny surfaces, and compute the light  transport between these tiny surfaces. This worked reasonably well for diffuse  light transport, matte objects if you will.   But, it was not great at rendering shiny objects.  And it gets even worse, look. You see these blocky   artifacts here, these come from the fact that the  scene has been subdivided into these surfaces,   and the surfaces are not fine enough  for these boundaries to disappear. But, yes, I hear you asking, Károly,   why talk about this ancient technique? I’ll tell  you in a moment. You’ll see, I promise. So, yes,   radiosity is old, some professors still teach  it to their students, it makes an interesting   history lesson, but I haven’t seen any use of  this technique in the industry in decades now.   If radiosity would be a vehicle, it would be a  horse carriage in the age of high-tech Tesla cars. So, I know what you’re thinking. Yes,  let’s have a look at those Teslas. Chapter 2. Neural rendering. Many  modern light simulation programs   can now simulate proper light transport with  shiny objects, none of these blocky artifacts,   they are, as you see, absolutely amazing.  They can render all kinds of material models,   detailed geometry, caustics, color bleeding,  you name it. However, they seem to start   out from a noisy image, and as we compute the  path of more and more light rays, this noisy image   clears up over time. But, this still takes a  while. How long? Well, from minutes to days. And then, neural rendering entered the  fray. Here you see our earlier paper   that replaced the whole light simulation  program with a neural network that learned   how to do this. And it can create these images  so quickly that it easily runs not in minutes   or days, but as fast as you see here. Yes,  in real time on a commodity graphics card.   Now note that this neural renderer  is limited to this particular scene. With this, I hope that it is  easy to see that we are now   so far beyond radiosity that it sounds like a  distant memory of the olden times. So, once again,   why talk about radiosity? Well, check this out. Chapter 3. Neural Radiosity. Excuse  me, what? Yes, you heard it right.   This paper is about neural radiosity. This  work elevates the old old radiosity algorithm   by using a similar formulation to the original  technique, but also, infusing it with a powerful   neural network. It is using the same horse  carriage, but strapping it onto a rocket,   if you will. Now you have my attention.  So, let’s see what this can do together. Look. Hmm! Yes, it can render really  intense specular highlights. And now,   hold on to your papers, and wow. Once again,  the results look like a nearly pixel-perfect   copy of the reference simulation. And, we now  understand the limitations of the old radiosity,   so, let’s strike where it hurts the most.  Yes, perfectly specular, mirror-like surfaces.   Let’s see what happens here. Well, I can  hardly believe what I am seeing here!   No issues whatsoever. Still, close  to pixel perfect. This new paper   truly elevates the good old radiosity  to the next level. So good! Loving it. But wait a second. I hear you asking, yes,  Károly, this is all well and good, but   if we have the reference simulation, why  not just use that? Good question. Well,   that's right, the reference is great, but that  one takes up to several hours to compute, and   the new technique can be done super quickly. Yet,  they look almost exactly the same. My goodness. In fact, let’s look at an equal-time comparison  against one of the Tesla techniques, path tracing.   We give the two techniques the same amount of  time, and see what they can produce. Let’s see.   Now that is no contest. Look. This is Eric Veach’s  legendary scene where the light only comes from   the neighboring room through a door that is only  slightly ajar. Notoriously difficult for any kind   of light transport algorithm. And yet, look  at how good this new one is in tackling it. Now, note that not even this technique is perfect.  It has two caveats. One, the training takes place   per-scene. This means that we need to give  these scenes to the neural network in advance,   so it can learn how light bounces off of this  place, and this can take from minutes to hours.   But, for instance, if we have a video game  with a limited set of places that you can go,   we can train our neural network on all of  them in advance, and deploy it to the players,   who can then enjoy it for as long as they wish.  No more training is required after that. But,   the technique would still need to be  a little faster than it currently is.   And two, we also need quite a bit  of memory to perform all this. And, yes! I think this is an excellent  place to invoke the First Law Of Papers,   which says that research is a  process. Do not look at where we are,   look at where we will be two more papers down  the line. And two more papers down the line,   who knows, maybe we get this in real time and  with a much friendlier memory consumption. Now, there is one more area that I think would  be an excellent direction for future work. And   that is about the per-scene training  of the neural network. In this work,   it has to get a feel of the scene before the light  simulation happens. So how does the knowledge   learned on one scene transfer to others? I imagine  that it should be possible to create a more   general version of this that does not need to look  at a new scene before the simulation takes place. And in summary. I absolutely love this  paper. I takes an old old algorithm,   blows the dust off of it, and  completely reinvigorates it   by infusing it with a modern learning-based  technique. What a time to be alive! So, what do you think? What would you use  this for? I’d love to hear your thoughts,   please let me know in the comments below. And when watching all these beautiful results, if  you feel that this light transport thing is pretty   cool, and you would like to learn more about it,  I held a Master-level course on this topic at the   Technical University of Vienna. Since I was always  teaching it to a handful of motivated students,   I thought that the teachings shouldn’t  only be available for the privileged few   who can afford a college education, but the  teachings should be available for everyone.   Free education for everyone, that’s what I want.  So, the course is available free of charge for   everyone, no strings attached, so make sure to  click the link in the video description to get   started. We write a full light simulation program  from scratch there, and learn about physics,   the world around us, and more. If you watch  it, you will see the world differently. Thanks for watching and for your generous  support, and I'll see you next time!"
579,Waymo's AI Recreates San Francisco From 2.8 Million Photos!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see if Waymo’s AI   can recreate a virtual copy of San  Francisco from 2.8 million photos. To be able to do that, they rely on a previous  AI-based technique that can take a collection   of photos like these, and magically, create a  video where we can fly through these photos.   This is what we call a NERF-based technique,  and these are truly amazing. Essentially,   photos go in, the AI fills in the gaps, and  reality comes out. And there are a lot of   gaps between these photos, all of which are  filled in with high-quality synthetic data. So, as you see with these previous methods, great  leaps are being made, but one thing stayed more or   less the same: and that is, the scale of these  scenes is not that big. So, scientists at Waymo   had a crazy idea, and they said, rendering  just a tiny scene is not that useful. We have   millions of photos laying around, why not render  an entire city like this? So, can Waymo do that?   Well, maybe, but that would take Waymo'. I am  sorry. I am so sorry, I just couldn't resist. Well, let’s see what they came up with. Look,  these self-driving cars are going around the city,   they take photos along their journey, and…well,  I have to say that I am a little skeptical here.   Have a look at what previous techniques could do  with this dataset. This is not really usable, so,   could Waymo pull this off? Well, hold on to  your papers, and let’s have a look together.   My goodness…this is their fully reconstructed   3D neighborhood from these  photos. Wow. That is superb. And don’t forget, most of this  information is synthetic. That is,   filled in by the AI. Does this mean that? Yes!  Yes it does! It means three amazing things. One, we can drive a different path that  has not been driven before by these cars,   and still see the city correctly.  Two, we can look at these  buildings from viewpoints   that we don’t have enough information about,  and the AI fills in the rest of the details.   So cool! But it doesn't end there, not even close. Three, here comes my favorite. We can also engage   in what they call appearance modulation.  Yes, some of the driving took place at night,   some during the daytime, so we have information  about the change of the lighting conditions.   What does that mean? It means that we can  even fuse all this information together   and choose the time of day for our virtual  city. That is absolutely amazing. I love it. Yes, of course, not even this technique  is perfect, the resolution and the details   should definitely be improved over time.  Plus, it does well with a stationary city,   but with dynamic moving objects, not so  much. But do not forget, the original,   first NERF paper was published just  two years ago, and it could do this.   And now, just a couple papers down the line, and  we have not only these tiny scenes, but entire   city blocks. So much improvement in just a couple  papers. How cool is that. Absolutely amazing. And with this, we can drive and play  around in a beautiful virtual world   that is a copy of the real world around us. And  now, if we wish it to be a little different,   we can even have our freedom  in changing this world   according to our artistic vision. I would  love to see more work in this direction. But wait, here comes the big question. What  is all this good for? Well, one of the answers   is sim2real. What is that? Sim2real means  training an AI in a simulated world, and   trying to teach it everything it needs to learn  there before deploying it into the real world. Here is an amazing example, look, OpenAI  trained their robot hand in a simulation   to be able to rotate these Rubik cubes. And then,  deployed this software onto a real robot hand,   and, look! It can use this simulation knowledge,  and now it works in the real world too.   But sim2real has relevance to self-driving cars  too. Look. Tesla is already working on creating   virtual worlds, and training their cars there. One  of the advantages of that is that we can create   really unlikely and potentially unsafe scenarios,  but, in these virtual worlds, the self-driving AI   can train itself safely. And when they deploy  them into the real world, they will have all   this knowledge. It is fantastic to see Waymo also  moving in this direction. What a time to be alive! So, what would you use this for? What do you  expect to happen a couple more papers down the   line? Please let me know in the comments  below. I’d love to hear your thoughts. Thanks for watching and for your generous  support, and I'll see you next time!"
580,Adobe’s New AI: Next Level Cat Videos!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. As you see here, I love Two Minute Papers, but  fortunately I am not the only one, Elon Musk   loves Two Minute Papers too. What’s more, even  cats love it. Everybody loves Two Minute Papers.   So, what was that? Well, these are all synthetic  videos that were made by this new, magical AI. So, here, we are adding synthetic data to an  already existing photorealistic video, and the   AI is meant to be able to understand the movement,  facial changes, geometry changes, and track them   correctly and move the moustache, the tattoos, or  anything else properly. That is a huge challenge. How is that even possible? Many AI-based  techniques from just a year or two ago   were not even close to being able to pull this  off. However, now, hold on to your papers,   and have a look at this new technique, now that  is a great difference. Look at that! So cool! This is going to be really useful  for at the very least two things.   One, augmented reality applications. For instance, have a look at this cat. Well, the  bar on the internet for cat videos is quite high,   and this will not cut it. But  wait…yes! Now we’re talking! The other examples also showcase that the progress  in machine learning and AI research these days   is absolutely incredible. I would like to send a  big thank you to the authors for taking the time   off their busy day to create these results only  for us. That is a huge honor. Thank you so much! So far, this is fantastic news. Especially  given that many works are limited to one domain,   for instance, for instance, Microsoft’s human  generator technique is limited to people. However,   this works on people, dogs, cats, and even Teslas.  So good. I love it. What a time to be alive! But wait, I promised two applications,  not one. So, what is the other one?   Well, image editing. Image editing? Really? What  about that? That is not that new…but…aha! Not just   image editing, but mass scale image editing!  Everybody can get antlers, tattoos, stickers, you   name it. We just chuck in the dataset of images,  choose the change that we wish to make, and out   comes the entirety edited dataset automatically.  Now that is a fantastic value proposition. But, of course, you, the seasoned  Fellow Scholar immediately knows,   that surely, not even this technique is  perfect. So, where are the weak points? Look.   Oh yes, that. I see this issue for nearly  every technique that takes on this task.   It still has troubles with  weird angles and occlusions. But, we also have good news. If you can navigate  your way around an online Colab notebook,   you can try it too! You know what to  do. Yes, let the experiments begin! So, what would you use this for? Who would  rock the best moustache? And, what do you   expect to happen a couple more papers down the  line? Please let me know in the comments below. Thanks for watching and for your generous  support, and I'll see you next time!"
581,OpenAI’s New AI Thinks That Birds Aren’t Real!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to explore what happens  if we unleash an AI to read the internet,   and then, ask it some silly questions and  we’ll get some really amazing answers. But how? Well, in the last few years, OpenAI set  out to train an AI named GPT-3 that could finish   your sentences. Then, they made Image-GPT, this  could even finish your images. Yes, not kidding.   It could identify that the cat here likely holds a  piece of paper and finish the picture accordingly,   and even understood that if we have a droplet  here and we see just a portion of the ripples,   then this means a splash must be filled in. So, in summary, GPT-3 is trained to  finish our sentences, or even your images.   However, scientists at OpenAI identified that  it really isn’t great at following instructions.   Here is an excellent example. Look, we ask it  to explain the moon landing to a 6-year old,   it gets very confused. It seems to  be in text completion mode instead of   trying to follow our instructions.  Meanwhile, this is InstructGPT,   their new method, which can not only finish our  sentences, but also, follow our instructions.   And, would you look at that! It does  that successfully. Good job, little AI. This was a really simple example. But of  course, we are experienced Fellow Scholars here,   so let’s try to find out what this AI is really  capable of in three really cool examples. Remember in The Hitchhiker's Guide  To The Galaxy where people could ask   an all-knowing machine the most important  questions of things that trouble humanity.   Yes! We are going to do exactly that.  Round 1. So, dear all-knowing AI,   what happens if you fire a cannonball  directly at a pumpkin at high speeds? Well. What? Do you see that? According  to GPT-3, pumpkins are strong magnets,   I don’t know which internet  forum told the AI that.   Not good. Now, hold on to your papers, and  let’s see the new technique’s answer together.   Wow, this is so much more informative. Let’s  break it down together. It starts out by hedging,   noting that is it hard to say there are  too many unpredictable factors involved.  Annoying as it might seem, it is correct to  say all this. Good start. Then, it lists some   of the factors that might decide the fate of  that pumpkin, like the size of the cannonball,   distance and velocity, yes, we’re getting  there, but please, give me something concrete.   Yes, there we go! It says that, quote “Some of the  more likely possible outcomes include breaking or   knocking the pumpkin to the ground, cracking  the pumpkin, or completely obliterating it.”   Excellent. A little smartypants AI  at our disposal. Amazing. I love it. Round 2, code summarization. While  DeepMind’s AlphaCode is capable of reading   a competition-level programming problem and coding  up a correct solution right in front of our eyes.   That is all well and good, but if we give GPT-3  a piece of code and ask it what it does, well,   the answer is not only not very informative,  but it’s also incorrect. InstructGPT   gives a much more insightful answer which shows  a bit of understanding of what this code does.   That is amazing. Note that it  is still not completely right,   but it is directionally correct. Partial credit. Round 3, write me a poem. About  what? Well, about a wise frog.  With GPT-3, we get the usual confusion. By round  3 we really see that it was really not meant to   do this. And, with InstructGPT, let’s see. Hmm…an  all-knowing frog, who is the master of disguise,   a great teacher, and quite possibly  the bringer of peace and tranquility   to humanity. All written by the AI. This is  fantastic. I love it. What a time to be alive! Now, we only looked at 3 examples here,   but what about the rest? Worry not for a second,  OpenAI’s scientists ran a detailed user study,   and found out that people preferred InstructGPTs  solutions way more often than the previous   techniques on a larger set of questions. That  is a huge difference. Absolutely amazing.   Once again, incredible progress  just one more paper down the line. So, is it perfect? No, of course not, so let’s  highlight one of its limitations. If the question   contains false premises, it accepts the premise  as being real, and goes with it. This leads to   making things up. Yes, really. Check this out. Why aren’t birds real? GPT-3 says….something.   I am not sure what this one is about. This almost  sounds like gibberish. While, InstructGPT accepts   the fact that birds aren’t real, and even helps us  craft an argument for that. This is a limitation,   and I must say, a quite remarkable one. An  AI that makes things up. Food for thought. And, remember, at the start of this episode,  also looked at this moon landing example.   Did you notice the issue there? Please  let me know in the comments below! So,   what is the issue here? Well, beyond not being  all that informative, it was asked to describe   the moon landing in a few sentences. This is not  a few sentences. This is one sentence. If we give   it constraints like that, it tries to adhere  to them, but is often not too great at that. And, of course, both of these shortcomings show  us the way for an even better followup paper,   which, based on previous progress in AI  research, could appear maybe not even in years,   but much quicker than that. If you are  interested in such a followup work, make   sure to subscribe and hit the bell icon to not  miss it when it appears here on Two Minute Papers. So, what would you use this for? What do you  expect to happen a couple more papers down the   line? Please let me know in the comments  below. I’d love to hear your thoughts. Thanks for watching and for your generous  support, and I'll see you next time!"
582,NVIDIA's New AI: Next Level Image Editing!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to do some incredible experiments with NVIDIA’s next level image editor AI. Now, there are already many AI-based techniques out there that are extremely good at creating new human, or even animal faces. You see here how beautifully this Alias-Free GAN technique can morph one result into another. It truly is a sight to behold. They are really good at editing images. You see, this was possible just a year ago. But, this new one, this can do more semantic edits to our images. Let’s dive in and see the all the amazing things that it can do! If we take a photo of our friends we haven’t seen in a while. What happens? Well of course, the eyes are closed, or are barely open. From now on, this is not a problem. Done! And, if we wish to add a smile or take it away, boom, that is also possible. Also, the universal classic, looking too much into the camera? Not a problem. Done! Now, the AI can do even more kinds of edits, hairstyle, eyebrows, wrinkles, you name it. However, that’s even not the best part. You have seen nothing yet! Are you ready for the best part? Hold on to your papers, and check this out! Yes, it even works on drawings… paintings. Oh my! And even statues as well. Absolutely amazing. How cool is that? I love it. Researchers refer to these as out of domain examples. The best part is that this is a proper learning-based method. This means that by learning on human portraits, it has obtained general knowledge. So now, it doesn’t just understand these as clumps of pixels, it now understands concepts. Thus, it can reuse its knowledge, even when facing a completely different kind of image, just like the ones you see here. This looks like science fiction, and here it is, right in front of our eyes! Wow. But, it doesn’t stop there. It can also perform semantic image editing. What is that? Well, look! We can upload an image, look at the labels of these images, and edit the labels themselves. Well, okay, but what is all that good for? Well, the AI understands how these labels correspond to the real photo. So, check this out, we do the easy part, edit the labels, and the AI does the hard part, changing the photo appropriately. Look! Yes! This is just incredible. The best part is that we are now even seeing a hint of creativity with some of these solutions. And, if you are also one of those folks who feel like the wheels and rims are never big enough, well, NVIDIA has got you covered, that’s for sure. And here comes the kicker it learned to create these labels automatically by itself. So, how many label to image pairs did it have to look at to perform all this? Well, what you do think? Millions? Hundreds of thousands. Please leave a comment, I’d love to hear what you think. And the answer is 16. What? 16 million? Nope. 16. Two to the power of four. That’s it. Well that is one of the most jaw-dropping facts about this paper. This AI can learn general concepts from very few examples. We don’t need to label the entirety of the internet to have this technique be able to do its magic. That is absolutely incredible. Real learning from just 16 examples. Wow. The supplementary materials also showcase loads of results, so, make sure to have a look at that. If you do, you’ll find that it can even take an image of a bird, and adjust their beak sizes, even to extreme proportions. Very amusing. Or, we can even ask them to look up, and I have to say, if no one would tell me that these are synthetic images, I might not be able to tell. Now note that some of these capabilities were present in previous techniques, but this new method does all of these with really high quality, and all this in one elegant package. What a time to be alive! Now, of course, not even this technique is perfect. There are still cases that are so far outside of the training set of the AI that the result becomes completely unusable. And this is the perfect place to invoke the first law of papers. What is that? Well, the First Law Of Papers says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. And don’t forget, a couple papers before this we were lucky to do this. And now, see how far we’ve come. This is incredible progress in just one year. So, what do you think? What would you use this for? I’d love to hear your thoughts, please let me know in the comments below. Thanks for watching and for your generous support, and I'll see you next time!"
583,OpenAI DALL-E 2: Top 10 Insane Results!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today I am so excited to show you this. Look. We are going to have an AI look at 650 million images on the internet, and then, ask it to generate the craziest synthetic images I have ever seen. And wait, it gets better, we will also see what this AI thinks it looks like. Spoiler alert it appears to be cuddly. You'll see. So what is all this about? Well, in June 2020, OpenAI created GPT-3, a magical AI that could finish your sentences, and among many incredible examples, it could generate website layouts from a written description. This opened the door for a ton of cool applications, but, note that all of these applications are built on understanding text. However, no one said that these neural networks can only deal with text information. And sure enough, a few months later, scientists at OpenAI thought that if we can complete text sentences, why not try to complete images too? And thus, Image-GPT was born. The problem statement was simple: we give it an incomplete image, and we ask the AI to fill in the missing pixels. If we gave it this image, it understood that these birds are likely standing on something. And it even has several ideas as to what that might be! Look, a branch, a stone, or they can even stand in the water, and, amazingly, even their mirror images are created by the AI. But then, scientists at OpenAI thought, why not have the user write a text description, and get them a really well done image of exactly that. That sounds cool, and it gets even cooler the crazier the ideas we give to it. The name of this technique is a mix of Salvador Dalí and Pixar’s Wall-e. So please meet Dall-e. This could do a ton. For instance, this understands styles and rendering techniques. Being a computer graphics person, I am so happy to see that it learned the concept of low polygon count rendering, isometric views, clay objects, and we can even add an X-ray view to the Owl. Kind of. And now, just a year later, would you look at that! Oh wow. Here is Dall-e 2! Oh my, I cannot tell you how excited I am to have a closer look at the results. Let's dive in together! So what can it do? Well, that’s not the right question. By the end of this video, I bet you will think that the more appropriate question would be “what can’t it do”? This one can take descriptions that are so specific, I would say that perhaps even a good human artist might have trouble with. Now, hold on to your papers, and have a look at 10 of my favorite examples. 1. “A panda mad scientist mixing sparkling chemicals”. Wow, look at that! This is something else. It even has sunglasses for extra street cred, and the reflections of the questionable substance it is researching are also present on its sunglasses. A+. But the mad science doesn’t stop there. 2. “Teddy bears mixing sparkling chemicals as mad scientists”. But at this point, we already know that doing this would be too easy for the AI. Let’s do it in multiple styles. First, steampunk, second, 1990s Saturday morning cartoon, and third digital art. It can pull off all of these. 3. Now, about variants. Give me ""A teddybear on a skateboard in Times Square."" Now, this is interesting for multiple reasons. For instance, you see that it can generate a ton of variants. That is fantastic. As a light transport researcher, I cannot resist mentioning how nice of a depth of field effect it is able to make, and, would you look at that! It also knows about highly sought-after signature effect of the lights blurred into these beautiful balls in the background. The AI understands these bokeh balls and the fact that it can harness this kind of knowledge is absolutely amazing. 4. And if you think that is too specific, you have seen nothing yet. Check this out! “An expressive oil painting of a basketball player dunking, depicted as an explosion of a nebula”. I love it. It also has a nice Space Jam quality to it. Well done, little AI. So good! 5. You know what? I want even more specific, and even more ridiculous images. “a propaganda poster depicting a cat dressed as french emperor napoleon holding a piece of cheese”. Now that is way too specific. Nobody can pull that off…there is no way that…wow! I think we have a winner here. When the next election is coming up, you know what to do. 6. And still, once again, believe it or not, you have seen nothing yet. Yes, that’s right! We can even get more specific. So much so that we can even edit an image that is already done. For instance, if we feel that this image is missing a flamingo, we can request that it is placed there, but, we can even specify the location for it. And, even the reflections are created for it, and they are absolutely beautiful. Now note that I think that if there are reflections here, then, perhaps there should have been reflections here too. A perfect test for one more paper down the line, when Dall-e 3 arrives. Make sure to subscribe and hit the bell icon, you really don’t want to miss it. 7. This one puts up a clinic in understanding the world around us. This image is missing something. Missing what? Well, of course, corgis. And, I cannot believe this. If we specify the painting as the location, it will not only have a painterly style, but one that already matches the painting on the wall. This is true for the other painting too. This is incredible. I absolutely love it. And, last test, does it? Yes it does! If we are outside of the painting at the photo part, this good boy becomes photorealistic. Requesting variants is also a possibility here, so, what do you think? Which is the best boy? Let me know in the comments below! And, number 8. If we can place any object anywhere, this is an excellent tool to perform interior design. We can put a couch wherever we please, and I am really looking forward to inspecting the reflections here. Oh yes. Oh yes. This is very difficult to compute this is not a matte, diffuse object, and not even a mirror-like specular surface, but a glossy reflection that is somewhere inbetween. But it gets worse, this is a textured object, which also has to be taken into consideration, and proper shadows also have to be generated in a difficult situation where light comes from a ton of different directions. This is a nightmare, and the results are not perfect, but my goodness, if this is not an AI that has a proper understanding of the world around us, I don’t know what is. Absolutely incredible progress in just one year. I cannot believe my eyes. You know what? Number 9. Actually, let’s look at how much it has improved since Dall-e 1 side by side. Now there is no contest here. Dall-e 2 is on a completely different level from its first iteration. This is so much better, and once again, such improvement in just a year. What a time to be alive! And, what do you think Dall-e 3, if it appears, will be capable of? What would you use this, or Dall-e 3 for? Please let me know in the comments below, I’d love to know what you think. Now, of course, not even Dall-e 2 is perfect. Look at that. Number 10. Inspect the pictures and tell me, what do you think the prompt for this must have been? What do you think? Not easy, right? Let me know your tips in the comments below. Well, it was “A sign that says deep learning”. Well, A+ for effort, little AI, but this is certainly one of the failure cases. And, you know what, I cannot resist. +1. If you have been holding on to your papers, now, squeeze that paper at least as hard as these scholars are squeezing their papers. So, which one are you? Which one resembles your reaction to this paper the best? Let me know in the comments below. And, yes, as promised, here is what it thinks of itself. It is very soft and cuddly. Or at least, it wants us to think that it is so. Food for thought! And, if you speak robot and have any idea what this writing could mean, make sure to let me know below. And one more thing. We noted that this AI was trained on 650 million images, and uses 3.5 billion parameters. These are not rookie numbers by any stretch of the imagination. However, I am hoping that with this, there will be a chance that other, independent groups will also be able to train and use their own Dall-E 2. Just in the last few years, OpenAI has already given us legendary papers, for instance, an AI that can play hide and seek, solve math tests, or play a game called DOTA 2 on a world champion level. Given these, I hereby appoint Dall-E into the pantheon of these legendary works. And I have to say, I am super excited to see what they come up with next. Thanks for watching and for your generous support, and I'll see you next time!"
584,NVIDIA's Ray Tracing AI - This is The Next Level!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today, we are going to take a light simulation  algorithm that doesn’t work too well,   and infuse it with a powerful neural network.  And what happens, bam! This happens. Wow. So, what was all this about? Well, when we  fire up a classical light simulation program,   first, we start out from…yes, absolutely nothing.  And, look, over time, as we simulate the path of   more and more light rays, we get to know something  about the scene. And slowly, over time, the image   cleans up. But there is a problem.  What is the problem? Well, look.   The image indeed cleans up over time, but no one  said that this process would be quick. In fact,   this can take from minutes, in the case of this  scene, to get this, even days for this scene. In our earlier paper, we rendered  this beautiful, but otherwise,   sinfully difficult scene and it took approximately   three weeks to finish. And it also took  several computers running at the same time. However, these days, neural network-based   learning methods are already capable of  doing light transport. Why not use them?   Well, yes they are, but they are not perfect.  And we don’t want an imperfect result. So, scientists at NVIDIA said that this  might be the perfect opportunity to use   control variates. What are those and  why is this the perfect opportunity? Control variates are a way to inject our knowledge  of a scene into the light transport algorithm.   And here is the key. Any knowledge is  useful, as long as it gives us a headstart,   even if this knowledge is imperfect. Here  is an illustration of that. What we do is   that we start out using that knowledge,  and make up for the differences over time.   Okay, so, how much of a headstart can we get with  this? Normally, we start out from a black image,   and then, a very noisy image. Now hold on to your  papers, and let’s see what this can do to help us.   Wow. Look at that. This is incredible! This is not  the final rendering, this is what the algorithm   knows, and instead of the blackness, we can  start out from this. Goodness, it turns out that   this might not have been an illustration, but the  actual knowledge the AI has of the scene. So cool! Let’s look at another example. In this bedroom,  this will be our starting point. My goodness,   is that really possible? The bed and the  floor are almost completely done. The curtain   and the refractive objects are noisy, but  do not worry about those for a second.   This is just a starting point, and it is still  way better than starting out from a black image. To be able to visualize what the algorithm has  learned with a little more finesse, we can also   pick a point in space, and learn what the world  looks like from this point and how much light   scatters around it. And we can even visualize  what happens when we move this point around.   Man, this technique has a ton  of knowledge about these scenes. So, once again, just to make sure. We start out  not from a black image, but from a learned image,   and now, we don’t have to compute  all the light transport in the scene,   we just need to correct the differences.  This is so much faster! But actually, is it? Let’s look at how this helps us in practice. And  that means, of course, equal time comparisons   against previous light transport simulation  techniques. And, when it comes to comparisons,   you know what I want. Yes! I want Eric Veach’s  legendary scene. See? This is an insane scene   where all the light is coming from not here, but  the neighboring room through a door that is just   slightly ajar. And, the path tracer, the reference  light transport simulation technique behaves as   expected. Yes, we get a very noisy image because  very few of the simulated rays make it to the next   room. Thus, most of our computation is going  to waste. This is why we get this noisy image.  And, let’s have a look what the new method  can do in the same amount of time. Wow!   Are you seeing what I am seeing? This is  completely out of this world. My goodness. Okay, but this was a pathological scene  designed to challenge our light transport   simulation algorithms. What about a more typical  outdoors scene? Add tons of incoming light from   every direction and my favorite, water caustics.  Do we get any advantages here? The path tracer   is quite noisy, this will take quite a bit of  time to clean up. Whereas the new technique…oh   my, that is a clean image. How close is it to the  fully converged reference image? Well, you tell   me, because you are already looking at that. Yes,  now, we are flicking between the new technique   and the reference image, and I can barely tell the  difference. There are some, for instance, here,   but that seems to be about it. Can you tell the  difference? Let me know in the comments below. Now, let’s try a bathroom. Lots of shiny surfaces  and specular light transport. And the results are…   look at that! There is no contest here. A  huge improvement across the whole scene. And, believe it or not, you still haven’t seen  the best results yet. Don’t believe it? Now, if   you have been holding on to your papers, squeeze  that paper, and look at the art room example here.   This is an almost unusable image with classical  light transport. And, are you ready? Well, look at   this. What in the world! This is where I fell off  the chair when I was reading this paper. Absolute   madness. Look. While the previous technique  is barely making a dent into the problem,   the new method is already just a few fireflies  away from the reference. I can’t believe my eyes. And this result is not just an  anomaly. We can try a kitchen scene   and draw similar conclusions. Let’s see.  Now we’re talking! I am out of words. Now, despite all these amazing results, of  course, not even this technique is perfect.   This torus was put inside a glass container, and  is a nightmare scenario for any kind of light   transport simulation. The new method successfully  harnesses our knowledge about the scene,   and accelerates the process a great  deal, but, once again, we get fireflies.   These are going to be difficult to get rid of  and will still take a fair bit of time. But   my goodness, if this is supposed to be a  failure case…then yes, sign me up, right now! Once again, the twist here is  not just to use control variates,   the initial knowledge thing,  because in and of itself,   it is not new. I, like many others have been  experimenting with this method back in 2013,   almost ten years ago, and back then, it was  nowhere near as good as this one. So, what is the   twist then? The twist is to use control variates,  and infuse them with a modern neural network. Infusing previous techniques with  powerful learning-based methods is a   fantastic area of research these days. For  instance, here you see an earlier result,   an ancient light transport technique called  radiosity. This is what it was capable of   back in the day. And here is the neural  network-infused version. Way better. I   think this area of research shows a ton of promise  and am so excited to see more in this direction. So, what do you think? What would you use  this for? I’d love to hear your thoughts,   please let me know in the comments below. And when watching all these beautiful results, if  you feel that this light transport thing is pretty   cool, and you would like to learn more about it,  I held a Master-level course on this topic at the   Technical University of Vienna. Since I was always  teaching it to a handful of motivated students,   I thought that the teachings shouldn’t  only be available for the privileged few   who can afford a college education, but the  teachings should be available for everyone.   Free education for everyone, that’s what I want.  So, the course is available free of charge for   everyone, no strings attached, so make sure to  click the link in the video description to get   started. We write a full light simulation program  from scratch there, and learn about physics,   the world around us, and more. If you watch  it, you will see the world differently. Thanks for watching and for your generous  support, and I'll see you next time!"
585,This New AI is Photoshop For Your Hair!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. AI research is improving at such a rapid pace,  that today, we can not only create virtual humans,   but, even exert some artistic control  over how these virtual humans should look.   But today, we can do something even cooler.  What can be even cooler than that? Well,   have you ever seen someone with a really cool  haircut and wondered what you would like with   it? Would it be ridiculous? Or would you be able  to pull it off? Well, hold on to your papers,   because you don’t have to wonder anymore. With  this new AI, you can visualize that yourself! Hmm, okay, but wait a second. Previous  methods were already able to do that,   so, what is new here? Well, yes! But look at  the quality of the results. In some cases, it   is quite easy to find out that these are synthetic  images, while for other cases, so many of the fine   details of the face get lost in the process  that the end result is not that convincing.   So, is that it? Is the hairstyle synthesis  dream dead? Well, if you have been holding   on to your papers, now squeeze that paper, and  have a look at this new technique. And, wow!   My goodness, just look at that. This is a huge  improvement just one more paper down the line.   And note that these previous methods  were not some ancient techniques,   these are from just one and two years ago. So  much improvement in so little time. I love it. Now let’s see what more it can do. Look… we  can choose the hairstyle we are looking for,   and, yes! It doesn’t just give us just one image  to work with, no! It gradually morphs our hair   into the desired target shape. That is fantastic,  because even if we don’t like the final hairstyle,   one of these intermediate images may give  us just what we are looking for. Loving it. And it has one more fantastic property. Look. Most  of the details of the face remain in the final   results. Phew! We don’t even have to morph into  a different person to be able to pull this off.   Now, interestingly, look,  the eye color may change,   but the rest seems to be very close to  the original. Not perfect, but very close. And, did you notice? Yes, it gets better!  Way better! We can even choose the structure,   this would be the hairstyle,  which can be one image,   and the appearance, this is more like the  hair color. So, does that mean that? Yes!   We can even choose them separately. That is,  take two hairstyles, and fuse them together. Now, have a look at this too. This one is an  exclusive example, to the best of my knowledge,   you can only see this here on Two Minute  Papers. A huge thank you for the authors   for creating these just for us. And I must  say that we could stop on any of these images,   and I am not sure if I would be able to tell  that it is synthetic. I may have a hunch due   to how flamboyant some of these hairstyles are,  but not due to quality concerns, that’s for sure. Even if we subject these  images to closer inspection,   the visual artifacts of the previous  techniques around the boundaries of the hair   seem to be completely gone. And I wonder  what this will be able to do just a couple   more papers down the line. What do you think? I’d  love to know, let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
586,DeepMind’s New AI Finally Enters The Real World!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Finally, today, DeepMind’s amazing AI,  MuZero that plays Chess and other games   has now finally entered the real world and has  learned to solve important real-world problems. This is a reinforcement learning technique that  works really well on games. Why? Well, in Chess,   Go and Starcraft, the controls are clear, we use  the mouse to move our units around or choose where   to move our pieces. And the score is also quite  clear: we get rewarded if we win the game. That   is going to be our score. To say that these worked  really well would be an understatement: DeepMind’s   MuZero is one of the best in the world in Chess,  Go, and Stacraft 2 as well. But one important   question still remains. Of course, they did not  create this AI to play video games. They created   it to be able to become a general-purpose AI  that can solve not just games, but many problems.   The games are just used as an excellent  testbed for this AI. So, what else can it do? Well, finally here it is! Hold on to your papers,  because scientists at DeepMind decided to start   using their MuZero AI to create a real solution  to a very important problem. Video compression.   And here comes the twist they said, let’s  imagine that video compression is a video game.   Okay, that’s crazy, but let’s accept it for now.  But then, two questions: what are the controls,   and what is the score? How do we  know if we won video compression? Well, the video game controller in our hand will  be is choosing the parameters of the video encoder   for each frame. Okay, but there needs to  be a score. So what is the score here?   How do we win? Well, we win if we are able to  choose the parameters such that the quality of   the output video is as good as with the previous  compression algorithms, but, the size of the video   is smaller. The smaller the output video,  the better. That is going to be our score. And, it also uses self-competition, which  is now a popular concept in video game AIs.   This means that the AI plays against previous  versions of itself, and we measure its improvement   by it being able to defeat these previous  versions. If it can reliably do that, we   can conclude that the AI is indeed improving. This  concept works on boxing, playing catch, Starcraft   and I wonder how this would  work for video compression? Well, let’s see. Let’s immediately  drop it into deep waters.   Yes, we are going to test this against a  mature, state of art video compression algorithm   that you are likely already using at this very  moment as you are watching this on Youtube. Well,   good luck little AI, but I’ll be honest, there  is not much hope here. These traditional video   compression algorithms are a culmination  of decades of ingenious human research.   Can a newcomer AI beat it? I am not  sure. And now, hold on to your papers,   and let’s see together. How did it go? So, a 4%  difference. So, a learning-based algorithm that   is just 4% worse than decades of human innovation?  That is great! But…wait a second, it's actually   not worse. Can it be that…yes! It is not 4% worse.  It is even 4% better. Holy mother of papers, that   is absolutely incredible. Yes, this is the corner  of the internet where we get super excited by a 4%   better solution, and understand why that matters  a great deal. Welcome to Two Minute Papers! But wait, we are experienced Fellow Scholars  over here, we know that it is very easy to   be better by 4% in size at the cost of decreased  quality. But having the same quality and save 4%   is insanely difficult. So, which one is it? Let’s  look together. I am flicking between the state   of the art and the new technique, and, yes, my  goodness, the results really speak for themselves. So, let’s look a bit under the hood and see  some more about the decisions the AI is making.   Whoa. That is really cool. So what is this? Here,  we see the scores for the previous technique   and the new AI, and here, they appear to be  making similar decisions on this cover song video,   but the AI makes somewhat better  decisions overall. That is very cool.   But, look at that! In the second half  of this gaming video, MuZero makes   vastly different, and, vastly  better decisions. I love it. And to have a first crack at such a mature  problem, and manage to improve it immediately,   that is almost completely unheard of. Yet, they  have done it with protein folding, and now,   they seem to have done it for video  compression too. Bravo, DeepMind! And note the meaning in the magnitude of the  difference here. For instance, OpenAI’s Dall-E 2   was this much better than Dall-E 1. That’s  not 4% better, if that was a percentage,   this would be several hundred percent  better. So, why get so excited about 4%?   Well, the key is that 3-4% more compression is  incredible given how well polished these state   of the art techniques are. VP9 compressors  are not some first crack at the problem,   no-no. This is a mature field  with decades of experience,   where every percent of improvement requires blood,  papers and tears, and of course, lots of compute   and memory. And this is just the first crack  at the problem for DeepMind, and we get not 1%,   but 4% essentially for free. That is absolutely  amazing. My mind is blown by this result. Wow. And, I also wanted to thank you for watching  this video. I truly love talking about these   amazing research papers, and I am really  honored to have so many of you Fellow Scholars   who are here every episode, enjoying these  incredible works with me. It really means a lot.   Every now and then I have  to pinch myself to make sure   that I really get to do this every day.  Absolutely amazing. Thank you so much! So, what do you think? What else could this be   useful for? What do you expect to happen  a couple more papers down the line?   Please let me know in the comments  below. I’d love to hear your thoughts. Thanks for watching and for your generous  support, and I'll see you next time!"
587,NVIDIA’s Robot AI Finally Enters The Real World!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Scientists at NVIDIA knocked it out  of the park with this one. Look,   they are training an AI that moves this virtual  robot arm to finish everyday tasks on this table. Let’s see. It picks up the toy school bus. So  far so good. But, wait a second! This is not good   at all! It should have grabbed the c-clamp  here, not the toy school bus. Why is that?   Well, it has only seen one demonstration to learn  from yet, so these are just the humble beginnings. Now, let’s look at what it can do after seeing 10  demonstrations. And…yes! That is so cool! Learning   is happening The c-clamp goes in the box  successfully. Wonderful! And now, for the crayon   box…that is not a crayon box. And neither is that  one. And it’s not that one either. Oh my goodness. Now, hold on to your papers, and let’s see what it  learned after looking at still as few as a hundred   demonstrations. C-clamp into the box, crayon  box too, toy school bus, get in that box. And,   yes, finally! The rest seems to work  fine too. I love it. Good job, little AI!   And if you were wondering,  this is a porcelain plate. And that is twist number one. Learning  happening from very few image-action pairs.   How many exactly? Well, it learned to do  this, and more, from only 179 image-action   pairs. That, in the era of learning-based  techniques, is almost nothing at all. Wow. And, it gets better. One of  the most mind-blowing results   is that from these very few examples, it also  learned to deal with previously unseen colors and   words. And not only that, but it was also able  to learn a wide variety of tasks from these 179   examples. And its capabilities just keep  going on and on and on. Absolutely amazing. Okay, now, that is all well and  good. But still, this is robot arm   can play a computer game. Is that it? Why make  a video about this? Why is this so important?   Well, and now, I give you the two more twists  that breathe new life into this project.   Twist number two. We can use natural language  to give instructions to these robots.   Yes! And twist number three. If you have  been holding on to your papers so far, now,   squeeze that paper, because the knowledge it  learned in the simulation transfers to reality.   We come come out of the video game world into the  real world, and the AI still works wonderfully. Let’s have a look. It can move chess pieces. Great  work little AI, I love it! But, this was a solid   chess piece, you know what, let’s try something  that requires a little more delicacy. Let’s see.   Yes, folding and unfolding a piece of  cloth is also not a problem. That requires   delicate movement. So good, I love it!  But, let’s try something that requires   even more finesse. Yes. Putting  cherries in a box. That is really cool! Sweeping also works… kind of. As long  as we are looking to sweep beans.   And, believe it or not, it can even read.  Again, kind of. You see, we can label two   boxes that will contain the good and bad  stuff. And then, instruct the AI as follows.   This worked well, and it will work, even if we  try to be a bit more tricky, and swap the labels.   And, as you see, it is not falling for it. I  will have to give partial credit for that drop,   but still, all this from less than 200  demonstrations? That is incredible. So cool! And, what you see here is what researchers refer  to as sim2real. Teaching an AI something in a   simulated world, and then, deploying it into the  real world. Okay, so what is all this good for? Here is an amazing example, look, OpenAI  trained their robot hand in a simulation   to be able to rotate these Rubik cubes. And then,  deployed this software onto a real robot hand,   and, look! It can use this simulation knowledge,  and now it works in the real world too.   But sim2real has relevance to self-driving cars  too. Look. Tesla is already working on creating   virtual worlds, and training their cars there. One  of the advantages of that is that we can create   really unlikely and potentially unsafe scenarios,  but, in these virtual worlds, the self-driving AI   can train itself safely. And when they deploy  them into the real world, they will have all   this knowledge. It is fantastic to see that  Waymo is also moving in this direction. As you see, sim2real is real. And  NVIDIA’s new technique also puts us   one step closer to a future  where this kind of intelligence   can be democratized and used by  everyone. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
588,DeepMind's New AI: A Spark Of Intelligence!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see what DeepMind’s amazing  AI can learn from humans doing silly things,   like playing the drums with a comb. Really, we have to do that? Well…okay,  if you insist. There you go. And? Now what? Well, if it had seen  enough of these instruction/video pairs,   hopefully it will be able to learn from it. And  here comes the fundamental question we are here   to answer today. If this AI does something, is  this imitation, or is this learning? Imitation   is not that impressive, but if it can learn  general concepts from examples like this,   now that would perhaps be a little spark of  intelligence. That is what we are looking for. So, how much did it learn?  Well, let’s see together. Nice, look at that! If we ask it, it can  now show us where a specific room is.   And if we get a little lost, it even helps us with  additional instructions to make sure we find out   way into the bathroom. Now note that the layout  of this virtual playhouse is randomized each time   we start a new interaction, therefore it cannot  just imitate the direction people go when they say   bathroom. It doesn’t work, because  next time, it will be elsewhere. It   actually has to understand what a  bathroom is. Good job, little AI! It also understands that we are holding a bucket  and it should put the grapes in that. Okay,   so it learned from instructions, and it can deal  with instructions. But if it had learned general   concepts, it should be able to do other things  too! You know what, let’s ask some questions. How many grapes are there in this scene?   2. Yes, that is correct, good job! Now, are they  the same color or different? It says different.  I like it, because to answer this question, it has  to remember what we just talked about. Very good! And now, hold on to your papers,  and check this out! We ask   what color the grapes are on the floor.  And what does it do? It understands what   it needs to look for to gather the required  information. Yes, it uses its little peepers,   and answers the question. A little  virtual assistant AI! I love it. But, it isn’t just an assistant. Here come  two of my favorite tasks from the paper.   One, if we dreamed up a catchy tune and we  just need a bandmate to make it happen. Well,   say no more! This little AI is  right there to help. So cool! And it gets better, get this, two, it even knows  about its limitations. For instance, it knows   that it can’t flip the green box over. This can  only arise from knowledge and experimentation   with the physics of this virtual world. This  will be fantastic for testing video games. So, yes, the learning is indeed thorough. Now  let’s answer two more super important questions.   One, how quickly does this AI  learn, and is it as good as a human? So, learning speed. Let’s look under the  hood together, and oh yes, yes, YES!   I am super happy. You are asking, Károly, why are  you super happy? I am super happy because learning   is already happening by watching humans mingle  for about 12 minutes. That is next to nothing!   Amazing. And, one more thing that makes me perhaps  even happier. And that is, that I don’t see   a sudden spike in the growth, I see nice,  linear growth instead. Why is that important?   Well, this means there is a higher chance that the  algorithm is slowly learning general concepts, and   starts understanding new things that we might ask  it. And if it were a small spike, there would be a   higher chance that it had seen something similar  to a human clearing a table in the training set,   and suddenly started copying it. But, with this  kind of growth, there is less of a chance that it   is just copying the teacher. I love it. And, yes  this is the corner of the internet where we get   unreasonably excited by a blue line. Welcome to  Two Minute Papers! Subscribe and hit the bell icon   if you wish to see more amazing works like this.  And if we look at the task of lifting a drum,   oh yes, also nice, linear growth, although,  at a slower pace than the previous task. Now, let’s look at how well it does  its job. This is Team Human, and MIA   is team AI. And it has an over 70% success  rate. And remember, many of these tasks require   not imitation, but learning. The AI sees  some humans mingling with these scenes,   but to be able to answer the questions, it needs  to think beyond what it has seen. Once again,   that might be perhaps a spark of  intelligence. What a time to be alive! But actually, one more question: A virtual  assistant is pretty cool, but why is this so   important? What does this have to do with our  lives? Well, have a look at how OpenAI trained   their robot hand in a simulation to be able to  rotate these Rubik cubes. And then, deployed this   software onto a real robot hand, and, look! It can  use this simulation knowledge, and now it works in   the real world too. But sim2real has relevance  to self-driving cars too. Look. Tesla is already   working on creating virtual worlds, and training  their cars there. One of the advantages of that is   that we can create really unlikely and potentially  unsafe scenarios, but, in these virtual worlds,   the self-driving AI can train itself safely.  And when they deploy them into the real world,   they will have all this knowledge. Waymo  is running similar experiments too. And, I also wanted to thank you for watching this  video. I truly love talking about these amazing   research papers, and I am really honored to  have so many of you Fellow Scholars who are here   every episode, enjoying these incredible  works with me. Thank you so much! So, I think this one also has great potential  for a sim2real situation. Learning to help humans   in a virtual world, and perhaps uploading  the AI to a real robot assistant. So,   what do you think? What else could this be useful  for? What do you expect to happen a couple more   papers down the line? Please let me know in the  comments below. I’d love to hear your thoughts. Thanks for watching and for your generous  support, and I'll see you next time!"
589,DeepMind’s New AI Learns Gaming From Humans!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we will see how and what DeepMind's AI is  able to learn from a human, but with a twist!   And the twist is that we are going to remove the  human teacher from the game, and see what happens.   Is it just memorizing what the human does,  or is this AI capable of independent thought? Their previous technique did something like  that, like guiding us to the bathroom, start   a band together, or even find out the limitations  of the physics within this virtual world.   However, there is a big difference. This  previous technique had time to study.   This one, not so much! This new one  has to do the learning on the job. Oh yes, in this project, we will see an  AI that has no pre-collected human data.   It really has to learn everything on the  job. So, can it? Well, let's see together! Phase 1 Pandemonium. Here, the  AI says, well, here we do…things,   I am not so sure. Occasionally, it gets a point,  but now, look, uh-oh! The red teacher is gone.   And…oh boy, it gets very confused. It does  not have much of an idea what is going on. But, a bit of learning happens, and later.  Phase 2 Following. It is still still   not sure what is going on, but  when it follows this red chap,   it realized that it is getting a much  higher score. Before, it only got 2, now,   look at it! It is learning something new  here. And, look! He is gone again, but   it knows what to do. Well, kind of. It still  probably wonders, why its score is decreasing? So, a bit later, Phase 3 Memorization. First,  the usual dance with the demonstrator. But, now,   when he is gone, it knows exactly what to  do, and just keeps improving its score. And then comes the crown jewel. Phase  4 Independence. No demonstrator   anywhere to be seen. This is the final  exam. It has to be able to independently   solve the problem. But here comes the twist.  For the previous phase, we said memorization,   and for this, we say independence. Why is this  different? Well, look. Do you see the difference?   Hold on to your papers, because we have switched  up the colors. So, the previous strategy   is suddenly useless. Oh yes! If it walks the same  path as before, it will not get a good score,   and initially, that's what it is trying to  do. But, over time, it is now able to learn   independently, and indeed, find  the correct order by itself. And, what I absolutely loved here is, look.  Over time, the charts verify that indeed,   as soon as we take away the teacher, it  starts using different neurons as right   after becoming an independent entity.  I love it. What an incredible chart. And all this is excellent news. So, if  it really has an intelligence of sorts,   it has to be able to deal with  previously unseen conditions and   problems. That sounds super fun  let’s explore that some more. Let’s give it a horizontal obstacle.  Good! Not a problem. Vertical?   Also fine. Now, let’s make the world larger! And,  it is still doing well. Awesome! So, I absolutely   love this paper. DeepMind demonstrated that  they can build an AI that learns on the job,   one that is even capable of independent thought,   and even better, one that can deal with unforeseen  situations too. What a time to be alive! So, what do you think? What  else could this be useful for?   What do you expect to happen a  couple more papers down the line?   Please let me know in the comments  below. I’d love to hear your thoughts. Thanks for watching and for your generous  support, and I'll see you next time!"
590,NVIDIA’s New AI Grows Objects Out Of Nothing!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how NVIDIA’s new AI transfers real objects into a virtual world. So, what is going on here? Simple, in goes just one image, or a set of images of an object. And the result is that an AI really transfers this real object into a virtual world almost immediately. Now that sounds like science fiction, how is that even possible? Well, with this earlier work, it was possible to take a target geometry from somewhere, and obtain a digital version of it by growing it out of nothing. This work reconstructed its geometry really well. But, geometry only. This other work tried to reconstruct not just the geometry, but everything. For instance, the material models too. Now, incredible as this work is, it is still baby steps in this area. As you see, both the geometry and the materials are still quite coarse. So, is that it then? Is the transferring real objects into virtual worlds dream dead? It seems so. Why? Because we either have to throw out the materials to get a high-quality result, or if we wish to get everything, we have to be okay with a coarse result. But, I wonder, can this be improved somehow? Well, let’s find out together. And, here it is! NVIDIA’s new work tries to take the best of both worlds. What does that mean? Well, they promise to reconstruct absolutely everything. Geometry, materials, and even the lighting setup, and all of this with high fidelity. Well, that sounds absolutely amazing, but, I will believe it when I see it! Let’s see together. Well, that’s not quite what we are looking for, is it. Yes, this isn’t great, but, this is just the start. Now, hold on to your papers and marvel at how the AI improves this result over time. Oh yes, this is getting better. And, my goodness! After just as little as two minutes, we already have a usable model. That is so cool! I love it. We go on a quick bathroom break and the AI does all the hard work for us. Absolutely amazing. And, it gets even better! Well, if we are okay with not a quick bathroom break, but taking a nap, we get this just an hour later. And if that is at all possible, it gets even better than that! How is it possible? Well, imagine that we have a bunch of photos of a historical artifact, and, you know what’s coming! Of course, creating a virtual version of it, and dropping it into a physics simulation engine, where we can edit this material, or embed it into a cloth simulation. How cool is that? And, I can’t believe it! It still doesn’t stop there. We can even change the lighting around it and see what it would look like in all its glory. That is absolutely beautiful. Loving it. And, if we have a hot dog somewhere, and we already created a virtual version of it, but now, what do we do with it? Of course, we engage in the favorite pastime of the computer graphics researcher. That is, throwing jelly boxes at it. And, with this new technique, you can do that too. And, even better, we can take an already existing solid object and reimagine it as if it were made of jelly. No problem at all. And, you know what it is final boss time. Let’s not just reconstruct an object. Why not throw an entire scene at the AI. See if it buckles. Can it deal with that? Let’s see. And…I cannot believe what I am seeing here. It resembles the original reference scene so well, even when animated, that it is almost impossible to find any differences. Have you found any? I have to say I doubt that, because I have swapped the labels. Oh yes, this is not the reconstruction, this is the real reconstruction. This will be an absolutely incredible tool in democratizing creating virtual worlds and giving it into the hands of everyone. Bravo NVIDIA. So, what do you think? Does this get your mind going? What else could this be useful for? What do you expect to happen a couple more papers down the line? Please let me know in the comments below. I’d love to hear  your thoughts. Thanks for watching and for your generous support, and I'll see you next time!"
591,DeepMind’s New AI Thinks It Is A Genius!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see what DeepMind’s AI is  able to do after being unleashed on the internet   and reading no less than 2 trillion words.  And, amusingly, it also thinks that it   is a genius. So, is it? Well, we are  going to find out together today. I am really curious about that, especially given  how powerful these recent AI language models are.   For instance, OpenAI’s GPT-3  language model AI can now write poems   and even continue your stories. And even  better, these stories can change direction,   and the AI can still pick them up  and finish them. Recipes work too. So, while OpenAI is writing  these outstanding papers,   I wonder what scientists at DeepMind are  up to these days? Well, check this out.   They have unleashed their AI that they call Gopher  on the internet and asked it to read as much as it   can. That is, 2 trillion words. My goodness, that  is a ton of text. So, what did it learn from it?   Oh boy, a great deal. But mostly, this one can  answer questions. Hmm…questions? There are plenty   of AIs around that can answer questions. Some  can even solve a math exam straight from MIT.   So, why is this so interesting? Well, while humans  are typically experts at one thing, or very few   things, this AI is nearly an expert at almost  everything. Let’s see what it can do together! For instance, we can ask a bunch  of questions about biology,   and it will not only be quite insightful, but  it also remembers what we were discussing a   few questions ago. That is not trivial  at all. So cool! Now note that not all   of its answers are completely correct. We will  have a more detailed look at that in a moment. Also, what I absolutely loved seeing when  reading the paper is that we can even ask what   it is thinking. And look it expresses that it  wishes to play on its smartphone. Very humanlike!   Now make no mistake that this does not mean  that this AI is thinking like a human is thinking.   At the risk of simplifying it, this is more  like a statistical combination of things   that it had learned that people say on the  internet when asked what they are thinking. Now note that many of these new works are so  difficult to evaluate because they typically do   better on some topics than previous ones,  and worse on others. The comparison of these   techniques can easily become a bit subjective  depending on what we are looking for. However,   not here! Hold on to your papers,  and have a look at this! Oh wow.   My goodness! Are you seeing what I am seeing? This  is OpenAI’s GPT-3, and this is Gopher. As you see,   it is a great leap forward not just here and  there, but in many categories at the same time! Also, GPT-3 used 175 billion parameters  to train its neural network, Gopher uses   280 billion parameters, and you see, we get  plenty of value for these additional parameters.   So, what does all this mean? This means that  as these neural networks get bigger and bigger,   they are still getting better. We are steadily   closing in on the human-level experts in many  areas at the same time, and progress is still   not plateauing. It still has more left in  the tank. How much more? We don’t know yet,   but as you see, the pace of improvement in AI  research is absolutely incredible. However, we   are still not there yet. Its knowledge in the area  of humanities, social sciences and medicine is   fantastic, but at mathematics, of all things, not  so much. You will see more about that in a moment. And, if you have been holding on to your papers,  now, squeeze that paper, because…would you look at   that! What is it that I am seeing here? Oh boy. It  thinks that it is a genius. Well, is it? Let’s ask   some challenging questions about Einstein’s field  equations, black holes and more and find out. Hmm. Well, it has a few things  going for it. For instance,   it has a great deal of factual knowledge, however,  at the same time, it can also get quite confused   by very simple questions. Do geniuses mess  up this multiplication? I should hope not! Also, have a look at this. We noted that it is not  much of a math wizard. When asked these questions,   it gives us an answer, and when we ask are you  sure about that, it says it is very confident.   But, it is confidently incorrect, I am afraid,  because none of these answers are correct. So, a genius AI? Well, not quite  yet. Human-level intelligence?   Also, not yet. But this is an incredible step  forward just one more paper down the line. And   just imagine what we will be able to do  just a couple more papers down the line.   What do you think? Does this get your mind going?  Let me know your ideas in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
592,This AI Makes You A Virtual Stuntman!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we will try to create a copy of  ourselves, and place it into a virtual world. Now this will be quite a challenge. Normally,  to do this, we have to ask an artist to create   a digital copy of us, which takes a lot of time  and effort. But, there may be a way out. Look.   With this earlier AI-based technique, we can take  a piece of target geometry, and have an algorithm   try to rebuild it to be used within a virtual  world. The process is truly a sight to behold.   Look at how beautifully it sculpts this piece of  geometry until it looks like our target shape! This is wonderful, but wait a second. If  we wish to create a copy of ourselves,   we probably want it to move around too. This  is, however, a stationary piece of geometry.   No movement is allowed here. So,  what do we do? What about movement? Well, have a look at this new technique,  where getting a piece of geometry   with movement cannot get any simpler than this  just do your thing, record it with a camera,   and give it to the AI. And, I have  to say that I am a little skeptical,   look, this is what a previous technique could get  us. This is not too close to what we are looking   for. So, let’s see what the new method can do  with this data. And! Uh-oh. This is not great. So,   is this it? Is the geometry cloning dream dead?  Well, don’t despair quite yet! This issue happens   because our starting position and orientation  is not known to the algorithm, but,   it can be remedied. How? Well, by adding  additional data to the AI to learn from. And now, hold on to your papers,  and let’s see what it can do now!   And…oh my goodness. Are you seeing what I am  seeing? Our movement is now replicated in a   virtual world almost perfectly! Look at that  beautiful animation. Absolutely incredible. And, if even this is not good enough, look  at this result too. So good! Loving it.  And, believe it or not, it has even more coolness  up the sleeve. If you have been holding on to   your papers so far, now, squeeze that paper,  because here comes my favorite part of this work.   And that is a step that the authors  call scene fitting. What is that?  Essentially, what happens is that the AI  reimagines us as a video game character, and sees   our movement, but does not have an idea as to what  our surroundings look like. What it does is that   from this video data, it tries to reconstruct our  environment, essentially, recreating it as a video   game level. And that is quite a challenge. Look,  at first, it is not close at all. But, over time,   it learns what the first obstacle should look  like, but still, the rest of the level...not   so much! Can this still be improved? Let’s have  a look together. As we give it some more time,   and our character a few more concussions,  it starts to get a better feel of the level. And it really works for a variety of  difficult, dynamic motion types. Cartwheels,   backflips, parkour jumps, dance moves, you name  it. It is a robust technique that can do it all.   So cool! And note that the authors of the  paper gave us not just the blueprints for   the technique in the form of a research paper,  but they also provide the source code of this   technique to all of us, free of charge. Thank  you so much! I am sure this will be a huge help   in democratizing the creation of video  games and all kinds of virtual characters. And, if we add up all of these together. We  get this. This truly is a sight to behold.   Look! So much improvement just one more paper down  the line. And just imagine what we will be able to   do a couple more papers down the line. Well, what  do you think? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
593,Google’s New AI: Flying Through Virtual Worlds!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going take a collection of photos like these, and magically, create a video where we can fly through these photos. So, how is this even possible? Especially that the input is only a handful of photos. Well, typically, we give it to a learning algorithm and ask it to synthesize a photorealistic video where we fly through the scene as we please. Of course, that sounds impossible. Especially that some information is given about the scene, but this is really not much. And as you see, this is not impossible at all through the power of learning-based techniques, this previous AI is already capable of pulling off this amazing trick. And today, I am going to show you that through this incredible new paper, with a little expertise, something like this can be done even at home, on our own machines. Why? Because now, research scientists at Google and Harvard also published their take on this problem. And they promise two fantastic improvements: Improvement number one. Unbounded scenes. No more front-facing scene with a stationary camera. They say that now, we can rotate the camera around the object, and their technique will still work. That is a huge deal…if it indeed works. We will see. You know what, hold on to your papers, and, let’s see together right now. Wow! This does not look like an AI-made video out of a bunch of photos. This looks like reality! My goodness. And, of course, you are experienced Fellow Scholars over there, so I bet you are immediately interested in the fidelity of the geometry, which truly is a sight to behold. I am a light transport researcher by trade, so I am additionally looking at the specular highlights. This is as close to reality as it gets. The depth maps it produces, which describe the distance of these objects from the camera, and, they are also silky smooth. Outstanding. So, is this a one-off result, or is this a robust technique that works on a variety of scenes? Well, I bet you know the answer by now. But wait, we talked about two promises. Promise number one was the unbounded scenes with the moving camera. What is promise number two? Well, promise number two is free antialiasing. Ooh boy! This is a technique from computer graphics that helps us overcome these jagged edges that are usually present in lower resolution images. And, it really works so well, check out this comparison against a previous work by the name mip-NERF. And it is just so much better, the new method truly seems to be in a league of its own. We get smoother lines. And pretty much every material, and every piece of the geometry comes out better. And note that this previous method is not some ancient technique. No-no! mip-NERF is a technique from just a year ago. Bravo Google and Bravo Harvard! And remember, all this comes from just a quick camera scan. And the rest of the details is learned and filled in by an AI. And this technique now truly has a remarkable understanding of the world around us. So, just using a commodity camera, walking around a scene, and creating a digital video game version of it? Absolutely incredible. Sign me up, right now! So, what do you think? What else could this be useful for? What do you expect to happen a couple more papers down the line? Please let me know in the comments below. I’d love to hear  your thoughts. Thanks for watching and  for your generous support, and I'll see you next time!"
594,NVIDIA Renders Millions of Light Sources!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how NVIDIA’s light  transport algorithm can render this virtual   amusement park that contains no less than  3.4 million light sources interactively.   Yes. Millions of light sources. How is this even possible? Well, to  understand what is going on here,   we need to talk about two concepts. If we wish to create a truly photorealistic scene,  in computer graphics, we usually reach out to a   light transport simulation algorithm, and then,  this happens. Oh yes, concept number one. Noise!   This is not photorealistic at all, not yet anyway.  Why is that? Well, during this process, we have to   shoot millions and millions of light rays into  the scene to estimate how much light is bouncing   around, and before we have simulated enough rays,  the inaccuracies in our estimations show up as   noise in these images. This clears up over time,  but it may take from minutes to days for this to   happen, even for a smaller scene. For instance,  this one took us 3 full weeks to finish. Weeks!   Ouch. And in a moment, you will see that NVIDIA’s  technique can render even bigger scenes than this,   not in weeks, but in milliseconds.  Yes, really. I am not kidding. And for now, concept number two. Biased  and unbiased. There are different classes   of light transport algorithms. Here you see an  unbiased technique that doesn’t cut any corners,   and now, look! Oh yes, this is a  biased technique, and as you see,   this one can give us a much cleaner  image in the same amount of time. But,   the price to be paid for this is that  some corners have been cut in the process. And with that, now, hold on to your papers,  and have a look at NVIDIA’s technique.   Oh yes, this was the scene with  the 3.4 million light sources,   and this method can really render not just an  image, but an animation of it interactively. That is absolutely incredible, and  it has even more to offer. Look!   The light sources can even be dynamic, or in  other words, they can move around as they please,   and the algorithm still doesn’t break a sweat.  Oh my goodness! That is absolutely amazing. Now, let’s see how it compares to a previous  technique. Oh boy. See the noise levels here? This   is so noisy, we don’t even know what the scene  should exactly look like. But, now, when we have   a look at this technique, oh yes, now we know  for sure that there are some glossy reflections   going on here. Even the unbiased version of the  new technique is significantly better in this.   However, look, if we are willing to let the  simulator cut some corners, this is the biased   version, and, oh my goodness! Those difficult  glossy reflections are so much cleaner! I love it. This amusement park scene contains a total of over  20 million triangles, and once again, the biased   version of the new technique makes it truly a  sight to behold. And, this does not take from   minutes to days to compute, each of these images  were produced in a matter of milliseconds! Wow. But it gets better! How? Well, the more  detailed comparisons in the paper reveal   that this method is 10 to a 100 times faster than  previous techniques, and it also maps really well   onto our graphics cards. Okay, but what is behind  all this wizardry? How is this even possible? Well, the magic behind all this is a smarter  allocation of these ray samples that we have   to shoot into the scene. For instance,  this technique does not forget what we did   just a moment ago when we move the camera a  little and advance to the next image. Thus,   lots of information that is otherwise thrown away  can now be reused as we advance the animation.   Now note that there are so many papers out there  on how to allocate these rays properly, this field   is so mature, it truly is a challenge to create  something that is just a few percentage points   better than previous techniques. It is very hard  to make even the tiniest difference. And to be   able to create something that is 10 to a 100 times  better in this environment? That is insanity. And, this proper ray allocation has  one more advantage. What is that?   Well, have a look at this. Imagine that you are  a good painter, and you are given this image.   Now your task is to finish it.  Do you know what this depicts?   Hmm…maybe. But knowing all the details of this  image is out of question. Now, look, we don’t have   to live with these noisy images, we have denoising  algorithms tailored for light simulations.   This one does some serious legwork with this noisy  input, but even this one cannot possibly know   exactly what is going on because there is so much  information missing from the noisy input. And now,   if you have been holding on to your papers so  far, now, squeeze that paper, because, look.   This technique can produce this image in the same  amount of time. Now we’re talking! Now, let’s give   it to the denoising algorithm, and…yes! We get  a much sharper, more detailed output. Actually,   let’s compare it to the clean reference image. Oh  yes, this is much closer. And the fact that just   one more paper down the line, we could go from  this to this absolutely blows my mind. So cool! But wait, we noted that the biased version of the  technique cuts some corners. What is the price to   be paid for this? What are we losing here?  Well, some energy. What does that mean? It   means that the image it produces is typically  a little darker. There are other differences,   but usually, they are relatively minor,  making this an excellent tradeoff. Adding it all together, this will be an excellent  tool in democratizing light transport algorithms,   and putting it into the hands of everyone.  And to think that years ago, we had this,   and now, we can get this running on a commodity  graphics card. I am truly out of words. The   pace of progress in computer graphics research is  nothing short of amazing. What a time to be alive! Now, if you also got excited by this. Look here.  This is not an implementation of this paper,   but the first author also has a nice little  webapp where you can create interesting 2D   light transport situations with lots of  caustics. And the results are absolutely   beautiful. The link is available in  the video description. Check it out! Thanks for watching and for your generous  support, and I'll see you next time!"
595,OpenAI’s DALL-E 2: Even More Beautiful Results!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to play some more with OpenAI’s  amazing technique DALL-E 2, where we can write   a text description, and the AI creates an  amazing image of exactly that. That sounds cool,   and it gets even cooler the  crazier the ideas we give to it. This is an AI endowed with a diffusion-based  model, which means that when we ask it something,   it starts out from noise, and over  time, it iteratively refines this image   to match our description better. And, over  time, magically, an absolutely incredible image   emerges. This is a neural network that is given  a ton of images, and a piece of text description   that says what is in this image. That  is one image-caption pair. DALL-E 2   is given millions and millions of these pairs.  So, what can it do with all this knowledge? Well, the best part is that it can combine  things! Oh yes, this is the key. This does   not copy images from this training data, but  it truly comes up with novel images. How?   Well, after it had seen a  bunch of images of koalas,   and separately, a bunch of images of motorcycles,  it starts to understand the concept of both,   and it will be able to combine the two  together into a completely new image. And here you can see how much DALL-E  2 has improved since DALL-E 1. It is   on a completely different  level from its first iteration.   This is so much better, and once again,  this is an AI that was improved this much   in just a year? I can hardly believe what  I am seeing here. What a time to be alive! And now that some time has passed, new,  interesting capabilities have emerged.   Now, hold on to your papers, and let’s  have a look at 10 more amazing examples. One, for instance, it can even create not  just images, but small videos. Videos?   How? Well, look at this video where a victorian  house is getting modernized! Wow. So what is going   on here? Well, we can enter a text prompt, get an  image, then change the text just a tiny bit, get a   slightly changed image, and repeat the process  over and over until we get an amazing video   like this. We can also run this process backwards  and victorianize a modern building as well. Two, if you don’t believe that it can  combine several existing concepts into   something that definitely does not exist, hold  on to your papers, and have a look at this one.   Oh yes, Apple products, Leonardo Da Vinci style.   This is truly insane. If all this does not feel  like humanlike intelligence, I don’t know what is. Three, here is how the AI imagines  a robot learning a new language. Four, it can create new kinds of drinks, and,  my goodness. Well, I am super happy now. You are   probably asking, Károly, why are you super happy?  Well, I am a light transport researcher by trade,   and I spend a great deal of my time computing  caustics. Oh yes, caustics are these beautiful   patterns that emerge when we hit a reflective  or refractive object with light just the right   way. And these can look especially magical  if we have an object with a complex geometry.   And the fact that the AI also understands this  about our world. I am truly speechless. Beautiful. Five, it can also create new  inventions. This is a toilet car.   You know, some people are quite busy and  if you have to go when you are on the go,   well, this one is for you.  What a time to be alive! Six, in this one, the AI puts up a clinic in  understanding combinations of concepts. Check   this out! This is plants surrounding a lightbulb.  Now, a lightbulb surrounding some plants.   Now a plant with a lightbulb inside.  And a lightbulb with plants inside.   It can follow these instructions  really well an all of these cases. Seven, it can also create larger images too, so  much so, that entire murals can be requested.   And here, not just the quality, but the  variety of results is truly a sight to behold. Eight, this is the famous Sith Lord, Darth Ant.   Oh yes. This is Darth Vader  reimagined as a robot ant. Loving it. Nine, if you remember, previously, when we  requested that it writes a piece of text on   a sign, it floundered a great deal. This  sign is supposed to say “deep learning”.   And, look! The amazing Peter Welinder found  a way to make it write things on signs   properly. And all this with an amazing depth of  field effect. Once again, this is an AI where a   vast body of knowledge lies within, but it only  emerges if we can bring it out with properly   written prompts. It almost feels like a new  kind of programming that is open to everyone,   even people without any programming or  technical knowledge. This is prompt engineering,   if you will. Perhaps a new kind of job  that is just coming into existence. Ten, now check this out. We can even give  instructions to it as a photographer would   instruct its camera, request mammatus clouds, we  marveled together at a simulation of those in an   earlier video. And, oh my goodness. That cannot  be true. Look. The hand has subsurface scattering.   What is that? That is the effect  of light penetrating the skin,   bouncing around, and either coming out on the  same, or the other side. It has this absolutely   beautiful halo effect. We worked on this a bit  in an earlier paper together with Activision   Blizzard, and it took a great deal of mathematics  and physics to perform this efficiently in a   computer graphics engine. And now, the AI just  knows what it is. I really don’t know what to say. And, as always, +1 because I couldn’t resist:  if you are interested in trying a reduced   version of DALL-E, check this out. The link is  available in the video description. Once again,   please note that this is a  highly reduced version of DALL-E,   but it is still quite fun.  Let the experiments begin! And, I also cannot wait to get access to the full  model, some Two Minute Papers mascot figures, and   obviously, images of wise scholars holding on  to their papers must to come into existence!   I am sure this tool will democratize art  creation by putting it into the hands of all   of us. We all have so many ideas and so little  time. And DALL-E will help with exactly that.   So cool. Just imagine having an AI artist that is,  let’s say, just half as good as a human artist,   but the AI can paint 10000 images a day for  free. Cover art for your album? Illustrations   for your novel? Not a problem. A little brain in  a jar for everyone, if you will. And, now, it’s   a really good one. Bravo OpenAI. I am starting to  believe that the impact of this AI is going to be   so strong, there will be the world as we know  it before DALL-E, and the world after it. So,   does this get your mind going? What else would you  use this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
596,Google’s New Robot: Your Personal Butler!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today, you are going to see how one area  of research can unlock new capabilities   on a seemingly completely different area. We  are now living the advent of amazing AI-based   language models. For instance, OpenAI’s GPT-3  technique is capable of all kinds of wizardry,   for instance, finishing our sentences, or creating  plots, spreadsheets, mathematical formulae,   and many other things. While their Dall-E 2 AI is  capable of generating incredible quality images   from our written descriptions. Even if  they are too specific. Way too specific.   Now note that all this wizardry is possible as  long as we are dealing with text and images. But how about, endowing a real  robot, moving around in the real   world with this kind of understanding of  language. I wonder what that could do? Well, check this out! Scenario 1. This little  robot it uses GPT-3 and other language models,   and it not only understands it, but it  can also use this knowledge to help us.   Don’t believe it? Have a look. For instance,  we can tell it that we spilled the coke,   and ask it how it can help us out? And,  it recommends finding the coke can,   picking it up, going to the trash can,  throwing it out, and bringing us a sponge.   Yes, we will have to do the rest of it ourselves,  but still, wow, good job, little robot. Now the key here is that it not only understands  what we are asking, propose how to help, but,   hold on to your papers, because here comes  Scenario 2. Oh my! It also looks around,   locates the most important objects required,  and now, it knows enough to make recommendations   as to what to do. And of course, not all of  these make sense. Look at that! It can say that   it is very sorry about the mess. Well, thank  you for the emotional support, little robot,   but we also need a little physical help  here. Oh yes, that’s better. And now, look,   it uses its eyes to look around, yes, I have my  hand, well, does it work? Yes it does, great, now,   coke can, trash can, sponge. Hmm…it’s  time to make a cunning plan. Perfect! And, it can also do plenty more. For instance,  if we’ve been reading research papers all day   and we feel a little tired, if we tell  it, it can bring us a water bottle,   hand it to us, and can even bring us an apple.  Now I’d love to see this…be gentle, and! Oh   my! Thank you so much! The amenities at Google  HQ seem to have leveled up to the next level. And, believe it or not, these were  just really simple things, it can do   way better. Look. This is a plan that  requires planning 16 steps ahead,   and it does not get stuck, and doesn’t mess  up too badly anywhere. This one is as close   to a personal butler as it gets. Absolutely  incredible. These are finally real robots that   can help us with real tasks in the real  world. So cool! What a time to be alive! Now, this is a truly amazing paper, but  make no mistake, not even this is perfect.   For instance, the success rate for the planning is  about 70%, and it can properly execute the plans   most of the time, but clearly, not all  the time. The longer term planning results   may also need to be a bit cherry-picked to  get a good one. It doesn’t always succeed. Also, note that all this is played at  10x speed, so, it takes a while. Clearly,   typing the message and waiting for the robot  still takes longer than just doing the work.   However, this is an excellent opportunity  for us to apply the First Law Of Papers,   which says, that research is a process. Do not  look at where we are, look at where we will be   two more papers down the line. If OpenAI’s image  generator AI, DALL-E looked like this, and just   a year and a paper later, it looks like this.  Well, just imagine what this will be able to do   just a couple more papers down the line.  And, what do you think? Does this get   your mind going? If you have ideas for cool  applications, let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
597,Google’s Imagen AI: Outrageously Good!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. I cannot tell you how excited I am by this  paper. Wow. Today you will see more incredible   images generated by an AI. However, not from  OpenAI, but Google! Just a few months ago,   OpenAI’s image generator AI called  DALL-E 2 took the world by storm. You could name almost anything, Cat Napoleon,  a teddy bear on a skateboard on Times Square,   a basketball player dunking as an explosion  of a nebula, and it was able to create an   appropriate image for it. However, there was  one interesting thing about it. What do you   think the prompt for this must have been? Hmm.  Not easy, right? Well, it was “A sign that says   deep learning”. Oh yes, this was one of  the failure cases. Please remember this. Now, we always say that in research, do  not look at where we are, always look   at where we will be two more papers down  the line. However, we didn’t even make it   two more papers down the line. What’s more,  we barely made two months down the line,   and here it is: this is Google Research’s  incredible image generator AI, Imagen. This technique also looks at millions  of image and text description pairs,   and learns what people mean when they  say this is a guitar, or this is a panda.   But, the magic happens here. Oh yes, it also  learned how to combine these concepts together,   and how a panda would play a guitar. The  frets and the strings seem a little wacky,   but what do we know, this is not human  engineering. This is panda engineering. Or a robot   engineering something for a panda.  We are living crazy times indeed. So, what is different here? Why do  we need another image generator AI?   Well, let’s pop the hood, and look inside. Oh yes.  Two things that will immediately make a difference   come to mind. One, this architecture is simpler.  Two, it learns on longer text descriptions,   and hopefully that also means that it can  generate text better. Let’s have a look.   What? Are you seeing what I am seeing?  That is not just some text, that   is a beautiful piece of text, exactly what  we were looking for. Absolutely amazing. And, these not the only advantages, you know, I  am a light transport researcher by trade, and I   promised to myself that I’ll try not to flip out.  But…hold on to your papers, and… holy mother of   papers. Look at that. It can also generate  beautiful refractive objects. That duck is   truly a sight to behold. My goodness. Now I will  note that DALL-E 2 was also pretty good at this. And, if we plug in DeepMind’s new Flamingo  language model, would you look at that!   Is this really happening? Yes, that is an  AI commenting on a different AI’s work.   What a time to be alive! We will have a  look at this paper too in the near future,   make sure to subscribe and hit the bell icon,  you really don’t want to miss it when it comes. And, you know what, let’s test it some more  against OpenAI’s amazing DALL-E 2. See how   they stack up against each other. The first prompt  will be a couple of glasses sitting on the table.   Well, with Google’s Imagen, oh my, these ones are  amazing, once again, proper refractive objects,   loving it. And, what about DALL-E 2? There is one  with the glasses that with an interesting framing,   but I see both reflections and refractions,  apart from the framing, I am liking this. And   the rest…well, yes, those are glasses sitting on  the table. But when we say a couple of glasses,   we probably mean these and not these. But that’s  really interesting, two AIs that have a linguistic   battle here. Imagine showing this to someone  just ten years ago. See how they would react.   Loving it. Also, I bet that in the future, these  AIs will work like brands and products today,   where people will have strong  opinions as to which ones they prefer.   The analog warmth of Imagen, or the  three-year warranty on DALL-E 4? And wait, you are all experienced Fellow  Scholars here, so you also wish to see the   two tested against each other a little more  rigorously. And we’ll do exactly that. The   paper checks the new technique against previous  results mathematically. Or, we can ask a bunch   of humans which one they prefer. And, wow. The  new technique passes with flying colors on both. And once again, DALL-E 2 appeared just about 2  months ago, and now, a new followup paper from   Google that tests really well against it.  This is not two more papers down the line.   Not even two more years down the line.  This is just two more months down the line.   And a year before, we had DALL-E 1, and  see how much of a difference OpenAI made   in just a year. Now I am almost certain that  this paper has been in the works for a while,   and they added the comparisons against DALL-E  2 at the end. But still, a followup paper this   quickly? The pace of progress in AI research is  absolutely incredible. What a time to be alive! So, does this get your mind  going? What else would you   use this new technique for? Let  me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
598,OpenAI DALL-E 2 - AI or Artist? Which is Better?,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how OpenAI’s DALL-E 2 AI’s images stack up to real works from real artists. This is going to be an amazing journey, so, hold on tight to your papers! DALL-E 2 is an AI that took the world by storm in the last few months. What does it do? Well, we can write it a text description, and hand it to the AI that creates an amazing image of exactly that. And now, I am lucky to say that, yes, I have ascended! Thanks to the generosity of OpenAI, I now have access to DALL-E 2. And my first thought was, that if it can come up with high-quality, and truly novel images, let’s put those to the real test. In other words, see how they stand up against the works of real artists. Our first subject will be my favorite artist in the world, and that would be my wife, Felícia Zsolnai-Fehér. She drew this chimpanzee. Oh my, that is beautiful. So, let’s try this against DALL-E 2. Okay, but how? Well, the first way of trying it will be through variant generation. What is that? Well, since this AI is trained to understand the connection between text and images, the understanding goes both ways. This means that we can not only input text and get an image, but input an image too. Does it understand what is depicted in this image? Let’s see. Oh yes. Wow, it not only understands what is going on, but it can demonstrate that by creating really good variants of it. But, not nearly as good as Felícia, I have to say. Let’s give it another chance. Here, I used a text prompt to get a hand-drawn chimp looking to the right. And I asked for lots and lots of detail. I’ve pleaded and pleaded for extra wrinkles to squeeze everything out of the AI, even cherrypicked the results, and these were the best I could get. You know, art is highly subjective, that’s what makes it what it is. But for now, I have to say Team Humanity, team AI, 1-0. At this point, I thought this is going to be easy. Easy easy easy. And boy, you’ll find out in a moment how wrong I was. Here is a work from one of my other favorite artists, Judit Somogyvári. Wow. That is one of her original characters, very imaginative, I love it. So, let’s ask for a variant of that. Does the AI understand what is going on? You know, there is a lot to take in here, elf ears, make up, and the…wow. I cannot believe that. I am lucky that the variants are working out so well, because writing a prompt for this would be a bit of a challenge. I am truly out of words. These are so good. But, not as good as Judit, in my opinion. Team Humanity vs Team AI, 2-0. That’s a good score, but it is getting harder. Now, let’s look at one of Judit’s other works. An ostrich that is also a formula one racer. I am loving this, especially the helmet and the goggles. I think I can whip up a good prompt for this. So, what did the AI come up with? Holy mother of papers! Is this some kind of joke? It cannot be that good. I checked the images over and over again. It is that good. Wow. Now, of course, scoring this is subjective, but clearly, both are excellent. I’ll give this one a tie. Team Humanity vs Team AI, 3-1. I told you, it is getting harder. Make sure to check out both Judit, and Felícia’s works, the links are available in the video description. And now, for the next test, please give a warm welcome to the King of All Donuts. Of course, that is the great Andrew Price, 3d modeler and teacher extraordinaire. Now, here are some of the donuts I came up with using DALL-E 2. And some from the prompts that I ran for my daughter. Loving the googly eyes there, by the way. So, these are some great donuts. Now, Andrew, it’s your turn. Let’s see…wow OpenAI’s was a good one, but this, this is a Michelin-star donut right there. The AI-made ones are no match for Andrew’s skills. So, Team Humanity vs Team AI, 4-1. Note that DALL-E 2 often isn’t as good as a real, world-class artist, but the AI can create 10000 of these images a day. That must be worth at least a point. So, 4-2. Whew! We made it. Now, all this testing is great fun, and of course, highly subjective, so don’t take it too seriously. And here comes the even more important lesson. In my opinion, this tool should not be framed at all as Team Humanity versus Team AI. I believe this should be framed as Team Humanity, supercharged by Team AI. Here’s why through 6 inspiring examples. Look. First, this is my second favorite prompt ever, a grumpy cat in a spaceship. That is incredible. Now, almost as soon as I posted it on Twitter, it inspired Fabian to create an animated version of it. And now, our adorable little Grump is ready for takeoff. Through the ingenuity of Team Human and Team AI together. So cool! What a time to be alive! So, I hear you asking, if this was my second favorite, what is the first? Well, two, have a look at this fox scientist. I cannot believe how well this came out. It has tons of personality. And, it also inspired others to repaint it. Here it is. You see, I don’t see this as a competition. I see this as collaboration. Three, it also inspired the great beardyman, who wanted to see a platypus studying the schematics for a cold fusion reactor. These are absolutely amazing, not just the quality, but the variety of the results. My goodness. Note that he asked for a one-eyed platypus, but this iteration of DALL-E is not that good with numbers. Yet. If it could talk, it would probably apologize for it. Four, I also ran a prompt for David Brown from Boyinaband which got real interesting real quick. He was looking for a visual pun. What is that? Is this a pan? Is this the pun, that we got a pan instead? I am not sure, but this is very cool nonetheless. And I also wonder what the AI is trying to say here. Five, James from Linus Tech Tips also ran a superb prompt. A time traveler insensitive to humanity's trivialities. What a great idea! Oh my, loving the framing and the thoughtfulness of these results. And finally, six, it inspired me too. I also made this. All hail the mighty Cyberfrog! Loving it. And we also have a tiny chihuahua in our home and in the Two Minute Papers labs. She is Rosie, the paper dog, if you will. This is how we see her. And now, we can also show how she sees herself when meeting other dogs. Oh yes, I think that is very accurate. And of course, as always, +1. You did not think we would leave without looking at some of you Fellow Scholars holding on to your papers, did you? Here you go. And as you see, these are some proper papers. You Fellow Scholars are loving it. But now, let’s have an even more explosive paper, like DALL-E 2. Oh yes! Now we are talking! This scholar is you, who knows that this paper is history in the making. So much so that I cannot stop thinking about and playing with this thing. It really keeps me up at night. This is an incredible achievement that is going to transform the world around us. And fast. So, artists, AI people, what do you think? Which results did you like better? Let me know in the comments below! Also, I will be on Twitter taking some requests from my followers, so make sure to follow us there and send some prompts! The best ones may appear in a future episode here. The link  is in the description. Thanks for watching and for your generous support, and I'll see you next time!"
599,DeepMind Takes A Step Towards General AI!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to have a look at DeepMind’s  new AI, GATO, which can do almost everything   at the same time. Playing games, controlling a  robot arm, answering questions, labeling pictures,   you name it. And the key is  that all this is done by one AI. Now, in the last few years, DeepMind  has built AI programs that solved   a number of extremely difficult problems.  Their chess AI is absolutely incredible,   as it plays better than any human. Then,  they proceeded to tackle Go, a game with an   even larger space of possible moves with great  success as they beat the reigning world champion. But, all of these are separate programs. They  share some components, but all of them require   a significant amount of engineering to  tailor to the problem at hand. For instance,   their amazing AlphaFold AI contains a lot  of extra components that give it information   about protein folding. Hence, it  cannot be reused for other tasks as is. Their StarCraft 2 AI is also at the very least  on the level of human grandmasters. This is also   a separate AI. Now, of course, DeepMind did  not spend years of hard work to build an AI   that could play just video  games. So, why do all this? Well, they use video games as an excellent  testbed for something even greater.   Their goal is to write a general AI that is  the one true algorithm that can do it all.   In their mission statement,  they often say, step number one   is to solve general intelligence, and step  number two, use it to solve everything else. And now, hold on to your papers, and I  cannot believe that I am saying this,   but here is their newest AI that  takes a solid step in this direction. So, what can it do? Well, for instance, it can  label images. And these are not some easy images,   I like how it doesn’t just say  that here are some children   and slices of pizza. It says that we have  a group of children who are eating pizza.   Now, the captions are not perfect, we will get  back to exactly how good this AI is in a moment. Okay, so, what else? It can also chat with us.  We can ask it to recommend books, ask it why a   particular recommended book is interesting, ask it  about black holes, protein folding, you name it.   It does really well, but note that it can still  be factually incorrect, even on simpler questions. And now, with this one, we are  getting there It can also control   this robot hand. It was shown how to stack  the red block onto the blue one, but now,   we ask it to put the blue one on the  green block. This is something that is   hasn’t been shown before, so, can it do it?  Oh yes. Loving it. Great job, little robot! So these were three examples. And now, we have  to skip forward a little bit. Why is that? Well,   we have to skip because I cannot go  through everything that it can do. And now,   if you have been holding on to your  papers so far, now, squeeze that paper,   because it can perform more than 600 tasks.  600! My goodness. How is that even possible? Well, the key here is that this is a neural  network where we can dump in all kinds of   data at the same time. Look. We can train it on  novels, images and questions about these images,   Atari game videos and controller actions,  and even all this cool robot arm data.   And, look! The colors show that all this  data can be used to train their system,   this is truly just one AI that  can perform all of these tasks. Okay, and now comes the most important question.  How good is it at these tasks? And this is   where I fell off the chair when I read this  paper. Just look at this. What in the world!  It is at least half as good as a human expert  in about 450 out of about 600 tasks. And, it is   as good as a human expert in about a quarter of  these tasks. That is mind blowing. And note that   once again, the best part of this work is that  we don’t need 600 different techniques to solve   these 600 tasks. We just need this one generalist  AI that does it all. What a time to be alive! Now clearly, it is not perfect. Not even  close. However, this is Two Minute Papers,   this is the land of Fellow Scholars, so we  will also apply the First Law of Papers,   which says that research is a process. Do  not look at where we are, look at where we   will be two more papers down the line. So how  do we do that? Well, we look at this chart,   and, oh my. Can this really be true? This chart  says that you have seen nothing yet. What does all   this mean? Well, this means that as we increase  the size of the neural network, we still see   consistent growth in its capabilities. This means  that DeepMind is just getting started with this.   The next iteration will be way better.  Just one or two more papers down the line,   and these inaccuracies that you have seen  earlier might become a distant memory. By then,   we might ask “remember when DeepMind’s AI  answered something incorrectly”? Oh yeah,   that was a completely different world back then,  because it was just two papers before this one. This one really keeps me up at night. What  an incredible achievement. So, does this get   your mind going? And what would you use this  AI for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
600,"NVIDIA’s Ray Tracer: Wow, They Nailed It Again!","Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how NVIDIA’s incredible light transport algorithm can render these notoriously difficult photorealistic smoke plumes, volumetric bunnies, and even explosions interactively. We are going to talk about this amazing paper in a moment, and given that NVIDIA can already render this absolutely beautiful marbles demo in real time, not only with real time light transport, but also with our other favorite around here, real time physics! This is just to say that tech transfer is happening here, the papers are real, the papers that you see here really make it to real applications that everyone can use at home. Please note that they won’t say exactly what tech is under the hood here, but looking at their best light transport papers might be a good starting point of what is to come in the future. Now, back to the intro. What you are seeing here should not be possible at all! Why is that? Well, when we use a light transport simulation algorithm to create such an image, we get a photorealistic image, but as you see, not immediately. Not even close. It is a miracle of science that with this, we can shoot millions and millions of light rays into the scene to estimate how much light is bouncing around, and initially, the inaccuracies in our estimations show up as noise in these images. As we shoot more and more rays, this clears up over time. However, there are two problems. One, this can take hours to finish. And in the case of volumetric light transport, these light rays can bounce, scatter, and get absorbed inside a dense medium, in that case, it takes much, much longer. Not only hours, sometimes, even days. Oh my goodness. So, how did NVIDIA pull this off? Well, they built on a previous technique of theirs that is capable of rendering 3.4 million light sources, and not just for one image, but it can even handle animation. Just look at all this beautiful footage. Now it would be so great to have a variant of this that works on volumetric effects, such as explosions and smoke plumes, but of course, anyone who has tried it before says that there is no chance that those could run interactively. No chance at all. Well, scientists at NVIDIA beg to differ. Check this out. This is a previous technique and it says 12 spp. That means 12 samples per pixel, which is more or less simulating 12 light rays for each pixel of these images. That is not a lot of information, so let’s have a closer look at what this previous method can do with that. Well, there is still tons of noise, and it flickers a lot when we rotate the camera around. Well, I would say there is not much hope here, we would need hundreds, maybe even thousands of samples per pixel to get a clean image, not 12. And, look, it gets even worse. What? The new technique runs not thousands, not hundreds, and not even 12 samples per pixel in the same amount of time, but…really? Can this be? 1 sample per pixel? Can you do anything meaningful with that? Well, hold on to your papers, and check this out. Wow, look at that! It can do much better with just one ray per pixel than the previous technique can do with 12. That is absolutely incredible. I really don’t know what to say. And it works not just on this example, but on many others as well. This is so far ahead of the previous technique that it seems like science fiction. I absolutely love it. However, yes I can hear you asking, Károly, but these are still noisy, so why get so excited for all these incomplete results? Well, do you remember what we did with the previous NVIDIA light transport paper? We took these noisy inputs, and plugged them into a denoising technique that is specifically designed for light transport algorithms. It tries to guess what is behind the noise. And as you see, these can help a ton. But you know, can they help so much that a still noisy input, a meager 1 sample per pixel can become usable? Well, let’s have a look. Oh my goodness. Look at that. The result is clearly not perfect, one light ray per pixel can hardly give us a perfect image, but after denoising, this is unreal. We are almost there right away! It has so much less flickering than the previous technique with much more samples, and we are experienced Fellow Scholars around here, so let’s also check the amount of detail in the image. And…whoa! There is no contest here. This technique also pulls off all this wizardry by trying to reuse information that is otherwise typically thrown away. For instance, with no reuse, we get this baseline result, and if we reuse information from a previous frame in the animation, we get this. That is significantly better than the baseline. If we reuse previous rays spatially, that is also an improvement. So are these different kinds of improvements? Well, let’s add them together, and oh yes, now this is what we are here for. Look at how much more information there is in this image. So now, even these amazing volumetric scenes can be rendered interactively? I am out of words. What a time to be alive! And, if you feel inspired by these results, I have a free Master-level course on light transport where we write a full light simulation program from scratch, and learn about physics, the world around us, and more. If you watch it, you will see the world differently. That is free education for everyone, that’s what I want. So, the course is available free of charge for everyone, no strings attached, check it out  in the video description. Thanks for watching and for your generous support, and I'll see you next time!"
601,Google AI Simulates Evolution On A Computer!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today is going to be all  about simulating evolution   on our computers. This can build virtual cars,  virtual creatures, even paint the Mona Lisa!   These all sound amazing, but,  evolution on a computer? How? A few years ago, a really fun online app surfaced  that used a genetic algorithm to evolve the   morphology of a simple 2D car with the goal of  having it roll as far away from a starting point   as possible. A genetic algorithm? What is  that? Well, it is a super simple technique   where we start out from a set of random solutions,  and likely find out that none of them really work   well. However, we start combining and mutating,  or in other words, changing small parts of this   solution, until, oh yes! Something starts to  move. Now as soon at least one wheel is placed   correctly, the algorithm will recognize  that this one rolled so much further,   and keep the genes of this solution in the  pool to breed new, similar solutions from it. A similar concept can also be used to design  the optimal morphology of a virtual creature   to make it skip forward faster. And,  look! That is so cool over time,   the technique learns to more efficiently  navigate a flat terrain by redesigning its legs   that are now reminiscent of small springs  and uses them to skip its way forward.   And here comes something even cooler if we  change the terrain, the design of an effective   agent also changes accordingly, and the super  interesting part here is that it came up with   an asymmetric design that is able to climb  stairs and travel uphill efficiently. Loving it! And, in this new paper, scientists at Google Brain  tried to turbocharge an evolutionary algorithm   to be able to take a bunch of transparent  triangles, and get it to reorganize them so they   will paint the Mona Lisa for us. Or any image in  particular. The technique they are using is called   evolution strategies, and they say that it is  much faster at this than previous techniques. Well, I will believe it when I see it. Hold  on to your papers, and let’s see together.   Here is a basic evolutionary algorithm after 10  thousand steps. Well, it is getting there, but   it’s not that great. And, let’s see how well their  new method does with the same amount of steps.   Wow. My goodness! That is so much better. In fact,  their 10 thousand steps is close to equivalent   to half a million steps with a previous algorithm. And we are not done yet. Not even close. This  paper has two more amazing surprises. Surprise   number one. We can even combine it with OpenAI’s  technique called CLIP, this learns about pairs   of images and their captions describing these  images, which is an important part of DALL-E 2,   their amazing image generator AI. This  could take completely outlandish concepts   and create beautiful, photorealistic images out  of it that are often as good as we would expect   from a good human artist. Scholars holding  on to their papers, a cyberfrog, you name it. So, get this, similarly to that, we can now  even write a piece of text, and the evolutionary   triangle builder will try to create an image  that fits this text description. With that,   we can request a triangle version of a  self-portrait, a human, and look at how similar   they are. Food for thought! But it can also try to  draw Walt Disney World, that is remarkable. Look   at how beautifully it boils it down to its essence  with as few as 200, or even just 25 triangles.   Loving it. Also, drawing the Google  headquarters? No problem at all. And here comes surprise number  two. The authors claim that it is   even faster than a differentiable renderer.  These are really powerful optimization techniques   that can even grow this beautiful statue out of  nothing. And do all that in just a few steps. So,   claiming that it is even faster than  that? Well, that is very ambitious.   Let’s have a look. And…oh yes, as expected the  differentiable technique creates a beautiful image   very quickly. Now, try to beat that! Wow…the new  method converges to the final image even quicker.   That speed is simply stunning. Interestingly, they  also have different styles. The differentiable   renderer introduces textures that are not  present in the original image, while the new   technique uses large triangles to create a smooth  approximation of the background and the hair, and   uses smaller ones to get a better approximation  of the intricate details of the face. Loving it. Now, let’s ramp up the challenge, and test  these a little more and add some prompts. And,   oh yes, this is why the differentiable  technique flounders on the prompts. Look.   It almost seems to try to do everything  all at once. While, the new technique   starts out from a fair approximation, and  converges to a great solution super quickly. I hope that this new take on evolution  strategies will be plugged in to applications   where differentiable techniques do well,  and perhaps do even better. That would be   absolutely amazing because these are a treasure  trove of science-fiction like applications.   For instance, we would be able to  almost instantly solve this billiard   game, where we would like to hit the white  ball with just the right amount of force   and from the right direction, such that the  blue ball ends up close to the black spot. Or,   simulate ink with a checkerboard pattern, and  exert just the appropriate forces so that it   forms exactly the Yin-Yang symbol shortly after. Or, here comes a previous, science-fiction like   example. This previous differentiable technique  adds carefully crafted ripples to the water,   to make sure that it ends up in a state that  distorts the image of the squirrel in a way that   a powerful and well-known neural network sees  it not as a squirrel, but as a goldfish. Wow. And, if this new evolutionary  technique could do all of these tasks,   but better and faster? Sign me up  right now. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
602,NVIDIA’s AI Nailed Human Face Synthesis!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. We are back, and today we are going to marvel at the capabilities of NVIDIA’s incredible image generation AI. And, we will see together that we can all become witchers. Previously, they were able to create an incredible AI that can take a piece of text from us, and create an appropriate image. Or, it could also give us a little more artistic control as it could even transform a rough sketch into a beautiful output image. Variant generation was also possible. Or, if we didn’t feel like drawing, it could also take segmentation maps, which specify which region should be what. The output was again, a beautiful photorealistic image. So these were mostly landscapes. And what about human faces? Can we generate human faces too? And I wonder if we could also edit them? If so, how? Well, NVIDIA already has incredible works for generating human faces. Look. This is their StyleGAN 3 AI, and as you see, my goodness. These are incredible high quality faces, and we even have a tiny bit of control over the images. Not a great deal, but a little control is there. It works on art pieces too, by the way. And, this new technique finally gives us a little more creative freedom in editing these images in 5 different ways. How? Well, check this out! Oh yes, one, we start sketching, and out comes an image of this quality. Wow. This truly is a testament to the power of AI these days. It doesn’t just generate images out of thin air, but here, we can really harness the power of this AI and the best part is that we don’t even need to be an expert at drawing. So cool! Two, if we don’t feel like drawing, but we would really like to see some well-known people as cubist paintings, this can do that too. Three, if we have an image with snow, and we feel like chocolate, vanilla, or cherry ice cream would be so much better for snowboarding, also, not a problem. And here comes my favorite. Four. We can make people into Witchers. I think I might make a good one. Look. This is Obi-Wan Károly, a synthetic image of me made by an AI with an added beard. And, look. Hm-hmm! You don’t want to mess with Witcher Károly, do you? Hmm, so who else would make a great witcher? Well, here are some of my favorites. For instance, Grimes. Beardyman, Skrillex, Robin Williams, Obama, and The Rock. All excellent witchers! And, five, it can not only produce these images, but even interpolate between them. What does that mean? This means that not only the final image, but intermediate images can also be generated. Look, we are going from a photo, to a sketch, to a Mona Lisa-style painting. While we are looking at these wonderful results, I would like to send a huge thank you to the authors and they were very patient and took quite a bit of time off their busy day to delight you Fellow Scholars with these animations. As a result, some of these animations you can only see here on Two Minute Papers. That is a huge honor. Thank you so much! So, how easy it is to use? So easy that it only takes a minute to produce such excellent results, and it likely means that this will be an excellent tool in democratizing artistic image editing and giving it into the hands of everyone. Now make no mistake, this is a research paper, not a product. Yet! But NVIDIA has an excellent record of transferring these works into real products. For instance, they published a similar Audio2Face paper in 2017, and now, just a few years later, it is out there for everyone to use. How cool is that? I really hope that this work will also have a similar fate! What a time to be alive! So, does this get your mind going? What would you use this for? Let me know in the comments below! Thanks  for watching and for your generous support, and I'll see you next time!"
603,Watch This Dragon Grow Out Of Nothing!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to learn how to take an object  in the real world, make a digital copy of it,   and place it into our virtual world. This can be done through something that we  call differentiable rendering. What is that?   Well, simple, we take a photograph, and have  an AI find a photorealistic material model   that we can put into our light simulation program  that matches it. What this means is that now,   we can essentially put this real material  into a virtual world. This earlier work   did very well with materials, but  it did not capture the geometry. Later, this followup work was looking to  improve the geometry side of things. Here,   let’s try to reproduce a 3D model from a  bunch of triangles. Let’s see….and. Look.   This is lovely. Seeing these images gradually  morph into the right solution is an absolutely   beautiful sight. But, of course, in  this case, the materials are gone. And, if we wish to focus more on materials,  and we don’t care about geometry at all,   we can use one of our previous papers for that.  This one uses a learning algorithm to find out   about your artistic vision and  recommends really cool materials.   Or, with this other work, you can whip up a fake  material in photoshop, and it will magically find   a digital photorealistic material that matches  it. But once again, no geometry. Materials only. But wait a minute. Am I reading this  right? With previous method number one   we get the materials, but no geometry. Or, previous method number two. No materials.   But, the geometry is much more detailed. Or, previous method number three. Excellent   materials, no geometry. We  can’t have it all, can we? So, is that it? We can’t really take a real object  and make a virtual copy of it? Is the dream dead?   Well, don’t despair quite yet! This new technique  might just be what we are looking for. Although I   am not so sure, this problem is super challenging.  Let’s try this chair and see what happens.   Here are a bunch of images of it.  Now, your turn, little algorithm! And, we start out from a sphere. Well, I will  believe this when I see it! Good luck! And, hmm.   The shape is starting to change,  and…wait! Are you seeing what I am seeing?   Yes! Yes! Yes! The material is also  slowly starting to change. We are getting   not only geometry, but materials too! Oh boy!  Now, based on the previous works, our expectation   is that the result will be extremely coarse.  Well, hold on to your papers, and let’s see   how much detail we end up with. Oh my. That is  so much better. Loving it! Actually, let’s check. This was the previous technique, and  this is the new one. My goodness,   I have to say there is no contest here. The  new one is so much more detailed! So good!   And, it works on a bunch of other examples  too! These are not perfect by any means, but   this kind of improvement, just one more  paper down the line. That is absolutely   incredible. The pace of progress in computer  graphics research is nothing short of amazing,   especially when we look at all the magic  scientists are doing in Wenzel Jakob’s lab. And, the whole process took less than an hour.  That is a considerable amount of time, but   an artist can leave a bunch of these  computations to cook overnight,   and by the morning, a bunch of already pretty  good quality virtual objects will appear   that we can place in our virtual worlds.  How cool is that? What a time to be alive! Make sure to check out the whole paper,  it has crystal-clear mathematics, and   tons of gorgeous images. The link is  available in the video description. I   believe this will become an excellent tool  in democratizing asset creation for games,   animation movies, and all kinds of virtual worlds. So, what would you use this for? Do you have some  cool ideas? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
604,OpenAI DALL-E 2 - Top 10 Best Images!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. These amazing images were all made by an AI  called DALL-E 2. This AI is endowed with a   diffusion-based model, which means that when  we ask it something, it starts out from noise,   and over time, it iteratively refines this image  to match our description better. And, over time,   magically, an absolutely incredible image emerges.  This is a neural network that is given a ton of   images, and a piece of text that says what is  in this image. That is one image-caption pair.   DALL-E 2 is given millions and millions of these  pairs. So, what can it do with all this knowledge? The key takeaway here is that it does no, or  very little copying from this training data, but   it truly comes up with novel images. How? Well,  after it had seen a bunch of images of koalas,   and separately, a bunch of images of motorcycles,  it starts to understand the concept of both,   and it will be able to combine the two  together into a completely new image. And luckily, due to the generosity of OpenAI,   I also have access and I tried  my best to push it to its limits. It can also generate an image of you  Fellow Scholars marveling at this paper,   or in the process of reading an even  more explosive paper. So good! I also   tried this fox scientist in an earlier  episode, and it came out extremely well   with tons of personality. And, hold on to  your papers because due to popular request,   you are getting some more variants from  my experiment with it. And DALL-E 2 did   not disappoint. This character can  be recreated as a beautiful sculpture   with several amazing variants, Dragonball  style, and it is also a true merch monster. But, we also tested it against the works of real  artists. My opinion is that it is not yet as good   as a world-class artist, here are some tests  against Judit Somogyvári’s work, and it is also   not as good as the king of donuts, Andrew Price,  however, DALL-E 2 is able to create 10,000 images   every day. That is a different kind of value  proposition. So, we ask today, after these amazing   results, does it really have more surprises up  the sleeve? Oh boy. By the end of this video,   you will see how much of a silly question that  is. So today, we are going to have a look at   10 recent amazing results, some of which are from  prompts that I ran for you on Twitter. Let’s see! For instance, one this is from you.  Schrödinger's cat gripping the latest   research papers. Loving the glasses and  the scholarly setting, and, wait a minute.   Don’t eat them! Come on! What results are  you trying to hide? We will never know. Two, this is a screaming tiger  trading cryptocurrencies. And,   yes, you are seeing correctly, the meme game is  strong here. The resemblance is uncanny! So good! Three, I am always curious about what it  thinks about itself. Well, let’s ask it   to make a self-portrait of itself looking in  the mirror. It identifies as a robot. Okay,   got it, that makes sense. And it is also… a  penguin? Excuse me? And a robotic frog too   that seems surprised by its own existence.  How cool is this? Also, food for thought. Four, it can create battle maps  for tabletop roleplaying games.   And I bet variants of these will  also inspire some of you Fellow   Scholars to dream up amazing new  board games or even video games. Five, we can’t have enough cats, can we? This  is one falling into a black hole. I love how   expressive the eyes are. The first two seem quite  frightened about this journey. First time? But the   third one, this chap seems really confident. And,  I love how it also inspired you Fellow Scholars   with these memes that are now starting to appear  on these DALL-E 2 images. Keep the game going! Six, I now introduce you, to the king of  life. A dabbing walrus with sunglasses.   This one here is an excellent testament to the  creative capabilities of DALL-E 2. Clearly,   its training set did not contain  images of walruses in sunglasses,   and much less ones that are dabbing. It made this  thing up on the spot. What a time to be alive! Seven, if you think that  this world is a simulation,   and you wish to see what it looks like from  the outside, DALL-E 2 has you covered. Oh yes! Now, eight. let’s put it to the real test with  Noam Chomsky’s classic sentence. Colorless green   ideas sleep furiously. He put this together  to show that you can create a sentence that   is grammatically correct, but semantically, it is  complete nonsense. No meaning whatsoever. Well,   DALL-E 2 begs to differ. Here are its  ideas. This idea seems is colorless,   with a bit of green paint from the bed,  and it seems that it was asleep and it is   now furious. This one makes me feel that the AI  can create anything. A true creativity machine. Nine, anything you say? Well, this one is  from the amazing DALL-E 2 subreddit. And   it showcases Bob Ross painting  himself in infinite recursion.   This is one of the most impressive  prompts I have seen yet. Wow. Ten, I really wanted to make a squirrel with a  malfunctioning laptop. I tried to push DALL-E 2 to   the limits in creating this with a huge variety  of different styles and to say that I was not   disappointed would be an understatement. Look  at these beautiful results! My goodness! It can   do Disney style, cartoon style, oil paintings,  I feel like this can do absolutely anything. And, +1 because I can never resist. This is Kermit  the frog in fight club. Oh yes. It seems to me   that he went for the method actor route and made  the role his own. Better not mess with this guy. And, you know what, if you have been holding on to  your papers so far, now, squeeze your papers, for   +2. I hear you asking, Károly, why are you posting  the thumbnail of an earlier Two Minute Papers   video here? What is this? Is it possible that…oh  yes. M-hmm! That is exactly right. Quite a few of   our last thumbnails were made by DALL-E 2. What  a time to be alive! My wonderful wife, Felícia   Zsolnai-Fehér does the rest of the design work on  these gorgeous thumbnails. I absolutely love them.   She is so good. Not only in graphic design, but  her pencil drawing was also compared to DALL-E 2,   and she was one of the amazing artists who was  able to beat DALL-E 2. At least, in my opinion. So, as you see, DALL-E 2 is transforming the world  around us. And fast. Just a couple years ago,   I was wondering whether we could  take an AI, just say what we want,   and get a high-quality thumbnail  for our videos, and I thought,   well, maybe in my lifetime. By the time I become  an old man, perhaps such a thing would exist. And,   boom, just a couple more papers down the line,  and here we are. I truly cannot believe how good   these results are. What a time to be alive! And  don’t forget, this was what DALL-E 1 was capable   of just a bit more than a year ago, and now,  one more paper down the line, and the results   are outstanding. I wonder what DALL-E 3 will be  capable of? I bet it is already in the works. So,   does this get your mind going? Which work did  you like best? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
605,NVIDIA’s New AI Trained For 10 Years! But How?,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today you will see an absolute banger paper. This  is about how amazingly NVIDIA’s virtual characters   can move around after they have trained for 10  years. 10 years? We don’t have 10 years for a   project! Well, luckily, we don’t have to wait for  10 years. Why is that? I will tell you exactly why   in a moment. But, believe me, these folks are  not natural born warriors. They are AI agents   that have to train for a long long time  to become so good! So, how does that work? Well, first, our candidates are fed a bunch of  basic motions, and then, are dropped into NVIDIA’s   Isaac, which is a virtual gym where they can hone  their skills. But, unfortunately, they have none.   After a week of training, I expected that they  would showcase some amazingly athletic warrior   moves, but instead, we got… this. Oh my goodness.  Well, let’s be optimistic and say that they are   practicing Judo where the first lesson is  learning how to fall. Yes, let’s say that. Then, after two months, we can witness some  improvement. Well, they are not falling,   and they can do some basic movement.  But, they look like constipated warriors. After 2 years, we are starting to see something  that resembles true fight moves. These are not   there yet, but they have improved a great deal.  Except this chap. This chap goes like “Sir, I’ve   been training for two years, I’ve had enough! And  now, I shall leave… in style.” I wonder what these   will look like in 8 more years of training. Well,  hold on to your papers, and let’s see together! Oh my! This is absolutely amazing! Now that’s  what I call a bunch of real fighters! See? Time   is the answer! It made even our stylish chap take  its training seriously. So, which one is your   favorite from here? Did you find some interesting  movements? Let me know in the comments below! Now, I promised that we will talk about the ten  year thing. So, did scientist at NVIDIA start this   paper in 2012? Well, not quite. This is 10 years  of training, but in a virtual world. However,   a real world computer simulates this virtual  world and it can do it much quicker than that.   How much quicker? Well, a powerful machine  can simulate these 10 years not in 10 years,   but in 10 days. Oh yes! Now  that sounds much better! And, we are not done yet. Not even close! When  reading this paper, I was so happy to find out   that this new technique also  has four more amazing features. One, it works with latent spaces. What is  that? A latent space is a made up place   where similar kinds of data are laid out to  be close to each other. In an earlier paper,   we used such a space to generate beautiful  virtual materials for virtual worlds. NVIDIA   uses a latent space to switch between the  motion types that the character now knows,   and not only that, but their AI also  learned how to weave these motions together,   even if they were not combined together  in the training data. That is incredible! Two, this is my favorite. It has to be. They  not only learned to fall, but in those 10 years,   they had plenty of opportunity to also learn  to get up. Do you know what this means?   Of course, this means the favorite pastime of  the computer graphics researcher. And that is,   throwing boxes at virtual characters. We like to  say that we are testing whether the character can   recover from random perturbations. That  sounds a little more scientific. And,   these AI agents are passing with flying  colors. Or flying boxes, if you will. Wow. Three, the controls are excellent.   Look. This really has some amazing  potential to be used in virtual worlds,   because we can even have the character face  one way, and move into a different direction   at the same time. More detailed poses can also  be specified. And, what’s more, with this, we   can really enter a virtual environment and strike  down these evil pillars with precision. Loving it. Four, these motions are synthesized adversarially.  This means that we have a generator neural network   creating these new kinds of motions. But, we  connect it to another neural network called the   discriminator, this watches it and ensures that  the generated motions are similar to the ones in   the dataset and seem real too. And, as they battle  each other, they also improve together, and in   the end, we take only the motion types that are  good enough to fool the discriminator. Hopefully,   these are good enough to fool the human eye too.  And, as you see, the results speak for themselves. If we wouldn’t be doing it this way,   here is what we would get if we  trained these agents from scratch. And, yes, while we are talking about training.  This did not start out well at all. Imagine   if scientists at NVIDIA quit after just 1 week of  training, which is about 30 minutes in real time.   These results are not too promising, are they?  But, they still kept going. And the result was   this! That is excellent life advice right there,  and, also, this is an excellent opportunity for us   to invoke the The Third Law Of Papers.  Not the first, the third one! This says   that a bad researcher fails 100% of the time,  while a good one only fails 99% of the time.   Hence, what you see here is always  just 1% of the work that was done. And, this is done by NVIDIA, so I am sure that  we will see this deployed in real world projects,   where these amazing agents will get democratized  by putting it into the hands of all of us.   What a time to be alive! So,  does this get your mind going?   What would you use this for? Let  me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
606,"Finally, Robotic Telekinesis is Here!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to perform something that seems  like a true miracle. Oh yes, you are seeing it   correctly. This is robotic telekinesis. Moving  objects from afar. So, what is going on here? Well, we take a human operator, who  performs these gestures, which are then   transferred to a robot arm, and then,  the magic happens. This is unbelievable!   So now, hold on to your papers, and let’s  see how well it can pull off all this! Level 1 Oh my, it comes out guns  blazing! Look at that! It is not a brute,   not at all! With delicate movements,  it can pick up these plush toys, or,   even rotate a box. That is a fantastic start. But, it can do even better! Level 2 Let’s try to pick up those scissors.  That is going to require some dexterous hand   movements from the operator and…wow! Can you  believe that? By the time I am saying this,   it is already done. And, it can also stack  these cups, which is a difficult matter as its   own finger might get in the way. This was a little  more of a close call, but it still managed. Bravo! So, with all these amazing tasks, what could  possibly be level 3? Well, check this out! Yes, we are going to attempt to open  this drawer. That is quite a challenge.   Now note that it is slightly ajar to make  sure the task is not too hard for its fingers,   and, let’s see! Oh yeah! Good job little robot! But what good is opening a drawer  if we aren’t doing anything with it,   so, here comes +1, the final boss level  open the drawer, and pick up the cup.   There is no way that this is possible through  telekinesis. And…oh my goodness. I love it. And note that this human has done  extremely well with the hand motions, but   is this person a robot whisperer, or can anyone  pull this off? That is what I would like to know.   And, wow! This works with other operators too,   which is good news, because this means that  it is relatively easy and intuitive to use.   So much so, and get this, that these people  are completely untrained operators. So cool! So, now is the part where we expect the bad  news to come. So, where is the catch? Maybe   we need some sinfully expensive camera gear to  look at our hand to make all this happen, right?   Well, if you have been holding on to your  papers so far, now, squeeze those papers,   because we don’t need to buy anything crazy at  all! What we need is just one, uncalibrated color   camera. Today, this is available in almost  every single household. How cool is that! So, if it can do that, all we need is  a bunch of training footage, right? But   wait a second…are you thinking what I am thinking?  If we need just one uncalibrated color camera,   can it be that? Yes! That is exactly right. We  don’t need to create any training data at all,   we can just use Youtube videos that  already exist out there in the wild! In a previous paper, scientists at  DeepMind harnessed the power of Youtube   by having their AI watch humans play games,  and then, they would ask the AI to solve hard   exploration games and it just ripped through these  levels in Montezuma’s revenge and other games too.   And here comes the best part what was even  more surprising there that didn’t just perform   an imitation of the teacher, no-no. It  even outperformed its human teacher! Wow. I wonder if we could somehow do  a variant of that in a future paper?   Imagine what good we could do with that. Virtual  surgeries? Any surgeon could perform a life-saving   operation on anyone, from anywhere else in  the world? Wow. What a time to be alive! Now,   for that, the success rate needs to be much  closer to a hundred percent here, but still,   this is incredible progress in AI research. And  of course, you are an experienced Fellow Scholar,   so you don’t forget to apply The First Law  Of Papers here, which says that research   is a process. Do not look at where we are, look  at where we will be two more papers down the line.   And two more papers down the line, I bet this  will not only be significantly more accurate,   but, I would dare to say that even full body  retargeting will be possible. Yes, we could   move around, and have a real robot replicate  our movements. Just let the computer graphics   people in with their motion capture knowledge,  and this might really become a real thing soon. So, does this get your mind going? What would you  use this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
607,NVIDIA GTC: When Simulation Becomes Reality!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see that the research papers  that you see here in these videos are real. So   real that a ton of them are already seeing use  in NVIDIA’s AI projects, some of which are so   advanced they seem to be straight out of a science  fiction movie, and even more of them are coming. We will talk about NVIDIA’s GTC event and as  always, we will try to have a scholarly angle on   it. For instance, in our Tesla AI Day recap video,  we marveled at how transformer neural networks,   a paper from 2017 saw use just a couple  years later in real self-driving cars   deployed all around the world. That  kind of tech transfer is extraordinary. So, around here, we talk about a ton of amazing  research papers, and under almost every video,   I see some of you asking, okay, this all looks  great, but when do we get to use it? That   is completely understandable question, and  finally, you’ll get your answers today. And it gets better, we will also learn  three lessons. Lesson one is that AI   is everywhere today and it is improving incredibly  fast. For instance, it can now train these virtual   warriors from scratch, or, as many of us have  these computers with weird camera placements,   it can recreate our image such that it  appears that we are holding eye contact.   Yes, that’s right! Parts of this image are  synthetic, parts are made on the fly by an AI,   and it is already almost impossible to notice.  And this is FourCastNet. This is a physics model   that can predict outlier weather events,  and it runs not in a data center anymore,   it runs on just one NVIDIA graphics card.  That is fantastic, lesson two, AI is opening   up completely new frontiers that we never even  thought of. And now, hold on to your papers,   and check this out. They can now also create a  virtual world where they can accurately simulate   infrastructure projects, down  to the accuracy of a millimeter. When talking about digital copies of real things,  this is a virtual assistant, which understands   English, the questions we ask it, and let’s be  honest, today, all that is expected. However,   what is not expected is that it can synthesize  and answer in a real persons’s voice,   this person is Jensen Huang, the CEO of NVIDIA,  and it also animates its mouth and gestures   accordingly. And all this in real time. Look at  the face of this proud man! This is priceless. And, we talked briefly about Tesla, but do you  know that NVIDIA is also making progress on   self-driving cars? This is how it sees the world.  Marvelous. But, the self driving part is nothing,   watch carefully, because here it comes! Oh yes,  lesson number three: everything is connected.   See the assistant there? It sees and identifies  the passenger, understands natural language. It   also understands which building is which  and what shows are played in them today. This is self-driving cars combined  with the virtual assistant, but, look!   Oh my! This is also combined with virtual  copies of real worlds too. By the end of 2024,   they expect to have a virtual, video game version   of all major North American highways, plus  Europe, and Asia. What in the world! Wow. So, I hear you asking, what are all  these virtual video game worlds good for?   Why do we need a video game world? Why not  just use the real one? It’s right here! Well,   one of the answers is domain randomization. Here,  we can reenact real situations for self-driving   cars, and even create arbitrarily complex  new situations, and these AIs will be able   to learn from them in a safe environment.  You see, once again, everything is connected. And there is still so much more to talk about, my  goodness. For instance, they also have this cell   visualization system that can show us real  cells splitting right in front of our eyes   in real time. Not so long ago such a  simulation would have taken days, and now,   it runs in real time. They can also create a  digital version of real fulfilment centers.   This virtual world can be simulated and optimized  before the real one is changed. Does this mean   that? Yes, that is exactly right now, companies  can find the optimal layout for their plans before   making any physical investments. Kind of like the  game factorio but in real life. How cool is that? Their vision system can also look at conveyor  belts and adjust their speed based on congestion.   And once again, you see that everything is  connected, the self driving engine can also be   put not only into a car, but into a little robot,  and thus, it can now navigate these warehouses.   So, whenever you see a new research paper, and it  does not immediately show how it could be used,   please do not forget everything is connected. They also have their Isaac Gym system,   in which these robots can train safely  before they are deployed into the real world.   And, these simulations can get insanely  detailed and accurate. For instance, here,   the physics and connections of 5400 parts are  simulated, and as a result, this virtual robot   works exactly as the real one. This application  would have been unfathomable just a few years ago. And, yes, this has huge ramifications.   For instance, in the future, whenever we have this  robot called Anymal pass a test in a simulation,   it is very likely that it will  pass in the real world too.   A simulation that is nearly the same as reality.  Just think about that. What a time to be alive! And once again, applications like this require  light transport simulations, cloud streaming,   and collaboration with an AI assistant. And now,  you can even stream these from the cloud if you   don't have a beefy graphics card at home. And the  results are absolutely amazing. Here you see what   this place will look like at noon. Now, how about  some trees nice and maybe more variation. We don’t   even need to be experts in 3D modeling, we just  say what we wish to see to the virtual assistant,   and, there we go! We can also ask, what does  this look like at night it looks spectacular. All that is truly amazing. My eyes were  popping out like these machines when seeing   these results for the first time. And, with  all of these remarkable results, we really   just scratched the surface. There is so much  going on, it is almost impossible to keep   track of all of these amazing projects.  Here are some more for your enjoyment. So, the papers that you see here in this series  are real. As real as it gets. We can get from a   research paper to a real product in just a couple  years. And everything that you saw here today   is already in production, or will be in  production in the near future. By the way   if everything goes well, I will hold my own  talk in this year’s GTC as well, and publish   a video of it on this channel. Make sure to  subscribe and hit the bell icon to not miss it. So, what do you think? Does  this get your mind going?   What would you use these for? Let  me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
608,NVIDIA’s AI Plays Minecraft After 33 Years of Training!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see whether NVIDIA’s  modern AI system they call MineDojo   can learn to play and build  things in Minecraft and more. In an earlier episode, we explored a previous  project that goes by the name GANCraft,   which performed world to world translation.  What is that? Well, simple, we give an AI a   very rough draft of a virtual world, and out  comes a much more detailed and beautiful one.   This almost seems like science fiction. Really,  just dream up something, and it makes it happen. And, it gets better, it created water,  it understands the concept of an island,   and it created a beautiful landscape,   also, with vegetation. Insanity. It even  seems to have some concept of reflections,   although they will need some extra work to get it  perfectly right. It even supported interpolation,   which means that we can create one landscape and  ask the AI to create a blend between different   styles. We just look at the output animations, and  pick the one that we like best. And, with this,   finally, we also have some artistic control over  the mood of the results. Absolutely amazing. But then they thought, why stop there? For  instance, DeepMind has a long history of   using AI systems to master all kinds of  games, from Chess, to Go, to StarCraft 2,   OpenAI has a DOTA2 AI project that is  able to challenge a world champion team,   so, NVIDIA thought, why not try their hands  at Minecraft? Minecraft is an open world game,   one of the most played games in the world,  which is kind of like a virtual sandbox.   Here, you can build things, explore, but really,  do whatever you wish. It’s a sandbox after all. So scientists at NVIDIA thought, let’s train an AI  to play this game and see what happens. Okay, but   how? Well, most AI systems of today require  lots and lots of training data. Wait a minute.   There is no shortage of that out there in the  internet, that’s for sure. Let me explain. We can have this little AI sit down and  watch hundreds of thousands of Minecraft   tutorial videos on Youtube, 33 years of footage  in total, my goodness, then, have it read over 7   thousand wiki pages, and then also becomes the  ultimate Reddit lurker. Wow. That makes a ton   of sense. Just think about it. It can learn  practical knowledge from the tutorial videos,   encyclopedic knowledge from the wiki, and  it can learn what makes the best creations   the best that are shared on reddit. That sounds  fantastic…on paper. But, there is so much to do   and to learn about this game, can an AI  really get an understanding of all this? Now,   hold on to your papers, and let’s see together  what this AI could learn from all this data! But wait a minute! This is a gamer AI  that uses the controller to move around in   a virtual world. But how do we instruct it?  Do we need to speak robot? If not, well,   it doesn’t even understand English text. What do  we do with that? Well, remember OpenAI’s GPT-3,   which is a neural network model that  has read almost the entirety of the   internet. This one has proper English  knowledge, so, we plug that in, and bam,   now you can read that wikipedia, and it gets  better, because we can now even give it text   instructions. Now let’s see how well it  fares through 5 of my favorite examples. One, we can ask it to explore an ocean monument.  I love this one because the text description   is sufficiently vague. How could a machine  understand what exploring means? Well, humans do,   and this learns from humans as there must be  tons of tutorial videos out there on exploration.   And, it seems to me that it was able to  learn the concept correctly. Loving it. Two, it can encircle these llamas with a fence.   That is excellent. It understands what object  it needs to use, where to move, where to look,   and that not even a tiny gap  is allowed. Very impressive. Three, this will be a more dangerous quest.  Now scoop a bucket of lava. Oh my goodness.   Be careful there! Do not fall!  We are getting there, and…got it! Four, we can even ask it to build a nether  portal. It builds the correct portal   frame and, did it use the correct materials?  Does it work? Yes and yes! Good job little AI! And, five. It is final boss time. Literally.  Oh yes. Someone was brave enough to ask the AI   to fight an ender dragon, essentially  the final boss of the game,   and the AI was brave enough to try it.  Well, apparently, this is a limited example   as it does not appear to be charging at the AI,  but, the AI seems to know what we are looking for,   and what fighting entails in this  game. Starting this game from scratch   and building up everything to defeat such a  dragon takes a long time horizon, and will be   an excellent benchmark for the next AI one more  paper down the line. I’d love to it perform this   start to end. Make sure to subscribe, if such a  paper appears, I’ll be here to show it to you. While we are looking at more examples of  what it could do, I have to note that we have   barely scratched the surface here.  For instance, it understands text,   yes, but if we attach a speech recognition AI  to this agent, we don’t even need to write.   It can essentially be a little virtual friend.  How cool is that? What a time to be alive! Okay, so this is MineDojo, an AI  agent that understands English,   and can learn to navigate these virtual worlds  so well, we can give it a task, and they would   execute it. And not just some simple ones, we  are talking a wide variety of complex tasks.   Now, all this footage looks great,  but how do we know if this is really   performing these tasks correctly? Well, if  you have been holding on to your papers, now,   squeeze that paper, because they had an  experienced human evaluator look at these results,   and agreed with the AI’s solutions about 97%  of the time. Wow. That is an incredible result. And don’t forget, NVIDIA is amazing at  democratizing these works and putting them   into the hands of everyone, and this  one is an excellent example of that.   If you have some programming knowledge,  you can give it a try right now. So, does this get your mind going? What would you  use this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
609,"NVIDIA’s Ray Tracer - Finally, Real Time!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. I can’t believe that I am saying but, but today   we are going to see how NVIDIA is getting  closer and closer to solving an almost   impossible problem. And that is, writing  real-time light transport simulations. So, what is that? And why is it nearly impossible?  Well, if we wish to create a truly photorealistic   scene, in computer graphics, we usually reach  out to a light transport simulation algorithm,   and then, this happens. Oh yes, concept number  one. Noise! This is not photorealistic at all,   not yet anyway. Why is that? Well, during  this process, we have to shoot millions   and millions of light rays into the scene to  estimate how much light is bouncing around,   and before we have simulated enough rays, the  inaccuracies in our estimations show up as noise   in these images. This clears up over time, but it  may take from minutes to days for this to happen,   even for a smaller scene. For instance, this one  took us 3 full weeks to finish. 3 weeks! Ouch.  Now, earlier, we talked about this technique which  could take complex geometry, and 3.4 million light   sources, and it could really render not just  an image, but an animation of it interactively. But how? Well, the magic behind all this is  a smarter allocation of these ray samples   that we have to shoot into the scene. For  instance, this technique does not forget what   we did just a moment ago when we move the camera  a little and advance to the next image. Thus,   lots of information that is otherwise thrown away  can now be reused as we advance the animation. And, note that even then, these smooth,  beautiful images are not what we get directly,   if we look under the hood and look at the  raw result that comes out of a simulation, we   get something like this. Oh yes.  Still, a noisy image. But wait,   don’t despair! We don’t have to live with these  noisy images, we have denoising algorithms   tailored for light simulations. This one does  some serious legwork with this noisy input. And, in a followup paper, they also went on to  tackle these notoriously difficult photorealistic   smoke plumes, volumetric bunnies, and even  explosions interactively. The results were once   again, noise filtered to nearly perfection. Not to  perfection, but a step closer to it than before. Now note that I used the word interactively  twice here. I did not say real time.   And that is not by mistake. These  techniques are absolutely fantastic,   one of the bigger leaps in light  transport research, but, they still   cost a touch more than what production systems  can shoulder. They are not quite real time yet. So, what did they do? Did they stop there? Well,   of course not, they rolled up the  sleeves and continued. And now,   I hope you know what’s coming. Oh yes! Have a  look at this newer paper they have in this area. This is their result on the Paris Opera House  scene, which is quite detailed, there is a   ton going on here. And, you are all experienced  Fellow Scholars now, so when you see them flicking   between the raw, noisy and the denoised results,  you now know exactly what is going on. And, hold   on to your papers, because all this takes about 12  milliseconds per frame. Yes yes yes! My goodness!   That is finally in the real time domain  and then some! What a time to be alive! Okay, so where is the catch? Our keen eyes see  that this is a static scene. It probably can’t   deal with dynamic movements and rapid changes  in lighting, can it? Well, let’s have a look.   Wow! I cannot believe my eyes. Dynamic movement,  checkmark. And here, this is as much changing in   the lighting as anyone would ever want, and  it can do this too. I absolutely love it. And, remember the amusement park scene  from the previous paper? The one with 23   million triangles for the geometry,  and over 3 million light sources?  Well, here are the raw results, and after  denoising, this looks super clean. Wow.   So, how long do we have to wait for this?  This can’t be real time, right? Well,   all this takes is about 12 milliseconds  per frame. Again. And this is where I fell   off the chair when reading this paper. Of  course, not even this technique is perfect,   the glossy reflections are a little blurry at  places, and artifacts in the lighting can still   appear. But if this is not a quantum leap in  light transport research, I don’t know what is. Plus, if you wish to see some properly detailed  comparisons against previous techniques,   make sure to have a look at the paper. And,  if you have been holding on to your papers,   now, squeeze that paper, because  everything that you see in this   paper was done by two people. Huge  congratulations Chris and Alexey! And, if you are wondering if we  ever get to use these techniques,   don’t forget that their marbles demo is  already out there for all of us to use.   And it gets better, for instance, not many know  that they already have a denoising technique that   is online and ready to use for all of us. This  one is a professional grade tool right there. This   is really incredible, they have so many tools  out there for us to use, I check what NVIDIA   is up to daily, and I still quite often get  surprised about how much they have going on. So, finally, real time light  transport in our lifetimes?   Oh yes, this paper is history in the making. Thanks for watching and for your generous  support, and I'll see you next time!"
610,OpenAI’s DALL-E 2: Top 5 New Results!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to talk about OpenAI’s  amazing image generator AI, DALL-E 2,   and you will see that it can do two things  that perhaps not many of you have heard of. So, what is DALL-E 2? This is a diffusion-based AI  model, which means that it starts out from noise,   and gradually changes it to become a beautiful  image like this one. The image can be anything,   so much so that we can enter a piece of text,  and magically, out comes an image that fits   this description. It would be hard to overstate  how transformative this will be to digital art.   Imagine that you could now illustrate any  novel…and not only that, but it wouldn’t even need   any additional work for you  to write up the text prompts.   Just take excerpts from the  novel, and there we go. Wow. And, if you feel like adding crazy prompts, as  you see, it won’t complain about that either. But,   it can do even more. For instance, if  we have an image that we really like,   variant generation is also possible. This means  that we can provide it an input image, it tries   to understand what it depicts, and create several  versions of it. And it does that spectacularly. But, that’s not the only little-known  feature is does, it also two other amazing   things. One is image inpainting. What is that? Inpainting means that we take an input image   or video, cut out an undesirable part, and let  an algorithm fill in the holes with data that   makes sense given the context of this image. Now,  previous techniques were already capable of this,   but I am very excited to see what DALL-E 2 can do  with this problem, because we know that it has a   really detailed understanding of the world around  us. So, can it use its knowledge of the world to   create high-quality inpaintings? Oh boy, you will  see in a moment that it can do it incredibly well.   Let’s give it a try. Oh yes, we can delete this  squirrel, and here comes the key: oh my! Remember   that I said that inpainting techniques fill  in the holes given the context of the image.   But DALL-E 2 does something even better! It fills  in the image with any context we can think of.   Let’s try that! How? Well, this image is  ridiculous squirrels are known not to be   able to operate computers, so, let’s add a hippo  instead. And there we go! Much better. Loving it.   Now let’s push it to the limits  through 5 outstanding examples! One. The ladybug. Here is a  photo, but, look. Unfortunately,   some parts of the photo are out of focus. Oh  yes, how about deleting those parts, and have   them inpainted with DALL-E 2? Could that work? It  knows that this is a ladybug after all. Let’s see.   Wow. That is much sharper and fits the  image really well. I would have to look   for quite a bit to find out the trickery. What  about you? Let me know in the comments below! So   this is fantastic. But you know what is even  more fantastic? Now hold on to your papers,   because it can also do not only image  inpainting, but image outpainting! What is that? Well, two. Ever wondered what might  be outside of this frame? Well, let’s ask DALL-E   2 and…that is insanity. It is really doing it. And  we can just zoom out and out and out and it feels   like it never stops. And it just gets crazier  over time. If a human would have made this,   I would say that human is very creative. But  this was done by a machine. Is that a hint of   creativity in a machine? Food for thought.  I love this one. What a time to be alive! Three, of course, we need to do that to the  Mona Lisa too. What could be outside the frame?   Ah, I see! This is a more modern take on  the old classic. Working at the office while   everyone else is enjoying life. Note that  once again, a prompt was used to generate   the theme of the outpainting here. These are  the prompts that were used for this example. And, these are so good, let’s do a  couple more. Four. Zooming out from   a house in the middle of a lush field to  this. Very artistic. And it turns out,   wow…that we are on a floating island in  the middle of the universe…with a twist. And, last, but not least. Five, the Last Supper.  But this time, let’s not zoom out, but let’s try   to answer one of history’s big questions: and that  is, what might be going on in the adjacent rooms.   Look. I love it. And I wonder what DALL-E  3 will be capable of? Remember, the DALL-E   2 version came out just about a year after the  first version, and it was leaps and bounds better.   And the competition is also coming, Google  already has their Imagen and Parti AI’s,   and even that is just scratching the surface. As  you see, the pace of progress in AI research is   absolutely incredible. These tools are going  to transform the world around us as we know it,   especially now that OpenAI is aiming to deploy  this AI to no less than 1 million people in the   near future. How cool is that! Democratizing  art creation for all of us. Soon. Bravo! So, does this get your mind going? Do you have  any thoughts as to what DALL-E 3 could do?   Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
611,OpenAI’s New AI Learned To Play Minecraft!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see what happens if  we unleash an AI to watch Minecraft videos   and then, ask this AI to play  it. And the big question is,   can it play as well as we, humans do?  Well, you’ll get your answer today! This is OpenAI’s project, which is not to be  confused with NVIDIA’s similar project called   MineDojo. So, what was that? That  was an AI project that watched a ton   of Youtube tutorial videos on Minecraft.  Minecraft is an open world sandbox game,   one of the most played games in the world, and  this is a true sandbox, which means that you   can do whatever you wish. You create the story of  this game. Gather raw materials, build something,   explore, or don’t, it’s all up to you. This is  a sandbox game, and your story after all. So,   what could NVIDIA’s AI do after watching  these tutorial videos? Well, a great deal. For instance, we could even use natural  language and ask it to explore an ocean   monument. I really liked this one because  the word “explore” is sufficiently vague   and open ended. A human understands what exploring  means, but does the AI understand it too?   Well, as you see, it does! Amazing. It could  also encircle these llamas with a fence.  Or, it could also be dropped into  an arena with an Ender Dragon,   the final boss of the game, if you will. However,   look, this is a very limited setting for this  task as the dragon is not charging at the AI.   But at the very least, NVIDIA’s AI  understood what fighting meant in this game. Now, I said the following, quoting: “Starting  this game from scratch and building up everything   to defeat such a dragon takes a long time  horizon, and will be an excellent benchmark   for the next AI one more paper down the line.  I’d love to see it perform this start to end.   Make sure to subscribe, if such a paper  appears, I’ll be here to show it to you.” And I thought, maybe a year or two later there  will be some activity here. And now, hold on   to your papers, because OpenAI’s scientists  are here with their own spin of this project,   not one year later, but just one week  later. Wow! And, with this paper,   I also got what I was looking for, which is  an AI that understands longer time horizons. They make a huge claim. They say that this is  the first AI that is capable of crafting Diamond   tools. That is very ambitious. Can that really  be? Well, I say that I will believe it when I see   it. Do you see this long sequence of subtasks?  We need to do all this correctly and in order   to be able to perform that. This takes up to 24000  actions and 20 minutes even for a human player.   So, let’s see if the AI is also up for the task!  It starts chopping wood right away, then, the   planks are coming along. Later, it starts mining,  finds iron ore, crafts a furnace, starts smelting,   and at this point I am starting to think that it  might make it. That would be absolutely amazing.   However, we still need some diamonds. Can  it find some diamonds? It takes a while,   and…oh my, got ‘em! And from then on, we have  established that this is a smart AI, so from   here on out, this is a piece of cake. There  we go. I can’t believe it. A diamond pickaxe. And here is my other favorite, it can also perform  pillar jumps. This is a cool technique where we   jump, and while mid-air, we quickly put a block  below us, and tadaa. We are now standing a little   higher. If we do this over and over again, we will  be able to reach places that otherwise would be   way out of reach. And the AI  learned this too. How cool is that? While we are looking at what else it is capable  of, I would like to bring up one more remarkable   thing here. Here goes: it learns from unlabeled  videos. In other words, this means that the AI is   not told what these videos are about, it is just  allowed to watch them and find out for itself.   And to be able to learn this much information  from this kind of unlabeled data just is an   incredible sign for future projects to come.  You see, playing Minecraft well is great,   but this is an AI that can learn new things  from unstructured data. In the future,   these agents will be able to perform more general,  non-gaming related tasks. What a time to be alive! For instance, the amazing DeepMind lab is  already walking this path. First they wrote an AI   that could learn to play Chess, Go, and Starcraft  2, and then, they used a similar system to solve   protein folding. Now, make no mistake, the  protein folding system is not exactly the   same system as the chess AI, but  they share a few building blocks. And here, having two amazing Minecraft AIs  appear within a week of each other is a true   testament to how quickly projects  are coming along in AI research.   So, let’s apply the first Law of Papers  what do you think this will be capable   of one more paper down the line?  Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
612,Samsung’s AI: Megapixel DeepFakes!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to make the Mona Lisa and  other fictional characters come to life through   this DeepFake technology that is able to create  more detailed videos than previous techniques. So, what are DeepFakes? Simple, we record a  video of ourselves, as you see me over here   doing just that, and with this, we can make  a different person speak. What you see here   is my favorite DeepFake application where  I became these movie characters and tried   to make them come alive. Ideally, expressions, eye  movements and other gestures are also transferred. Some of them even work for animation movie  characters too. Absolutely lovely. This can   and will create filmmaking much more accessible  for all of us, which I absolutely love. Also,   when it comes to real people, just imagine how  cool it would be to create a more smiling version   of an actor without actually having to re-record  the scene. Or maybe even make legendary actors   appear in a new movie. The band Abba is already  doing these virtual concerts where they appear   as their younger selves, which is a true  triumph of technology. What a time to be alive! Now, if you look at these results, these are  absolutely amazing, but the resolution of the   outputs is not the greatest. They don’t seem to  have enough fine details to look so convincing.   And here is where this new paper comes into play.   Now, hold on to your papers and have a look at  this. Wow. Oh my! Yes, you are seeing correctly,   the authors claim that these are the first one  megapixel resolution deepfakes out there. This   means that they showcase considerably more  detail than the previous ones. I love these,   the amount of detail compared to  previous works is simply stunning. But, as you see, these are not perfect by any  means. There is a price to be paid for these   results. And the price is that we need to trade  some temporal coherence for these extra pixels.   What does that mean? It means that as  we move from one image to the next one,   the algorithm does not have a perfect memory of  what it had done before, and therefore some of   these jarring artifacts will appear. Now,  you are all experienced Fellow Scholars,   so here, you know that we have to invoke The  First Law Of Papers, which says that research   is a process. Do not look at where we are, look  at where we will be two more papers down the line. So, megapixel results huh? How  long do we have to wait for these?   Well, if you have been holding on to your papers  so far, now, squeeze that paper because these   somewhat reduced examples work in real time. This  means that we can point a camera at ourselves,   do our thing, and see ourselves become  these people on our screens in real time. Now, I love the film directing  aspect of these videos, and also,   I am super excited for us to be able to even  put ourselves into a virtual world. And clearly,   there are also people who are less interested in  these amazing applications with these techniques.   To make sure that we are prepared  and everyone knows about this,   I am trying my best to teach political decision  makers from all around the world to make sure   that they can make the best decisions for  all of us. And I do this free of charge. As   any self-respecting Fellow Scholar would do,  you can see me holding on to my papers here   at a NATO conference. At such a conference, I  often tell these political decision makers that   DeepFake detectors also exist, how reliable  they are, what the key issues are, and more. So, what do you think? What actor  or musician would you like to see   revived this way? Prince or Robin Williams  anyone? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
613,Google’s New AI Learned To See In The Dark!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going take a collection of photos  like these, and magically, create a video   where we can fly through these photos. And  we are going to do all this, with a twist. So, how is this even possible? Especially  that the input is only a handful of photos.   Well, typically, we give  it to a learning algorithm   and ask it to synthesize a photorealistic video  where we fly through the scene as we please.   Of course, that sounds impossible. Especially  that some information is given about the scene,   but this is really not much. And as you see, this  is not impossible at all through the power of   learning-based techniques, this previous AI is  already capable of pulling off this amazing trick.   And today, I am going to show you  something even more incredible. Now, did you notice that most of these were shot  during the daytime, and these are all well lit   images. Every single one of them. So, our question  today is, can we perform view synthesis in the   dark? And my initial answer would be a resounding  no. Why? Well, in this case, we not only have to   deal with less detail in the images. It would also  be very difficult to stitch new views together if   we have images like this one. Luckily, we have  a choice. Instead, we can try something else,   and that is, using raw sensor data instead. It  looks like this. We get more detail, but, uh-oh,   now we also have a problem. Do you see the problem  here? Yes, that’s right. In the raw sensor data,   we have more detail, but also, much more  noise that contaminates this data too. We either have to choose from less detail and  less noise or from more detail more noise. So,   I guess that means that we get no  view synthesis in the dark, right? Well, don’t despair, not everything is lost yet.  There are image denoising techniques that we can   reach out to. Let’s see if this gets any  better. Hmm! It definitely got a lot better,   but I have to be honest, this is not even close to  the quality we need for view synthesis. This one   denoises a single image. However, yes, finally,  there is an opening here! Remember, in the world   of NERFs, we are not using a single image, we  are using a package of images. A package contains   much more information than just one image,  and hopefully, it can be denoised better. So   this is what the previous method could do,  and now, hold on to your papers, and let’s   look at this new technique, called RAWNerf!  Can it pull this off? Wow! Seemingly, it can. So, now, can we be greedy and hope that view  synthesis works on this data? Let’s see.   My goodness. It really does! And, we are not done  yet. In fact, we are just starting. It can do even   more! For instance, it can perform tone mapping  on the underlying data to bring out even more   detail from these dark images, and here comes  my favorite. Oh yes. We can also refocus these   images, and these highly sought after depth  of field effects will start to appear. I love   it. And what I love even more is that we can even  play with this in real time to refocus the scene. This is a very impressive set of features, so   let’s take this out for a spin and marvel  together at 5 amazing examples of what it can do. Yes, once again, this is extremely noisy.  For instance, can you read this street sign?   Not a chance right? And, what about now?   This looks like magic. I love it. Now let’s start  the view synthesis part, and this looks really   good given the noisy inputs. The previous,  original NERF technique could only produce   this, and this is not some ancient technique.  Nuh-uh. No sir. This is from just 2 years ago,   and today, a couple papers down the line, and  we get this. I can’t believe it. We can even see   the specular highlight moving around around  the badge of the car here. Outstanding. Two, actually, let’s have a closer look at  specular highlights. Here is a noisy image,   the denoised version, and the view synthesis.  And the specular highlights are once again,   excellent. These are very difficult to capture  because they change a great deal as we move the   camera around, and the photos are spaced out  relatively far from each other. This means a   huge challenge for the learning algorithm, and  as you see, this one passes with flying colors. Three, thin structures are always a problem.  Look, an otherwise excellent previous technique   had a great deal of trouble with the  fence here, even in a well lit scene. So,   let’s see. Are you kidding me? Doing the same  with a bunch of nighttime photos? There is not   a chance that this will work. So, let’s  see. Look at that! I am out of words.   Or you know what’s even better, let’s be really  picky, and look here instead, these areas are   even more challenging. And even these work  really well. Such improvement in so little time. Four, as I am a light transport researcher  by trade, I would love to look at it resolve   some more challenging specular highlights. For  instance, you can see how the road reflects the   street lights here and the result looks not  just passable, this looks flat out gorgeous. Now, talking about gorgeous scenes. Let’s look at  some more of those. Five, putting it all together.   This will be a stress test for the new technique.  Let’s change the viewpoint, refocus the scene,   and play with the exposure at the same time.  That is incredible. What a time to be alive! And you are saying that it does all this from a  collection of 25 to 200 photos? We can shoot these   in seconds! Now, clearly, not even this technique  is perfect, we can see that this does not match   reality exactly, but going from a set of extremely  noise raw images to this is truly a sight to   behold. The previous, two-year old technique  couldn’t even get close to these results. Bravo! And, this is an excellent place for  us to apply the First Law Of Papers,   which says research is a process. Do not look  at where we are, look at where we will be two   more papers down the line. So, what do  you think? What will we be able to do   two more papers down the line? And what would you  use this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
614,Microsoft's New AI: Virtual Humans Became Real!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, we are going to see Microsoft’s AI looking at a lot of people who don’t exist, and then, we will see that these virtual people can teach it something about real people. Now, through the power of computer graphics algorithms, we are able to create virtual worlds, and of course, within those virtual worlds, virtual humans too. So, here is a wacky idea. If we have all this virtual data, why not use these instead of real photos to train a new AI to do useful things with them? Hmm…wait a second. Maybe this idea is not so wacky after all. Especially because we can generate as many of these virtual humans as we wish, and all this data is perfectly annotated. The location and shape of the eyebrows is known, even when they are occluded, and we know the depth and geometry of every single hair strand of the beard. If done well, there will be no issues about the identity of the subjects, or the distribution of the data. Also, we are not limited by our wardrobe or the environments we have access to. In this virtual world, we can do anything we wish. So good! But of course, here is the ultimate question that decides the fate of this project. And that question is: does this work? What is all this good for? And the crazy thing is that Microsoft’s previous AI technique could now identify facial landmarks of real people, but, it has never seen a real person before. How cool is that! But, this is a previous work, and now, a new paper has emerged, and in this one, scientists at Microsoft said, how about more than 10 times more landmarks? Yes, this new paper promises no less than 700. When I saw this, I thought are you kidding? Are we going 10x just one more paper down the line? Well, I will believe it when I see it. Let’s see a different previous technique from just two years ago. You see that we have temporal consistency issues, in other words, there is plenty of flickering going on here, and there is one more problem: these facial expressions are really giving it a hard time. Can we really expect any improvement over these two years? Well, hold on to your papers and let’s have a look at the new method and see for ourselves. Look at that! It not only tracks a ton more landmarks, but the consistency of the results has improved a ton as well. So, it both solves a harder problem, and it also does it better than the previous technique. Wow! And all this just one more paper down the line. My goodness. I love it! I feel like this new method is the first that could even track Jim Carrey himself. And, we are not done yet! Not even close it gets even better! I was wondering if it still works in the presence of occlusions, for instance, whenever the face is covered by hair or clothing, or a flower. And, let’s see. It still works amazingly well! What about the colors? That is the other really cool thing it can, for instance, tell us how confident it is in these predictions. Green means confident, red means that the AI has to do more guesswork, often because of these occlusions. My other favorite thing is that this is still trained with synthetic data. In fact, it is key to its success. This is one of those success stories where training an AI in a simulated world can be brought into the real world, and it still works spectacularly. There is a lot of factors at play here, so let’s send out a huge thank you to computer graphics researchers as well for making this happen. These virtual characters could not be rendered and animated in real time without decades of incredible graphics research works. Thank you! And now comes the ultimate question: how much do we have to wait for these results? This is incredibly important. Why? Well, here is a previous technique that was amazing at tracking our hand movements. Do you see these gloves? Yes? Well, those are not gloves. This is how a previous method understands our hand motions, which is to say, that it can reconstruct them nearly perfectly. Stunning work. However, these are typically used in virtual worlds, and we had to wait for nearly an hour for such a reconstruction to happen. Do we have the same situation here? You know, 10x better results in facial landmark detection, so what is the price that we have to pay for this? One hour of waiting again? Well, not at all! If you have been holding on to your papers, now, squeeze that paper, because it is not only real time, it is more than twice as fast as real time. It can churn out 150 frames per second and it doesn’t even require your graphics card, it runs on your processor. That is incredible. Here is one more comparison against the competitors. For instance, Apple’s ARKit runs on their own iPhones, and thus, they can make use of the additional depth information. That is a goldmine of information. But, this new technique doesn’t, it just takes color data, that is so much harder, but in return, it will run on any phone. Can these results compete with Apple’s solution with less data? Let’s have a look. My goodness, I love it. The results seem at the very least comparably good. That is, once again, amazing progress in just one paper. So cool! Also, what I am really excited about is that variants of this technique may also be able to improve the fidelity of these DeepFake videos out there. For instance, here is an example of me becoming a bunch of characters from Game of Thrones, this previous work was incredible because it could even track where I was looking. Imagine a new generation of these tools that is able to track even more facial landmarks, and democratize creating movies, games and all kinds of virtual worlds. Yes, with some of these techniques, we can even become a painting or a virtual character as well, and even the movement of our nostrils would be transferred. What a time to be alive! So, does this get your mind going? What would you use this for? Let me know in the comments below! Thanks for watching and for your generous support, and I'll see  you next time!"
615,Microsoft’s New AI: The Selfies Of The Future!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to use an AI  to take a collection of photos,   and hopefully create a digital human out of them. However, not so fast! For instance, have a look  at NVIDIA’s amazing face generator AI. As you see,   it can generate beautiful results,  and they even show us the illusion   of these characters moving around, however, this  does not have a strong concept of 3D information.   These are 2D images, and 3D consistency  is not that great. What is that?   Well, look. This person just one frame away is  not exactly the same person. For instance, the   hair moves around. This means that these images  are not art directable, or at least, not easily. As you see here, it has improved  a great deal over two years and   one more research paper down the  line, it’s still not quite there.   So, fine details, checkmark, 3D  consistency, not quite there. So let’s have a look at solutions that have  more 3D consistency. To be able to do that,   we can take a collection of photos like  these, and magically, create a video   where we can fly through these photos. It is  really crazy, because this is possible today,   for instance, here is NVIDIAs method that can be  trained to perform this in a matter of seconds.   This is remarkable, especially that  the input is only a handful of photos.   Some information is given about the  scene, but this is really not much. So, can we have a solution with 3D consistency?  Yes, indeed, for these, 3D consistency,   checkmark. However, the amount of fine  details in these outputs is not that great. Do you see the pattern here?   Depending on which previous method we use, we  either get tons of detail, but no 3D consistency,   or we can get our highly coveted 3D  consistency, but then, the details are gone. So, is the dream dead? No digital humans for us?   Well, don’t despair, and have a look at this new  technique that promises one megapixel images that   are also consistent. Details and consistency  at the same time! When I first saw this paper,   I said I will believe it when I see  it, so, let’s have a look together. This is StyleNERF, a previous technique, and we  see the problems that we now expect the hair   is not consistent, the earring is flickering  a great deal, and there are other issues too.   So, are the authors of the new paper  claiming that they can solve all this? Well,   now, hold on to your papers, and have a  look at this. This is the new technique.   Oh my goodness. The earring, facial features  and the hair stay still as we rotate them,   and it indeed shows the new  angles correctly. I love it! Let’s examine this phenomenon   in a little more detail. Look at the hair  here with StyleNERF. It looks decent,   but it’s not quite there. And I really wonder why  that is. Let’s zoom in and have a closer look.   Oh yes, upon closer inspection, this hair is a  bit of a mess. And, with the new technique, now   that is what I call consistency across the video  frames. Smooth hair strands everywhere. So good! And what I absolutely loved about this new work  is that it outperforms this previous technique,   which is from how many years ago? Well, if  you have been holding on to your papers, now   squeeze that paper, because this work is  from not even one year ago, just from 8   months ago. And you already see a meaningful  improvement over that. That is insanity. Now, not even this technique is perfect. I  don’t know for sure if the hair consistency   is perfect here and we are dealing  with video compression artifacts,   or whether this is still not a 100% there,  but this truly is a great step forward. Also, here, you see that the images are  still not as detailed as the photos,   but this seems to be a roadblock to me that is  much easier to solve than 3D consistency. My   impression is that with this, the most difficult  part of the task is already done, and just one or   two papers down the line, and I am sure we will  be seeing even more realistic virtual humans. So, can we enter into a virtual world as a  digital human from just a collection of photos?   Oh yes! Time to meet our beloved ones from afar,  or meet new people, and play some games together.   What a time to be alive! So,  does this get your mind going?   What would you use this for? Let  me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
616,OpenAI’s DALL-E 2 - AI-Based Art Is Here!,"Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. These amazing images were all made by an AI called DALL-E 2. This AI is endowed with a diffusion-based model, which means that when we ask it something, it starts out from noise, and over time, it iteratively refines this image to match our description better. And, over time, magically, an absolutely incredible image emerges. This is a neural network that was given a ton of images, and a piece of text that says what is in this image. That is one image-caption pair. DALL-E 2 is given millions and millions of these pairs. So, what can it do with all this knowledge? The key takeaway here is that it does no, or very little copying from this training data, but it truly comes up with novel images. How? Well, after it had seen a bunch of images of koalas, and separately, a bunch of images of motorcycles, it starts to understand the concept of both, and it will be able to combine the two together into a completely new image. Previously, we also compared these works to what real human artists are capable of. And, in my opinion, this tool should not be framed at all as Team Humanity versus Team AI. I believe this should be framed as Team Humanity, supercharged by Team AI. Here’s an example of that. And the results are so good, and it is so easy to use, I said, on several occasions that this is going to change the world of art creation as we know it. And, you are an experienced Fellow Scholar, so you may ask the question, okay, but how? Here are 5 beautiful examples of how artists are already using it. One, texture synthesis. Oh yes. Whenever we see these beautiful animation movies or video games, we see a lot of real-looking content in there. The roads are realistic, the buildings and hills are also realistic. We can get a great deal of help in making them realistic by…taking photographs of real objects and importing them into our virtual world. But there is a problem. What is the problem? Well, let’s see together. Let’s take a photo. There is a stain on the wall, is that the problem? No, that’s not the problem, we can cut that out. Now, we need to extend this texture, but since this is just a small snippet, and we have long walls and big buildings to fill in a video game. So, what do we do? We start tiling it. And, uh-oh. Now that is the problem. It is quite visible what happened to this texture, it has been extended, but at the cost of seams and discontinuities appearing in the image. So, how do we get a seamless texture from this? Well, DALL-E 2 hopefully understands that this is a rock wall, and, can it fix this? Now, hold on to your papers, and let’s see together. Yes it can! Amazing! Now let’s have a look at it attached to a real virtual object and see how it looks in practice. Oh my. This is gorgeous. I love it. Here is an even more difficult example with paint peeling off the wall. This is the real-world phone photo. And the tiled version with the seams. And now, DALL-E 2. Wow! I could never tell that this image is not a real photo. And, have a look at it added to a video game. So good. By the way, these are Stan Brown’s works, who used DALL-E 2 to great effect here. Make sure to check out his work, for instance, he also made these amazing materials for one of my favorite games of all time, Path Of Exile. The link to his work is available in the video description. Just imagine using the free 3D modeling tool, Blender and this plugin to create roads, and you can make it an asphalt road, dirt road, or make it look like pavement in just a couple clicks with DALL-E 2. Two, DALL-E 2 is now able to generate real human faces too, and sometimes they come out really well. And sometimes, not so much. No matter, in these cases, we can use a face restoration program to fix some of the bigger flaws and run it through a super-resolution program to add more details, and bam. There we go. Fantastic. And, if we have a portrait that works for us, we can plug it into an amazing tool called Deep Nostalgia to create a virtual animated version of this person. How cool is that. What a time to be alive! Three, this one speaks for itself. The Cosmopolitan magazine made their first cover from an image that was made by an AI. No tricks, no frills, just a good old image straight out of DALL-E 2. And the kicker is that some of them are already good enough to become magazine covers. That is incredible. Talk about the age of AI image creation! Four, this tool can even help us create new product designs. And I mean not this one, although this toilet car might have some utility if you need to go when you are on the go, but I meant this one instead! And, finally, five. We can also combine DALL-E 2 with other tools that take a 2D image and try to build an understanding as if it were a 3D scene. And the results are truly stunning, we can get so much more out of these images. And one of the important lessons here is that not only AI-generated art is improving at an incredible pace, but an additional lesson is that everything is connected. So many of these tools can be made even more powerful if we use our imagination and combine them correctly. This is huge. For instance, NVIDIA is already building virtual copies of the real world around us. Why is that? Well, because this way we can reenact real situations for their self-driving cars, and even create arbitrarily complex new situations, and these AIs will be able to learn from them in a safe environment. Also, self-driving can also be combined with a number of other technologies that make it an even more amazing experience. See the assistant there? It sees and identifies the passenger, and understands natural language. It also understands which building is which and what shows are played in them today, so we might jump into the car not even knowing where we wish to go, and it would not only help us find a good show, but it would also drive us there. And, +1. Remember the shoe design example? How about trying on pieces of clothing, but not one by one, as we do in real life, but trying on…all of the clothes, at the same time. Yes, these are all AI-generated images, and it is not on just one still image, but video footage where a camera moves around. This is spectacular. Imagine what buying clothes will look like when we have algorithms like this! Wow. Or, if an actor did not have the appropriate attire for a scene in a movie? Not to worry, just change it in post with the AI. How cool is that? You see, once again, everything is connected. So, does this get your mind going? What would you combine with DALL-E 2? Thanks for watching and for  your generous support, and I'll see  you next time!"
617,"A 1,000,000,000 Particle Simulation!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to find out  whether it is possible to write   a fluid simulation on a computer that  showcases no less than 1 billion particles!   So, is it this one? Maybe! Of  course, I will tell you in a moment. But note that there are plenty of fluid and  physics simulation research papers out there. This   is a mature research field, and we can do so  much today. Here are 4 of my favorite works   in this area, and then, we will talk  about what is the problem with them. One, in reality, we can experiment with whatever  objects we have at our disposal, but in a   simulation, we can do anything. Including changing  the physical parameters of these objects, and   thus, this previous work can simulate three jello  blocks of different stiffness values. So cool! Two, this technique simulates strong two-way  coupling. What does that mean? Well, you see a   simulation here that doesn’t have it. So is this  correct? Well, not quite. The honey should be   supporting the dipper, and it not only falls, but  it falls in a very unnatural way. Instead, this   technique can simulate strong two-way coupling,  and finally it gets this right. And not only that,   but what I really love about this is that it also  gets small nuances right. I will try to speed up   the footage a little, so you can see that the  honey doesn’t only support the dipper, but the   dipper still has some subtle movements both  in reality and in the simulation. A+. Love it. Three, we can even simulate the physics of baking  on a computer. Let’s see…yes, expansion and baking   is happening here. Are we done? Well, let’s have  a look inside. Yup, this is a good one. Yum! And perhaps my favorite work in this area is  this. Four, now, you may be wondering, Károly,   why are you showing a real video to me? Well,  this is not a real video. This is a simulation.   And so is this one. And hold on to your papers,  because this is not a screenshot taken from the   simulation, nuh-uh. No sir. This is a real photo.  How cool is that? Whenever you feel a little sad,   just think about the fact that through the  power of computer graphics research, we can   simulate reality on our computers. At least,  a small slice of reality. Absolutely amazing. However, there is a problem. And the  problem is that all of these simulations   have a price. And that price is time.  Oh yes, some of these works require   long all-nighters to compute. And, unfortunately, as a research   field matures, it gets more and more difficult to  say something new, and improve upon previous work.   So, can we expect no more meaningful speedups  at this point? Are we at a saturation point? Well, just two years ago, we talked about  a technique that was able to simulate   100 million particles. My goodness, this was  absolutely amazing. And now, just a couple   more papers down the line, we go 10x. Yes, that’s  right! Here is an even better one that is finally   able to simulate 10 times as many particles. Yes,  1 billion particles. Now, hold on to your papers,   and let’s marvel together  at this glorious footage. Yes, this is a scene with 1 billion. Can that  really be? Well, I can hardly believe it,   but here it is. Look, that is a beautiful  simulation. I love it. Now, this technique is   not only able to simulate this many particles, but  it also does it faster than previous techniques.   The speedup factor is typically 3x, but  not on honey buckling scenes. Why is that? Well, the more viscous the fluids  are, at the risk of simplification,   let’s say that the more honeylike they are,  the higher the speedup. How much higher?   What? Yes, that’s right. On this honey buckling  scene, it is 15 times faster! What took   nearly 2 all nighters now only takes an hour?  That is insanity. What a time to be alive! Now, note that these simulations still  take a while, about half a minute per   frame for 150 million particles, and about  ten minutes per frame for a billion particles.   But for a simulation of this quality? That  is not a lot at all! Sign me up, right now! So, as you see even for such a mature research  field as fluid simulations in computer graphics,   the pace of progress nothing short of amazing.  You see a lot of videos on what AI techniques   are capable of today, and what you see here  is through the power of sheer human ingenuity. So, what do you think? Does this get your  mind going? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
618,"Stable Diffusion: DALL-E 2 For Free, For Everyone!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to look at images, and  even videos that were created by an AI,   and honestly, I cannot believe  how good some of these are. And,   it is possible that you can run this AI  at home yourself? You’ll find out today. This year, through OpenAI’s DALL-E 2, we are  entering the age of AI-driven image generation.   Most of these techniques take a text  prompt, which means that we can write   whatever we wish to see on the  screen, and first, a noise pattern   appears that slowly morphs into  exactly what we are looking for. This is what we mean when we say we are  talking about diffusion-based models. Now, OpenAI’s DALL-E 2 can create  incredibly creative images,   and Google’s Parti and Imagen AIs  are also at the very least as good.   Sometimes they even win linguistic  battles against OpenAI’s solution. But there is a problem. All of them are missing  something. And that is the model weights,   and the source code. This means  that these are all closed solutions,   and we cannot pop the hood and look around inside. But now, here is a new solution called  Stable Diffusion, where the model weights and   the full source code are available. I  cannot overstate how amazing this is.   So, to demonstrate why this is  so amazing, here are two reasons. Reason number one is that with this, we  can finally take out our digital wrench   and tinker with it. For instance, we  can now adjust the internal parameters   in ways that we cannot do with the  closed solutions like DALL-E 2 and   Imagen. So now, let’s have a look together at 10  absolutely amazing examples of what it can do!   After that, I’ll tell you about reason  number two of how it gets even better. One, dreaming. Since the internal parameters  are exposed, we can add small changes to them,   create a bunch of outputs that are similar,  and then finally, stitch these images together   as a video. This is so much better for exploring  ideas, just imagine that sometimes you get an   image that is almost what you are looking for,  but the framing, or the shape of the doggy is   not exactly perfect, well, you won’t need to throw  out these almost perfect solutions anymore. Look   at that. With this, we can make the perfect good  boy so much easier. I absolutely love it. Wow. Two, interpolation. Now hold on to your papers,  because we can even create a beautiful visual   novel like this one by entering a bunch  of prompts, like the ones you see here,   and we don’t go from just one image to the  next one in one jarring jump, but instead,   these images can now be morphed into the next one,  creating these amazing transitions. By the way,   the links to all of these materials  are available in the video description,   including a link to the full version  of the video that you see here. Three, its fantasy imagery are truly something  else. Whether you are looking for landscapes,   I was quite surprised by how competent Stable  Diffusion is at creating those, these tree houses   are amazing too, but that’s not when I fell  off the chair. I fell off the chair when I saw   these realistic fairy princesses. I  did not expect it to be able to create   such amazingly realistic humans. How cool is that! Four, we can also create a collage.  Here, we can take a canvas,   enter several prompts, and select a range for  each of them. Now the issue is that there is   space between the images, and there is another  problem, even if there is no space between them,   they won’t blend into each other.  No matter, Stable Diffusion can also   perform image inpainting, which means  that we select a region, delete it,   and it will be filled in with information  based on its surroundings. And the results are   spectacular. We don’t get many separate  images, we get one coherent image instead. Five, you know what, let’s look at a few more  fantasy examples. Here are some of my favorites. Six, now these are diffusion-based models, which  means that they start out from a bunch of noise,   and slowly adjust the pixels of this  image to resemble our input text prompts   a little more. Hence, they are very sensitive to  the initial noise patterns that we start out from.   Andrej Karpathy found an amazing way to take  advantage of this property by adjusting this   noise, but just a tiny bit, and create many  new, similar images. When stitched together,   it results in a hypnotic video like this one.  Random noise walks if you will. Loving it! Seven, it can generate not only images, but  with a little additional work, even animations.   You are going to love this one. Look. This was  made by creating the same image with the eyes open   and closed, and with a little additional  work of blending them together,   it looks like this. Once again, the  links to all of these works are available   in the video description if you wish  to have a closer look at the process. Eight, you remember that it  can create fantastic portraits,   and it can interpolate between them. Now,  putting it all together, it can create portraits,   and interpolate between them, creating these  sometimes smooth, sometimes a little jumpy videos. And don’t forget, nine, variant generation   is still possible. We can  still give it an input image,   and since it understands what this image depicts,  it can also repaint it in different variations. And finally, ten. The fact that these amazing  images come out of Stable Diffusion does not mean   that we have to use them in their entirety. If  there is just one part of an image that we like,   be it the knight on a horse, or the castle,  that is more than enough. We can discard   the rest of the image and just use the parts  that we love best, and make an awesome montage   out of it. I would say that here, very few  humans would be able to tell the trick. Now, we discussed that we can now  pop the hood and tinker with this AI,   that was one of the amazing reasons behind  these results, but I promised two reasons   why this is so good. So what is reason  number two? Is it possible that? Yes!   This is the moment you have been waiting for.  You can now try it yourself. If you are patient,   you can engage in changing the internal  parameters here and get some amazing variants. You might have to wait for a bit, but as of the  making of this video, it works. Now what happens   when you Fellow Scholars get over there, who  really knows, we have crashed plenty of websites   before with our Scholarly Stampede. And, if you  don’t want to wait or run some more advanced   experiments, you can run the model yourself at  home on a consumer graphics card. Loving it. And if you are unable to try it, don’t despair,   AI-based image generation is only getting cheaper  and more democratized from here on out. So,   a little open-source competition for  OpenAI and Google. What a time to be alive! And, please, as always, whatever you do, do  not forget to apply the First Law of Papers,   which says that research is a  process. Do not look at where we are,   look at where we will be two more papers  down the line. For instance, here are some   results from DALL-E 1, and just a year later,  DALL-E 2 was capable of this. Just a year   later. That is unbelievable. Just imagine  what we will be able to do 5 years from now.   If you have some ideas, make sure  to leave a comment about that below. So, finally Stable Diffusion, a free and open  source solution for AI-based image generation.   Double thumbs up. This is something  for everyone out there and it really   shows the power of collaboration as us,  tinkerers around the world work together   to make something amazing. I  love it. Thank you so much! And note that all this took about 600 thousand  dollars to train. Now, make no mistake, that is a   lot of dollars, but, this also means that creating  an AI like this does not cost tens of millions of   dollars anymore, and the team at Stability AI is  already working on a smaller and cheaper model   than this. So, we are now entering not only  the age of AI-based image generation, but the   age of free and open AI-based image generation.  Oh yes! And for now, let the experiments begin! Thanks for watching and for your generous  support, and I'll see you next time!"
619,Google’s New AI: Fly INTO Photos!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today, we are able to take a bunch of photos,  and use an AI to magically, create a video   where we can fly through these photos. It is  really crazy, because this is possible today,   for instance, here is NVIDIAs method that can be  trained to perform this in a matter of seconds. Now, I said that in these we can fly through  these photos. But here is an insane idea:   what if we used not multiple photos, but just  one photo, and we don’t fly through it, but fly   into this photo. Now, you are probably asking,  Károly, what are you talking about? This   is completely insane, and it wouldn’t work with  these NERF-based solutions like the one you see   here, these were not designed to  do this at all! Look. Oh yes. That. So, in order to fly into these photos, we  would to invent at least 3 things. One,   image inpainting. Look, if we are to fly into  this photo, we will have to be able to look at   regions between the trees. Unfortunately, these  are not part of the original photo, and hence,   new content needs to be generated intelligently.  That is a formidable task for an AI,   and luckily, image inpainting techniques  already exist out there. Here’s one. But inpainting is not nearly  enough. Two. As we fly into a photo,   completely new regions should also  appear that are beyond the image. This   means that we also need to perform image  outpainting, creating these new regions.   Continuing the image, if you will. Luckily, we are  entering the age of AI-driven image generation,   and this is also possible today, for  instance, with this incredible tool. But even that is not enough. Why is that? Well,  three! As we fly closer to these new regions,   we will be looking at fewer and fewer pixels  and from closer and closer, which means…this.   Oh my, another problem. So, we surely can’t solve  this, right? Well, great news we can! Here is   Google’s diffusion-based solution to super  resolution, where the principle is simple:   have a look at this technique from last year. In  goes a course image or video, and this AI-based   method is tasked with…this! Yes. This is not  science fiction. This is super resolution,   where the AI starts out from noise and  synthesizes crisp details onto the image. So, this might not be such an insane idea after  all! So, does the fact we can do all three of   these separately mean that this task is easy?  Well, let’s see how previous techniques were able   to tackle this challenge. My guess is that  this is still sinfully difficult to do. And…oh   boy. Well, I see a lot of glitches and not a lot  of new, meaningful content being synthesized here.   And note these are not some ancient techniques,  these are all from just two years ago.   It really seems there is not a lot of hope here.  And now, hold on to your papers, and let’s see   how Google’s new AI puts all of these  together, and lets us fly into this photo.   Wow, this is so much better! I love it. Clearly, not perfect, but I feel that this   is the first work where the flying into  photos concept really comes into life. And it has a bunch of really cool features too,  for instance, one, it can generate even longer   videos, which means that after a few seconds,  everything that we see is synthesized by the   AI. Two, it supports not only this boring linear  camera motion, but these really cool, curvy camera   trajectories too. Putting these two features  together, we can get these cool animations   that were not possible before this paper. Now,  the flaws are clearly visible for everyone,   but this is a historic episode where we can  invoke the three Laws of Papers to address them. The First Law Of Papers says that research  is a process. Do not look at where we are,   look at where we will be two more  papers down the line. With this concept,   we are roughly where DALL-E 1 was about a year  ago. That is an image generator AI that could   produce images of this quality. And, just one year  later, DALL-E 2 arrived, which could do this! So,   just imagine what kind of videos this will be  able to create just one more paper down the line. The Second Law of Papers says  that everything is connected.   This AI technique is able to learn  image inpainting, image outpainting,   and super resolution techniques at the same time,  and even combine them creatively. We don’t need   3 separate AIs to do these, just one  technique. That is very impressive. And finally, the Third Law Of Papers says  that a bad researcher fails 100% of the time,   while a good one only fails 99% of the time.  Hence, what you see here is always just 1%   of the work that was done. Why is that? Well, for  instance, this is a neural network-based solution,   which means that we need a ton of training data  for these AIs to learn on. And hence, scientists   at Google also needed to create a technique to  gather a ton of drone videos on the internet   and create a clean dataset also with labelings as  well. The labels are essentially depth information   which shows how far different parts of the image  are from the camera. And they did it for more   than 2 million images in total. So, once again,  if you include all the versions of this idea   that didn’t work, what you see here  is just 1% of the work that was done. And now, we can not only fly through photos, but  also fly into photos. What a time to be alive! What do you think? Does this get your  mind going? What would you use this for?   Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
620,DeepMind’s AlphaFold: 200 Gifts To Humanity!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today you will see history in the making.  Yes, we are going to have a look at AlphaFold   from DeepMind, perhaps one of the most  important papers of the last few years.   This is a true gift to humanity. And, if  even that is not good enough, it gets better,   because today you will see how this  gift has now become 200 times bigger. So, what is AlphaFold? AlphaFold is an AI that is  capable of solving protein structure prediction,   which we will refer to as protein folding.   Okay…but what is a protein  and why does it need folding? A protein is a string of amino acids, these are  the building blocks of life. This is what goes in,   which in reality, has a 3D structure. And that is  protein folding. Letters go in, a 3D object comes   out. This is hard. Really hard. For instance,  DeepMind’s earlier AI programs were able to play   hard games like Chess, Go, and Starcraft 2, and  none of these AIs were a match to protein folding. Yet, it has become the world’s  best solution at the CASP event,   I’ve heard DeepMind CEO Demis Hassabis  call it the Olympics of protein folding.   If you look at how teams of scientists prepare  for this event, you will probably agree that yes,   this is indeed the Olympics of protein folding.  And what is even more incredible is that it has   reached a score of 90, which means two amazing  things. One, in absolute terms, AlphaFold 2   is considered to be about three times better than  previous solutions. And, it gets even better, the   score of 90 means that we can consider protein  folding as a mostly solved problem. Wow.   This is not something that I thought  I would be able to say in my lifetime. Now, we said that this is a  gift to humanity. Why is that?   Where is the gift part here?  What makes this a gift? Well, a little after publishing the paper,  DeepMind made these 3D structure predictions   available for free for everyone. For instance,  they have made their human protein predictions   public. Beyond that, they already have  made their predictions public for yeast,   important pathogens, crop species, and more. I also said that AlphaFold 2 is not the  end, but a start of something great.   I promised to tell you how it improves over  time, so here are two incredible revelations. One, we noted that AlphaFold will likely help  us fight diseases and develop new vaccines. And   hold on to your papers, because this is not  just talk, this is now a reality today: look,   this group was already able to accelerate their  research work a great deal due to AlphaFold,   in fighting antibiotic resistant bacteria. They  are studying how to fight deadly bacteria that   modify their own membrane structure so that  the antibiotic can’t get in. That is a hugely   important project for all of us. They noted that  they have been working vigorously on this project   for 10 years, and now that they have access  to AlphaFold, they could get a prediction done   in 30 minutes, and this is something that was not  possible to do for 10 years before that. That is   absolutely incredible. This is history in  the making. I love to be able to live through   times like this. So thank you so much  for sharing this amazing moment with me. So this was one project, and what is this here?   Oh yes, these are some more  of these AlphaFold predictions   that were already referenced from other scientific  research papers. This is making big waves. And two, this gift to humanity has now  become bigger. So much bigger. Initially,   the protein prediction database they published  contained 1 million entries. And now, if you have   been holding on to your papers, now squeeze that  paper, because they have grown it from 1 million   to 200 million structures. That is 200x within  less than a year. Wow. So just imagine how many   more of these amazing medicine research projects  it will be able to accelerate. I cannot even   fathom how many lives this AI project is going  to save in the future. What a time to be alive! Here is the distribution of the data  across different categories, and these   small circles still represent what was available  before, and the big ones show what is available   from now on. Absolutely amazing. It seems  clearer and clearer that we are now entering   the age of AI-driven art creation, but it  might also be possible that we will also   soon enter the age of AI-driven medicine. These  tools can help us so much! How cool is that? So, does this get your mind going? What would you  use this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
621,NVIDIA’s AI: Amazing DeepFakes And Virtual Avatars!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see that the  research papers that you see here   in this series are real. Here you see NVIDIA’s  game changing videoconferencing AI. So what does   this do? Why is this so interesting? How does  this transfer a video of us over the internet?   Well, here is a crazy idea. It doesn’t! What?  Transmitting video without transmitting video?   How is that even possible?  Well, now, it is possible! What they do in this work, is take only the  first image from the video, and they throw away   the entire video afterwards! But before that,  it stores a tiny bit of information from it,   which is, how our head is moving over time,  and how our expressions change. That is an   absolutely outrageous idea, except the fact that  it works. It not only works, but it works really   well. And because this is an amazing paper,  it does not stop there it can do even more. Look at these two previous methods  trying to frontalize the input video.   This means that we look to the side a little,  and the algorithm synthesizes a new image of   us as if the camera was right in front of us.  That sounds like a science fiction movie, except   that it seems absolutely impossible given how  much these techniques are struggling with the   task…until we look at the new method. My goodness.  There is some jumpiness in the neck movement   in the output video here, and some warping issues  here, but otherwise, very impressive results.   Now if you have been holding on to your papers  so far, now, squeeze that paper, because these   previous methods are not some ancient papers  that were published a long time ago. Not at   all! Both of them were published within the same  year as the new paper. How amazing is that. Wow. And it could also perform these deepfakes too.  Look! We only need one image of the target person,   and we can transfer all of our gestures to  them, in a way that is significantly better   than most previous methods. Now, of course,  not even this technique was perfect,   it still struggled a great deal in the presence  of occluder objects, but still, just the fact   that this is now possible feels like we are  living in the future. What a time to be alive! Now, I said that today we are going to  see that this paper is as real as it gets,   so, what does that mean? Well, today,   anyone can try this technique. Yes, just  one year after publishing this paper,   and it is now available as a demo. The link to  it is available in the video description below. And, get this, some people at NVIDIA are already  using it for their virtual meetings. And by “it”,   I mean both the compression engine, where  the previous industry standard compression   algorithm could do this when given very little  data. That is not much. But the new technique,   with the same amount of data, can now  do this. That is insanity. Loving it. And, they also use their own gestures to create  these deepfakes and make virtual characters   come alive, or to frontalize their videos when  talking to each other. It almost feels like we   are living in a science fiction movie.  And all this is out there for us to use. Also, these technologies will soon  be part of the NVIDIA Video Codec SDK   as the AI Face Codec, which means that it will be  soon deployed to an even wider audience to use.   These companies are already using it. So,  this is one more amazing example that shows   that the papers that you see here in Two Minute  Papers are real. Sometimes so real that we can go   from a research paper to a product in just  a year. That is absolutely miraculous. So, what do you think? What would you use  this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
622,"NVIDIA’s New AI: Beautiful Simulations, Cheaper!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to talk about an amazing  new technique to make these beautiful, but   very expensive computer simulations cheaper. How  much cheaper? Well, I will tell you in a moment. You see, around here, we discuss research  papers like this and this. We talk a great   deal about how difficult and computationally  expensive it is to simulate all this physics,   but that’s not the only thing that needs to  be done here. All this data also has to be   stored too! That is a ton of geometry  information. How much exactly? Well,   15 gigabytes for a simulation is not uncommon, and  my fluid simulation that you see here took well   over a hundred gigabytes of space to store.  Some of the more extreme examples can fill   several hard drives for only a minute of physics  simulation data. That is an insane amount of data. So, we need something to help compressing down all  this data. And the weapon of choice for working   with this kind of volumetric data is often a  tool called OpenVDB. It has this hierarchical   structure within, and scientists at NVIDIA had a  crazy idea. They said, how about exchanging this   with all of these amazing learning-based methods  that you are hearing about everywhere? Well,   okay, but why just throw a neural network at  the problem? Is this really going to help?  Well, maybe. You see, neural networks have  exceptional compression capabilities. For   instance, we have seen with their earlier paper  that such an AI can help us transmit video data   of us by, get this: only taking the first image  from the video, and just a tiny bit of extra data,   and they throw away the entire video afterwards! So, can these techniques do some more magic in   the area of physics simulations? Those are  much more complex, but who knows, let’s see! And now, hold on to your papers, because it  looks like this. Now that is all well and good,   but here is the kicker, it takes about 95% less  storage than the previous method. Yes, that is 20x   compression while the two pieces of footage still  look the same. Wow. That is absolutely incredible. And the previous OpenVDB solution  is not just for some hobby projects,   it is used in many of the blockbuster  feature-length movies you all know and   love. These movies won many-many academy  awards. And, we are not done yet. Not   even close it gets even better! If you  think 20x is the best it can do, now look,   we also have 50x here. And sometimes, it  gets up to even 60x smaller. My goodness! And this new NeuralVDB technique from NVIDIA  can be used for not only these amazing movies,   but for everything else OpenVDB was  useful for. Oh yes! That means that   we can go beyond these movies, and use it for  scientific and even industrial visualizations. They also have the machinery to read and  process all this data with a graphics card,   therefore it not only decreases the  memory footprint of these techniques,   but it even improves how quickly they run. My  mind is officially blown. So much progress in   so little time. And this is my favorite kind  of research, and that is when we look into the   intersection of computer graphics and AI. That’s  where we find some of the most beautiful works. And don’t forget, these applications are  plentiful. This could also be used with   FourCastNet, which is a physics model that  can predict outlier weather events. The   coolest part about this work is that  it runs not in a data center anymore,   but today, it runs on just one NVIDIA  graphics card. And with NeuralVDB,   it will require even less memory to do  so. And it will help with all of these   absolute miracles of science that you see  here too. So, all of these could run with   an up to 60 times smaller memory footprint?  That is incredible. What a time to be alive! Huge congratulations to Doyub Kim, the first  author of the paper and his team for this amazing   achievement. And once again, you see the power of  AI, the power of the papers, and tech transfer at   the same time. And, since NVIDIA has an excellent  record in putting these tools into the hands of   all of us, for instance, they did it with the  previous incarnation of this technique, NanoVDB,   I am convinced that we are all going be able to  enjoy this amazing paper soon. How cool is that? So, does this get your mind going? What would you  use this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
623,"Google's New AI: Dog Goes In, Statue Comes Out!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today you are going to see how Google’s new  AI just supercharged art generation. Again! Yes, this year, we are entering the age  of AI-based art generation. OpenAI’s   DALL-E 2 technique is able to  take a piece of text from us,   and generate a stunning image  that matches this description.   Stable Diffusion, a similar, but open source  technique is also now available for everyone   to use. And the results are so good that artists  are already using it around the world to create   illustrations for a novel, texture synthesis  for virtual worlds, product design, and more. So, are we done here? Is there  nothing else to improve other   than the visual fidelity of  the results? Well, not quite! Have a look at two of my favorite  AI-generated images, this scholar   who is desperately trying to hold on to his  papers. So, I am very happy with this image,   but imagine that if we were creating a comic,  we would need more images of this chap doing   other things. Can we do that? Well, we  are experienced Fellow Scholars here,   so, we know that variant generation comes  to the rescue, right? Well, have a look. Clearly, the AI has an understanding of the image,   and can create somewhat similar images: let’s  see we get someone with a similar beard,   similar paper which is similarly on fire.  The fact that the AI can have a look at such   an image and create variants is a miracle of  science…but not quite what we are looking for. Why is that? Well, of course,   this is a new scholar. We are looking for  the previous scholar doing other things. Now let’s try our fox scientist too and see if  this was maybe just an anomaly? Maybe DALL-E 2   is just not into our Scholarly content? Let’s see!  Well, once again, the results are pretty good. It   understood that the huge ears, gloves, labcoat  and the tie are important elements of the image,   but ultimately, this is a different  fox scientist in a different style. So, no more adventures for these scientists,   right? Have we lost them  forever? Should we give up? Well, not so fast! Have a look at Google’s new  technique, which promises a solution to this   challenging problem! Yes, they promise that if we  are able to take about 4 images of our subject,   for instance, this good boy here, it  will be able to synthesize completely   new images with them. Let’s see, this  is the same dog in the Acropolis. That is excellent. However… wait a minute! This  photo is very similar to this one. So basically,   there was a little synthesis going on here,  but otherwise, changing the background carries   the show here. That is now new. We can do  that with already existing tools anyway. So, is that it? Well, hold on to your  papers, because the answer is no,   it goes beyond changing backgrounds. It  goes so far beyond that that I don’t even   know where to start! For instance, here  is our little doggy swimming, sleeping,   in a bucket, and we can even give this good boy  a haircut. That is absolutely insane. And all of   these new situations were synthesized by using  only 4 input photos. Wow. Now we’re talking! Similarly, if we have a pair of stylish  sunglasses, we can ask a bear to wear it,   make a cool product photo out of it, or put  it in front of the Eiffel Tower. And as I am   a light transport researcher by trade, I have  to note that even secondary effects, like the   reflection of the glasses here are modeled really  well. And so are the reflections here. Loving it. But, it can do so much more than this. Here  are 5 of my favorite examples from the paper. One, we can not only put our favorite teapot  into different contexts, or see it in use,   but we can even reimagine an otherwise  opaque object and see what it would look   like if it were made of a transparent  material, like glass. I love it,   and I bet that product design  people will also love it too. Two, we can create art renditions of our  test subject. Here, the input is only   three photos of a dog, but the output, the  output is priceless. We can commission art   renditions from legendary artists of the past,  and all this nearly for free. How cool is that? Three, I hope you liked this teapot property  modification concept, because we are going   to push it quite a bit further! For  instance, before repainting our car,   we can have a closer look at what it would look  like, and we can also reimagine our favorite   little pets as other animals. Which one is  your favorite? Let me know in the comments   below. Mine is the hippo. It has to be  the hippo. Look at how adorable it is! And if even that is not enough, four, we can also  ask the AI to reimagine our little dog as a chef,   a nurse, a police dog, and many others.  And all of these images are fantastic! And finally, five it can perform no less than  views synthesis too! We know from previous papers   that this is quite challenging, and this one only  requests four photos of our cat, and of course,   if the cat is refusing to turn the right  direction, which happens basically every time,   well, no matter, the AI can resynthesize an  image of it looking into our desired directions.   This one looks like something straight out of a  science fiction movie. What a time to be alive! And this is a huge step forward, you see,   previous techniques typically struggled  with this, even in the best cases,   the fidelity of the results suffer. This alarm  clock is not the same as our input photo was.   And neither is this one. But, with the new  technique, now we’re talking! Bravo Google! And once again, don’t forget, the First  Law of Papers is on full display here:   this huge leap happened just one more paper  down the line. I am truly stunned by these   results and I cannot even imagine what we will  be able to do 2 more papers down the line! So, what do you think? What would you use  this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
624,Google’s New Self-Driving Robot Is Amazing!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to give instructions  to Google’s new self-driving robot,   and see if it is smart enough to deal with these  nasty tasks. Look at that. There is no way, right? Well, with our current tools, I am not so  sure. But let’s be ambitious, and think about   a possible Dream Scenario for a self-driving  robot. For this, we need 3 puzzle pieces. You see, whenever we talk about self-driving  techniques, usually, the input is an image,   and the output is a question  like, okay self-driving car,   what would you do next? But note that the input  is not a sentence. These AIs don’t speak English,   so puzzle piece number one. Language. For a  possible solution to that, have a look at OpenAI   and NVIDIA’s AI’s that can play Minecraft, and  here comes the best part: they also understand   English. How do we do that? Well, we can  use OpenAI’s earlier technique called GPT-3,   which has read a huge part of the internet,  and get this, it now understands English. However, we are not done yet. Not even close.  Puzzle piece number two. Images! If we wish it   not only to be able to understand English, but  to understand our instructions, it needs to be   able to connect these English words to images. It  knows how to spell “stop sign”, but it hasn’t the   slightest clue what a stop sign really looks  like. So, we need to add something that would   make it understand what it sees. Fortunately,  we can also do this, for instance, using the   recent DALL-E AI that took the world by storm by  being able to generate these incredible images   given our text descriptions. This has a back and  forth understanding between words and images, this   module is called CLIP, and it is a general puzzle  piece that can fortunately be reused here too. But, we are not done yet. Now, puzzle piece number  three. Of course, we need self-driving. But make   it really hard for this paper for instance,  train the AI on a large, unannotated self-driving   dataset, which means that it does not get a great  deal of help with the training. There is lots of   data, but not a lot of instructions within this  data. Then, finally let’s chuck all these three   pieces into a robot, and see if it became smart  enough to be able to get around the real world. Of course, there is no way, right?  That sounds like science fiction and   it might never happen. Well, I hope you know  what’s coming, now hold on to your papers,   and check out Google’s new self-driving robot,  which they say has all three of these puzzle   pieces. So, let’s take it out for a ride, and  see what it can do in two amazing experiments. First, let’s keep things light. Experiment number  one. Let’s ask it to recognize and go to a white   building, nice, it found and recognized it. That’s  a good start. Then, continue until you see a white   truck, now, continue, until you find a stop  sign. And, we are done! This is incredible,   we really have all three puzzle pieces working  together. And this is an excellent application   of the The Second Law of Papers, which says that  everything is connected. This AI can drive itself,   understands English, and can see an  understand the world around it too. Wow.    So now, are you thinking what I am thinking?  Oh yes, let’s give it a really hard time! Yes,   final boss time. Look at this  prompt! Experiment number two. Let’s start following the red-black building until  we see a fire hydrant. And, haha, let’s get a   bit tricky! Yup, that’s right. No fire hydrant  anywhere to be seen. Your move, little robot!   And, look at that! It stops…turns around,  and goes the other way, and finally finds   that highly sought after fire hydrant. Very  cool. Now, into the grove to the right.   Okay, this might be it, and now, you are  supposed to take a right when you see a   manhole cover. No manhole cover yet, but, there  were ones earlier that it is supposed to ignore,   so good job not falling for these. And now,  the search continues. But, no manhole cover.   Still, not one in sight. Wait, got it!  And now, to the right, and find a trailer.   No trailer anywhere to be seen. Are we even at the  right place? This is getting very confusing. But,   with a little more perseverance, yes, there  is the trailer! Good job little robot! This is absolutely incredible. I think this  description could have confused many humans,   yet the robot prevailed. All three  puzzle pieces are working together   really well. This looks like something  straight out of a science fiction movie. And a moment that I really loved in this  paper, look. This is a scientist who is   trying to keep up with the robot in the  meantime. Imagine this person going home,   and being asked honey, what  have you been doing today? Well,   I followed a robot around all day.  It was the best. How cool is that! So, with DALL-E 2 and Stable Diffusion, we are  definitely entering the age of AI-driven image   creation, and with DeepMind’s AlphaFold,  perhaps even the age of AI-driven medicine,   and we are getting closer and closer to the  age of AI self-driving and navigation too.   Loving it. What a time to be alive! And, before we go, have a look at this.  Oh yes. As of the making of this video,   only 152 people have looked at this paper. And  that, Dear Fellow Scholars, is why Two Minute   Papers exists. I am worried that if we don’t  talk about some of these works in these videos,   almost no one will. So thank you so much  for coming along this journey with me,   and if you feel that you would like more  of this, please consider subscribing. Thanks for watching and for your generous  support, and I'll see you next time!"
625,Google’s New AI: Fly INTO Photos…But Deeper!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to use an AI to fly  into this photo. And we will see how   much better this technique has become  in just one year. It will be insanity. Yes, in a previous video, we explored an insane  idea: what if we could take just one photograph   of a landscape, and then, we would fly into this  photo like a bird. Of course, that is a big ask,   because to be able to do this, we would need to  invent at least 3 things. One is image inpainting.   Look, when we start flying, the regions between  the trees are missing. We need to generate those. Two, information is also missing not just within,  but around the photo. This is a huge problem,   because completely new regions should  also appear that are beyond the image.   This means that we also need  to perform image outpainting,   creating these new regions from scratch.  Continuing the image, if you will. And three, as we fly closer to these new regions,   we will be looking at fewer and fewer pixels  and from closer and closer, which means…this.   For this, we would need super resolution in  goes a coarse image or video, and this AI-based   method is tasked with…this! Yes. This is not  science fiction. This is super resolution,   where the AI starts out from noise and  synthesizes crisp details onto the image. So, last year, scientists at Google  created an amazing AI that was able   to learn and fuse all these three  techniques together, to create this.   Wow, so this is possible after all. Well, hold on  to your papers, because it is not only possible,   but the followup paper is already here, we are  now one more paper down the line, in less than   a year later! I know you’re itching to see  it, me too, so let’s compare them together! These methods will all start from the same point,  and oh my, look how quickly they deviate. The two   earlier methods quickly break down, and this  is the work that we talked about a few weeks   ago. This is clearly much better, however, as  every single one of you Fellow Scholars can see,   it lacks temporal coherence. What is that?  Well, this means that the AI does not have   a long-term vision of what it wishes to do, and  barely remembers what it did just a few frames   ago. As a result, these landscapes start morphing  into something completely different very quickly. And now, you know what’s coming, so hold onto  your papers and let's look at the new technique!   My goodness, I love it! So much  improvement in just a year. Now,   everyone can see that these are also not perfect,   but this kind of improvement just one more  paper down the line is nothing short of amazing. Especially that it doesn’t only have better  quality. No-no, it can do even more. It offers   us more control too! Now we can turn the camera  around and whenever we see something interesting,   we can decide which direction we wish to go.  And the result is that now, we can create and   also control these beautiful, long aerial  videos where after the first few frames,   every single thing is synthesized by an AI.  How cool is that? What a time to be alive! And, it doesn’t stop there. It gets even better.  If you have been holding on to your papers so far,   now squeeze that paper, because this new AI  synthesizes these videos…without ever having   seen one. That’s right, it had never seen a video.  The previous work was trained on drone videos,   but training this one only requires  a collection of single photographs.   Multiple views and the camera position  are not required. That is insanity. This   AI is so much smarter than the previous  one that was published just a year ago,   and it requires training data that is much  easier to produce at the same time. And,   I wonder what we will be capable of just  two more papers down the line. So cool! Thanks for watching and for your generous  support, and I'll see you next time!"
626,Google's AI: Stable Diffusion On Steroids!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to have a look at how  this new paper just supercharged AI-driven   image generation. For instance, you will see  that it can do this, and this, and even this. And today, it also seems clearer and  clearer that we are entering the age   of AI-driven image generation. You see,  these new learning-based methods can do   something that previously was only possible  in science-fiction movies. And that is,   we enter what we wish to see, and  the AI paints an image for us. Last year, this image was possible,  this year this image is possible.   That is incredible progress in just one  year. So, I wonder what is the next step?   Beyond the regular quality increases,  how else could we improve these systems? Well, scientists at Google had  a fantastic idea. In this paper,   they promise prompt to prompt editing. What  is that? What problem does this solve? Well,   whenever we create an image, and  we feel mostly satisfied with it,   but we would need to add just a little change  to it, we cannot easily do that. But now,   have a look at 5 of my favorites examples  of doing exactly this with this new method. One, if we create this imaginary cat riding  a bike, and we are happy with this concept,   but after taking some driving lessons, our  little imaginary cat wishes to get a car now,   well, now it is possible. Just change the prompt,  and get the same image with minimal modifications   to satisfy the changes we have made. I love  it. Interestingly, it has also become a bit   of a chonker in the process. A testament to  how healthy it is to ride the bike instead! And two, if we are yearning for bigger  changes, we can use a photo, and change   its style as if it were painted by a child.  And I have to say this one is very convincing. Three, and now, hold on to your papers and behold  the Cake Generator AI. Previously, if we created   this lemon cake, and wished to create other  variants of it, for instance, a cheese cake,   or apple cake, we got a completely different  result. These variants don’t have a great deal   to do with the original photo. And, I wonder would  it be possible with the new technique that? Oh my   goodness. Yes! Look at that. These cakes are not  only delicious, but, they are also real variants   of the original slice. Yum! This is fantastic. So,  AI-generated cuisine, huh? Sign me up right now! Four, after generating a car at the side of  the street, we can even say how we wish to   change the car itself. For instance, let’s  make it a sports car instead. Great. Or,   if we are happy with the original car, we can  also ask the new AI to leave the car intact,   and change it surroundings instead. Let’s  put it on a flooded street, or, quickly,   before water damage happens, put it in  Manhattan instead. Excellent. Loving it. Now, of course, you see that not even this  technique is perfect, the car still has changed   a little, but that is something that will surely  be addressed a couple more papers down the line. Five, we can even engage in mask-based  editing. If we feel that this beautiful   cat also deserves a beautiful shirt, we  can delete this part of the screen, then,   the AI will start from a piece of noise  and morph it until it becomes a shirt.   How cool is that? So good! It works for  many different kinds of apparel too. And while we marvel at some  more of these amazing examples,   I would like to tell you one more  thing that I loved about this paper. And that is, it describes a general  concept. Why is this super cool? Well,   it is super cool because it can be applied  to different image generators. If you look   carefully here, you see that this concept  was applied to Google’s own closed solution,   Imagen here. And I hope you know what’s coming  now. Oh yes, a free and open-source text to   image image synthesizer is also available and it  goes by the name Stable Diffusion. We celebrated   it coming into existence a few episodes ago.  But, why am I so excited about this? Well,   with Stable Diffusion, we can finally take out our  digital wrench and tinker with it. For instance,   we can now adjust the internal parameters  in ways that we cannot do with the closed   solutions like DALL-E 2 and Imagen. So  let’s have a look at why that matters. Do you see the prompts here? Of course  you do. Now, what else do you see?   Parameters! Yes, this means  that the hood is popped open,   we can not only look into the inner workings of  the AI, but we can also play with them, and thus,   these results become reproducible at home. So  much so that there is already an unofficial,   open-source implementation of this new  technique applied to Stable Diffusion.   Both of these are free for everyone to run.  I am loving this. What a time to be alive! And once again, this showcases the power of the  papers, and the power of the community. The links   are available in the video description,  and for now, let the experiments begin! Thanks for watching and for your generous  support, and I'll see you next time!"
627,"Wow, A Simulation That Looks Like Reality!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. I am so happy to tell you that today, my  dream came true. I can’t believe it. So,   what happened? I am going to show you my new  paper that just came out. This is a short   Comment article published in Nature Physics,  where I had the opportunity to be a bit of an   ambassador for the computer graphics research  community. You see, this is a small field,   but it has so many absolutely amazing papers,  I am really honored for the opportunity to show   them to the entire world. So, let’s marvel  together today as you’ll see some amazing   papers throughout this video. For instance, what  about this one? What are we seeing here? Yes,   what you see here is a physics simulation from a  previous episode that is so detailed, it is almost   indistinguishable from reality. So, what you  are seeing here should be absolutely impossible. Why is that? Well, to find out, we are going  to look at two different branches of fluid   simulations. Scientists who work in the area  of computational fluid dynamics have been able   to write such simulations for more than 60 years  now, and these are the really rigorous simulations   that can accurately tell how nature works in  hypothetical situations. This is super useful,   for instance, for wind tunnel tests for cars,  testing different aircraft and turbine designs,   and more. However, these simulations can  easily take from days to weeks to compute. Now, here is the other branch. Yes, that  is computer graphics. What is the goal for   computer graphics simulation research? Well,  the goal is exactly what you see here. Yes,   for us graphics people, a simulation does not need  to be perfect. The goal is to create a simulation   that is good enough to fool the human eye. These  are excellent for feature-length movies, virtual   worlds, quick visualizations, and more. And  here is the best part, in return, most computer   graphics techniques don’t run from days to weeks,  they run in a few minutes to a few hours at most. So, computational fluid dynamics, slow but  rigorous, computer graphics, fast but approximate.   These are two completely different branches. Now, of course, any self-respecting Scholar   would now ask, okay, but Károly, how do you  optimize an algorithm that took weeks to run,   to now run in a matter of minutes? Well,  here are two amazing ideas to perform that. One is spatial adaptivity. What does  that do? Well, it does this! Oh yes,   it allocates most of our computational  resources to regions where the real   action happens. You see these more turbulent  regions with dark blue. The orange or yellow   parts indicate calmer regions where we can  get away with less computation. You see,   this is an approximate model, but a really  good one which speeds up the computation   a great deal at a typically small cost. The  hallmark of a good computer graphics paper. Two, in my opinion, this is one of the most  beautiful papers in all of computer graphics,   Surface-Only Liquids. Legendary work. Here,  we lean on the observation that most of the   interesting detail in a fluid simulation  is visible on the surface of the liquids,   so why not concentrate on that. Now, of course,   this is easier said than done, and  this work really pulled it off. But, we are not stopping here. We can also use  this amazing paper to add realistic bubbles to   a simulation. And you will see in a moment that  it greatly enhances the realism of these videos.   Look at that. I absolutely love  it. If the video ends here,   it is because I cannot stop looking  at and running these simulations. Okay, it took a while, but I am back, I have  recovered. So, the key to this work is that   it does not do the full surface-tension  calculations that are typically required   for simulating bubbles, no-no. That would be  prohibitively expensive. It just says that look,   bubbles typically appear in regions where air  gets trapped by the fluid. And it also proposes   an algorithm to find these regions. And the  algorithm is so simple, I could not believe   my eyes. I thought that this was some kind of a  prank. It turns out, we only need to look at the   curvature of the fluid, and only nearby, which  is really cheap, simple and fast. And it really   works! I can’t believe it. Once again, this  is an approximate solution, not the real deal,   but it is extremely simple and fast, and it can  even be added as a post-processing step to a   simulation that is already finished without  the bubbles. How cool is that? Loving it! Now wait, we just said that surface-tension  computations are out of reach. That just costs   too much. Or, does it? Here is an incredible  graphics paper from last year that does just   that. So what is all this surface-tension thing  good for? Well, for instance, it can simulate   this paperclip floating on water. That is quite  remarkable because the density of the paperclip   is 8 times as much as the water itself, and  yet, it still sits on top of the water. We   can also drop a bunch of cherries into water and  milk and get these beautiful simulations. Yes,   these are not real videos, these are all simulated  on a computer. Can you tell the difference? I’d   love to know, let me know in the comments below!  And get this, for simpler scenes, we only need to   wait for a few seconds for each frame. That  is insanity. I told you that there are some   magical works within computer graphics. I am so  happy for the opportunity to share them with you! However, wait a second. Remember, we  said that these graphics works are fast   and approximate. But is this really true?  Can they really hold the candle to these   rigorous computational fluid dynamics  simulations that are super accurate,   but take so long? That is impossible,  right? A quick simulation on our home   computer cannot possibly tell us anything  new about aircraft designs, can it? Well,   hold on to your papers, because it can! And it  not only can, but you are already looking at it! This is a devilishly difficult test, a real  aerodynamic simulation in a wind tunnel. In   these cases, getting really accurate results is  critical. For instance, here we would like to   see that if we were to add a spoiler to this car,  how much of an aerodynamic advantage we would get   in return. Here are the results from the real wind  tunnel test, and now, let’s see how the new method   compares. Wow. Goodness! It is not perfect by any  means, but seems accurate enough that we can see   the wake flow of the car clearly enough so that  we can make an informed decision on that spoiler. Yes, this, and even more is possible with computer  graphics algorithms. These approximate solutions   became so accurate and so fast, that we are seeing  these two seemingly completely different branches,   computational fluid dynamics, and computer  graphics getting closer and closer to each   other. Even just a couple years ago, I did not  dare to think that this would ever be possible.   And yet, these fast and predictive  simulations are within arm’s reach,   and just a couple more papers down the line,  and we might be able to enter a world where   an engineer is able to test new ideas in aircraft  design every few minutes. What a time to be alive! So, all this came about because I am worried that  as the field of computer graphics is so small,   there are some true gems out there, and if we  don’t talk about these works, I am afraid that   almost no one will. And this is why Two Minute  Papers, and this paper came into existence. And   I am so happy to have a Comment paper accepted  to Nature Physics, and now, to be able to show   it to you. By the way, the paper is a quick  read, and you can read it for free through   the link in the video description. I think we are  getting so close to real life-like simulations,   everyone has to hear about it. So cool! If  you wish to help me spread the word about   these incredible works, please consider sharing  this with your friends and tweeting about it. I would also like to send a big thank you for  this amazing opportunity to write this paper,   and to Christopher Batty who sent super  useful comments. So, what do you think?   What would you use these techniques  for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
628,Watch NVIDIA’s AI Teach This Human To Run!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today is going to be a really fun day  where we do this, and this, and this. In a previous episode, we talked about creating  virtual characters with a skeletal system,   adding more than 300 muscles and teaching  them to use these muscles to kick, jump,   move around and perform other realistic human  movements. You see the activated muscles with red. I am loving the idea, which, turns out,  comes with lots of really interesting   corollaries. For instance, this simulation  realistically portrays how increasing the   amount of weight to be lifted changes  what muscles are being trained during   a workout. These agents also learned to  jump really high and you can see a drastic   difference between the movement required  for a mediocre jump and an amazing one. And now, scientists at NVIDIA had a crazy idea.  They said, what if we take a similar model, and   ask it to learn to walk from scratch.  Now that is indeed a crazy idea,   because they proposed a model that is  driven by over 150 muscle-tendon units,   and is thus very difficult to  control. So, let’s see how it went. First, it started to…umm…hello? Well, A+ for  effort, but unfortunately, this is not a great   start. But don’t despair! A little later, it  learned to…well, fall in a different direction,   however, at least some learning is hopefully  happening. Look. I wouldn’t say that it has   finally taken the first step, but at least it  is attempting to take a first step. Is that good   news? Let’s see! Oh yes, yes it is! Because a  little later, it learned to jog. This concept   really works! And, if we wait for a bit longer,  we see that it learned to run as well. Fantastic! Now, let’s have a closer look and see if the  colors of the muscles indeed show us which   ones are activated at a given moment. And that’s  right! When slowing the footage down, we see the   difficulty of the problem and that is, different  tendons need to be moved every single moment. So, while we look at this technique learning  other tasks, we ask one of the most important   questions here, and that is, how fast did it learn  to run? It had to control a 150 different tendons   continuously over time, without falling. So how  long did it take? Did it take days? And now,   hold on to your papers, because it hasn’t taken  days. It only takes minutes. After starting out   like this, by the 17-minute mark, it has learned  so much that it could jog. How amazing is that? And that is one of the key value propositions of  this paper. It can not only teach this AI agent   difficult tasks, but it can also learn up to  15 to 17-times faster than previous techniques.   That is absolutely amazing. Bravo! So, it seems  that we now have learning-based algorithms that   could teach even a complex, muscle-actuated  robot to walk. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
629,Ray Tracing: How NVIDIA Solved the Impossible!,"Dear Fellow Scholars, this is Two Minute Papers  with Dr. Károly Zsolnai-Fehér. Or not quite. To   be more exact, I have had the honor to hold a talk  here at GTC, and today we are going to marvel at   a seemingly impossible problem, and 4 miracle  papers from scientists at NVIDIA. Why these 4?   Could these papers solve the impossible?  Well, we shall see together in a moment.    If we wish to create a truly gorgeous   photorealistic scene, in computer graphics,  we usually reach out to a light transport   simulation algorithm, and then, this happens.  Oh yes, concept number one. Noise! This is   not photorealistic at all, not yet anyway. Why is  that? Well, during this process, we have to shoot   millions and millions of light rays into the scene  to estimate how much light is bouncing around,   and before we have simulated enough rays, the  inaccuracies in our estimations show up as noise   in these images. This clears up over time, but it  may take from minutes to days for this to happen,   even for a smaller scene. For instance, this one  took us 3 full weeks to finish. 3 weeks! Yes,   really. I am not kidding. Ouch.    Solving this problem in real time seems absolutely  impossible, which has been the consensus in the   light transport research community for a long  while. So much so, that at the most prestigious   computer graphics conference, SIGGRAPH, there  was even a course by the name: Ray tracing is   the future and ever will be. This was a bit of a  wordplay yes, but I hope you now have a feel of   how impossible this problem feels.    When I was starting out as a first year PhD  student, I was wondering whether real-time   light transport will be a possibility within  my lifetime. It was such an outrageous idea,   I usually avoided even bringing up the question in  conversation. And boy, if only I knew what we are   going to be talking about today. Wow.    So, we are still at the point where these images  take from hours to weeks to finish. And now,   I have good news and bad news. Let’s go with  the good news first. If you overhear some   light transport researchers talking, this is  why you hear the phrase “importance sampling”   a great deal. This means to choose where to  shoot these rays in the scene. For instance,   you see one of those smart algorithms here,  called Metropolis Light Transport. This is one   of my favorites. It typically allocates these rays  much smarter than previous techniques, especially   on difficult scenes. But, let’s go even smarter!  This is my other favorite Wenzel Jakob’s Manifold   Exploration paper at work here. This algorithm is  absolutely incredible, and the way it develops an   image over time is one of the most beautiful  sights in of all light transport research.    So if we understand correctly,   the more complex these algorithms are, the smarter  they can get, however, at the same time, due to   their complexity, they cannot be implemented so  well on the graphics card. That is a big bummer.   So, what do we do?    Do we use a simpler algorithm and take  advantage of the ever-improving graphics   cards in our machines, or write something  smarter and miss out on all of that?    So now, I can’t believe I am saying this,   but let’s see how NVIDIA solved the impossible  through 4 amazing papers. And that is, how they   created real-time algorithms for light transport.    Paper number one. Voxel Cone Tracing. Oh my, this  is an iconic paper that was one of the first signs   of something bigger to come. Now, hold on to your  papers, and look at this. Oh my goodness. That is   a beautiful, real-time light transport simulation  program. And it gets better, because this one   paper that is from 2011, a more than 10-year-old  paper, and it could do all this. Wow! How is that   even possible? We just discussed that we’d be  lucky to have this in our lifetimes, and it seems   that it was already here, 10 years ago!    So, what is going on here? Is light transport  suddenly solved? Well, not quite. This solves   not the full light transport simulation  problem, but it makes it a little simpler.   How? Well, it takes two big shortcuts. Shortcut  number one: it subdivides the space into voxels,   small little boxes, and it runs the light  simulation program on this reduced representation.    Shortcut number two, it only computes 2 bounces   for each light ray for the illumination. That is  pretty good, but not nearly as great as a full   solution with potentially infinitely many bounces.    It also uses tons of memory, so, plenty of things  to improve here, but my goodness, if this was not   a quantum leap in light transport simulation, I  really don’t know what is. This really shows that   scientists at NVIDIA are not afraid of completely  rethinking existing systems to make them better,   and boy, isn’t this a marvelous example of that.  And remember, all this in 2011. 2011! More than 10   years ago. Absolutely mind-blowing. And one more  thing: this is the culmination of software and   hardware working together. Designing them for  each other. This would not have been possible   without it.    But, once again, this is not the full light  transport. So, can we be a little more ambitious,   and hope for a real-time solution for  the full light transport problem? Well,   let’s have a look together and find out! And here  is where paper number two comes to the rescue. In   this newer work of theirs, they presented an  amusement park scene that contains a total of   over 20 million triangles, and it truly is  a sight to behold. So let’s see, and! Oh,   goodness! This does not take from minutes to days  to compute, each of these images were produced in   a matter of milliseconds! Wow.    And, it gets better. It can also render this  scene with 3.4 million light sources, and this   method can really render not just an image, but  an animation of it interactively. What’s more,   the more detailed comparisons in the paper reveal  that this method is 10 to 100 times faster than   previous techniques, and it also maps really well  onto our graphics cards. Okay, but what is behind   all this wizardry? How is this even possible?    Well, the magic behind all this is a smarter  allocation of these ray samples that we have to   shoot into the scene. For instance, this technique  does not forget what we did just a moment ago when   we move the camera a little and advance to the  next image. Thus, lots of information that is   otherwise thrown away can now be reused as we  advance the animation. Now note that there are   so many papers out there on how to allocate  these rays properly, this field is so mature,   it truly is a challenge to create something  that is just a few percentage points better   than previous techniques. It is very hard to  make even the tiniest difference. And to be able   to create something that is 10 to a 100 times  better in this environment? That is insanity.    And, this proper ray allocation   has one more advantage. What is that? Well, have a  look at this. Imagine that you are a good painter,   and you are given this image. Now your task is  to finish it. Do you know what this depicts?   Hmm…maybe. But knowing all the details of this  image is out of question. Now, look, we don’t have   to live with these noisy images, we have denoising  algorithms tailored for light simulations. This   one does some serious legwork with this noisy  input, but even this one cannot possibly know   exactly what is going on because there is so much  information missing from the noisy input. And now,   if you have been holding on to your papers so far,  squeeze that paper, because, look. This technique   can produce this image in the same amount of  time. Now we’re talking! Now, let’s give it   to the denoising algorithm, and…yes! We get a  much sharper, more detailed output. Actually,   let’s compare it to the clean reference image.  Yes, yes yes! This is much closer. This really   blows my mind. We are now, one step closer  to proper, interactive light transport!    Now note that I used the   word interactively twice here. I did not say real  time. And that is not by mistake. These techniques   are absolutely fantastic, one of the bigger leaps  in light transport research, but, they still cost   a little more than what production systems can  shoulder. They are not quite real time yet.    And, I hope you know what’s coming.   Oh yes! Paper number three. Check this out!    This is their more recent result on the Paris  Opera House scene, which is quite detailed,   there is a ton going on here. And, you are all  experienced Fellow Scholars now, so when you   see them flicking between the raw, noisy and the  denoised results, you now know exactly what is   going on. And, hold on to your papers, because  all this takes about 12 milliseconds per frame.   That is over 80 frames per second. Yes yes yes! My  goodness! That is finally in the real time domain   and then some! What a time to be alive!    Okay, so where is the catch? Our keen  eyes see that this is a static scene.   It probably can’t deal with dynamic movements  and rapid changes in lighting, can it? Well,   let’s have a look. Wow! I cannot believe my  eyes. Dynamic movement, checkmark. And here,   this is as much change in the lighting as  we would ever want, and it can do this too.    And, we are still not done yet. At this point,   real time is fantastic, I cannot overstate how  huge of an achievement that is. However, we need   a little more to make sure that the technique  works on a wide variety of practical cases.   For instance, look here. Oh yes! That is a ton of  noise, and it’s not only noise, it is the worst   kind of noise! High-frequency noise. The bane of  our existence. What does that mean? It means these   bright fireflies. If you show that to a light  transport researcher, they will scream and run   away. Why is that? It is because these are light  paths that are difficult to get to, and hence,   take a ton more ray samples to clean up.    And, you know what is coming! Of course, here  is paper number four. Let’s see what it can   do for us. Am I seeing correctly? That is so much  better! This seems nearly hopeless to clean up in   a reasonable amount of time, and this…this might  be ready to go as is with a good noise filter.   How cool is that!    Now, talking about difficult light paths,  let’s have a look at this beautiful caustics   pattern here. Do you see it? Well, of course  you don’t! This region is so undersampled,   we not only can’t see it, it is hard to  even imagine what should be there. So,   let’s see if this new method can accelerate  progress in this region. That is not true. That   just cannot be true. When I first saw this paper,  I could not believe this, and I had to recheck the   results over and over again. This is at the very  least a 100 times more developed caustic pattern.   Once again, with a good noise filter, probably  ready to go as is. I absolutely love this one. Now, note that there are still shortcomings. None  of these techniques are perfect. Artifacts can   still appear here and there, and around specular  and glossy reflections, things are still not as   clear as the reference simulation. However, we now  have real time light transport and not only that,   but the direction we are heading to is truly  incredible, and amazing new papers are popping   up what feels like every single month. Don’t  forget to apply The First Law Of Papers,   which says that research is a process. Do not look  at where we are, look at where we will be two more   papers down the line.    Also, NVIDIA is amazing at democratizing  these tools and putting them into the   hands of everyone. Their tech transfer  track record is excellent. For instance,   their marbles demo is already out there,  and not many know that they already have   a denoising technique that is online and  ready to use for all of us. This one is a   professional grade tool right there. Many  of the papers that you have heard about   today may see the same fate. So, real-time light  transport for all of us? Sign me up right now!  Thanks for watching and for your generous  support, and I'll see you next time!"
630,"Google’s New AI: DALL-E, But Now In 3D!","Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how this new AI  is able to take a piece of text from us,   anything we wish, be it a squirrel  dressed like the king of England,   a car made out of sushi, or a humanoid  robot using a laptop, and, magically,   it creates not an image like previous techniques,  but get this, a full 3D model of it. Wow. This is absolutely amazing. An AI that can not  only create images, but create 3D assets? Yes,   indeed, the result is a full 3D model that  we can rotate around and even use in our   virtual worlds. So, let’s give it a really hard  time and see together what it is capable of. For instance, OpenAI’s earlier  DALL-E text to image AI was capable   of looking at a bunch of images of koalas, and  separately, a bunch of images of motorcycles,   and it started to understand the concept of  both, and it was be able to combine the two   together into a completely new image.  That is, a koala, riding a motorcycle. So, let’s see if this new method is also capable  of creating new concepts by building on previous   knowledge. Well, let’s see…oh yes! Here is a  tiger wearing sunglasses and a leather jacket,   and most importantly, riding a motorcycle. Tigers  and motorcycles are well understood concepts,   of course, the neural network had plenty of these  to look at in its training set, but combining the   two concepts together, now that is a hint of  creativity. Creativity in a machine. Loving it. What I also loved about this work is that it makes  it so easy to iterate on our ideas. For instance,   first, we can start experimenting with a  real squirrel, or, if we did not like it,   we can quickly ask for a wooden carving,  or even a metal sculpture of it. Then,   we can start dressing it up,  and make it do anything we want. And sometimes, the results are nearly good enough  to be used as-is even in an animation movie   or in virtual worlds, or even in the worse cases,  I think these could be used as a starting point   for an artist to continue from. That would save  a ton of time and energy in a lot of projects! And that is huge. Just consider all the  miraculous things artists are using the   DALL-E 2 text to image AI and Stable Diffusion  for, illustrating novels, texture synthesis,   product design, weaving multiple images  together to create these crazy movies,   you name it. And now, I wonder what unexpected  uses will arise from this being possible for   3D models? Do you have some ideas?  Let me know in the comments below! And just imagine what this will be capable of just  a couple more papers down the line. For instance,   the original DALL-E AI was capable of this, and  then, just a year later, this became possible. So, how does this black magic work? Well, the  cool thing is that this is also a diffusion-based   technique, which means that similarly to the text  to image AIs, it starts out from a piece of noise,   and refines this noise over time to  resemble our input text a little more.   But this time, this diffusion process  is running in higher dimensions, thus,   the result is not a 2D image, but a full 3D  model. So, from now on, the limit in creating   3D worlds is not our artistic skill, the limit  is only our imagination. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
631,Intel’s New AI: Amazing Ray Tracing Results!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. This is my happy episode. Why is that? Well, of  course, because today we are talking about light   transport simulations, and in particular, Intel’s  amazing new technique, that can take this, and   make it into…this! Wow. It can also take this, and  make it into…this. My goodness, this is amazing. But wait a second, what is going on here?  What are these noisy videos for, and why? Well, if we wish to create a truly gorgeous  photorealistic scene, in computer graphics,   we usually reach out to a light transport  simulation algorithm, and then, this happens.   Oh no! We have noise. Tons of it. But why? Well,  during the simulation, we have to shoot millions   and millions of light rays into the scene to  estimate how much light is bouncing around,   and before we have simulated enough rays, the  inaccuracies in our estimations show up as noise   in these images. This clears up over time, but it  may take a long time. How do we know that? Well,   have a look at the reference simulation footage  for this paper. See? There is still some noise in   here. I am sure this should clean up over time,  but no one said that it would do so quickly. A   video like this might require hours to days  to compute. For instance, this is from our   previous paper that took 3 weeks to finish and  it ran on multiple computers at the same time. So, is all hope lost for these beautiful  photorealistic simulations? Well,   not quite! Instead of waiting for hours or  days, what if I told you that we can just   wait for a small fraction of a second, about 10  milliseconds. It will produce this. And then,   run a previous noise filtering technique  that is specifically tailored for light   transport simulations, and what  do we get? Probably not much,   right? I can barely tell what I should be seeing  here. So, let’s see a previous method. Whoa! That   is way better. I was barely able to guess what  these are, but now we know: gratings. Great! So, we don’t have to wait for hours  to days for a simulated world to   come alive in a video like this, just a few  milliseconds. At least for the simulation,   we don’t know how long the noise filtering takes. And now, hold on to your papers, because this  was not today’s paper’s result. I hope this one   can do even better. And, look, instead, it can do  this. Wow. This is so much better! And the result   of the reference simulation for comparison,  this is the one that takes forever to compute. Let’s also have a look at the videos and compare  them. This is the noisy input simulation, wow,   this is going to be hard. Now, the previous  method. Yes, this is clearly better, but there   is a problem. Do you see the problem? Oh yes,  it smoothed out the noise, but it smoothed the   details too. Hence, a lot of them are lost. So,  let’s see what Intel’s new method can do instead.   Now we’re talking! So much better. I  absolutely love it. It is still not as   sharp as the reference simulation, however,  in some regions, depending on your taste,   it might even be more pleasing  to the eye than this reference. And it gets better! This technique does not only  denoising, but upsampling too. This means that   it is able to create a higher resolution image  with more pixels than the input footage. Now,   get ready, one more comparison and I’ll  tell you how long the noise filtering took. Whoa, I wonder what it will do with this noisy  mess. I have no idea what is going on here.   And neither does this previous technique.  And this is not some ancient technique,   this previous method is the Neural Bilateral  Grid, a learning-based method from just two   years ago. And now, have a look at this.  My goodness! Is this really possible? So   much progress just one more paper down  the line! I absolutely love it. So good! So, how long do we have to wait for an  image like this? Still hours to days? Well,   not at all. This all runs not only in real  time, it runs faster than real time! Yes,   that means about 200 frames per second for  the new noise filtering step. And remember,   the light simulation part typically takes 4-12  milliseconds on these scenes, this is the noisy   mess that we get. And just 5 milliseconds later,  we get this. I cannot believe it. Bravo! So,   real-time light transport simulations  from now on? Oh yes, sign me up right   now! What a time to be alive! So, what do  you think? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
632,Google’s New Robot: Don't Mess With This Guy!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to give a really hard time  to a robot. Look at this, and this. You see,   Google’s robots are getting smarter and smarter  every year, for instance, we talked about this   robot assistant where we can say “please help  me, I have spilled the coke”, and then…it creates   a cunning plan! It tries to throw out the coke  can, well, almost. Then, it brings us a sponge. And, if we’ve been reading research papers all  day and we feel a little tired, if we tell it,   it can bring us a water bottle, hand it  to us, and can even bring us an apple. A different robot of theirs  has learned to drive itself,   also understands English, and can thus,  take instructions from us and find its   way around. And these instructions  can be really nasty, like this one. However, there is one thing that  neither of these previous robots can do,   but this new one can. Let’s see what it is. Let’s ask for a soda again, and once  again, it comes up with a cunning   plan. Go to the coke can, pick it up, and  haha! Checkmate, little robot! Now what?  Now comes the coolest part! It realizes that  it has failed, and now, change of plans,   it says the following: there is no coke, but  there is an orange soda. Is that okay with us? So, are we going to be kind? No-no, we are  going to be a little picky here and say no and   ask for a lime soda instead! The robot probably  thinks, oh goodness, a change of plans again,   let’s look for that lime soda. And it is, of  course, really far away to give it a hard time,   so let’s see what it does…wow, look at that!  It found it in the other end of the room,   recognized that this is indeed the  soda, picks it up, and we are done!   So cool! The amenities at the Google  headquarters are truly next level. I love it. This was super fun, so you know what? Let’s try  another task. Let’s ask it to put the coke can   into the top drawer. Will it succeed? Well,  look at that! The human operator cannot wait   to mess with this little robot, and aha!  Sure enough, that drawer is not opening.   So, is this a problem? Well, first, the robot  recognized that this was not successful, but now,   the environment has changed, the pesky human  is gone, so, it tries again. And this time,   the drawer opens, in goes the coke can,  it holds the drawer with both fingers,   and now I just need a gentle  little push, and, bravo! Good job! So we tried to confuse this robot  by messing up the environment,   and it did really well, but now, get this,  what if we mess with its brain instead?  How? Well, check this out. Let’s ask it to throw  away the snack on the counter, it asks which one,   and to which our answer is of course, not the  apple, no-no. Let’s mess with it a little more.   Our answer is that we changed our mind, so please  throw away something on the table instead. Okay,   now as it approaches the table, let’s try to  mess with it again, you know what? Nevermind,   just finish the previous task instead.  And it finally goes there and grabs   the apple. We tried really hard, but we  could not mess with this guy. So cool! So, a new robot that understands English, can  move around, make a plan, and most importantly,   it a can come up with a plan B when plan A  fails. Wow. A little personal assistant. The   pace of progress in AI research is nothing  short of amazing. What a time to be alive! So, what would you use this for?  Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
633,Google’s Video AI: Outrageously Good!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. I cannot believe that this paper is here. This  is unbelievable. So, what is going on here? Yes,   that’s right, we know that these modern  AI programs can paint images for us,   anything we wish, but today, we are going to find  out whether they can also do it with video. You   see an example here, and here. Are these also  made by an AI? Well, I’ll tell you in a moment. So, video? That sounds impossible. That is so much  harder! You see, videos require a much greater   understanding of the world around us, so much more  computation, and my favorite, temporal coherence.   What is that? This means that a video is not just  a set of images, but a series of images that have   to relate to each other. If the AI does not do  a good job at this, we get this. Flickering. So, as all of this is so hard, I thought, we  will be able to do this maybe in 5-10 years, or   maybe never? Well, scientists at Google say not so  fast. Now hold on to your papers, and have a look   at this. Oh my goodness! Is it really here? I am  utterly shocked, but the answer is yes. Yes it is! So now, let’s have a look at 3 of my  favorite examples, and then I’ll tell   you how much time this took. By the way,  it is an almost unfathomably short time. Now, one, the concept is the same: one simple text  prompt goes in, for instance, a happy elephant   wearing a birthday hat walking under the sea, and  this comes out. Wow. Look at that! That is exactly   what we were asking for in the prompt, plus, as  I am a light transport researcher by trade, I am   also looking at the waves and the sky through the  sea, which is absolutely beautiful, but it doesn’t   stop there I also see every light transport  researcher’s dream there. Water caustics. Look   at these gorgeous patterns. Now, not even this  technique is perfect, you see that temporal   coherence is still subject to improvement, the  video still flickers a tiny bit, the tusk is also   changing over time. However, this is incredible  progress in so little time. Absolutely amazing. Two, in good Two Minute Papers fashion, now  let’s ask for a bit of physics, a bunch of   autumn leaves falling on a calm lake forming  the text “Imagen Video”. I love it. You see,   in computer graphics, creating a  simulation like this would take   quite a bit of 3D modeling knowledge, then,  we also have to fire up a fluid simulation. Now, this does not seem to do a great deal of  two-way coupling, which means that the water has   an effect on the leaves, you see it advecting  this leaf here, but the leaves do not seem to   have a huge effect on the water itself. This  is possible with specialized computer graphics   algorithms like this one, and I bet it will  also be possible with Imagen Video 2. Now,   I am super happy to see the reflections of  the leaves appearing on the water. Good job,   little AI! And to think that this is  just the first iteration of Imagen Video,   wow. By the way, if you wish to see how  detailed a real physics simulation can be,   make sure to check out my Nature Physics comment  paper in the video description. Spoiler alert:   the surprising answer is that they can  be almost as detailed as real life. I was also very happy with this  splash. And with this turquoise   liquid’s movement in the glass too. Great  simulations on version 1. I am so happy! Now, three, give me a teddy bear  doing the dishes. Whoa! Is this real?   Yes it is! It really feels like we are living  inside a science fiction movie. Now, it’s not   perfect, you see that it is a little confused by  the interaction of these objects, but if someone   told me a few weeks ago that an AI would be able  to do this, I wouldn’t have believed a word of   it. It not only has a really good understanding  of reality, but it can also combine two previous   concepts, a teddy bear and washing the dishes  into something new. My goodness. I love it. Now, while we look at some more beautiful results,   we noted that this is incredible progress in  so little time. But, how little exactly? Well,   if you have been holding on to your  papers so far, now, squeeze that paper,   because the OpenAI’s DALL-E 2 text to image AI  appeared in April 2022, then, Google’s Imagen,   also text to image appears one month later, May  2022, that is incredible, and get this, now, only   5 months later, by October 2022, we get this.  An amazing text to video AI. I am out of words! Of course, it is not perfect, the hair  of pets is typically still a problem,   and the complexity of this ship battle is  still a little too much for it to shoulder,   so version one is not going to make  a new Pirates of The Caribbean Movie,   but maybe version 3 two more  papers down the line? Who knows? Ah yes, about that. The resolution of these  videos is not too bad at all, it is in 720p,   the literature likes to call it high definition.  These are not in 4k like the shows you can watch   on your tv, but this quality for a first crack  the at the problem is simply stunning. And don’t   forget that first, it synthesizes a low-resolution  video, then upscales it through super resolution,   something Google is already really good at,  so I would not be surprised for version 2   to easily go to full HD, and maybe even beyond.  As you see, the pace of progress in AI research   is nothing short of amazing. If like me,  you are yearning for some more results,   you can check out the paper’s website in the video  description where as of the making of this video,   you get a random selection of results. Refresh  it a couple times and see if you get something   new! And if I could somehow get access to this  technique, you bet that I’d be generating a ton   more of these. Update: I cannot make any promises,  but, good news, we are already working on it.   A video of a scholar reading exploding papers  absolutely needs to happen. Make sure to subscribe   and hit the bell icon to not miss it in case  it happens! You really don’t want to miss that. So, from now on, if you are wondering what a  wooden figurine surfing in outer space looks like,   you need to look no further. What a time  to be alive! So, what do you think? Does   this get your mind going? What would you use  this for? Let me know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
634,NVIDIA’s Amazing AI Clones Your Voice!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to clone real human voices  using an AI. How? Well, in an earlier NVIDIA   keynote, we had a look at Jensen Jr., an  AI-powered virtual assistant of NVIDIA’s CEO,   Jensen Huang. It could do, this! Look  at the face of that proud man! I love   how it also uses hand gestures that  go really well with the explanation. These virtual AI assistants are going to appear  everywhere to help you with your daily tasks.   For instance, in your car, the promise  is that they will be able to recognize   you as the owner of the car, recommend  shows nearby, and even drive you there. These Omniverse avatars may also help us order our  favorite burgers too. And, we won’t even need to   push buttons on a touchscreen. We just need to  say what we wish to eat and the assistant will   answer and take our orders, perhaps later even  in a familiar person’s voice. How cool is that? And today, I am going to ask you to imagine a  future where we can all have our Toy Jensens   or our own virtual assistants with our own  voice. All of us! That sounds really cool! So,   is that in the far future? No-no, not at all!  Today, I have the amazing opportunity to show   you a bit more about the AI that makes  this voice synthesis happen. And yes,   you will hear things that are only  available here at Two Minute Papers. So, what is all this about? This work is an  AI-based technique that takes samples of our   voice, and can then clone it. So, let’s give it a  try. This is Jamil from NVIDIA who was kind enough   to record these voice snippets. Listen. Okay, so  how much of this do we need to train the AI? Well,   not the entire life recordings of the test  subject, but much less, 30 minutes of these   voice samples. The technique asks us to say  these sentences and analyzes the timbre,   prosody and the rhythm of our voice, which is  quite a task. And, what can it do afterwards? Well, hold on to your papers, because Jamil,   not the real one, the cloned AI Jamil has  a scholarly message for you that I wrote. Now, of course, it is not perfect, I think  that most of you are able to tell that these   voice lines were synthesized, but my opinion is  that these are easily good enough for a helpful   humanlike virtual assistant. And, really, how cool  is that? Cloning a human voice from half an hour   worth of sound samples. What a time to be alive!  And note that I have been really tough with them,   these are some long, scholarly sentences that  would give a challenge to any of these algorithms. Now, if you are one of our earlier Fellow  Scholars, you might remember that a few years ago,   we talked about an AI technique called Tacotron,  which can perform voice cloning from a really   short, few second-long sample. I have only heard  examples of simpler, shorter sentences for that,   and what is new here is that this new  technique takes more data, but in return,   offers higher quality. But it does not stop there!  It does more! This new method is easier to train   and it also generalizes to more languages better.  And these are already good enough to be used in   real products, so I wonder what a technique like  this will look like just two more papers down the   line. Maybe my voice here on Two Minute Papers  could be synthesized by an AI? Maybe it already   is? Would that be a good thing? What do you think?  Also, a virtual Károly for you to read your daily   dose of papers? Actually, since NVIDIA has a great  track record of putting these amazing tools into   everyone’s hands, if you are interested, there  is already an early access program where you can   apply. I hope that some of you will be able to  try this and let us know in the comments below! Thanks for watching and for your generous  support, and I'll see you next time!"
635,Ubisoft’s New AI: Breathing Life Into Games!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Earlier we talked about AI-based techniques  that can learn to clone your voice, and then,   we can perform text to speech. So, it would learn  this. And then, the AI would generate this. Yes,   this means that we can write something,  and the AI says it in our voice. So that   is text to speech. But get this, what  about speech to gesture? What is that? Well, scientists at Ubisoft had a crazy  idea. They said, let’s create a dataset   of characters where the AI can see gestures for  agreement, disagreement, being relaxed, neutral,   scared, and more, learn from it, and apply  it to new virtual characters intelligently. Sounds good, right? But, not so fast. Ubisoft  is not the first to try this idea, here is a   technique from just two years ago. Look. There  is something here, but we are humans, at least,   most of us anyway, and our eyes are highly attuned  to the gestures of other humans, which means that   if even the smallest part of the animation  is off, we will immediately notice. And I   think that every single one of you Fellow Scholars  indeed noticed that something is off here. And   based on the quality of the animation that is  required today to keep the illusion up, I am not   sure if this is coming to fruition anytime soon.  Just look at these examples. We are so far away. But in any case, let’s have a look at the new  technique together. The concept is the same,   first, in goes a speech sample, and it will be  able to generate long sequences using a style of   our choosing. Like this. That looks incredible.  Let’s compare it to the previous method.   Yes, my goodness! A night and day difference! Such incredible progress in just two years. Yes,   this is from only two years ago. And  that would already be pretty cool,   but, it gets crazier than that. Much crazier. For instance, we can plug in a new  speaker the AI hasn’t heard about yet,   and this happens. This is really good. But  that’s still nothing. It generalized to not   only new speakers, but hold on to your  papers, to new languages too. Listen. Wow, and the list of features just keeps on  going. For instance, we can also exert some   artistic control here. If we feel that the  hands are too high up, we can lower them.   Or, we can even ask for more or less  hip movement during the monologue. And I have to say, it does a lot of things really  well from the lowest energy level gestures, up to   the highest. And it still doesn’t stop there, it  is not only better than the previous technique,   is not only more controllable than the previous  technique, but is it also about 7 times faster,   and even better, it is done with a neural network  that is significantly simpler than that one. How many hours of these gestures did  the neural network get access to to   learn it? How big was the training set size?  Actually, it requires very little information.   All it was given was about 2 hours of these  gestures, and it is not copy-pasting, but,   it can continue these gestures for a long-long  monologue seamlessly. And as a cherry on top,   even some creative control is allowed. So,  these amazing virtual worlds are going to   be even more full of lifelike  characters, and with this work,   this process is going to be even easier and more  accessible for all of us. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
636,OpenAI’s New AI: Video Game Addict No More!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to use OpenAI’s  new language model AI to get it to   write our homework essays for us, see  how well it does that, and get this,   we will even ask it to write some computer  code for us. And it gets even better,   it does all this for free! But not in the way  you think. You will find out in a moment why. Now, let’s see. First, we write, “When I  was young, I only liked to p”. And then,   let’s say something comes here between, and  then, “and that’s how first I got interested in   AI research.” Now we ask the AI to fill in the  middle part. Let’s see what happens together. This indeed filled in the middle. It says,  when I was young, I only liked to play   video games. I would play sometimes more  than 13 hours per day. The rush, novelty,   and variety were beyond anything real life  could offer. (Whoa, hold it right there,   little AI!) It continues I loved the  challenge and I excelled at it. I would   often skip classes and go to and that’s  how first I got interested in AI research. But there is a problem. Did you notice the  problem? Yes, the video game addiction is   kind of a problem. But, that’s not what  I meant. It gets worse. Grammatically,   it kind of connects to the suffix, the text  that comes after, it is not nearly perfect,   but, perhaps the worst thing is that there  is no logical connection between them. It   does not explain how we ended up being an AI  researcher. Although Demis Hassabis, the CEO of   the DeepMind AI lab often says in interviews that  he played a ton of video games back in his day,   so this AI may be up to something. But  this new method can do way better. Look. “When I was young, I only liked to play video  games. Over time, I started thinking if it’d be   possible to make bots to play better than any  human can ever play these games. I eventually   decided I liked working on the latter more than  playing the games themselves and that’s how first   I got interested in AI research.” Now that’s  a scholarly completion! Excellent. Loving it. And, it has more absolutely magical  capabilities. For instance, here,   we can start diving into our narrative  immediately by talking about Wikipedia,   and, look. If we give it some space  before it, the AI will recognize that   telling the audience what Wikipedia is is  necessary, and it does it. Really cool! And, if we feel even lazier, we can write  the start of the story, the end of the story,   and now, off you go little AI, and you  do the hard work. Let’s read it. The   commercial diver finally thought he’d snagged  a big catch when he saw something white. But   then he quickly realized it wasn’t a fish  − he was wrangling an alligator. That is   a great story. Bravo. I made sure to look  it up, white alligators apparently exist. And it does not stop at writing  essays. Now hold on to your papers,   because it can also write  computer code for us. Let’s see. It can finish our code that sorts a list, computes  the average of numbers, or even performs prime   factorization. But how does it do all this?  Is this some crazy mind-reading AI? Well,   not quite. It is very good, but it does not read  our thoughts. It reads our comments. These are   not part of the computer code, these comments are  a crutch to explain what the next block of code   is about to do. And, with that, the AI knows how  it can help us out. This feels like magic. Just   imagine how much easier this makes a bunch of  coding work for us. And what incredible speedup   in productivity we can expect from techniques  like this. Mind-blowing. What a time to be alive! But wait, of course, not even this technique is  perfect. Sometimes it just does not know when   to stop. And sometimes, it just keeps repeating  itself. Alright, we get it, little AI. Thank you! And, there is one more feature that I loved  when reading this paper. You see, normally,   you would need to train a separate neural network  for this fill in the middle capability. However,   the coolest thing here is that it can  generate continuous text from left to right,   just like the good old GPT-3 AI and  other variants. And, at the same time,   this can also fill in the middle  without being explicitly trained   to do that. Oh yes! That’s right we get  this amazing capability essentially for free. So, what do you think? Does  this get your mind going? Let   me know in the comments below,  and let the experiments begin! Thanks for watching and for your generous  support, and I'll see you next time!"
637,NVIDIA’s New AI: Generating 3D Models!,"Dear Fellow Scholars, this is Two Minute  Papers with Dr. Károly Zsolnai-Fehér. Today we are going to see how this new AI can  create entire virtual worlds out of thin air,   and to do all that, we don’t even need to  be a professional artist. This can do so   much! But how much exactly? Well, let’s give  it a hard time and see what it can do through   4 really cool examples, and then, it gets  even better, I’ll tell you in a moment why! One, as you see here, it can generate a  great variety of different object types,   cars, animals, chairs, you name it. And,  would you look at that! I am very happy,   because I am already seeing two really nice  features here. Do you see them too? One is   that these are textured objects, most previous  techniques just give us the geometry, with this   one, we get nicely textured objects, that is super  nice, but, look! These are also regular 3D meshes,   which means a data structure that can be used  by most 3D modeling systems right away as it is.   That is not true for all previous techniques,  for instance, some of them even give us point   clouds instead. And textures aside, if you  compare the quality of the results to these   previous methods, there is a night and day  difference. This is so much better. So cool! Now, two, the fact that it can generate a  great variety of objects is all well and good,   but what if we kind of like this object, find it  to be almost exactly what we like, but we would   wish to change it a little. Can we do that? Well, that’s tough. For that, we would need   a latent space interpolation technique that is  akin to this earlier font generation method,   or like this material generator from our  earlier paper where we can walk around on   this 2D plane and fine-tune a material that we  already like. Doing the same with these objects   would be fantastic, but let’s not kid ourselves.  That is significantly harder in this context. So,   can it do it? And the answer is, look at that!  Yes it can! And what is even more impressive   is that nearly all of the intermediate results  make sense. That is the hardest part. Loving it! And, if it stopped there, that would  already be amazing, but it doesn’t stop   there. There is so much more! Three, get  this, it even understands the difference   between geometry and texture. This means  that whenever we modify the geometry,   the lamps still have the appropriate texture  and colors, it also understands that as we   create an SUV from a smaller car, the doors  and windows now have different placements.   That is an excellent showcase of an AI technique  that actually understands what it is generating. And now, here comes my favorite, they also  promise text-guided shape generation. Does   this ring a bell? Oh yes, we can prompt this  technique like we can prompt a text to image AI   like DALL-E 2. That would be amazing, but, have a  look. This is a previous technique trying to give   different animals tiger stripes, or actually, I am  not even sure what this is trying to do, or, make   these dogs a bit fluffier. Those are noble goals,  but unfortunately, this previous technique could   not pull this off. Not even close. And now, hold  on to your papers and let’s have a look at the new   technique’s results. Oh my goodness! There is no  contest here. Tiger stripes, fluffy dogs, you name   it. With this new method, we can create a tiger  or a panda out of any other animal, or ask for a   brick house, or burn it down by just writing these  two words and pressing enter. This is incredible   we just write the text, the AI does all the hard  work, and we take credit for it. How cool is that? And wait a second, we noted that this was the  best of what a previous method could do. How   long ago is this paper from? And this is  where I fell off the chair when reading   this new paper. This previous method is  not even a year old. It was published   approximately 11 months ago. And this kind  of progress in just a year? I am stunned. And wait, I promised to tell you how it gets even  better. Well, it does all these amazing things,   and not only that, but normally,  with previous handcrafted techniques,   if we wanted to do these 4 things, we would have  needed 4 different algorithms for that. Years ago,   if we wanted to conquer Chess and Starcraft and  Dota 2, these are three different applications,   and thus, need 3 different handcrafted  algorithms to solve. However, this is   not the case here. Because this new technique  can do all of them at the same time. This is   not 4 tasks, and 4 techniques, this is just one  technique. That is super important. Why? Well,   there are many definitions for artificial  intelligence, but most of them contain   some sort or generality. A good AI technique  should be able to learn from a large dataset,   and obtain general knowledge from it. And  this is an excellent testament of that. And, just imagine what this technique will be able   to do just a couple more papers down  the line. What a time to be alive! Thanks for watching and for your generous  support, and I'll see you next time!"
